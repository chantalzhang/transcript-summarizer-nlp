{
    "Topic 1": " Last class, remember, we were still talking about lexical semantics . In compositional semantics, what we to do is we to talk about, say the sentence level where sentences have meanings that you can derive by looking at the parts of the sentences . Language is compositional, but it's also not perfectly compositional in particular idioms or expressions whose meanings cannot be predicted from their parts .  There is an order to which you apply things, that this whole thing is within the scope of that universal quantifier . And then we'll also define a procedure to combine those pieces of logic together in order to form the overall meaning of the sentence . We need to have some algorithm for constructing these logical formulas at the sentence level from the its parts .  syntactic composition goes hand in hand with this, semantic composition, and you're also doing some function application in lambda calculus . The idea here is every terminal rule you take those semantic attachments, and then afterwards is a mechanical process of looking at your augmented Cfg with those . semantic attachments and then running those procedures .",
    "Topic 2": " A lot of search engines are still based on word of co-occurrences . People often discard negative values in the Pmi because it's not exactly the space of events that you're considering . The more popular one is the skip ground model, which is that you take the middle again and you use that to predict the middle word that's missing .  Montegovian cement is the assumption that natural language can be made as precise as logic . If you say all wugs are blorks and all blorks are cute, then you can conclude that all wug are cute . Wugex is asking, is this a Wug? And the implies arrow here is a logical connective, ? Truth values, the one thing before and the thing after .",
    "Topic 3": " Inference is to make something explicit that was implicit before in language. If you want to have a natural language interface to all of that information, it's possible to comprehend the syntax and semantics of both kinds of languages, with a single natural and mathematically precise theory . If you have a midterm conflict, please send me email, by the end of today .  Logic consists of the predicates and function names and arity . If it's ever unclear, you can add parentheses to make clear the scope. This part is really confusing because it's not intuitive, and the 1st time you see it is confusing. But the reason, remember that the reason we're using a logic ultimately is to relate language to the world.",
    "Topic 4": " The term context matrix is based on the distribution of words in order to infer something about the meanings of those words . It's just some a score or some a log ratio of ratio of likelihoods . You estimates the probability of both of those . words occurring . and you divide that by estimates of the probabilities of each of the words occurring separately of each . independently . And you can get pretty good levels of performance if you implement some ir system some basic ir system, that you want to creates a version of your term context . matrix .  The tradition that we're going to discuss today and class is to use logic to model sentence meaning . In 1st order logic, it has predicates which map elements of the domain of discourse to truth values . The difference between predicates and functions is what they return .  The general idea that we're going to pursue is that each of these words. this is the high level idea, which is that you have these words . It's important to . distinguish between these 3 and the names of A, B and C here. You can rename one of them, say, rename the 1st one to be . lambda, a constant, it gives you an element in your domain of discourse. Is this the domain of all variable factors? This is the domain . of all variables . And then there's functional application. You apply to some scenario with this interpretation, and then you check what's going on in .  The augmented grammar always applies another qualified into the inside. that's part of designing this augmented grammar. If you have a different rule that works in a different way, you have to follow that. You have to be consistent with that. That's why you have an associated population, it always applies to the inside .",
    "Topic 5": " We need to figure out if we need to book a room for midterm for the makeup midterm and forth . But first, st it'd be good, to just quickly summarize and recap what we did last class . This is the 1st iteration of this distributional approach to modeling meaning, and which eventually led to large language models .  Montague says, there's no, in his opinion, no important theoretical difference between natural languages and artificial languages of magicians . Predicates give you a truth, value true or false, and or and implies and bi-directional entailment . The function that you use to compose and get the overall meaning is complex and is dependent on the parts themselves .  We just have a syntax tree. We need to associate every rule that results in a leaf node, , involving a terminal . And we need the logic, because that's what we're aiming for in this whole exercise . We can look at each subparts of the syntax tree and build up the meaning representations bit by bit, until we get to the sentence level. And then hopefully, it'll be a bit clearer ."
}