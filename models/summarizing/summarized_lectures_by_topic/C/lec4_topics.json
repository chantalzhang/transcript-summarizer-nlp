{
    "Topic 1": " Programming assignment assignment one will be released . It could be naive base, logistic regression, neural network, perceptron, SVM, anything . Logistic regression formulation is very simple and also about the naive basis, very simple assumption that some of you were able to give examples why this would not hold .  There's no notion of similarity. And that's why nowadays we focus more on sentence representation . If the language model is able to fill in the gap properly, that means it has learned a very good representation of many words in the large text composure provided . This concept is very related to transfer learning, which I will cover in 2 minutes .",
    "Topic 2": " Logistic regression is a learning model which automatically learns nonlinear functions from input to output . In the case of logistic regression, you want to look for a certain weight that allows you to model your input X to give you Y . The more data we have, neural network has not disappeared because it is able to model any function provided you have large enough data . And also it's transformed the field of the feedforward neural networks .  SVM can also work for nonlinear data if you use kernel functions, we should see that you need a non linear function that can separate your data . The size of your output depends on the size of the vocabulary, the number of types that you have in your vocabulary . In the case of language model, your input vocabulary size is equal to your output vocabulary size .  For every Y you have, you try to compute what is the cross entropy between the Y, the original Y and what you have predicted . And you sum it all over every instance of Yi that you need . You need the loss function to know how far away are you from the words from the correct answer . One of the most popular loss functions used for classification is cross entropy loss .  Neural networks are able to learn better relationship within input and output . It reduces the need for future engineering . Another difficult thing is that neural networks tends to work very , but it's difficult to interpret. neural networks for NLP, there are many open questions on how to use linguistic structure .",
    "Topic 3": " If you want to do a classification task, then you still need to use a function to convert it to a probability distribution . The more data you have, the better you are going to be able to model this . Simple models SVM, excel where you have less data .  We need to come up with a better representation for every word in our vocabulary that have the notion of similarity . In the early 2000s, we started working on word embeddings . There are other ways of what is called component wise vector multiplication and there are also more sophisticated options concatenation .",
    "Topic 4": " The more data you have is just the better the performance in general . You can train a model for multiple tasks at the same time to solve multiple tasks . Multimaskask is that if you train a language model on Wikipedia or a mass language model, you can also use it for other tasks answering and answering questions .  There's sometimes when if you incorporate domain knowledge, you can boost your performance than a lot of engineering with neural networks, how to do better, multitask and transfer learning to new domains . The C plus evaluation metric you can use to evaluate your model if it's good is accuracy . And the problem for spam classification is that spam appears rarely, it's very rare .",
    "Topic 5": " The data set is very similar to the test sets, but you still have to do the calculation . There are a lot of smotting and there's souvenir smoothing techniques that we typically use in NLP . The weight of every layer is what you use randomly . And then you modify it randomly to get better .  GPT can do many things. It needs a lot, a lot of data and by you should be aware of that . Also, there are many hyper parameters to tune. There's no notion of interpretability. And when is linguistic feature engineering good? If you use the notion of classification . accuracy, your classifier can just ignore all the spam output and still have a good accuracy ."
}