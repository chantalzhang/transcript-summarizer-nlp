{
    "Topic 1": " Today, we'll consider sequence modeling with features with a focus on linear chain conditional random fields conditional rental fields . And then you can boost the performance by one or 2 points. And then we also have observed variables . For the case of Patos Beach. it will be the States . For example, for the . case, for example, the States will be . the pos tax. For the . example, here you have what is called the start of the entity. on the board you have, 3 kinds of scheme. The idea of adding features to our modem engineering requires a new feature .  There's no analytical emery solution. why aren't we normalizing here? Given the sense you minimize but of course there are other metals the conjugate gradient, or what is called the LBFgs. You minimize the negative log, likely. And then you do this for a while. And here we have to sum over all the States wide information of solution of team 5, 1, oh. I'm if you want to speak with the regulative one. We really have the submission of vaccine here. one t, 1 t management .",
    "Topic 2": " We find Llms to still struggle a bit with talking classification tasks. especially for a token classification task. We have a Markov model, where we have different states q and the observed variables will be the words . And I want to emphasize in generative model. Given an input X for generative . to to compute the joint probability. to to . compute Y . This is the idea of generating model was, as , in practice, this is not possible .  In the first, st we're trying to address the problem. And then we just say, . every entity start with iod iod. and then you have the inside of the entity, which is the eye. If there's no scheme here, it's very difficult to know the entity. If you want to know if something is capitalized, . instead of having yt if you . want to have yt, instead of a . number one x . here you find out that we can learn a function of Fk. and I'm going to define what will be F of K. This is what we're going to find after taking the derivatives. This will be equal to one over 0 x. Theta . This will  Theta K divided by sigma squared is based on the law of derivatives where the 2 above is going to cancel the 2 below . But this is, there are webs, ? Think that's also a demo. It's just a number, ? It's a number .",
    "Topic 3": " Some of the most important models that dominated the field for more than 5 years are still very useful to today . And some of these methods can still be useful in practice in practice . And today we're going to see how we can go from Hmms to crf. , they are not they're still very helpful to today. And there are many, many features you can add.  We're just taking the maths. You'll find that you're gonna have this set of things. we keep moving from one state to the other. And for a gradient asset. But of course you can also do gradient descents where you have a negative of that. And then you can estimate new values of your data after taking the derivatives. Then you are good to have summation. K. , this is very easy to estimate, but this will. This could take more time. This call is the explanation .",
    "Topic 4": " When deep learning came around it was still very important to have conditional random fields to improve performance . You can combine it with Crm by just appending the Crf layer to the the last layer of your network . The idea for any error is that you need to detect spams of multiple words that are relevant to the entity . David Ifeoluwa Adelani: Do you have questions?",
    "Topic 5": " Today we'll talk about some of the shortcomings of Ed Markov models, and then we're going to move from generative tasks to more discriminative tasks and of course, we examine the linear chain conditional fields . The States will be the different ner tax, we examined last time. We also talk about the ner models .  The one on the left hand side can be seen as the empirical distribution of the future K.case. This means that finding parameter estimate by gradient descent is equivalent to telling our model to predict the features in such a way that they are found in the same distribution as in the gold standard . Another thing is that you can have regularize a regularizer to this ."
}