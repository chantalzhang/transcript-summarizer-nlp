{
    "Topic 1": " It's the study of meaning as they relate to the lexicon, and lexicon is this idea that we have some mental storage of entries related to language which typically store words, but also stores, phrases sometimes, or also morphemes . You collect a corpus involving lots of instances of the word hand, and then you ask annotators to label each of those contexts with which sense of hand is indicated there and then . After that you feed that into some classifier .  It's to do a better job than using a static lists of patents . These Hearst patterns have also been discovered and used for other relations, such as between cause-effect relations . Hearst pattern depends on both of those words occurring in the same context . But then, by using that as your seed pattern. , that was 3 algorithms. But it doesn't really reduce the data cost. and then you extract a much larger set of unannotated data and apply your classifier then . And then you can still do this bootstrapping in that setting .",
    "Topic 2": " In general, we have this very good clue about how to disambiguate for the intended word sense by looking at the other words in the context . In the second sentence, flowing and graceful might be words that indicate the style of handwriting . But really you can imagine a version of this task which is called all words word census ambiguation, where you try to do all of these decisions together .  The problem, the phenomenon itself is not very understood, we're going to cover the 4th algorithm or task area for today . It's about the relationship between these words or noun phrases that co-occur in the same context where the words and those relations don't tend to tend to occur directly . And then you would accumulate all of those counts when it's the center of your attention and you're looking at all the words around it .  Word embeddings are these trained vector space representations of words to predict words in context . Skipgram based approach is equivalent to a version of the count based model, where you apply a singular value decomposition to that . And it also just goes to show you that, having different views of a problem and thinking about things in different ways .",
    "Topic 3": " The idea of lemmatizing and counting lemmetized Lemma overlap is, is implementing that idea of bootstrapping . This is how the model pulls itself up by its own bootstraps, which is it gradually improves itself through multiple rounds of training on automatically labeled data .  The idea of bootstrapping with reinforcement learning is the same as reinforcement learning . For every word it has 2 roles: target word is target word or context word . The target words are just the words you see in the Corpus that you want to model every time you see a new word, you can add it to a new row for it .",
    "Topic 4": " We are going to continue our discussion today about lexical semantics and last class we talked about . Lexical semantics is something in our mind that holds words and their meanings, and also their characteristics and behaviors . We'll talk a little bit about detecting semantic relationships . And then we'll continue, stretch, stretch and then we can talk about how we find these lexical relations in some lexical pairs .  The idea behind Hearst patterns is that if we can identify expressions this such as . then we can discover hyponym, hyperym pairs automatically . Because you can use a corpus based approach to help you expand that . What you can do is you can have an initial seed set of words in a hyponym hyperym relation . And then, once you have those contacts you can find which contexts are very common .  We will return to this in 10 slides or . But I'll talk about the things that people have done. I don't necessarily mean taxonomic similarity. And there are many possible choices of this gold standard. How are annotators handling that? That by considering the most similar sense that they can think of. And then there's a lot of math and structure behind it .",
    "Topic 5": " Jackie Cheung, Professor of Computer Science, gives a look at some heuristic algorithms . Heuristic algorithms are based on knowledge about the problem and your intuitions and build them into computational algorithms . Yarowski's algorithm, Lusk's algorithm is from the 80 s. and that you use dictionary definitions of a word senses in order to help you disambiguate in context . And then you compute an overlap between the context and the signature, and then you pick that one .  The heuristic that Jarowski proposed in his algorithm is to pick one other word that will correlate and co-occur with that sense with very, very high probability . In modern times, though we solve harder problems, and not just the binary word sense disambiguation problem, then, it's slightly supervised or you can do something else what you say .  The most common function that people computes to compare the meanings of 2 words is by computing their cosine similarity . If you see a word that occurs a lot with in or at or something, it's a time, word or location . The general idea here is that you understand a term by the distribution of words that appear near that term .  The problem is that word. Vectors have no objective inherent value that we can evaluate if you had 2 vectors for the word linguistics, is point 4.3 negative point 2 better? All we can do is evaluate the similarity of vectors to each other by similarity . And then if you have a trained word vector model, you can get your word vector to also give you similarity scores with cosine similarity ."
}