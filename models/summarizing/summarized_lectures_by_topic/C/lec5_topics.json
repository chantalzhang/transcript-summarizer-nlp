{
    "Topic 1": " How can we model language far in context of test classification? Some people are able to join how, how to do , on downstream tasks . If a word is in the sentence. does that word depend on previous words or not. Zip's law . The same thing is to do what's called lemmatization and stemming .  The longer model can also predict an incorrect fat. you prefer that it suggests what's for you when you're typing . ASR is a very a big application of language model and also machine translation . We find a solution that maximizes a combination of tax-specific quality. If you are just shooting for tax specific quality, it may not work on different tasks .",
    "Topic 2": " In slides number 8, we can compute word frequency, or we, the popular word that we use is what is called time frequency, where you want to compute the number of words in your compost . And then you use this to build probability distribution, where we define a random variable W equals a small W . Given a context C, the random variable here is W, which can take any word in our dictionary . The last 2 words of the format that is a trigram distribution. that is the unit distribution.  If you want to know what's the cross entropy between the true distribution P and the model distribution in Nlp, what happened is that we don't know the true . The model distribution should be close to the true. distribution . But since you don\u2019t know the . true distribution, you use the . model distribution instead of the true, and then you compute the cross . entropy based on this .",
    "Topic 3": " We do not fully care about the context of the of the words, we only treat them as bag of words or bag of engrams . If you have a world football? Is this 1 word or 2? Is it foods plus ball? And I'll be because it's a 1 of the 1st acts they have to tackle is how to do word segmentation in Chinese language . We're going to examine more statistical language modeling based on n grams, and then we'll do or maximum likelihood by relative frequency .  Can you compute a unigram and background language model, using the following sentence . You combine the diagram, the . diagram, and the unigrams together? Yes, you have to tokenize it . And then, if you want to compute the probability for every unique . unique types there and graph types, what are you going to get? And that is how you have 2 over 5 what's the . background probability of that is 2 or 5 .",
    "Topic 4": " Language models can also factor in the context or the previous words before the word we're trying to predict . A word that is infrequent might be around 30,000 times. Practical implication of this most war types are very rare. We have a long tail and sometimes this long tail are very important that if you miss them you will not be able to recover .  There's a nice connection between cross entropy and publicity . There's no need to waste a lot of that beats to encode that information ? This is by definition, I'm not adding anything. the information, why is it locked to a 1 over? You want me to talk more about this. And if you do this over if you are able to, , you're trying to say the if you have a good model .",
    "Topic 5": " In bidirectional oil, you can do it both in the forward direction and the backward direction . The last 2 words that proceed are . That means you have D, followed by what the cat divided by the Count of B. All these are what we call the embali estimates .  Cross entropy is defined in the cross entropy, . and cross entropy is also defined this way . Cross entropy also defined in this way, and is defined here in the . cross entropy . defined . . . and the cross . entropy is . defined as the entropy of a given set ."
}