{
    "Topic 1": " Part of speech used to be a very important task in the 90s because it was very essential for many applications from machine translation to speech recognition and many tasks, and also for named entity recognition . NLP workshop, NLP in the era of generative AI will be from October one to third .  The probability of an outcome is that you count all the outcomes and then divide by all the events . But there's an actor directory's name is Scratch, and you can store all the big files there . If you think the table is better, but they can edit that section .",
    "Topic 2": " We talk about smoothing techniques that you can address words . We show two different task sets, using Pentry Bank and using universal dependency . In language model, when you have transition between words that make up a sentence. this is an example of the markup model where you can have words and then you have interaction between the words and all interaction, all in this graph, they're directed graphs because it depends on the other one .  In graphical models, instead of saying hidden variables, we often use what is called a latent variable . In this model, we are not using the context of the words in this modeling, in this simplified one, this is a very, very simplified one . But from class we're going to talk about different algorithms on what is the best way to tag a sequence and. we have computed all the probabilities .",
    "Topic 3": " We'll be focusing on part of speech in English because I believe if you're attending this class, you understand this language . English language is following the structure of SVO, you need a subject which is typically a . determiner and a noun . And then we have order schemes that reduce this a bit to 18 called universal dependencies .  The first thing is try to compute \u03c0 highs for DT and VBD followed by what are the transition probabilities? The transition probabilities are very easy to compute . But in actual fact, you have to estimate this over the entire training corpus . You look at the entire . training corpus in different sentences . You compute unigram probability for every part of speak tax . What's the probability of a pronoun in my corpus? What's . probability of proposition in your entire corpus?",
    "Topic 4": " We're going to examine some things coming from motivated, from probability theory . We have an NLP workshop at Miller. Miller is the Montreal AI institute. Why do you need to smooth? giving a sequence a sequence of words. We have adjectives good modifier of a noun. This is more an adjective to compare to compare 1 hamburger to the other. Punctuation should be another one which is OK, it's obvious but punctuation is the last category .  The LDA.model is a very simple bigram. model. It's a very, very popular, not only for supervised tasks, even for unsupervised tasks the LDA . Do you have a question before we do the exercise? Yes, yes, yes . I plan to finish the first 3 instructions .",
    "Topic 5": " Professor Bengio: Hidden Markov models used to be very popular until deep learning overtook it . He explains how the Markov chain is used to model parts of speech as something hidden that you don't know, but you have to model it . And then you have transition probabilities from a tag to a word to generate a new world .  Given the transition probability, given the probability on that arrow, you can generate that we have to go from the terminal to . And then you have the tag set Q1Q2 to Q5 . Given an emission probability, you have to generate a word . It's possible if you don't have a good probability, if you are not able to estimate good probabilities, . And after you try this stage, we will have another score ."
}