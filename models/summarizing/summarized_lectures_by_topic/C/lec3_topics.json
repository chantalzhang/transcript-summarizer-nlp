{
    "Topic 1": " Last week we described how to train a classifier on a training set . Today we'll talk about how to learn a classifying system . We also did some feature selection, how to come up with features for your classifier . The log of the product of log of log descent will be used to optimize this gradient descent . And then if you have very basic tasks test classification tasks, topic classification or sentiment classification, you can easily run this .  The last thing I want to talk about is support vector machine, which is a very, very important algorithm . Given enough training data, they tend to perform very well . For current language models, people are trading for over six months, over a year, and if something is wrong, we have to start that over again. It's really expensive and often requires a lot of data .",
    "Topic 2": " We'll start with linear classifiers. Output Y can be a discrete outcome or categorical variable. We discussed linear regression last week. We already differentiated what's the difference between classification and regression . At the class, we're going to try to do a discriminative task instead of a generative task .  Many people believe that neural network is inspired by the human brain . Some people believe there's some interaction about you have neuron and the dendrites and the ASEAN in the brain . in the class I can ask you about what about K nearest neighbor decision trees, random forests, and on.",
    "Topic 3": " Theta is all your set of weights or your parameters you want to learn in the model . Some parameters cannot be learned automatically, you have to fix them . The most important thing that you're evaluating here will be what is your validation accuracy across the different experiments which you are going to average for a particular hyperparameter . That means you can get the true parameter of your model and then you can prove it .  Probability.variables are independent, probability of AB equals to probability of A. This is what it means if you say A&B are independent given C and you assume conditional independence. This would be probability of a given . given C multiplied by probability of B given C . and then you have a product between probability of XI given Y given Y and the product of Y . Bayes rule can you compute what would be the probability of Y given these three features .  For a generative model, we learn a distribution for all of the random variables involved, which is a joint distribution probability of X, Y . The only thing you care about is the parameter. You learn the parameter Theta. And that is the idea for most algorithm algorithms that have developed, they estimate the Theta such a way that the loss is low .  You want to compute what's the probability of Y given X but given this parameter of Theta because you want to find those parameters and those parameters in the logistic regression are A1A2A3A to an . Log likelihood is very important in NLP because it's what you use to compute things publicity of a language model .",
    "Topic 4": " When you are trying to learn a function this, typically you have X, Theta. You try the second hyperparameter, you try the third, the fourth, the fifth, it could be 10 . Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression . Another popular technique is naive base .  SVM can work in different tasks and settings, usually giving very little training data . What you have to do is to try different algorithms . Here we generalize the A1A2A3 that I told you into what is called the weight matrix . And here you can also stack different perceptions together .",
    "Topic 5": " You can split the training data into K folds and then test it into a different model . The idea of training is that you want to select the hyperparameter that minimize the error on your training data . The lowest you can get is 0.01 and one or not bigger than one . And then you train for the K fold, roughly equals size ."
}