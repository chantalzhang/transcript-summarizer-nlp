{
    "Topic 1": " We have language models that they try to scale, to 500 languages, but in terms of performance we I don't think we have a good luck with that . It's more on multilingual nlp and crosslingual transfer . There are 2 ways to fine tune. if you have labeled data, you could just fine tune this multilingual bits . And then, if you want to do multilingual transfer posting .  Language is covered in multilingual birth. the lesson is, I try to cover as many scripts during pre-training, that you will be it will be easier to adapt to new languages in the future . And then you can copy all these tokens into the . you can tune the model for that new vocabulary. And then things learning rates also matters. The tokenizer is not in your network. And you try to get as many tokens, but not too many tokens .",
    "Topic 2": " All of the models must be based on the transform architecture that we discussed . All of them must be . based on . the left hand side part of the architecture of the transformer only . You have to also construct a task, an example of a task used to pre train and then go down . Then, it's very easy to transfer another one that is even more popular is different tasks . you can train your model on a different task and then use it for another task . And then use this for classification .  Models Chatgpt just 1 min had much worse results than just training on English . But if that language is similar to a high resource language, the transfer still work very . If a language is not same, and there's no relative or causing then it's gonna give you a poor result . We have bite level models, which I did better .  Nigerian pigeon and Yoruba with predicted if we use the brute force approach . Yoruba was also predicted for hours, even by the language RAM model . If you train on the top 2 predicted languages, you'll find out all the time . You train an adapter for the target language and then for the task specific adaptation .",
    "Topic 3": " Ifeoluwa Adelani: There's still a lot of work to be done. we could also speed them. There's no translation of some words in English language. But this is a very expensive annotation process, it would take them a huge year to have a huge benchmark . The closest to our work is Beli Beli, Beli and other languages .  Different Mlp. which shows that the transfer learning works but is not better than a single most layer perception . They often have very high performance . Some languages where the language is unseen and also the script is unseen, and some are still very accurate . The performance of languages that were not seen by how similar those languages are not seen .  Arabic German, we see some interesting transfer to some African languages . In the Francophone region of Africa, we find that they do transfer very to each other . And in historically, Swahili has borrowed a lot of words from Arabic due to due to trade . If you combine this by training on multiple best transfer languages to enable further boost your performance .",
    "Topic 4": " There are over 7,000 languages in the world, and or over 400 of them, are spoken by 1 million speakers . But we don't have a single technology that works for 400 languages per world . Most of what we do is based on what is called language model. You have, decoder, only model. An example of encoder only models would be birth model .  A larger model are the best results . A simple Mlp get better results than using a bird's model that was trained for 2 weeks on a Gpu server . But for a language that you don't have a lot of text on the web just using simpler approaches naive base or something they already are not appropriate .  The model is also very big, and up to half of the model size is in the embedding part of it . We create a better model than previous strong multilingual models by Google and Md. Will . In general, what we saw is that vocabulary compression was better than knowledge distillation .",
    "Topic 5": " Some languages are not supported by keyboard spell checkers, morphological analyzers and dictionaries . Europe and European languages have been more favored because they are one of the early adopters of the Internet technology . There's lack of legal data for downstream task and also for many languages .  Models are just getting bigger, bigger, and then nobody can serve them . But one thing you can do about this is to do what is called adaptive fine tuning . We have the small version, 270 million parameters . We also find languages that are more. but because they are in the same geographical location, they have the same script .  That's the end of the ledger. Thank you for waiting till the end. , Jackie, with the go ahead. That's all you need to do with the rest of the show. That will be the first time you've heard from this side of the record ."
}