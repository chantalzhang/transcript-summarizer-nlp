{
    "Topic 1": " Co-reference is the relation between a referring expression and its 2 different mentions in the passage . It's a phenomenon, where one linguistic expression interpretation depends on another linguistic expression . This is just another term, because the interpretation of his . interpretation of they depends on the fact that it co-refers with citizen. That is, reference, the phenomenon of reference .  The algorithm is based on how there are soft cues about how you tend to refer to the subject of the previous sentence . It would encounter the noun phrase node first . Then the algorithm would find Alice, a reference focal reference . And then the algorithm could be used to predict the co-reference resolution step after mentioned .  Some of my Phd students, Ian Parada compared encoder and decoder models . and we find that encoder models tend to be more efficient and perform better . And if you do the proper parameter it predicts shift, which means, let's process the sentence . Oh, and shift means you're done processing the sentence all .",
    "Topic 2": " There is some dichotomy or opposition between language things that you can . These are things in language that will point to something else . lexical chains is just about relatedness. We try to do this a bit with the module and semantics, ? we all have different mental representations of things that are going on in the world . And then these linguistic expressions are said to co-refer. This is called an anaphore .  We can figure out what are the relevant linguistic cues and features that we might want to condition on . Hobbes Algorithm, the very early work from the 19 seventies, is a heuristic algorithm that puts together all of these cues and comes up with a method for co-reference resolution based on that . Later on, we'll look at machine learning algorithms that might be able to do a better job and resolution .  It gives you a score, for whether it's a mention or not, and you can do that, do supervised learning on this . What they do is they implement the proposal that we had from the class earlier, which is, you pass it a passage and then extracts a bunch of mentions, and it also extracts what these mentions point to previously in the context .",
    "Topic 3": " Co-reference is when you have multiple mentions or referring expressions that all points to the same thing in the world . It's hard to model this and do this for impractical settings, because, . One thing that you could do that is still useful is to use language models with the large language models .  We learn with hearse patterns. you're saying that Hearst patterns might give you patterns that help you figure out co-reference links. that would be more for the whole problem. But you could it would output a probability there or something being mentioned or not. But, yes, would any sequence modeling framework work for this task?",
    "Topic 4": " Natural language doesn't occur as individual sentences or utterances one at a time . We only look at the sentence level or below . We can talk about some basic distinctions the difference between a monologue versus a dialogue . There are other phenomena that appear at different levels of frequencies across languages .  A restricted co-reference resolution can be used to solve complex problems in machine learning . The algorithm is quite complex, with quite a bit of detail at a high level . But I won't go through each of the steps in detail, because it doesn't really add much to our discussion .  This ete model is quite highly performing more recently than the more recent developments, as you would expect is to do something similar, but with a transformer style, with a pre trained large language model with transformers . Then the second class of new models that have been proposed is what was mentioned earlier . This was supposed to come later . And this gets a much better performance .",
    "Topic 5": " We're going to focus on one particular phenomenon related to discourse, which is co-reference resolution . Coherence is the relation that logical relation between them . There are also discourse markers that we use to help structure a passage . These discourse markers help us explicitly mark what are the discourse relations between different parts of that discourse .  The high level idea of Hobbes algorithm is, st, you search within the current sentence to find a noun phrase that matches . The algorithm is designed to find the best match in terms of number and gender . Hobbes: It's obviously not perfect design because they tried to design it to be perfect .  The algorithm looks at the previous sentence, and the 1st noun phrase it encounters, which matches is the other . It'll propose she, but it doesn't match. Then they'll propose this, she in the second sentence . And then the noun phrase . encounters it in number and gender. And then, a huge noun phrase. And , the other examples are similar. It could be a fine-tuning approach ."
}