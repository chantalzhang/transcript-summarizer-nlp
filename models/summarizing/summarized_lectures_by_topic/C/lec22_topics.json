{
    "Topic 1": " Lstm is a model that can generate some outputs, but it's fine tuned on the specific problem and task you're interested in . There's no separate modeling of each of those steps, at least in the most extreme form of that . There are no guarantees that it'll generate something that's either semantically coherent or syntactically formed. It's just empirical that it happens to often work for many of the tasks that we try .  The downside is, you have no guarantee the constraints would be respected . In practice, you can still solve reasonable sized problems with, especially with industrial strength, implementations of these solvers that are highly optimized and very, very fast . And you can interpret the output back into your application contact, setting for your task .",
    "Topic 2": " We are going to look at how natural language generation has been approached with different methodologies and different approaches in the literature in the past . We were looking at automatic summarization focusing on extractive summarization . But if you want to describe the overall distribution of opinions, , of that product, and synthesize and the thoughts that people have on specific aspects, then you're going to have to do some abstraction . And then micro planning is to decide how all of these words fit together in clauses and sentences, and this is called sentence planning .  The output sentence is still faithful to the inputs material, if you have 2 sentences he studied sciences with pleasure, or he studied math and physics with Bohr . Then you can merge those 2 nodes that it's a single node for studied, and then all of the dependency, relations subject or prepositional phrases or object . And then you can just do some simple rescoring with a language model, even with a very bad language model to linearize it, and you'll get something reasonable .  There's 1 line of work which is related to that, I would say, at a high level which is this, chain of thoughts, work where you have a model, generate outputs, and then you condition on the model generator output to generate something else . And it also allows it to inspect and choose, to correct something and fix something . The other major trend is you try to fix these issues with human feedback .",
    "Topic 3": " We're going to talk about natural language generation, but it's good to learn about them to expand your toolkit . Natural language generation and abstraction has many use cases . We can divide up Nlg into data to text versus text to text, depending on the source, the input .  Each of these steps involves rules . The advantage of this type of system is that you have a lot of control over what happens . You can do it with lots of data. You can just feed it some data and . get it to run and adapt. You don't need to handcraft anything. This is a very different approach to thinking about AI problems .  Training of large language models these days is to figure out how to find good content source, appropriate contents, and to filter out the inappropriate content that exists on the Internet for training . The idea here is that you just prepend the special token that indicates the type of content you want to generate . and then at test time, you can control the output indirectly by prepending the . token corresponding to the target property as . which is to tell them decoder what not to generate. That's the standard log likelihood objective .",
    "Topic 4": " We're going to look at techniques that are not currently, the trendiest or the most popular . Instead, we're looking at a broad range of approaches . And we can talk about that which is a more declarative approach to optimization . We contrasted extractive summarization, which is where you take snippets of the source text and concatenate them together .  In this second approach of a neural nlg. You just assume that the pre trained language model, , can figure all of that out . This model is very accessible. It is not accessible, because, . it may be different levels of accessibility requiring different things . But if you ignore that, then it's a task independent where it was the neural energy approach . And then what you could do is you could formulate the inputs . each word is associated with a word embedding and an embedding . This is one instance of a broader class of methods and a more general approach .  Microsoft's Chatbot was released into the wild on the Internet . Microsoft had to take down the Chatbot one day before it had to be taken down . Other approaches include unlikelihood training, contrastive learning and reinforcement learning . These days are happening at a larger scale, and with a slower timeframe .",
    "Topic 5": " Professor Jackie Cheung, Professor: We're going to look at shared machine learning systems . We focus on extraction because it was easier to think about . And then surface realization, something called surface realization . Nlp is still based on rule writing rules and thinking about grammars and stuff, but at least you still need to know this stuff .  Generative models are highly structured, detailed, rule-based models . Generative model is a model that gives you a joint probability distribution over everything of interest, over your inputs and your labels . It's easy to make a mistake in engineering this. It's it's not easy to get these structures. It doesn't make sense it's a lot of work even to convert whatever input you happen to have .  A corpus of text labeled with a property you care about polite versus impolite, could you please do something versus? rather than single tokens, it started off being single tokens with special meanings . It could be some overall larger plan for what you're going to generate, in terms of some a cross entropy loss or whatever . It turns out this unlikelihood training can be a little bit tricky to get it to work ."
}