{
    "Topic 1": " The main topic of today's lecture is to assign a label or category to a piece of text . This is a common research paradigm these days in NLP, which is first, find an . Piece of text that new situation, the word run can be a verb or a noun . And then you're fitting the parameters of the model, the coefficients A and the bias term B . Involves a bunch of calculus in linear algebra, that's why you have to learn those .  The idea is called N grams, the idea of N grams . It turns out that different people have different writing patterns that are subtle, in how we use punctuation and how you use function words . There are other scalings of these counts that you can run which are more sophisticated and which can sometimes give you much better results . Once you have bigrams, you have v ^2 number of possible bigrams .",
    "Topic 2": " NLP has to do with understanding natural language as a phenomenon using computational methods . NLP is about the engineering aspects, developing useful, beneficial applications . In NLP we're analyzing into some output form that is discrete, or we're generating texts and texts you can think of as a series of discrete output . This was one of the earliest applications of NLP .  Using a system in order to predict whether the new e-mail is spam or not, hopefully it does a good job and it can filter out most of the actual spams without missing any of the legitimate non spams for you . And in fact, usually we can record or often we can try other strategies rather than just recording the word itself . And then you can turn them into a feature vector by recording their frequencies . You can choose higher orders of N, you can choose n = 2, and then it's called bigrams or n = 3 .",
    "Topic 3": " We'll talk about how do we formalize this a little bit better . Machine learning basics will be useful and important for much of the rest of the course . We'll focus on the major parts of each of these steps, with a focus in particular on step #2 . And then lecture, David will talk more about step numbers 3 and 4 .  Each e-mail can either be a non spam 0 or a spam one . The whole point of learning is that we want to make fewer assumptions . In practice they'll always be some abstraction of the full contents of the inputs . Sometimes losing information might not be a bad thing, as we'll discuss over the series of lectures .  The way to think about it is that each of these different schemes that we can choose will end up resulting in a different model after training . The number of unknown words is an indicator of whether something is. And this is where a lot of machine learning happens. This is where there are whole courses on machine learning.",
    "Topic 4": " We're not going to cover all of text classification in one lecture, but we'll focus today on feature extraction and some common processing that people in working with textual data do when trying to process text and pass them into a text classification system . Machine learning is one potential way we can or approach we can take in order to solve some NLP problem .  We have document classifier, document label. We specify what the possible YS mean and then we're going to train a classifier on top of the document. Or you can have a million features if you want, and then you have some values which are some numbers that are filled in at each dimension . And then in your training set you have the output labels associated with them. And then we have to also talk about the inputs. This is what is going on. And the idea is to chop off some endings and sometimes sometimes glue some glue back on some endings .  The main key behind both limitization and stemming is to rewrite the rules of a rule . We'll come back to part of speech tagging in a few weeks, but there's a way to do that . There's a common tag set that people use in English, which you can check out here in this link .  If I formulated a sentiment analysis, what we're solving is a . sentiment analysis problem of . what is that particular piece of text described? This is something that you can think of as a validity problem . We're solving one problem, but we're interested in another problem which is very different .",
    "Topic 5": " NLP is not machine learning and machine learning is not NLP, but it's about analyzing the internal word structure and how words are composed . In the supervised setting, a model has access to your input data as as its corresponding output label for the purposes of training . Unsupervised learning is very special in that you can solve for the best value of A&B analytically by solving the least squares equation .  Feature extraction refers to taking your input textual document and then extracting features from there that you think the system designer thinks might be relevant and useful to pass on to the classifier . You can think of it as matrices and vectors that your training set corresponds to an entire matrix, where each row represents a different sample, each row is a difference .  Machine learning is based on something that is wrong. I'm just saying that it'll be very, very difficult to do a better job than them fast enough to make money . We can still fix this by expanding the set of features. We condition on where it's where the piece of text comes from and forth."
}