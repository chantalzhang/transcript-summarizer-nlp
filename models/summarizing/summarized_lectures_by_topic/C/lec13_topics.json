{
    "Topic 1": " The scheme is called 1st and order order and can interpolate, and can do any other order and back order . It's a mini language model involving those symbols that would appear in the hand side of a rule . The probability of this whole thing is equal to the probability of Vp going to start advert phrase, and then Vp, going to advert phrase Vbd, and Vp . And then you have some subcategorizations and some probability that a verb is modified by a prepositional phrase or it takes on some object .  We're gonna talk about another really big topic which is semantics . It's a function that takes in an object from the world . And then there are many, many other infinitely many other objects in the world which are not telephones . You can express them logically as . But I'm going to define a whole bunch of terms and we'll just go through them .  Second context, something flowing and graceful would help inform that this hand is referring to the handwriting style. The handwriting style is a natural expression for a person to use in the context of this hand style. Second context would suggest that the hand style is flowing, graceful and flowing .",
    "Topic 2": " Each non terminal symbol is a syntactic category . For each of these non-terminal symbols they need to rewrite it into something else . We'll take a more automated I'll call it , a more algorithmic approach to do this that we can get a lot of the same effects without needing to learn too much linguistics but you can add adverbs .  Markovization is rather than modeling the entire thing all at once, we're not gonna do that . The standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter . You have the vertical marketization. Order of , , V equals . And the horizontal Markovizations markup order of , here's the standard Pcf with infinite context .  There's no 2 words or expressions that truly mean exactly the same thing . It's very difficult to separate synonyms from antonyms using many computational techniques . Another lexical semantic relation is something called homonomy . In Wordnet, there are 6 different senses in each word associated with the word .  One computational task that you might want to solve is called word sense disambiguation . One reason might be, you think it's in inherently interesting to figure out how do we figure out the intended sense of all of these words that have multiple senses? And that often different senses of a word should be translated differently when you translate to a different language .",
    "Topic 3": " Each non-terminal symbol forms a categorical distribution over all the rules with that non terminal symbol as the left hand side ? The idea here is that you have a particular way of estimating the probabilities of rules through a maximum likelihood estimation . But do it algorithmically, which is, we're gonna split up categories vertically and split up the rules horizontally . This is a quick fix for the vertical problem. We're gonna pretend that every hand side is a Mini Markov chain that we need to learn .  Each non-terminal symbol only is involved in one rule. We've wrapped up structure and parsing. We have two questions about syntax and parsing? although in a slightly different way, and the probabilities are all messed up. This method and this procedure is about training of the model and learning the parameters of the . model .  The example sentence is due to her superior education.project over the past 2 decades or something. The example sentences are due to the fact that she has a better education than her peers. Project over the last 2 decades has been a success in the past two decades of her education.",
    "Topic 4": " We're going to start by focusing on the meanings of words and how they construct sentences and how to construct these meanings of phrases . And we're also talking about how we can use these representations to derive new conclusions through some approach that's not only specific to Nlp, but that can help you make decisions or do whatever you want to .  The 1st was frege in 1892 he was one of the 1st to distinguish between the sense of a term, and its reference where the sense is more the intentional stuff about the . And the thing that denotes fewer things in the world is called the hyponym . The word that denotes more things is called a hypernym. It can be very difficult to distinguish . between harmonymy versus Polysemy .  The edges in this graph correspond to lexical semantic relations between syn sets . You can have a table of contents and actuarial table, a calendar file, allocation, table, periodic table, and on. arguably, one of the most obvious things you can do. Once you have those resources, you can try to disambiguate a word into which word sense was meant by that word .",
    "Topic 5": " There's a correlation between subject position and some semantic role of entities that tend to do things or have actions in the world and cause changes . The Cnf model is for the unigram model, which was already started with something that was already in something already .  The intentional definition is talking about the conditions, the necessary and sufficient conditions for something to mean something . However, there are different senses, because the morning star might be that bright thing in the sky that appears in the morning . These are synonymy and autonomy . synonym means that 2 words roughly mean the same thing . antonyms and autonomy is words that roughly mean opposite thing .  Application oriented reasons to work on this task . Application-oriented reasons to do this task. Application-orientated reasons to be able to work with this task: \"Application-orientation oriented reasons\" for working on the task.application oriented reasons for the task to be done."
}