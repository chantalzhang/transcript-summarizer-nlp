{
    "Topic 1": " Last class we talked about syntax and structure and hierarchical structure in natural language . It's a, it's a model of formal grammar that we can apply to model the syntax of natural languages . And, in fact, even broader question is . instead of for each individual language, English, French, or German, is, does there exist a Cfp for it?  The natural languages are ambiguous in many, many different ways across many different levels . There are usually multiple parses that can correspond to any sentence of a language . And the sentence is, I shot the elephant in my pajamas, where there's a prepositional phrase that attaches to shot there are two parses for that sentence .  Each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words . The entry at each cell is the list of non terminals that can span those words according to the grammar . The structure is different because we are assuming a different structure topologically, for us, the base case corresponds to individual words .  We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect . For shot it would be V. for a shot, for the for the For V, for N, N. or in it . Or in it would . Or N, for my it would. Or for my . Or for a . shot, we already checked that ? there's nothing we can build there, that cell is empty OK, for shot .  The basic idea is, if a cell is empty, then that's not possible to build something there . The rule is just you're just checking that break point . If things worked out properly, there should be a bunch of S's here . The other rule that works is NPPP, NpVP, NPVP, and NPVP .  Probabilistic parsing recover the best possible parse for a sentence along with its probability . The probability of a parse tree is going to be the product of the probabilities of all of the rules in that parse tree . You only need to keep the Max probability and the Max back pointers of the span, left hand side and non terminal symbol .  If they're two possible tokens we can create, and one of them has a higher probability than the other, then we keep both . The standard assumption of the PCFGS does a very poor job of modeling all of these situations . There are many different ways you could build that verb phrase .",
    "Topic 2": " We are going to continue our discussion about syntax . We talked about these things called constituents, which are groups of words that acts together as a unit in a sentence . And we also talked about tests for constituency, if you Hey? Do you all remember that . The other thing we talked about is we talked . about context-free . grammars? The first lecture presented some examples of that in the first lecture .  You see in the sentence you're trying to. the possible rewrite rules and you try to find a way to rewrite the non terminal symbols to get finer and finer grained until you get to rules that let you generate the actual words that you see . using the rules by matching the words 1st to non-terminals and then the non terminals to bigger non terminals . But we're just going to cover one, because these days you're lucky in that, parsing is no longer as popular .  The Np to N, yes, one important thing to note, a beef pure it'll be clear if I just delete it. One important thing is that you keep the original rules, too. The reason you need to keep it is that your tree could have gone to I an elephant in pajamas through N. The key idea here is to take advantage again of chomsky, normal form in that all rules that produce phrases are of the form, a rewrites to B and C. And the key idea is to set up our data structure from WI to J plus 10 indexed .  You can do one diagonal at a time, do the diagonal, and then to do the things to the diagonals and forth . If you tilt your head 45\u00b0 this way, you can even see the parse lines directly in the chart . And then every time a starting symbol is used, you use the back pointers to trace through all of possible paths .  The sum of all of the rules involving that Np. NP rewrites to anything else must sum up to one . The probability of a tree is going to be the product of all the rules that make up that tree . This is called a tree bank . A tree bank is a collection of trees in a in a bank, with . a with sentences, with, with a, with sentences .",
    "Topic 3": " The goal that we have is to recover all possible parses of the sentence . In practice for all of these cross serial dependencies, the M's and the n's are never greater than 2 dependencies . The other strategy that you can have is a bottom up strategy, which is, you start from the input words . And anything else is very arbitrary and contrived and artificial, and you never see it attested in corpora .  We're going to set up a 2 dimensional table that will store all of the constituents that can be built from contiguous spans within the sentence . The algorithm allows us to deal with ambiguity because you can store multiple things in each cell and then you can use all of them to build bigger . Cyk algorithm is to run the algorithm to keep track of probabilities in the table itself .  If you create an Np constituent here. We already know that we won't use it, we're throwing that out . But if there are 2 possible tokens we can create. They might be, they might the probabilities are not all independent of each other. But you add some adverbs we talked about kick the ball versus. kick the . ball softly or, or . or quickly . Then you can use what we already know from everything that from everything we discussed to do with estimating values of parameters, with Hmms. And Ngram models, and forth.",
    "Topic 4": " We're going to talk about CKY parsing or CYK parsing . And we're gonna talk about an algorithm to do that . And then we're also going to motivate why we need that . I'll also post the reading assignments soon . I know it's been over a week, and that's an eternity and we've all forgotten everything already .  For us, we're going to talk about an algorithm to recover all parses according to ACFG. And that's called a top down strategy. and a bottom up strategy. There's an early parser that goes top down. We're talking about one called Cyk. And we're just going to cover one parsing algorithm. But you still should know about it, because it's very good pedagogically.  All of those algorithms forward algorithm, backward algorithm, Viterbi, those are also dynamic programming algorithms . And in our grammar it's going to be Np. you can build a rule with the constituents that you've already found there . If we're able to build this bigger chunk A, that means we have already found the smaller chunks B&C and we can combine them to form the bigger chunk . And we just need to check all possible breakpoints to make sure that we cover all of the possible ways to build that bigger chunk. Then you can use all of them to build bigger chunks, bigger constituents .  The 1st possible breakpoint is 4 to 5, 5 to 7. The 1, 2, 3, 3 to 7 is not possible . The goal is not to just recover all possible of a sentence but to recover the most likely to recover all parses . And this is how you can recover all of the parses is how . You'll see the depth of the depth .  We are no longer computing all possible parses because of because of this . a real PCFG situation, OK? But there's no, the theory doesn't give you any reason to prefer one part over the other in that situation . The the the one that I would look at is to do top down parsing with the early algorithm .",
    "Topic 5": " Jackie Cheung, Professor: There's 1 topic from last class we didn't quite manage to cover, which would be interesting to talk about that . There's gonna be an S node which is the starting symbol which is . The other strategy that you can have is a bottom up strategy, which is you start from the input words and then you build ever bigger sub trees .  We need a base case and we need a recurrence step . If you care about recovering all possible parses, then you have to store both . The harder step is the recursive step, because this step corresponds to multiple words . We're gonna do it together for practice .  Npx, one is not a thing, and NPX 1 is not an NPX . NVPI don't think MVP is a thing. I hope I'm not. Something, but I hope it's something . CF GS become PCF Cfgs become Pcfgs, which has a 0 point 2 probability, and on, and forth .  We modify CYK algorithm directly to incorporate probabilities as we go along, that by the end you'll just end up with the most probable parse tree as is . That'll be more efficient because then you don't have to explicitly create all of the ambiguous trees and evaluate the probabilities for all . This is just another way of rewriting that I don't know what's you prefer in terms of being more human interpretable, but it's just the same thing ."
}