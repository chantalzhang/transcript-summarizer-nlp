{
    "Topic 1": " We often do not know the true distribution of the test set, we assume our model is good enough, and then we try to estimate, just based on our model, which is our cube? I have a good model that can represent your test set very . Oftentimes, if you can achieve a place of 15 on English Wikipedia, a better model than having perplexity of 15 using a brown compass . That means the model is better .  The concept I want to explain is out of vocabulary . Very rare words, even a 1 Zip also used to be very rare in your corpus . I would treat them as a node. all other words that are infrequent can be replaced by unknown . The art theta smoothing technique is what is called the art thea smoothing. And this is what we call back off language model .  Then you average a trade off. Between what corpus am I going to use to build my language model for this language. Is it a very big one or a very small one? The more data you have oftentimes the better it is. But for statistical language model.then you average .",
    "Topic 2": " David Ifeoluwa Adelani: If you always make your probability to be equal 0, then it's going to ruin your estimate every time . We said that everything we are calculating is what is called the maximum likelihood estimate but today we're going to see how we can derive also a general MLE . We can have an exact calculation of what will be the Mlb. Of PQ.  To modify our goal is to modify our accounts in such a way that we reserve probability ? The way we compute accounts is that we say c plus one multiplied by the frequency of the count plus one divided by F of C . Everything here should be equal to 50. But that doesn't seem . 4 by 3, divided by 50. Fc, would be one, because soccer. because C. Star is C plus one .",
    "Topic 3": " This is what we want to cover, because if you're able to estimate probabilities for your language model for every combination you have in your training set . How do you compute the unigram distribution for a particular world? that would be count of every time cat appears divided by count of all the words in your couples . And we also went to an example in the last class to clarify this .  How do you estimate for infrequent words, we need to slightly modify this probability distribution . The idea of smoothing is the probability distribution to shift some probability mass to cases that we haven't seen before, or we are unsure of that maximize the distribution . With unknown words, often unknown words don't have here, but you assume that your no wants to be f 1 .",
    "Topic 4": " The passport language model is to predict the world . We always make the assumption of conditional independence . The only difference is that you have what different values of the probability of a BC good . You have one at one of K discrete outcomes . If you flip a coin, you can either be the head or what, or the tail .",
    "Topic 5": " Cross entropies are trying to compare to distribution our care . Given the context C, and that will be probability of N. Probability of B times probability of B is that clear? for Model 2, it will be very similar to the probability for the uniform distribution you have computed it . You can make it depend on just the last token or the last previous 2 tokens .  WTWT. And then you can compute the infinite probability. Terms of more that complexity trade off . You have to think carefully about this. You have a lot of time to think about it. Think carefully about what you want to do. Consider the complexity trade-offs ."
}