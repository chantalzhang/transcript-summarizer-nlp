{
    "Topic 1": " We'll continue our discussion about part of speech tagging . We use, , , , pr ovn they have various different symbols . And also, , we made that assumption which we call the Markov assumption . And then, we relate this to Markov chains . We also call it the . Markov . assumption . We're trying to create a table of all possible state sequences .  The em algorithm is that you 1st assume a set of parameters randomly . And that you try to estimate what will be the value of theta or at that time step with that value . And for the maximization, then you find a better, better, Theta k plus one at the time .",
    "Topic 2": " In Markov chains we talk about how we decompose the joint probability which by has been removed from the board . The Union variables are Union variables and the o's are the observations what you observe . And to compute it, it's very important to do this revision and then you have the a high J, which is computed that given . What would be the likelihood of a sequence of observation which is the probability of all given Theta and this Theta .  Emi guardian can be used for different tasks, such as finding a word, organization, and the old tax, signifying that there is no entity located is not an entity . We have a gamma I of T, which you can decompose to be this probability of Qt. I given what you observe and the parameter you are looking for . and we can then use that to estimate what is our Gamma T. Of T. Gamma, high of T . and after that, and then the probability of Q equals? I .",
    "Topic 3": " We examine 2 popular schemes? where we use nlp for proper nouns and for universal dependency? With some initial probability you can then generate what will be the observed word which would be, o 1, and also, if you want to say probability of OQ, where all and q are random variables, and all are all your observed . This is in a very unsupervised way. We are going to examine things the em algorithm that is very popular in mushroom .  When you're searching for an important thing on the web, you want to get the entity that you can get the information . This will already have an entry on Wikipedia, and then you can extract information about the entity . The iob 2 tagging scheme is the most popular tagging technique, where you clearly define the beginning of an entity and the ending .",
    "Topic 4": " This is an example of the pantry bank scheme . And why, what was the motivation for having a more universal dependency scheme? try to generalize across other languages . We have the initial probabilities. And then we have the transition probabilities from . Plus one which we can also compute using mle. And here we also talk about the model parameters which we . estimate using an mle .  You need to have some initial values to do your calculations for the afford algorithm, this is very clear because you can assume the initial state probabilities . But for the backward algorithm we don't know. we can just assume the maximum value you can get for probability is one . For the forward backward algorithm here you just out to multiply what it offers and the betters together . And the second reason is because you are at the same States, for the backwater gardening .  We want to compute new values for, alphas! where we have the joint counts divided by the count over the initial states before we move to the State . And here you are trying to compute what is the transition probabilities . We also have some other sequence modeling task chunking where you find syntatic chunks in the sentence .",
    "Topic 5": " We are going to be going through some algorithms that are popularly used for this task . We have verbs and adjectives, we have preposition adverbs and determinar . The part of speech depends on the previous part of . speech . And the simple answer is that you just have to marginalize over all the state sequences . Once you have calculated all the . state values, you can marginalize the entire values to calculate what is the probability of all given theta .  We are trying to just learn everything unsupervisedly from the data . There are no labels to train on . There's no guarantee that you're going to get very good results . The only supervised setting, which is about vash a guardian with no labor data often gives very poor results ."
}