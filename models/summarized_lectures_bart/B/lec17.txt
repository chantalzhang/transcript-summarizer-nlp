David Ifeoluwa Adelani: Today we'll be talking about the machine translation machine translation which is a very interesting direction. It used to be a very, very difficult problem, but I'm not sure it's very difficult. Even the current models doesn't solve the task. The outline of today's talk will be Oh, there's another announcement. If you are supposed to take a makeup examination. This will be on Wednesday. oh, 4, 30, that means you are taking it. machine translation, we'll talk about why, it is a hard problem inferability and superior wharf hypothesis. Then we talk about the vocros triangle. Some languages do not have plural and some languages they don't have. You cannot even say for plural, , in our language you have to really specify the number to indicate popularity. Some words such as again stop, or more. you suppose, or contain an assumption about the world. in Zulu you can also have suffix. And then this would change the meaning of the world, and then the prefix might change present tense to past tense and future tense. We also have languages with noun classes which a good example, would be the Bantu languages in Africa. and also we have Syntac differences what other differences? some words, we have the X view. Is it possible to have perfect translations within subgroups of languages? Is it possible between French and Arabic, it's, is it possible? Is the problem they'll come perfect is the problem. You can have high accuracy, no highly accurate translation. but saying something is perfect. The language you speak affects your thoughts. The strong version is that language determines and constrains all human interaction and thoughts. We are not at a point of worrying about Sapia worth identity. But what we can do is to just minimize the complexity and say, can we just achieve translating about events. You have the text in English, which is the reference. and then your machine translation model produce another one which we can call the hypothesis or the output. And then you want to compare this hypothesis with a reference. And how do you compare? You want to do a matching of n-gram matching of the words. And one way to do this is using the bluescope the metric was introduced in 2,002.  blue is focused on what is called precision. It's precision oriented for each engram in the proposed translation you have to check if it if it is found in the reference translation. In practice, blue incorporates an additional brevity, penalty and a geometric mean over several values of N. This is a regression task. That's why it's changes in Miss Square Arrow. And based on this, you can create an estimator that you can use for estimating the performance of a machine translation task. , this is useful for some of the projects that you'll be working on. the Interlingua is a conceptual space common to all languages, that if you can take the source text to this intelliga, you'll be able to do the translation. The advantage is that you can use to develop a general empty system. a system that is trained on and works for a set. The key thing is that we need to do an alignment. You need to match one word to another. And then you can count the number of times the world in the. world is associated with another world because they always call together. You can use this to create an alignment model to do the translation. , if language use a similar autography, an example of a cognate word is , , reference and reference. You have to think about things possibility of translation. Many, too many mapping is impossible to do the mapping directly. Another thing that is very important, especially if you move to different languages is the word order. Olympian exam asks you to perform a task in a language given some rules. They provide the rules based on your on your linguistic knowledge and the rules that have provided. And then you have we? It will only have one example, but it would be included. Ibm developed a series of 5 differential models that make increasing powerful assumption. Decode is what we change from, understand to understood this. Think the past tense uses we, and the current tension is . does everybody agree with that? , your answer seems very possible, because that's the only thing we can say here. Probability of, , a French. What given English you can materialize over the alignment which is a and this one to one based on probability theory. And and then you can also oh. use the Bayes rule to say. this is a joint probability given E. And this will give you the probability of F. Given Ea. Multiplied by probability of a given E, of course we are make we are. Is just counts the probability that this word occurs both in English and French. You normalize by the count in English. using the number I made. and after that you can multiply it. But the question is that let's assume you don't. What are you going to do? You have different ways of doing this. it would be a language based rules and not dependent on the length of the sentence. a and all its possibilities consider both the language and thelength of the site. it possible to give an example of how? there might be an . This is just using em a guardian. and then you can create different alignment rules to match to compare the sentence in English and sentence in French. and then you cannot apply the Ibm model, which is just based on the statistical model. But as in the class, there's a way we can adapt this that you can have something. There's more there's slightly better. And this would be IBM model 2. And after that we're going to examine the last model which would be neural based.