Today is the lecture 10, and from class we have another professor taking it, Professor Jackie Sean. We have a reading break week, of course, already about the Thanksgiving on Monday, and then we have the entire week. We don't have any lectures. but there will be lecture on Wednesday. And then we discuss recurrent neural networks. and after that we'll move to long, short term memory networks. And at the end we are going to talk about Lstm Crfs, which is also the topic of your reading assignment. We are interested in learning the likelihood or probability of X given theta. In the Lccrf, we're interested in Z of X for the vertebra algorithm. What is the parameter of the model that we can take? That will help us to learn the joint probability. We try to compute the mle for this by taking derivatives. , going through this lecture. We want to review artificial neural networks. And if you remember, in enough, . The second week of the 3rd week we're talking about different classifiers. We talk about naive base. Then we discuss a support vector machine. And also we discussed artificial, neural, network, artificial neural network. , you can always connect any directed graph into a matrix. , all these connections shows you. you can have a matrix based on the dimension of the input and thedimension of the eating layer. You multiply with the weight matrix which the weight mattress describes all these. connections. Once you learn all the weight Matrix, all the. weight matrix, , you can do computation from x 1 to X. This is a revision for stochastic gradient descent. We are only interested in one training, example, and then we can find gradients of the loss functions with respect to the parameters. We use an algorithm called back propagation. It's a very old algorithm. But we have not been able to find a better algorithm. You multiply the input by the 1st weight matrix plus the intercept. Then you take the output as an impute to the second weight matrix. And then you do the multiplication. And at the last stage you have another activation function. This G 3, , if it's a multi-class classification can be a softbox function. and you compute the loss function between what has been predicted and your gold label. Recurring environmental is a better way to incorporate previous information into the current model. The time information can be easily incorporated into a recurrence information without using a time delay neural network. The idea of recording analytics work is that you have all these different cells you have cell one cell, 2 and then you have the state vector and the outputs. Which is the initial state. Then you pass this information of your x 1 to hen, and what you will get at the end of the day is that. You will get all this state vector, information s. 1, to Sn, and also you get all the Y information, which is your output. y, 1 to N, this is a simple idea. , but just to compare the Lc Crf and Rnns. if you last class, Lccrf our linear chain crf, a more linear model. If you are building a classifier, as you will see in the assignments, the reading assignment is that you can just replace your linear layer at the last layer and replace it with that of the Crf. Many to many is correct for language modeling. You will use many to many that's correct. For part of speed tagging, which menu to menu would you use for longer? The 1st one or the second one? Shidan Javaheri: I feel you would capture the context of the entire thing that's being said. For the Lstm which is the most popular Rnn architecture or Nlp, the model includes what is called a memory cell. We can try to store a lot of information there to be able to capture long dependencies between words. And inside each memory cell, we have a vector of weight in the evening layer. And then we want to learn all this information. the sigmoid function, it always gives you a value between 0 and one. If it's near 0, you will likely forget that information. You need to be careful to have all this information within a certain range. you can have problems in your recurring neural networks what is called exploding gradients and then vanishing gradients. In Lstms we can propagate a cell state directly to fix the what is called the vanishing gradient program. Apart from that, you can also not only have a single Lstm, but you can have bi-directional. That is a way to capture different context of information. Lstm is a language model that is going forward way and backward way, by Lstm model. Once you have all this information from the forward layer and the backward layer, you can concatenate them together to make the final prediction. If you train a by Lsdr model with a simple linear layer, and you might achieve something 88%. If you replace this linear layer with a crf, your accuracy can jump one or 2 points. The Lstm Crf is a popular architecture in 2015, 2016. You can combine this with the Crf by using what is the transition probabilities between the tasks? Because this is what you get for free from Crm. And you can add this information to improve your performance. I'm really excited about this bi-directional Nstm, it used to be a very strong model before birth came. Some people are still trying to revive Lstm to see if it can be competitive with transformers. It's really important to try to connect the old architecture we have been using for long in Nlp with a more new architecture. David Ifeoluwa Adelani: for machine translation, you need to encode all the sentence information for language, modeling for every word you have seen. That's why the last one here is the better one. Shidan Javaheri: I had imagined that would also be helpful for large from language modeling. David Ifeoluwa Adelani: Your role is in time, you are predicting the word also alongside. I will still be back at some point after every 4 weeks, of course. to have some more advanced talks. and to talk about things machine translation and crosslingual transfer.