There are over 7,000 languages in the world, and or over 400 of them, are spoken by 1 million speakers. Some languages are not supported by keyboard spell checkers, morphological analyzers and dictionaries. According to Achnolog, you could also categorize them, based on which languages are institutional. One of the most popular ways to categorize languages is based on Jewish classification. This is a Sys class categorization of languages based on how much unlabeled data are there on the web. English, Spanish, Chinese. win us in terms of the amount of data that is available. The focus of the encoder parts is to have a very good representation of the model. Even with attention, is still trying to do the same job. But better job of passing a very encoded information to the decoder is all used for generation. you can generate a new, a new text. The problem is that you cannot really do have a good, successful, self-supervised training if you don't have a lot of data. You can leverage what is called transfer learning, which is very popular in the last 3 years less popular , because we're prompting. But that you can transfer the knowledge of a model to another task to another domain, to another language. There are 2 ways to fine tune. if you have labeled data, you could just fine tune this multilingual bits. If you want to do multilingual transfer posting transfer, you need an encoder that is multilingual. The difference is mass language model has been performed on multilingual text. Just append, and then train the same. And then, if you have trading data, you can find some directly in Americ. And if you don't have training data, one thing you can do, is just do 0 shot transfer to the language. There are over 7,000 languages in the world. We don't have a lack of data set for many of these languages. We also have issues of parameter inefficiency. And there are also issues of what would be the best source language. The Masaka Project has created a data set for 10 African languages. The data set covers different tasks from question answering text to speech sentiment, classification, machine translation, news topic, part of speech, name, density, recognition. And we have this collaborative projects where we have native speakers and Mls. There's no translation of some words in English language. But the models are smart enough to bypass that. One word, and they're a lot more complex and translates to a phrase. The answer is , the issue of classes. entities here, you have something using ? Nigerian pigin is a language in Nigeria called Nigerian pigin. This is a clear language, and then you have because it's very similar to English. Even if you don't speak this language. you're able to record this. And then this is how we develop this Masaka news data set. Siv 200 is a labor data set for many, many languages. It uses an existing data set called Flores, that is based on machine translation. The project has been on for more than 5 to 10 years, and they are slowly adding 2, 3 languages every year. Kappa score was pretty low, just because we're dealing with very short sentences, and people disagree a lot, even for these short sentences. Congo, Indo-europia, was the largest, followed by, Atlantic, Congo. and Austronesia, and all that. A larger model are the best results. and if you go to topic language, you also have very good results. And on average this preaching language models also give the best result. If a language is not covered during pre-training, they often have lower performance, and then we have categorization. , you still have a high performance while you have some languages where the language is unseen, and also the script is unseen. We also compare prompting with 0 crosslingual transfer. And here you find out that , models Chatgpt just 1 min had much worse results than just training on English. 0 shot transfer to all the languages. Just , site level quotation. , that's a good question. Yes, we do have character level models. And we have bite level models, which I did better. But it has not been widely adopted. , for the languages that are unseen and script unseen. There's some that are still very accurate. This is due to what is called cost of multilinguality. If you have a word based model tokenization. And then for a non-sympathy scope Americ, you're gonna have a 4 score of 0 because you're just using one level. But one thing you can do about this is to replace the vocabulary completely. You can train a tokenizer, and then you can copy all these tokens. you trade the tokenizer on. It's a heuristic way that a guardian of tokenizing the text ? And then you have to ensure that the size match. if the original model has 250 K. Vocabulary tokens. Then you can redo the alignments by further training. Just trying to get as many importance as possible. We are a bit guided, based on what other people have done previously. you just don't do a million tokens or something. how to address this limitation of lofts which we call left? A a very simple way is just to return your own new model. Xmr was trained on very diverse scripts, very, very diverse languages. The model is also very big, and up to half of the model size is in the embedding part. We are the number of parameters of our model went from 270 million parameters to 140 million parameters without. there was some drop in performance. The only language we did not see improvement in performance is English, because the model is already good for English, there's no need to adapt English again. When we have a smaller model size than when we move to a bigger model size, there was no difference in performance. Similar for Yoruba, for Yorub. We even had better results, Kate. , we create a better model than previous really strong multilingual models by Google and Md. And the model is a login face. you can have an empty 5 model, and then you can specialize it also to region of languages. And also, you have similar effects where you can also improve the performance even for generation task. how to adapt an English Llm. you train a model on Chinese, and then you evaluate on the rest. On the remaining 41 languages. You train amodel on English, anyhow. model on English,. then you evaluation on the other 41. languages. And each map transfers call this. Arabic German, we see some interesting transfer to some African languages. We only have 45 languages. , from Shona to Zulu, you have better result. Some people have been thinking about this problem. In 2019, there was a paper from Cmu on what is called language rank. that you can train a ranking model, a ranker model to predict what would be the best transfer language for a new language. Linguistic distance is a combination of distance measures, genetic distance and data dependent feature. If you have a language Indi to, Swahili, they're very far from each other. The top one rank is better for some languages just using English is better, but if you do what is called co-training. That means if you train on the top 2 predicted languages, you'll find out all the time. This is better than using English every time. And the last thing is which is called parameter, efficient, fine tuning. you, if you're able to get the best transfer language, you can combine it with this framework called parameter efficient fine-tuning. The best transfer language here seems to be wall off for are not English for Euroba. But if you go to Zulu, all of us transfer very to other languages. And those little details are important. If you can get it, this can already boost your performance. And if you combine this with a different parameter, efficient approach. you can also further boost the performance.