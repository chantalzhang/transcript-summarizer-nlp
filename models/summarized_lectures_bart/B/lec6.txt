David Ifeoluwa Adelani: today we'll be talking about smoothing. someone asked a question last time about this, and then I can explain in details about what is called smoothing where you try to redistribute the some probabilities to unknown tokens. Last time we talked about how you can evaluate. , you could use things cross entropy or publicity. Today we're going to see how we can derive also a general MLE. For a. for a particular purpose. If we know the probability distribution. we can have an exact calculation of what will be the Mlb. Entropy gives us a number in bits, perplexity gives us the number of possible numbers you can represent with those bits, because it's 2 to the power. If you have a good model that can represent your test set very . the perplexity will be lower. Oftentimes, if you can achieve a place of 15 on English Wikipedia, a better model. how would you compute the modal, the probability of the model distribution? Can I get the final formula for complexity? The final formula is here ? 2 to the power of minus one over 3, because the size of the corpus is 3. We assume that, given a training data. and you have already estimated unigron probabilities. as P is probability of a equals. 0 point 3 probability of B equals 0 point 4 probability of C equals 0 points 3. If you have computed this, , if you are very fast in computing this. you can you tell me which is a better model for the test set. And why is it the model with the higher publicity, or the model. with the lower perplexity? We know the size of our worth, and for different tasks. You have a few number of classes that needs to be predicted. and for a given context they are additional independence and identically distributed. for a categorical, random, variable. categorical catalog variable is defined as this. If you flip a coin, you can either be the head or what, or the tail. Log of theta plus and or log of one minus theta. I believe this is planned . If you have questions, feel free to stop me. And if you take the derivative of this formulation. what are you going to about? The derivative of log of theTA is, what about theta? a large multiplier, if you . after you have computed the ultimate. parameter of theta, it also computes the ultimate parameter for this a ranch multiplier, which is a landlord. What we suggest is, this will be. and then this is the probability based on your corpus. Give them theta minus this Lagrange multiplier. , and if you put 3 and 6 and combine them together, you'll be able to find what is the optimal parameter of Ni equals Ni. The problem is that if you are going to test it for an unseen data. you are very likely to have a lower performance. When you're computing what is the count of every word in your couples? And you want to build a vocabulary. One thing you can do is that you can select a threshold and say anything less than 5. The idea of smoothing is the probability distribution to shift some probability mass to cases that we haven't seen before. If you want to resolve some probability mass to an unknown world, you have to formulate off what is called data. data can be, it's it can be a very small parameter. Some people can use one divided by the size of your corpus as data. or one where, by theSize of your vocabulary has data. Laplace discounting is when the letter is one. Data is defined to be equal to one that's Laplace moving. The easiest way you can do is just to do what's called interpolation. If at the test time, the way to think about it that you can understand is that you have already estimated. This is a very simple way to combine the trigon probability with the background. An example of a more advanced smoothing technique is what is called good sewing. and is a little bit more sophisticated for handling the unseen events. If you are unable to estimate probability at the diagram level, you can go back to Unigram. you're very likely to estimate. which we already see in the Zip's law. an on synongram should behave a lot . and grabs that only occur once in your corpus should behave other engrams that occur a lot. here we can do a ranking and said, the event frequency. It's your count, the count we have here. and the number of events with that frequency. that at a test phase you'll be able to address the issue of unknown. every word should appear one once, if it's in your training data. With unknown words, often unknown words don't have here, but you assume that your no wants to be. When you want to evaluate the model, ? what's the probability of unknown? If you are able to get for soccer, you'll be able to forget for count. What's the probability of count? c plus one would be 3, ? What is F of 3 F of 2 is 2. . what are you going to get? you're gonna get one over 10. And the last one is probability of pass. Star, divided by N. here you are going to say, 4 c. Plus 1, 4 times F of 4 FO, 4 is what f of one F of four 0, and you already know the answer. 4 is a all . Do you have question, or everything is yes. If you want to compute. Count of Wtr. you, want to computed Count Star. This will be the count of Wt. Multiply by the frequency of that diagram at position C of that WTWT. Of that background. Minus one plus one divided by F of C. Simple, good turning often fail. There is a trade off between how the model expresses expressibility. If you use, and a very, very expressive model, , the one with high values of n in angular modeling. It's very easy to overfit because you all often need to do smoothing and use more data.