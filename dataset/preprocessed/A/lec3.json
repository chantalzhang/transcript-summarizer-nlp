[
    "I. Hello I'm trying to fix this thing and it's not working.",
    "Hi everyone,  for the delay in starting the lecture.",
    "who can remind me what you have been taught last week?",
    "last week was Jackie, I  introduced you to test classification.",
    "what's the difference between supervised and unsupervised learning?",
    ", , I didn't post it this morning, but I will post it after the class.",
    "I have to speak louder.",
    "what's the difference between supervised, unsupervised?",
    "you don't have output for the unsupervised.",
    "what's the difference between classification and regression task?",
    "you  also did some feature selection, how to come up with features for your classifier.",
    "who wants to comment on why do you need things  lemmatization or stemming one beauty features?",
    "What do you mean by non necessary features?",
    "you  want to reduce the number of types that you have  that you have  more compact vocabulary.",
    "All ,  today we'll continue with, we'll start with linear classifiers.",
    "last week  we described classification where you have an input X.  an input X can be a vector, can be a matrix, can be anything.",
    "Then you want to learn a function F that will give you the output Y.",
    "Output Y can be a discrete outcome or categorical variable.",
    ", if you want to know you want to classify a particular sentence into spam or non spam  you can learn a function F, It could be a simple function  linear regression.",
    "we discussed linear regression last week.",
    "Or it can be a more complicated function  a deep learning algorithm.",
    "It could be a transformer architecture.",
    "you want to learn a function.",
    "It could be as simple as linear regression or more complicated as artificial neural networks.",
    ", you want to detect if a movie has a positive or a negative sentiment and for clustering you want to just give it an input.",
    "You want to cluster the data into different categories.",
    "we already differentiated what's the difference between classification and regression.",
    "let me ask a question.",
    "What is the use of training set, a validation set or a test set?",
    "Why do you need this?",
    "I know you may know this  before, but , that's what is that validation or test set  they can generalize, not sure.",
    "OK, you want to try validation set?",
    ", that's would be the test set.",
    "Test your model or , learning function.",
    "All ,  quickly, let me go through cross validation.",
    "let's assume you have a very small data and you don't have you only have training data.",
    "You don't have another split for test data, another split for validation data.",
    "We have a principle called cross validation which is also used for model selection.",
    "And  that you can split your training data into different chunks or subsets.",
    "And then you can, let's assume you have  500 samples and you have  5 fold validation.",
    "And  you can split it into  100 hundred 100.",
    "And then you can train on the 1st 400 and evaluate on another subset.",
    "Let's make it very simple.",
    "Let's assume we have  threefold cross validation.",
    "You can split it to generalize.",
    "You can split the training data into K folds and then test.",
    "Or  you split it into K folds.",
    "You train on K -, 1, and then you evaluate on the last one.",
    ", OK,  for the first fold, you pick the first fold as your test set and then you train on fold 2 and fold three.",
    "You combine them together and train.",
    "The experiment too is that you train on fold one and fold 3 and then you evaluate on fold 2.",
    "And the last one is that you can train on fold one and fold 2 and then evaluate on fold 3.",
    "And by doing this you can  aggregate the test accuracies and then try to pick your model parameters.",
    "It could be you're trying to pick an hyperparameter and then by doing this K4 cross validation you'll be able to choose what will be the  hyperparameter even if you don't have a validation set.",
    "Is this simple enough or do you have questions?",
    "Do you mean by a?",
    "when you are trying to learn a function  this, typically you have X, Theta.",
    "Theta is all your set of weights or your parameters you want to learn in the model.",
    "And some parameters cannot be learned automatically, you have to fix them.",
    "in the case of SVM, there are some parameters that you have to just fix and you don't know this parameter, but you  theoretically know the set of values that this parameter can take.",
    "it could take  between  0.01 and one or not bigger than one.",
    "since you are not sure, you can try to tune your model to pick what is the  hyperparameter.",
    "Should it be 0.5 S?",
    "And then you can have an equal split of the range of values between 0.01 and one.",
    "And then after training your model on using this different hyperparameter, you can choose what would be the best hyperparameter in that case.",
    ", if you are training a simple spam classifier for SVM, we have a hyperparameter and then you can say let's set this to 0.01.",
    "And then you train for the K fold.",
    "And then you have the average error for that hyperparameter.",
    "You try the second hyperparameter, you try the third, you try the fourth, you try the fifth, it could be 10.",
    "Then you select which one has the highest accuracy on this, and  you can  pick that hyper parameter as every parameter you're going to use to train the entire model.",
    "Or roughly equals size because.",
    ", it's going to be different model, but you're going to just train on the same data.",
    "you trained for the experiment one, you're going to have Model 1.",
    "Every training results into a different model.",
    "It will be different models.",
    "But the most important thing that you're evaluating here will be what is your validation accuracy across the different experiments which you are going to average for a particular hyperparameter.",
    "OK, I hope that answers your question.",
    "OK, you have to set it as a person performing the experiment.",
    "And if you don't know the hyperparameter, you can have a set of values.",
    "every method have some theoretical back in and they have some hyperparameter that needs to be turned.",
    "In the case of neural networks, you have things  learning rates, but you don't know what should be the  value of the learning rate.",
    "you have to give it different set of values and then try it out to determine which would be the best hyperparameter.",
    "Yes, yes, you would choose the model, you would choose the hyperparameter with the best validation accuracy, yes.",
    "OK,  this is a very small example which you can try.",
    "How many people are not familiar with SKLN?",
    "OK, It's used to be very, very, very popular,   10 years ago.",
    "Because the best models we have, even neural networks, sometimes they still struggle on some tasks.",
    "And you could, it's a very small package which you can install.",
    "And then if you have very basic tasks  test classification tasks,  topic classification or sentiment classification, you can easily run this package on it.",
    "And then you can  practice some of these things you have been taught because #1 if you have a test input, you have to determine what will be the features.",
    "you have to have a vocabulary.",
    "You have to use principles  TFIDF, which is a way to normalize your data  that you don't have unimportant tokens.",
    "last week we also talked about things  stop words, and sometimes you can  exclude these stop words because they don't influence the performance of your model.",
    "this  package allows you to try out different feature selection.",
    "But nowadays we don't really do a lot of feature selection because we learn a big model  neural networks that  tries to learn the features automatically given a text.",
    "It just tries to learn what are important features based on this.",
    "this is a very simple way.",
    "If you install this package SQL line and then you have a very small data X here you just  have two features 2 dimensional and then you have this output which is either 0 or one and then you can fit the model and predict.",
    "this will give you the logic and then you can add a softmax on this to determine what will be the probability.",
    "OK. OK. Today we'll talk about how to train a classifier on a training set.",
    "But today we are focusing on linear models.",
    ", last week  this will be another revision where we already have  feature extraction.",
    "You can determine the features you want to use for a particular text, and then your inputs can be a document or a sentence, and then you have a document label.",
    "OK,  if you think a little bit abstractly on this, your function can be any classifier.",
    "in the 90s or 80s, people have developed ,  many methods.",
    "One of the most popular methods in the 90s rather was support vector machine and became  popular that people even didn't care about neural networks because neural networks was not making progress for  many years and they were making a lot of progress with support vector machines.",
    "And one of the reason is because it's convex.",
    "That means you can  get the true parameter of your model and then you can prove it.",
    "And if you are mathematically inclined, you  something that you have an exact proof and  how your model will behave.",
    "support vector was very popular, but also it has limitation that it can only work very  on very basic tasks  test classification.",
    "And that's one of the reasons why we have this  packages  SQL.",
    "Another popular technique is naive base.",
    "And for  many years it was very difficult to beat this simple baseline of naive base.",
    "But nowadays  you can fairly beat them easily.",
    "But I will argue that even if you build a classifier for a very basic task, you should still have some of these baselines.",
    "you don't need to build an LSTM or a transform architecture to have a 90% accuracy, while if you just train naive base you can already achieve that score.",
    "Here's a very simple task.",
    ", most test classification data set  sentiment or spam detection.",
    "If  about the Oron is it Eron data set, it's a very popular spam detection data set.",
    "You can even for using these basic methods, you can already achieve close to 98% out of 100.",
    "And then this task was not very trivial.",
    "I would encourage you to also try this out.",
    "Although  you don't need it  much these days, but it's good to try out.",
    "OK,  for naive base,  I said, when you're learning a model, you need to add  a parameter teeter.",
    "Teeter can be all the weights you want to learn.",
    "Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression.",
    "if you have a very, very simple logistic regression classifier, you just need to learn things  what is the bias of the model and things  that.",
    "even for this  basic model, you still need an, you still need a set of parameters.",
    "the idea of training is that you want to select the hyperparameter that minimize the error on your training set or maximize the likelihood on your training data.",
    "You maximize the likelihood, but you minimize the error.",
    "likelihood is  you're learning a probability, what is the best probability that can fit this data?",
    "that's the idea of likelihood and the probability, the higher the better, the closer you are to one.",
    "And for the error, , the lowest you can get is 0.  if you continue to minimize your error, then you can get a very good model.",
    "OK, do we have questions?",
    "OK, yes, Theta is a parameter.",
    "OK.  everyone is familiar with Bayes rule.",
    "OK,  I believe the first equation here you can do the math the probability of Y given X&X.",
    "Here you have to look at it as a vector or  at minimum at the vector because X can be the set of features.",
    "If you are trying to do  a spam classification, every feature can be  the types.",
    "If you lemmatize then different words,  let's pick an English word, use, and then you can have different variants used using and all that will be converted to words to the root word.",
    "And then you have use and then you have different features or different words that can be categorized as your features in other tasks.",
    "I'm going to give an example that is more concrete, but I want you to look at X as not just a single variable, but  a vector of the different features you want to use to learn your probability of Y given X.",
    "And then if you do the math,  the probability of a joint of a conditional probability would be probability of the joint divided by the one that is conditioned on.",
    "this is a very simple Bayes rule, and if you don't know it, just master it.",
    "It's as simple as that.",
    "Probability of Y given X is probability of XY, the value of probability of X.",
    "Then you can rearrange this using the same idea and then you can  say this is probability of Y multiplied by probability of yx given Y divided by probability of X.",
    "I can use the board if it's not clear,  it's just a way of rewriting the same thing.",
    "Let's start with the drive probability of a.",
    "This can also be written as this,  if you just  replace this with XY,  I believe you'll be able to see it.",
    "here we have  see this as the.",
    "Rule you need to follow.",
    "can you all see it?",
    "I don't know if yes.",
    "see those two things as the rules you need to follow.",
    "Probability of AB equals probability of a, divide given B, probability of B, and then if you do the math, probability of A given B will  be probability of the joint AB divided by probability of B.",
    "And this is the exact thing we did here.",
    "we just changed the variable.",
    "probability of Y given X equals probability of XY, probability of X.",
    "Yes, they are exactly the same.",
    "And that's the same idea we used in  rearranging it here,  that you  have  saying probability of XY will not be written as probability of Y, probability of X given Y and then you still have probability of X below.",
    "Probability of X is estimating the entire probability over all the set of features.",
    "But for the naive base, typically we don't estimate this probability.",
    "The reason is because this is constant when you are trying to pick what is the best, what is the best Y given X. I'm going to explain a bit further.",
    "naive base can be seen as a simple generating model and I'm going to show you the data generating process in a minute.",
    "The idea there are two assumptions, but the most important assumption is this independence assumption.",
    "for each sample, that means for each sample of Y you generate a vector X by generating each feature independently conditioned on Y.  that means every feature of X is independent.",
    "that means if you have X to be 5 different features, they are not dependent on each other.",
    "The way it's been generated is  this.",
    "X is a vector, you have X1X2 to SK and then a probability of XY is  a probability of Y and the product of probability X given Y.",
    "And you are able to do this because they are independent.",
    "this is the original formulation for.",
    "This is a set of variables,  X1 to X5.",
    "this time we written as probability of X1X 2X3,  to XI given I.  the independent assumption says if two random variables are independent, probability of AB equals to probability of A. Probability.",
    "This is why you say.",
    "This is what it means if you say A&B are independent given C and you assume conditional independence.",
    "this would be probability of a given , given C multiplied by probability of B given C and then.",
    "If you do the math .",
    "This would be probability of Y, probability of X1 given Y, probability of X2 given Y, and  on the probability of XI given Y.",
    ", OK.  and this is what we call you assume independence between the variables.",
    "then  you have a product between probability of XI given Y and it's a very strong assumption which doesn't hold everything is that you assume all the features are independent of the output.",
    "Who can give me an example of where this will not hold this independence assumption time series and you appreciate?",
    ", because it depends on the previous thing.",
    "if you assume this independence assumption, it will fail for some applications and that's why people decide to work on better algorithms for other data sets.",
    "here for the naive based model parameters, the parameters to the model Theta consist of what we call the prior class, which is probability of Y.",
    "We call this prior class because although we want to find the probability of Y given X, we make an assumption that we are generating this data as a generative model.",
    "That first you have less as you want to train a data, , you want to construct a data.",
    "The way it's been constructed is that you have the label and then you provide an example.",
    "it's  you want to create an example of a spam e-mail and then you say, OK, for spam give me an example of a spam e-mail or for no spam, give me an example of no spam.",
    "This is the generation process.",
    "And for the spam e-mail that has been generated, you can  say each of the words which are features are independent.",
    "that means every word in it can  lead you to predict that this is spam.",
    "This is the idea because you are saying that they are independent of each other.",
    "that's the data generation process.",
    "And because you start with Y, we call it the prior class distribution.",
    "parameters of each feature's distribution are conditioned on the class.",
    "And this is the probability of each feature XI given the class.",
    "if you have  a discrete data, we assume that a distribution of P of Y&P XI are given Y are categorical distribution.",
    "This is a very simple assumption.",
    "for categorical distribution, an example, a very good example of a categorical distribution, if you still remember your stats probability theory is Bernoulli distribution.",
    "The idea of probability is that  that the probability must sum up to 1.  for a categorical random variable, it follows this distribution if it can take one of the K outcome each with a certain probability.",
    "if you have, if you are flipping a coin, this is a very good example of a Bernoulli distribution.",
    "It's called Bernoulli because you have just two outcomes and then you perform this experiment once.",
    "If you perform the experiment multiple times, then you have  a binomial distribution.",
    "But typically you can also assume that this categorical distribution is a multinomial distribution because you don't have two outcomes, you can have more than two outcomes.",
    "for natural language processing our outcomes, we assume that we have very, very large features, which is the size of our vocabulary and the size of our vocabulary.",
    "For some applications it could be  30,000, for some we could  100,000.",
    "It's  all the possible combination of words that you have in a language.",
    "it's  you want, if you pick an English Dictionary, whether the one from Oxford or Cambridge, all the words there is  this is  a dictionary.",
    "And this will  signify all the features that you need to estimate the probability for.",
    "OK, how do we train a naive based classifier?",
    "that we have to compute the likelihood.",
    "what is the likelihood on fitting?",
    "The idea of likelihood is that you want to fit on all your training data.",
    "And that's why we are computing this sum.",
    "this is a product, but if you put log it to be a sum, you are computing this product over your entire data set.",
    "For every X&Y you pick from your data, you want to estimate what is the probability of the joint probability.",
    "If you assume this independent assumption for each of your data, then you are going to have the product over all the data.",
    "Multiply the products you have on the individual which also consists of the products across all the features.",
    "Do you have a question?",
    "if you reuse this formulation on the board,  you will be able to do the same.",
    "It's just read it multiple times.",
    "OK, for a categorical distribution, if you really want to train your naive base classifier, you want to say what's the probability of AY as taking one of these values?",
    "what's the probability that Y takes a value of spam or Y taking the value of no spam for this  binary classification task and the same thing.",
    "You are going to do the same thing for this discrete probability, which is what's the probability of X taking the value of each of the features given the value of the Y you want to predict?",
    "Yes, OK. ; Is just saying that you need, you need to find this parameter of Theta to estimate this joint probability.",
    "If you can find this Theta, you can estimate the joint probability of PX, Y and that's why it's not together because if it's together it means something else.",
    "And for the training,  that for every after you have trained the model, you compute the likelihood over the entire data.",
    "You can  predict a new.",
    "For a new document you want to classify or for a new sentence you want to classify, you just reuse the base rule probability of Y given X equals the probability of the joints divided by P of X.",
    "And then you assume that every Y we pick a value whether spam or no spam, which is our running example.",
    "And then you also have different values for the features and then you marginalized over.",
    "to calculate P of X, the normal thing you are going to do is to marginalized over all the random variables.",
    "There's another rule that I have not written on the board to marginalized is you marginalized over the joint probability.",
    "that's the how to marginalized.",
    "That's a very simple example over the joints, OK,  and that's how you apply the base rule.",
    "Remember all the features are independent and I'm going to take you through a running example.",
    "this helps to clarify things with an example.",
    "OK. OK, this is an example, , probability of PX1, .",
    "if you have if Y can take 2 values.",
    "Of XY equals to 0 plus probability of XY.",
    "OK, I know it's a bit a lot of content but this is an exercise.",
    "I want to see if you can attempt it.",
    "Then I will show you the my own solution.",
    "I will give you  5 minutes table of whether a student will get a or not based on their habits.",
    "You have a nominal data and a Bernoulli distribution.",
    "Bernoulli distribution I told you has two values.",
    "Is that a yes or no?",
    "Is that a yes or no?",
    "you have if a student reviews notes, does assignments and asks questions and this is the grade,  what is the probability that this student gets an A?",
    "If and that's the last one, it doesn't review notes.",
    "He has not performed any assignments but always ask questions.",
    "Using the knowledge of Bayes rule, can you compute what would be the probability of Y equals grade A or not A given these three features which is review notes, does assignment and ask questions.",
    "Do you have a question on the task?",
    "that means you have to compute.",
    "you have to compute what you have below.",
    "What's probability of Y given X, which is the product of probability of Y and the product of the conditional.",
    "What will be the great for this student?",
    "You can get a probability of X if you use the marginal, but you can ignore it.",
    "It doesn't affect your results because it's constant.",
    "probability of X is the marginal distribution.",
    "you just have to sum over the joints to get a probability of X is a vector of the features.",
    "in that case you have to compute the joint probability of every X. , OK, I get what you mean.",
    "typically for naive base, it's not that you cannot estimate it, but typically you don't need to.",
    "Because you need to compute this over every single features and the calculation for every feature is different, ?",
    ", but I can assure you that for this task, even if you don't compute it, you will still get your answer.",
    "I don't know if anybody was able to solve it.",
    "What the answer is the grade.",
    "Is the a or not a?",
    "OK, OK. How did you arrive at not a?",
    "the two probabilities, if you can give me the probability of a given X and probability of not a given X, what are your probabilities?",
    "OK, all , I will give you an int of the answer.",
    "that for you have to compute what is the probability of Y, , what's the probability of grade of y = a given?",
    "And then you compute what's the probability of y = A given those assignments, probability of a given as question.",
    "the internal answer is this.",
    "And if you apply this rule,  you want to compare these two probabilities, ?",
    "Probability of y = A and probability of Y equals not a given X, ?",
    "This is what you want to compute.",
    "Then  that if you do the math, this probability of y = A equals probability of X, which is a vector given Y equals a.  that if you do the math, , what is the probability of what's probability of review notes?",
    "What's the probability of review notes given grade?",
    "A. OK, let me start from the basic.",
    "What's the probability that Y is equals to A and this 3 / 5, ?",
    "And what's the probability of not a 2 / 5, ?",
    "what's the probability of review notes equals yes, given grade equals a what, 2 / 3?",
    "And then what's probability of DOS assignment?",
    "What's the probability of DOS assignment given grade equals A2 over 3?",
    "And what's the probability of ask question given probability of y = a one over 3.  we're able to compute the probability.",
    "Because once you're supposed to compute is not yes, yes, yes, all the time.",
    "What you're supposed to compute is probability of.",
    "It doesn't review notes and probability of no assignment and probability of ask question, .",
    "if you say, what is the probability of?",
    "if you say, and this is if you compute these individual probabilities, , , 2 / 3 should be 1 / 3, OK, .",
    ", this should be 1 / 3, and then the other one should be 1 / 3, and the last one would be 1 / 3.  if you say, , OK, that's correct.",
    "you're going to have 1 / 45.  what's the probability?",
    "Dozen review notes, Probability of dozen review notes is if you pick the first tree, the first tree lines, Yes, appears two times, .",
    "And if it's no, it will be one over the probability,  the number of times you have grade and that's 1 / 3, ?",
    ", the one that has the highest probability between probability of.",
    "Equals and probability of y = a or probability and probability of Y equals not a.",
    "The one that has the highest probability will be your answer.",
    "Yes, this is my approach.",
    "P of X of the vector.",
    "my calculation is even wrong.",
    "here you have, I don't think my calculation OK.   one, I have two, which should be 1 / 3, one over three, 1 / 3 and then you have 1 / 45 and then you compare to the second one.",
    "you have OK.  the way you compute it is what's the probability of N in the first column?",
    "That means it doesn't review notes.",
    "And if you compare all the ones that has grade of A  you have 1 / 3 and the other one, what's probability of no assignment?",
    "You only have one N out of the first 3 where you have a grade of A and that's another 1 / 3.",
    "And the last one asks question.",
    "You have Y, which is yes, ask question out of three possibilities.",
    "And that's 1 / 3.",
    "And that's how I got the first one.",
    "in order to show, it should be 3 / 5 * 1 / 3 * 1 / 3 * 1 / 3, and then you're supposed to have 1 / 45.",
    "Let me just update the slide.",
    "OK, I'm not able to edit on this laptop, but ,  1 / 45.",
    "And then the other part, which is probability of Y equals not A, you're going to do the same.",
    "The probability of not a is 2 / 5, ?",
    "And the probability of dozens review notes will be 1 / 2 because you have two appearances of not A and that's 1 / 2, the second one 1 / 2, and the third one is 1 / 2.",
    "And then if you compare these two, which one is greater 1 / 45 and 1 / 20?",
    "1 / 20 is greater than 1 / 45.",
    "And that's how the answer is not a.",
    "No, no, it's not Theta, but that's why I said ignore probability of X, because if you put it in inequality, you can just remove them, ?",
    "If you're comparing, you can remove them.",
    "Which one are you talking about?",
    "The notation seems correct to me.",
    "The only mistake here is just where I have 2 / 3.",
    "It should be 1 / 3. .",
    "that if you already have this  this table of results, you can compute all the different probabilities and at the test, at the test time, you can estimate the probability for the new document and then decide what would be the category.",
    "OK. , No, it's not going to sum to one because you have not estimated the probability of X, ?",
    "Yes, yes, if you estimate the probability of X.",
    "What I'm just saying is that even without computing the probability of X, you can already decide which one is bigger.",
    "All ,  the answer is not a.",
    "And I hope this gives you an idea of all the maps we wrote on the book.",
    "OK,  quickly I will rush through the remaining of the slide before we go out of time.",
    "there's a simple distinction between type and token.",
    "the idea of type is that the identity of the word, which is the count of unique words and token is every single instance.",
    "for an example is below, you have YO appearing 2 times, ?",
    "if you have completed the probability of that token, you're also going to say what's the probability of spam, probability of YO given spam.",
    "And then you're still going to repeat it because this is computed over the probability of token, not the probability of types when we are doing this  calculation.",
    "please take note of that.",
    "There's an important distinction between that.",
    "another distinction is generative versus discriminative task.",
    "At the  class, we're going to try to do a discriminative task instead of a generative task.",
    "for a generative model, we learn a distribution for all of the random variables involved, which is a joint distribution probability of X, Y.",
    "And for a discriminative model, the only thing you care about is the parameter.",
    "You learn the parameter Theta.",
    "If you can get the Theta, you already solved the problem.",
    "and that is the idea for most algorithm ML algorithms that have been developed, they just focus on how do you estimate the Theta such a way that the loss is low.",
    "And once you can do this, who cares about having the estimation of the  probability, because sometimes the probability of XI just showed you.",
    "Sometimes is not very easy to compute for some application.",
    "But early days of machine learning, most researchers are split into two.",
    "Some are only working on discriminative tasks because they just want to have the best model.",
    "Some continue to focus on how do you estimate the  probability.",
    "Because if you can get what is the  prior distribution, what is the  distribution that you can use to fit your data, then you can get a very accurate result.",
    "But nowadays  people that focus on discriminative seems to win.",
    "But of course, there are two ways of thinking about a problem.",
    "for the task of logistic regression.",
    "the idea again, although it has regression in the name, it is not a regression technique, it is a classification technique.",
    "linear regression has continuous values, but for you for you to make it a probability distribution, you have to apply a simple function that will convert the output into a probability distribution  that everything can sum to 1.  the probability mass function can be written as this.",
    "And what when you want to try to solve which is also called logic?",
    "When you want to try to solve a problem using logistic regression?",
    "OK,  this is how it is if you convert it to a probability  that  everything can go between zero and one.",
    "I'm going to show you how you will compute what is the likelihood in logistic regression in a minute.",
    "Here I want to clarify that features can be anything.",
    "in the example I showed you here, we use a very simple example where the features are  review notes, dose assignment, and ask question.",
    "It's a big question on how do you get your features.",
    "We typically just default to what we have, which is the words we have and then use them as features because that's all we have.",
    "And then we can do counting.",
    "How many times does this word appear?",
    "How many times does that word appear?",
    "if the word yo that we say yes signifies is more correlated with spam, if it appears  many times in your document, it should be a good feature.",
    "we typically just default to this.",
    "you can design your feature.",
    ", you can say for different tasks  nameless recognition, you can say capitalization is very important, can be an example of a feature, the length of the document can be an example of a feature, the number of stop walls can be an example of a feature, and anything can be a feature.",
    "OK,  in practice the features depend on both the documents and the proposed class.",
    "Does the document contain the word money?",
    "if money is always associated with spam, then money will be a good feature for detecting spam.",
    "for the parameters of the logistic regression here, the Theta will just be once you fit the model, you need to compute what is the parameters.",
    ", it's very simple, all the values A1A2A3 to an and of course and the intercept.",
    "And our idea is that we want to compute what is the likelihood, what is the condition and likelihood on our training campus, which is very similar to what we did in the naive base.",
    "You want to compute what's the probability of Y given X but given this parameter of Theta because  you want to find those parameters and those parameters in the logistic regression are A1A2A3A to an.",
    "And if you had a log to this, you can have what is called the log likelihood.",
    "And log likelihood is very important in NLP because it's what you use to compute things  publicity of a language model and  many things  that.",
    "and the idea here is that if you say if you have log of the products of two probability, this will give you the sum of the probabilities.",
    "I hope this is clear.",
    "And once you have done that, you can  optimize this using gradient descent.",
    "I won't go too much into this because this is more of a machine learning concept, but if you do the math,  you'll be able to arrange this very quickly with this idea of the products.",
    "The log of the product of probability will be the sum of the log of the probabilities.",
    "I hope I'm saying it .",
    "OK,  another technique that is very proper.",
    "Do you have question on logistic regression?",
    "I was, I was a bit in a hurry because of the time, but do you have a question I can try to clarify very quickly?",
    "OK, the last thing I want to talk about is support vector machine, which is a very, very important algorithm.",
    "And  that you want to project your data in a high dimensional space and then you want to draw a line that separates the different classes.",
    "you have the blue dots and then you have the red dots.",
    "Can we, if we project it in a high dimensional space, are we able to draw a line that clearly separate them?",
    "And this line can be a very hard margin or a soft margin.",
    "how can we draw this line that separates the two examples?",
    "And then here we can have a hard margin, which is the red 1 or a stuffed margin, which is  the green one, because you can have some dots that will be very, very close to that.",
    "And this is the idea of, this is the major idea of the SVM.",
    "our SVM is generative for a discriminative model.",
    "There's no assumption of what is the probability distribution on this.",
    "All you care about is just solving the problem.",
    "What is the best hyper parameter?",
    "What is the best parameter of the model that can help you to separate your classes in a high dimensional space?",
    "how do you decide naive based logistic regression?",
    "SVM can work  in different tasks and settings, usually giving very little training data.",
    "Naive base is your good.",
    "It's always a good bet and you can  do the math.",
    "It's very simple to do this in practice.",
    "What you have to do is to try different algorithms.",
    "You can try logistic regression, SVM, boosting and  many techniques.",
    "OK,  the last one here is the perceptron that leads us to it's a very simple extension of the logistic regression.",
    "Here we generalize the A1A2A3 that I told you into what is called the weight matrix.",
    "And  we focus on what is the  weight matrix and the  intercept that will give me that can be used to fit my data.",
    "And here you can also stack different perceptions together.",
    "And from this is where we  move to what is called neural network.",
    "Because neural network is more  a generalization of this concept for artificial neural network, there are  many people that believe that it's inspired by the human brain.",
    "people do not believe in this.",
    "you can completely ignore everything I say here.",
    "Some people believe that there's some interaction about you have the neuron and the dendrites and the ASEAN in the brain.",
    "The way they interact also  model how artificial neural network works.",
    "And this is  one of the most successful algorithm that we have currently in machine learning.",
    "One, we have to we have some advantages of neural network.",
    "Number one, they can learn very complex functions.",
    "There is only a theoretical proof that says that they can learn any function.",
    "They can learn any function.",
    "And there are many different network structures that you can  come up with for neural networks.",
    "as you go on in your career, you can come up with another fancy architecture that will become famous.",
    "Better than Transformer are also leveraging this idea of neural networks.",
    "Given enough training data, they tend to perform very .",
    "even typically your large language models  ChatGPT are still based on neural networks.",
    "The disadvantage of this is that training can take a very long time.",
    "Sometimes you can train a machine translation model for a month.",
    "For current language models, people are trading for over six months, over a year, and if something is wrong, we have to start that over again.",
    "It's really expensive and often requires a lot of data.",
    "OK. And of course, we have different other kinds of classification algorithms that will not be covered, but you can read them up  K nearest neighbor decision trees.",
    "in the  class I can ask you about what  about K nearest neighbor decision trees, random forests, and  on."
]