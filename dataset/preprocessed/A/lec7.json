[
    "OK. All .",
    ", everyone.",
    "All .",
    "OK, I think we can start.",
    "I think lecture 7.",
    "Can you, can you all hear me?",
    "OK,  Lecture 7.  today we are going to be talking about part of speech tagging.",
    "Part of speech tagging.",
    "Early in the 90s, this used to be very important task.",
    ", it's hard to hear .",
    "Is still better, Slightly better.",
    "OK, , how about ?",
    "Is he OK?",
    "Path of speech tagging.",
    "Today we'll be talking about part of speech tagging.",
    "Is it better?",
    "All , Hopefully you can manage.",
    "I was saying, it used to be a very important task in the 90s because it was very essential for many applications from machine translation to speech recognition and  many tasks, and also for named entity recognition.",
    "And it's also a popular feature.",
    ", if you need additional features, you try to get part of speech tags of all the words and then you use this as additional features for order NLP task from word sense disambiguation to named entity recognition to order information extraction task.",
    "The first, the first announcement is that October 2nd, the lecture is cancelled.",
    "We have an NLP workshop at Miller.",
    "how many people don't know Miller?",
    "Miller is the Montreal AI institute.",
    "I think you should just Google Miller Quebec.",
    "it's an institute that focuses on artificial intelligence.",
    "Edit by Professor Yoshua Bengio.",
    "And  we are having an NLP workshop, NLP in the era of generative AI.",
    "As  from this lecture, we have not really talked about very trendy topics.",
    "if you want to know about trendy topics in AI, in NLP, I want to encourage you to attend this.",
    "it will be from October one to third.",
    "Unfortunately, there are  many people that are interested in this workshop,  you can no longer attend in person, but you can attend online if you register .",
    ",  far in the lecture we have been talking about how to do test classification, talking about different algorithms  naive base, support vector machine, logistic regression and neural networks.",
    "today  also last time I think last week we discussed about language modeling.",
    "We talk about N gram language modeling.",
    "We talk about smoothing techniques  that you can address words.",
    "Why do you smooth?",
    "Why do you need to smooth?",
    "Yes, for unknown words.",
    "Also to prevent having a probability of 0.  ruining your computation for several tasks.",
    "today we'll be making a series of predictions from a sequence.",
    "giving a sequence a sequence of words.",
    "How can you predict what is the tag for each of the words?",
    "OK,  this is the sequence labeling.",
    "There are other tasks that are sequence labeling,  name density recognition is also sequence labeling where you try to identify what is a personal name, what is an organization, what is a location in a text.",
    "today we'll be focusing on part of speech in English because I believe if you're attending this class, you understand this language.",
    "And then we have part of speech tagging as a sequence labeling problem.",
    "Then we're going to examine some things coming from  motivated,  from probability theory.",
    "It's  a continuation of naive base.",
    "In the naive base, we make an assumption which is the independence assumption or to say concretely we made a conditional independence assumption.",
    "And since this technique has been very successful, it has been extended to other techniques  to other tasks  part of speech tagging where you can also make the assumption that the part of speech tag, you can generate another part of speech based on that.",
    "And also you can generate a word based on that very similar to the naive based assumption.",
    "the only difference  is that you have a random variable in the Markov chain that can generate either A tag or a word.",
    "And this is the idea of hidden Markov models, which used to be very popular until deep learning overtook it.",
    "All .",
    "this would be a little bit different from what  from maybe primary school if you are not familiar with part of speech.",
    "we talk about the different part of speech in English language.",
    "The common ones  nouns  restaurant, dinner.",
    "We have verbs  find it is.",
    "We have adjectives  good modifier of a noun.",
    "We have prepositions  very, very common stop words  in of up above.",
    "And  prepositions are also the most common, probably one of the most common in your corpus and determinants.",
    "And also you have adverbs  quickly,  vary and  many.",
    "what is part of speech?",
    "it's a way of defining a syntactic category that tells you about the grammatical properties of the world.",
    "if you have the dash was delicious.",
    "you would know that  typically a noun, we fit this .",
    "English language is following the structure of SVO.",
    "subject, verb and object.",
    "in this case, you need a subject which is typically  a determiner and a noun.",
    "And then you have a verb which is worse here.",
    "And then you have the objects delicious.",
    "that maybe a food was delicious would be appropriate.",
    "The food was delicious.",
    "Here we were appropriate, which is a noun.",
    "And here you have the hamburger is dash than that one.",
    "this is more  an adjective to compare to compare 1 hamburger to the other.",
    "And there are sometimes that some you can have the same structure that one may be more grammatical and the other is not grammatical.",
    "you have the cat's head and then you have the cat enjoyed, which is less grammatical compared to the first one.",
    "OK.  , it's very important to know that this might be slightly different from what  from your grad grade school or high school.",
    "where we know that.",
    "OK, what is a noun is  a name of a person, Anima.",
    ".",
    "And things  that place or things that is  a noun.",
    "This is how it was defined for me when I was in primary school.",
    "And then you have  verbs, which is  action word.",
    "But  we have a broader category.",
    "you could have part of speech tag covering  37 categories including punctuation.",
    "And then we have order schemes that  reduce this a bit to  18 called  universal dependencies.",
    "that because you have different parts of speech for different languages.",
    "And at some point, people came together to say, let's unify this, which also has some disadvantage, which we're going to talk about that later, OK?",
    "And of course, nouns can also be action or events  wedding,  construction, there are events and then it's also noun.",
    "And then verbs may not necessarily be our actions, for example, to be developed to be OK.  one of the most popular tree banks that have been useful for many, many years until recently when universal dependency caught up with it, is the Penn Treebank tag set, which , by Penn,  where it's coming from.",
    "we have all these different symbols and at some point it looks very, very, very confusing.",
    "You have CC to be coordinating conjunction, CD to be cardinal number, determiner.",
    "But there are some that are more common than the other.",
    "And then you could also group them into different classes.",
    "Some are open classes of path of speech and some are  closed tags, but many of them are  intuitive.",
    "if you remember the very common ones  determiner, cardinal number are coordinating conjunction.",
    "We have preposition adjectives, adjectives.",
    "That's a very interesting symbol, JJ.",
    "And then we have also we have noun and then you could have plural of nouns, which is NN versus NNS and you have proper noun NNP.",
    "And then you have Internet PS.",
    "And then you have other things  pronoun that is also very popular.",
    "We have processing pronoun, we have symbol, and then you have interjection, which is interesting.",
    ", and then you have verb.",
    "You also have, you can have verb in past tense or in past participle.",
    ", when you start adding these  rules,  that this will not transfer very  If you go to some other languages, Some languages do not have the concept of past tense.",
    "the language I speak, Yoruba, doesn't have the concept of past tense.",
    "And then some languages also do not have the concept of plural.",
    "And then also you find that this pantry bank may not work across different languages.",
    ".",
    "Punctuation should be another one which is  OK, it's obvious but punctuation is the last category.",
    "OK,  we have other part of speech  you can distinguish between modal verbs and auxiliary verbs  the car and wheel.",
    "And you can also have different  conjunctions.",
    "You can have main conjunctions  hand or bolt yet and you can also have something  subordinate conjunction that happens sometimes  after that can connect.",
    "Two clauses together for example 2 phrases together.",
    "We also have  particles that sometimes you have particles that can be parts of a verb and depending on the tag sets.",
    "Also they are standard definition of what each part of speech tag is.",
    "OK, And  first I want to distinguish between open class and closed classes.",
    "Closed classes are  part of speech that is very difficult to add new words to them while open, they're  open.",
    "You can always add new words to it.",
    "For example, before Google, there was nothing  Google.",
    "But  you could use Google as a verb.",
    "And then you can also say Photoshop.",
    "Instead of saying I want to edit this picture, you can say I want to Photoshop it as another verb.",
    "And then you have nouns  Twitter, Facebook, and you have  adjectives that are not very common.",
    "But there's this website where you could see, you could browse to see our new words that have been added with their part of speech.",
    "All ,  for the cluster R class we have, these are  ingrained in the language,  they are very difficult to expand.",
    "For example, pronounce, IE.",
    "The only thing you can have is that maybe nowadays people will probably use one more than the other.",
    "And then you have determinants, We have quantifiers, we have conjunctions that are very difficult to expand.",
    "closed classes tend to convey grammatical information and they tend to be more functional words or stop words.",
    "Do you have questions?",
    "I believe this is very good.",
    "Yes, yes.",
    ", if you don't put them in a sentence, you cannot form good sentences.",
    "They are important, but maybe for future engineering, if you want to do test classification, maybe they are not  important.",
    "But in some other good question, in some other tasks they may be very important even for test classification.",
    "if you are trying to distinguish between spam and no spam, maybe function ones are not very important.",
    "But if you want to distinguish who is the author of this article, you can have an author that uses more function words than the other, and then maybe they will form important features to  distinguish one author versus the other.",
    "they are not completely useless depending on the task, ?",
    "OK, Yes, OK, ,  I want to tell you about this universal dependency task set, because nowadays people try to work on this instead of the Penn Tree Bank.",
    "Penn Tree Bank is very, very, very old.",
    "it's in the 90s.",
    "At some point, we have different tag sets.",
    "Stanford has A tag set, which is  a Stanford dependency tag set.",
    "And then we have some from Czech Republic, we have some from Japan.",
    "And then there's a lot of confusion of which is the  task set to use, especially if you want to expand to new languages.",
    "It's not very clear if you should use the French style, Japanese style, or the US style or the British style, anyone.",
    "there's a project started.",
    "They started a project by a couple of researchers from Stanford and many universities and many, many universities, and they came together to start this universal dependency project.",
    "And it's a very ongoing project.",
    "it's been ongoing for more than five years.",
    "this is driven by linguist and competition linguist.",
    "And then they  have a more simplified tag set.",
    "we have  the open classes, the closed classes.",
    "And the interesting thing with this universal dependency tag set is that it's fairly easy to extend to many different languages.",
    "Of course, there are some issues.",
    "two years ago we tried to expand universal dependencies to a bunch of African languages, and we faced  many,  many challenges.",
    "I'm not saying that there are no problems, but this is probably a bit more scalable than you have the Penn Tree Bank.",
    "The Penn Tree Bank has  distinction between their past tense and some languages don't have this .",
    "we have a distinction between singular noun, plural nouns.",
    "Some languages don't have this, but this one just has  noun which is noun.",
    "And then you have proper noun, you have verb and also you have the open classes and the closed classes, very similar to what we have in the pantry bank.",
    "in the open class we have the adjective, erverbs, interjection, noun, proper noun and verb.",
    "you can easily extend this based and add new words to this.",
    "For closed classes they are more very similar to the grammar and to the way the language is being used.",
    "And also you have other classes that are very difficult to put them into either open or closed classes  your punctuation.",
    "You asked about symbols and XX is just , we don't know what should be the  category for this, but anything that doesn't belong into all this, we just call it X.  most of the time when you are doing this  annotation, X is just maybe there is a grammatical error in the word and then it doesn't make any sense.",
    "And then there are some jargon or maybe there's some strange hashtag on Twitter that you want to update and it doesn't fit into anything.",
    "this will be X or some links URL will be X. OK, yes, OK. For corpus differences, depending on the corpus, you could have more fine grain tags than another corpus.",
    "For example, PTB, which is the pantry bank doesn't distinguish between intransitive verbs and transitive verbs.",
    "And then you have something  listened versus heard.",
    "And then if you look at Brown corpus has  87 tags for this, why PTB only has 45?",
    "also another thing is about language differences, which is very, very, very important.",
    "And this is one of the motivation for the universal dependency project.",
    "For example, in Japanese, which I don't speak, there's no great decision between nouns and pronouns.",
    "Pronouns are also open class rather than closed class in English.",
    "this is one major difference between, for example, path of speech tagging in Japanese versus in English and wallow.",
    "For example, verbs are not conjugated.",
    "you don't have things  for person and tense.",
    "having  different annotation for past tense and present tense and continuous perfect is very, very different from that of English.",
    "And then in this language called celicial languages in Pacific Northwest, also the distinction between nouns and verbs.",
    "in most languages you'll be able to easily distinguish between nouns and verbs, but in some languages this is also very difficult.",
    "OK, I have an exercise for you.",
    "Can you give a cross POS tag labels to the following passage?",
    "I think the difficulty here is you are seeing these tags for the first time and then makes it probably difficult.",
    "this is I don't know which one issue.",
    "?",
    "Should I write a sentence on the word and then show you the tax sets?",
    ", OK,  I will show you the text sets.",
    "OK, let's use the complicated one first and then we will use the universal dependency.",
    "I think my solution is very similar to this, but maybe I'm wrong.",
    "This is my solution.",
    "Maybe I'm wrong, but  if we take geography as , are you saying JJJJ and N maybe , but I think you get the idea.",
    "another example is  this one sensation in Iceland after evolved in social media.",
    "I think we also have very similar situation  the first example where sensation is  NN, you have you have preposition, they have  a proper noun, then you have another preposition, you have a pronoun, you have a verb, you have a determiner, you have an adjective and then a noun.",
    "And in the last example, you have influencer, which is a noun, and then you have a preposition.",
    "And then you have what?",
    "Proper noun?",
    ", you have a pronoun, adjective, noun, preposition, determiner, noun, and penetration.",
    "let's say we want to convert this to universal dependence .",
    "OK,  let's use this tag set.",
    "What are you going to get for that on the board for what you have on the board using a different task set?",
    "A will still be what will still be  this will be Det, ?",
    "We have two kinds of vets.",
    "This is difficult.",
    "We don't have position, All , OK.  we  show two different task sets,  using Pentry Bank and using universal dependency.",
    "one of the second example that would be what if you want to convert this to universal dependency?",
    ", I have to show you that.",
    "this would be also  and then ad position.",
    "You have proper  ad position.",
    "Then you have  that's after maybe something else.",
    "It may be the subordinate conjunction in universal dependency.",
    "And then you have pronoun, verb, determiner, adjective, and noun.",
    "OK, The last one will be also very similar.",
    "We have noun acquisition.",
    "You have preposition.",
    ".",
    "You have  preposition, pronoun, adjective, noun, apposition, and then you have determiner,  apposition.",
    "OK, I believe it's clear, ?",
    "OK, All ,  let's go to the algorithms.",
    ", I think by  you already know the task we are working on.",
    ", we are working on part of speech, , different task sets that can be used.",
    "for the rest of the lecture we are still going to continue with the pantry bank.",
    "If you are interested in the project universal dependency, you can go to the website.",
    "There is a way you can join the project if you are interested.",
    "And you can do this for different languages, for different data sets or for different domains.",
    "All , , , for path of speech tagging, if we have some data that is labeled with path of speech tagging, what  problem is this?",
    "Yes, , it's supervised and it's classification.",
    "OK.  from the knowledge of language model in lecture, this is also very important for path of speech because in language model.",
    "You have a context and then you want to predict the  word.",
    "The interesting thing is for part of speech, there's also dependency between a Canadian.",
    "For example, there's a lot of dependency between Canadian geography nerd.",
    "And then there's dependency there.",
    "And if you want to predict what will be the  tag, you need to understand what is the tag of the previous word.",
    "Very very similar to laggard model.",
    "the only difference  is that you need to predict both the tag and the  word.",
    "This is the way we model this.",
    "In longer model our concern is just how to predict the  word.",
    "But  we want to predict the  tag.",
    "But the way we will do the modeling is that we want a model that will be able to predict both the  word and the  tag correctly.",
    "OK,  I saw the dash, the team won the match and several cards.",
    "There's some dependence on what would be the  word that will come.",
    "here we have an example here.",
    "And when we are doing part of speech, even if you want to model this, I think in the  few lectures, we're going to discuss things  conditional random field, which was very, very, very popular in the early 2000s.",
    "And if you want to model part of speech, you have to consider what is the previous context and the current word.",
    "You can even consider what is the  word to be able to determine what is the  tag.",
    "having this different word information, you can use it to predict what will be the  tag for the word you are trying to predict.",
    "And this brings us to a framework that can help us to model this, which is called the Markov chain, the Markov chain.",
    "The idea is what is called the Markov process, and the idea is that we can change this probability of predicting the  word and use the Markov process.",
    ", we have States and we have transition probabilities from one state to the other.",
    "And in this case, your state will be the part of speech.",
    "And then you can  say what would be the probability of predicting the  word?",
    "What would be the probability of predicting the  stack?",
    "And we have already seen some  Markov processes because we are trying, for example, in the naive base, we have a very simplistic Markov chain where you have the class, for example, with a spam or no spam, and then you want to predict each word that will be generated from spam or no spam.",
    "It's a very simple example of the Markov process.",
    "in morphology where you are transitioning between phonemes that make up a word.",
    "In language model, when you have transition between words that make up a sentence.",
    "In other words, they are  finite state automata.",
    "this is an example of the markup model where you can have words and then you have interaction between the words and all these interaction, all these arrows in this graph,  they're directed graphs because it depends on the other one.",
    "all these arrows, you also have transition probabilities and these transition probabilities will sum up to one because it's a probability.",
    "this is an example of a bigram model.",
    "What will be  can you give an example of how the trigram will look ?",
    "Yes, , you have  3 errors here.",
    "if you unroll it in time steps because  it's easier to follow.",
    "If you unroll it, then you have the car of hands run.",
    "it's very easy to see that for you to predict the word of car, it depends on the previous word, which is a determiner, and then to predict the  one depends on the previous one.",
    "And each of these arrows we are going to have what is called the transition probabilities.",
    "and based on that we can have a concept of what is called the eating variables very similar to what we have in the naive base.",
    "we can consider part of speech to be predicted as eating variables because you don't see them during test time.",
    "For example.",
    "We have other phenomena that  are very similar to that of parts of speech you have  encrypted symbols and output of Ed messages.",
    "you can see, you can look at parts of speech as something hidden that you don't know, but you have to model.",
    "And this is where the idea of hidden Markov model is coming from.",
    "Something is hidden, you model it as an Ed variable, but you still have to model the hidden variable because you don't know it at this time.",
    "And then you have an assumption for that hidden variable, You have a probability distribution assumption for that hidden variable.",
    "another example is gains are output of functional relationships, whether it's the output of hidden climate conditions that we don't know and stock prices are output of market conditions, Yes.",
    "I said, if we want to model part of speech, then we have the part of speech as the part of speech will  be the states.",
    "Those states can be anything, any word.",
    "verb can be any word, can be, be or have or do.",
    "And then you have also the different probabilities, different transition probabilities, going from one node to the other of going from one state to the other.",
    "And then we have a modal transition between parts of speech and output for a part of speech to how do I explain this for a part of speech?",
    "Because it's  an hidden variable that you're using to model words.",
    "something is hidden and it's  you generate a word.",
    "See it as more  a generating model.",
    "given a part of speech, you want to generate a word.",
    "This is the way we model it.",
    "And there are different words that can be generated given different probabilities.",
    "OK, let me connect this to language model.",
    "In language model, you want to know what is the probability of predicting the  word, ?",
    "And here given a part of speech, what is the probability of generating this world?",
    "we are modeling both parts of speech and words together that A tag can generate the  tag and that same tag can generate a new world.",
    "And this is the way the modeling is being done.",
    "And then you have transition probabilities from 1 tag to the other and then you have another probability to emit the world.",
    "Is it clear?",
    "Yes.",
    "hidden variable.",
    "The idea is that it's an assumption that because you don't know this variable at Test time.",
    "let's say you want to say what is the path of speech for this sentence for each word.",
    "You don't know the part of speech that is an hidden variable because you don't know it at this time.",
    "However, when you want to model it, the assumption is at the modeling stage is as if  it and is that this part of speech depends on a previous part of speech in the time step.",
    "when you unroll this out,  this means that the verb depends on another part of speech, and the word that will be generated depends on the part of speech.",
    "the way it's being modeled is that there are many possibilities for this part of speech to generate a word, because that is the class of that word.",
    "given a part of speech, you can generate different words that can fit that part of speech, but you don't know the word.",
    "You can only determine the word if  the transition, the emission probabilities, Yes, But at the modeling stage, this is how it's being modeled.",
    ".",
    "And , you have a question.",
    ", I can, yes.",
    "we want to learn the different probabilities of how possible is it that this part of speech will predict another part of speech, and how possible is it that this part of speech will predict a word given our corpus is expected to predict it because  you have a corpus.",
    "given an annotated corpus, I have estimated all the different probabilities.",
    "Can you predict what would be the part of speech and test time?",
    "Yes, there's a question.",
    ".",
    "Mm.",
    "Yes.",
    "it depends on the corpus how many, what's the probability of having  followed by  given the corpus, .",
    "in this assumption, we assume we have a training data that we can use to estimate all these probabilities, ?",
    ".",
    "the way the trigram will work is that  you say it doesn't depend on just one previous tag, it depends on two previous tags, .",
    "Very, very similar to how we computed probabilities for the language model.",
    ".",
    "if we unroll it, the time steps.",
    "this is the way it's modeled.",
    "And you can see  each part of speech, there's a possibility for them to generate what is the  part of speech.",
    "Given the transition probability, given the probability on that arrow, you can generate that we have to go from the terminal to .",
    "And given an emission probability, you can generate a word.",
    "And this is the way it's being modeled.",
    "And once you have done that, you can repeat the same process.",
    "That noun can  generate another part of speech, which is preposition, and also can generate another word with some probabilities.",
    ", Yes, .",
    "It's possible if you don't have a good probability, if you are not able to estimate good probabilities, , If you could, you are not able to estimate good probabilities.",
    "It's possible to predict the wrong thing.",
    ".",
    "Yes.",
    ", Let me ask.",
    "OK. No, this is just dependent on the tag.",
    "This is  a bigram.",
    ", it's a very simple bigram.",
    ".",
    "You have a question.",
    "OK, , yes, this is a very simplistical diagram.",
    "that means it just depends on the previous tag or the word depends on the tag and then the  tag depends on the previous tag.",
    "OK,  we have a probability.",
    "if you want to say what is the probability of D is the terminal, then car is a noun and of is preposition and is noun with a plural and then ran is a verb that is in the past tense.",
    "OK.  the way it's  you have to 1st have the initial probability, which is the probability of DT multiplied by.",
    "It's  having a unigram probability multiplied, ?",
    "What is the probability that it's going to emit the word D?",
    "And what's the probability that it's going to transition to the noun?",
    "And then you repeat the same thing because this is  a bigram model.",
    "the  tag will be what's the probability that the noun is going to emit a car, which is the  word times?",
    "What's the probability that it's going to transition into the  tag?",
    "Yes, PDT, you are going to estimate it based on your corpus, what I'm going to show you in another slide here, OK. And you repeat the same thing.",
    "you just multiply everything.",
    "Very similar to what we did in the language model.",
    "But  we are  saying that it does not, that the tag, the  tag, would depend on a previous tag and a word would depend on A tag.",
    "And then you multiply the probabilities together.",
    "Very, very similar, yes, They are unaware there's no relationship in this formulation, Yes.",
    "But of course there are ways you can make it depend.",
    "if you have  you expand the context size here and then you multiply the hidden state transition and the observation emissions together.",
    "And of course here we are also still taking the independence assumption.",
    "OK,  there's a way we can generalize everything into what is called a graphical model if we represent each state as a random variable.",
    "the Q here, which is a random variable for every tag, probability of QT equals A tag, and then the observed variable, which would be your words, can also be a random variable and the OT.",
    "you see that we  distinguish between two things, the observed variable, which is  shaded and then the latent variable.",
    "In graphical models, instead of saying hidden variables, we often use what is called a latent variable.",
    "It's  the same thing.",
    "And also the assumption is conditional independence.",
    "there are a lot of algorithms that have been developed for graphical models that are very, very popular, not only for supervised tasks, even for unsupervised tasks  the LDA.",
    "OK,  in any Markov representation, this is what you have.",
    "just to generalize, we have the observed which is the observed random variables O1, O2.",
    "In our case this would be the words.",
    "And then you have the tag set Q1Q2 to Q5.",
    "And how do we estimate the joint probability?",
    "Because we are making an independence assumption, all we have to do is to multiply all the transition probabilities times all the emission probabilities.",
    "you're saying that the probability of Q2 given Q1.",
    "And then if you want to estimate, if you want to predict Q3, it would be the probability of Q3 given Q2 multiplied by the probability of Q4 given Q3 multiplied by probability of Q5 given Q4.",
    "This is  a bigram model and at the same time you multiply it by the emission probabilities.",
    "Probability of observing O1 given Q1 times probability of observing QO 2 given Q2 times probability of observing O3 given Q3 times probability of observing O4 given Q4.",
    "That's probability of observing O5 given Q5, yes.",
    ", you have to also estimate that from your corpus.",
    "Yes, OK. You have to 1st, you have to estimate the what is called  the initial probability.",
    "It's  the unigram.",
    "How many times does this Q1 occur in my entire corpus?",
    ".",
    "How about some words that occur?",
    ", then , your probability will be 0, ?",
    ", your probability will be 0.",
    "But if you apply smoothing, your probability will be non 0 and you can consider with your calculation.",
    ", yes, yes, , yes, .",
    "But it's they are both equivalent, .",
    "The two things you said  I cannot, what's the difference?",
    "Yes, , .",
    "the word depends on that POS, Yes.",
    "But we are not using the context of the words.",
    "We are not using the context of the words in this modeling, in this simplified one.",
    ", this is a very, very simplified one, just  that you get the idea, ?",
    ".",
    "OK.",
    "I know you asked about the initial probability.",
    "the initial probability of Q1, you can estimate it.",
    "for every part of speech that you have, you can estimate what would be the initial probability based on your corpus, .",
    "And then you say what's the probability of a verb, What's the probability of a determiner?",
    "What's the probability of a pronoun in my corpus?",
    ", Yes.",
    "Relative frequencies.",
    ".",
    "In your sentence.",
    "In the sentence.",
    "this is queue one.",
    "What is the probability that this part of speech begins a sentence?",
    "This is queue one, yes.",
    ".",
    "This is queue one because queue one is the beginning.",
    "what is the probability that the terminal starts the sentence?",
    "What's the probability that verb starts the sentence?",
    "This is your initial probabilities for Q1.",
    "once you have these initial probabilities, the  one is that you can  estimate your transition probability.",
    "What's the probability of Q2 given Q1?",
    "What's the probability of Q3 given Q2?",
    "And  on.",
    "And the last one is the emission probability, which depends on the word.",
    "What's the probability of a word given the path of speed tag?",
    "OK,  how do you train  path of speed tagging?",
    "Since you have a label corpus, this is  a supervised task.",
    "In the , the very simplified 1, you can estimate all these probabilities very similar to how we estimate all the probabilities for naive base.",
    "You can estimate the initial probability distribution.",
    "You can estimate the transition probability distribution.",
    "You can also estimate the emission probability.",
    "Do you have a question before we do the exercise?",
    "Yes, yes, , , of the whole thing.",
    "That's the joint probability.",
    "It's  a language model.",
    "it's  probability of all the observed variables and probability of all the observed taxes.",
    "in this case, it's just , all .",
    "Yes.",
    "Probability of the entire corpus, .",
    "Yes, it will be per sequence.",
    ".",
    "Per sentence , , this one, if you want to estimate the same thing, this one should be the entire corpus.",
    "But you can also estimate this per sentence, , because there's some probabilities that you can only estimate per sentence, ?",
    "this probability of Q to given Q1.",
    "But no, no, , the other one, probability of O1Q1.",
    "But in actual fact, you have to estimate this over the entire training corpus.",
    "You look at the entire training corpus in different sentences.",
    "How many times does O1?",
    "How many times do you have the word D given determiner?",
    "How many times you have the word A given determiner in the entire corpus, whether it's sentence one or sentence two, you have to calculate this.",
    "Yes, .",
    "Because Q1, this is your initial probabilities.",
    "you compute  unigram probability for every part of speak tax.",
    "What's the probability of proposition in your entire corpus?",
    "What's the probability of the terminal in your entire corpus?",
    ", .",
    ", I missed the , I missed the initial one.",
    ", yes, yes, because they are also parts, it's treated as another token.",
    ", yes.",
    "why would you want to do that?",
    ", you can, but why?",
    "But there's a dependency.",
    ", this is our model.",
    "there's a dependency between this.",
    "that's why.",
    ", but of course you can do that.",
    "But what will it mean?",
    ", yes, we're trying to say it depends on it.",
    ",  this is a simplified 1 here.",
    "We're just saying bigram assumption, it can also depends on the last two  part of speech.",
    "But say that it doesn't depend, it's   false, ?",
    ", .",
    "this is the way we estimate the probability for the initial probabilities.",
    "you remember the way we compute probabilities.",
    "The probability of an outcome is that you count all the outcomes and then you divide by all the events.",
    "if you want to estimate the probability of Q1, this will just be number of times you have Q1 starting, starting the sentence.",
    "How many times do you have preposition starting the sentence?",
    "How many times do you have determinants starting the sentence divided by all the possible sentences you have in your corpus?",
    "I believe that is clear, ?",
    "That is the Pi I and then you have the transition probabilities.",
    "How many times does these two parts of speech Co occur together in your corpus in different sentence, in different sentences?",
    "How many times do you have this core together divided by the number of times you have I, which is that part of speech?",
    "it's very similar to how we completed the, for example, the naive piece.",
    "And the last one is for the admission probabilities.",
    "How many times do you have this word K and that's tag together every time.",
    "Do you have it?",
    "How often do you have them together divided by the number of times you have the tag?",
    "And then  you say, you can also still do smoothing because some probabilities will be 0 and we don't want them to be 0.  you have to also perform smoothing for the OOV events.",
    "OK,  this is the exercise and then we'll wrap up the class.",
    "given this exercise and this is your corpus, you have how many sentences in your corpus?",
    "you have 4 sentences in your corpus.",
    "Can you compute what would be the initial probability distribution, what would be the transition distribution and what would be your emission distribution for just two tags DT and VBD?",
    "I'll give you  a few minutes and then we'll solve this together.",
    "how do you start for this  exercise?",
    ", you have to estimate all these probabilities first, the initial probability π highs.",
    "the first thing is try to compute π highs for DT and VBD followed by what are the transition probabilities?",
    "What's the probability of having a ?",
    ", what's the probability of having a noun given a determiner?",
    "What's the probability of having a noun given a verb?",
    "you estimate all these probabilities, all the different combinations, and then you estimate also the admission probabilities.",
    "No, no, no, not this one.",
    "you have to do it differently for DT and differently for VBD.",
    ".",
    ", ,  for them, for each of them differently, , each different things given DT.",
    ",  for  we can exclude the ones that are obviously 0 because you're going to smooth anyways.",
    "The idea is that you have to compute the probabilities for every combination in your corpus.",
    ", you have a question, but only a few cases will be non 0,  I will only focus on that in this exercise.",
    "they are very easy to compute.",
    "I believe by  you should have computed the initial probability.",
    "what's the initial probability for DT?",
    "What?",
    ", 3 / 4.",
    "Is that what you got?",
    ", OK, .",
    "what is the initial probability for VBD 0?",
    "Great.",
    "OK, at least  I know some people are following.",
    "All .",
    "what is the transition probabilities from OK,  let's think about different combination from DT to NN.",
    "Yes, four out of six, that's correct.",
    "what are from which other combination do we have?",
    "I don't think we have any other combination that is non 0. .",
    "DD to JJ, Yes, 2 / 6.",
    "OK, OK. And there's no more.",
    "OK, .",
    "And what about for VBD?",
    "What is the transition probabilities from VBD to preposition, for example, to iron?",
    "Yes, 2 / 4. , also from VBD to JJ adjective 1 / 4.",
    "OK what are from VBD to DT 1 / 4?",
    "OK,  the last one.",
    "Let's talk about the initial probabilities.",
    "what's the probability to have D given DT one?",
    ", because the three the three sentences you're always have the , OK.  what's the probability of for VBD  what's the probability that from VBD you go to SAT?",
    ", 1 / 4 And what's the probability from VBD you're going to worse 3 / 4?",
    ".",
    "OK.",
    "I have this calculation.",
    "I think I'm missing one things, one more calculation, I don't remember which one, but I think I'm missing I think DT to JJ.",
    "I think that's the only one I'm missing which you can compute here.",
    "OK, OK. And then, All ,  in terms of the inference, how do we  do the inference  that we have a model, how do we  tag a new sentence?",
    "I think this is what we cannot finish in this class.",
    "But from  class we're going to talk about different algorithms  on what is the best way to  tag a sequence and.",
    "We have the following questions which we need to estimate.",
    "we have computed all the probabilities  we have our Theta and  that we have our Theta we can compute what is the observation given Theta.",
    "And then we can also use some algorithm to determine how to estimate the  tags given all what we have observed and given the parameter we have estimated.",
    "And then we have all these algorithms, forward algorithm, backward algorithm, Viterbi algorithm that we're going to discuss in the  class.",
    "OK, if you have a question, what you can ask.",
    "Otherwise, thank you for attending the class.",
    "I , you're using just the one question for storage, you are mainly using your home directory or scratch.",
    "There's, I think there's a, there's a directory other than home.",
    "what after we log in, we are automatically directed to home directory.",
    "But there's an actor directory's name is Scratch, and you can store all the big files there.",
    "And I think the space for Scratches around.",
    ".",
    "I'll give you a score.",
    "Thank you.",
    "And after you try this stage, we will have another score.",
    "And.",
    "I don't know.",
    ", , I think the paper is supposed to.",
    "Peter said he will be responsible for the experiment section.",
    ",  the result section, but because he said he can because and Peter said he can do that and he reported some of the scores to me and it doesn't seem because for example, for the accuracy on multiple SQ was just 26%.",
    ", I think, I think the first thing will be after tonight because tonight about I plan to finish the I plan to finish the first 3 instructions and the first half of the first.",
    "Maybe it's better for me to just use the pointed out the point at the three end.",
    "you would finish the 1st for other parts.",
    "All other stuff it shall be mostly done.",
    "And I think you what you can do is after you can in the polishment you Polish the 1st 4 pages and there are some figures that can be added.",
    "For example to help illustration, you can first read what I wrote and write a figure.",
    "I'm   how I how I ordered the databases during training phase for example to help smoothing the training where they was interleaving databases from different languages.",
    "SO11 database from English, one database from French and then this for just many that I have many forms of data and each of them is to contain one database from each language.",
    ", , that's what I did.",
    "is just use, but if you think that's better to use table, you can edit that part.",
    "But  is just to say, OK, for the stuff we use this data set and I just added script and I specify which language which set of languages for we use.",
    "For example, if a language containing many languages, but we just use for two or three languages.",
    "But if you think the table is better, , but they can edit that section.",
    "I have it.",
    "It's been years since I just took  a couple."
]