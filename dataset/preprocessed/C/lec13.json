{
    "Topic 1": [
        "Jackie Cheung, Professor: . . . I'm I started a bit late. The latest version of the draft of Durafsky and Martin. ? Quiz, not for marks. All . And . . it's a description of transitions between tokens other than not tokens. ,   instead of tokens those we'll call them non terminals. anything else. I think we're still missing somebody else, maybe. ? . ? you might have rules that where the verb phrase rewrites to just the verb, because it's an intransitive verb. or the verb phrase rewrites to the verb plus the direct object. ? If it's a transitive verb. there are a whole bunch of different options you can have for what's underneath a verb phrase, node. Otherwise, the  bit is gonna not make any sense at all. . But what I claimed at the end, and I didn't elaborate on because we ran out of time is that even with smoothing and regularization and all that, this approach doesn't work very . I mentioned this last time, too, but I'll go over it in more detail. We mentioned the case of pronouns. ,  if you speak Finnish, for example, then there's  15 or 16, or I've lost track of how many cases they have. 1st case. . ? To this. You can add prepositional phrases. To this. On the other hand, you can also have,  other subcategorizations. ? . and some probability that a verb is  modified by a prepositional phrase, or it takes on some object. ? We already talked about how Hmms. Or something else. ? Will tend to be closer to the top of the tree where the node above it might be an S. Node. Would be the way we've been drawing syntax trees, it'll be closer to the bottom of the tree. where its parent will be a verb phrase node. horizontally across the  hand side of a production. ? and then you also indicate the parents of that node. With a parent of S,  this would correspond to a subject position for a noun phrase. With a subject  with a parent of Vp. And again, you don't need to learn any linguistics to do this because this can be read off from the structure of the tree in the tree bank. In your supervised learning procedure in the Treebank data, you can directly see what was the parent node. That's . . ,  . Very sadly Piravincin passed away, , I think, last year or 2 years ago. . here the S node at the root. . But . you  just look at. And then  you can run the same procedure to do supervised learning. ? But then, the technically, we've already covered all of these, all of this technique, because this is just an end ground model. And  you can again look into your tree bank and you have all of those trees that are already annotated. Start up, multiply. . that's all. You can interpolate, do everything, everything we talked about with N. Gram language models you can do. All  is. . ,  here. Structure. , I can. I can also show it in. , let me show it in a more easy to read tree form. . . . and then you can go further. But of course, usually in practice, people don't go that far because of your model is  expressive and  complex that it's you don't have enough data to estimate that. , . this was proposed 20 years ago. And then wait. and you can see it makes a difference in terms of the pricing performance. the number here represents the accuracy of parse. and then underneath it. If you do. Then you get  a improvement up to  79, and your model is a little bit more expressive , and it has more parameters to learn, but not  that, much more . This has  a real effect on Parson. you look at every pair. ? I think it is somewhat specific in that. It's about. And Gram language models which we already know about. . . you can do this 1st and come up with  a model. and that might require a bit of thought. We have to reconvert it to. ? And make sure all the probabilities of the entire tree are preserved. And  , we're gonna talk about another really big topic which is semantics. ? how? . ? but that these superficial answers will at least give us some beginnings of  guidelines for how we might want to model things computationally. But let's  explore this. These these ideas a little bit more. . . ? . . ? However. And then it returns to recall? . . Did I miss something. . , , you could. ? That has a certain sense. You can evaluate that, and it points to a different referent in the real world. ,  that was our very superficial look at detour into the philosophy of language. A synonym, ? And there are many others that you can do. You can express them logically as . But . and we'll just go through them. And then hypo is  lower down. , then, here I assume you've heard of these ones. These are synonymy and autonomy. I'll be  back. . No. I forget the name of this comic, but the whole humor from that comic comes from rephrasing common, everyday expressions, using paraphrases that are supposed to mean the same thing. But they sound humorous if but if you do a synonymmy replacement exchange planet or something I forget, . again. Often. or happy and sad or descendant and ancestor. And again. that's why it's  synonym. It's about. there are different kinds of homonyms. There's a homophone which again break it down. ? If you speak a widely spoken dialect of English, . ? , here's another one. . It turns out that newspaper has many, many different senses. ? The 1st one is about that physical object with things printed on it. When it began to rain he covered his head with a newspaper. What other newspapers do people read? or firm that publishes, that produces regular articles for consumption. . . and I'm sure that the boundary is a little bit fuzzy. and I'm sure we can reconstruct some chain such that, , it seems  they're related to each other. But then, at other points. ? the 1st sense of position given here is  the particular portion of space occupied by something. that clearly seems related to the 1st sense. ? It's just that one is about physical location, and the other is about  a location in some more abstract sense. You're . You're creating this abstract space, and you're  locating points of view in that abstract space. And then this last one is really interesting of  position as  a job in an organization  at 1st glance, it would seem to me that it's just entirely distinct from  the physical location, ? One is about physical locations, the other is about your job in an organization. You can think about jobs within an overall abstract space of  a bunch of jobs in the organization. You can do to create this. , more lexical, semantic relations. We're not done yet. , . Here's 1 that's very relevant to us. It's a concept of  some geographical location or something. This is. . these are relatively  standardized examples. ,  this is one of the mechanisms in which these things form. ? it's both. ? it involves substituting. if you are in the situation involving sailing boats or whatever, and the captain says all hands on deck the captain doesn't mean just literally, all of your hands must be on the deck. or many swear words involve sensored body parts. ? The  relation is Hallonomy and meronomy. ,  we just mentioned whole part relations. holyms are the whole. ? these are all whole part relations of different kinds. ? Something could be. . And in this case, with Haronomy and Morontomy, is that similar to  we saw on the 1st slide about this. is Hallonomy versus meronomy, the same as hipernomy and hyponomy. while they the structure is similar, because it's it seems to be about  greater things and smaller things. What's the part. . Yes,  there are, which is a some  homonyms. hair and head. . the head is the holyms, and the hair is the marinym enemy. . And above you just said it. You just said the 1st 3 letters. Someone anyone who has not. . can you? You can. ? . Last one. Which one is which. . And you've got one part of the midterm down. ? And in fact, they take it a bit further. and they organize everything in terms of these senses rather than in terms of the words themselves. the primary organization is in terms of senses. ? and they have a different hierarchies for different parts of speech, but the one that's by far the most  developed is the one for nouns, and then the other ones they happen to. Someone. ? ? ? 0, hey! Here's another case, ? This is  a this is a metonym which has become  conventionalized that it's  listed as a separate sense. He entertained the whole table with his witty remarks or the arrangements. ? And . . It's not that big, but it's still takes a bit of time. And then you can look up synsets in Wordnet. ? And we'll continue this  class. ? . which is a noun of the 1st sense, the 1st sense of that. and it corresponds to  your actual hand physically. , wait. , it's not here. You can still try to come up with application oriented reasons to work on this task."
    ],
    "Topic 2": [
        "good evening. Hi, everybody welcome. Don't worry. That that's by following a probability distribution. and what this last condition property is saying is that process follows a categorical probability distribution. if you still need me to clarify, please ask. then the and at the end of the last lecture we discussed probabilistic, context-free grammars in their most basic form, which sometimes people call vanilla Pcfgs informally. And it's because this approach doesn't model the context very  around the rules, and it's also pretty bad in that. we know in language that there are different syntactic positions within a sentence. in English you have differences between I and me. But and in other languages they have a lot more of these cases. Distribution in terms of what you Gen. All the internal parts that you generate with a Pcfg. that's a problem. , for example, humans or animals, or maybe other moving objects or things that are perceived culturally to have some  , , intentionality or movement, or something  that. And, in fact, that's exactly the solution that we'll explore. But we'll do it in a way that doesn't require linguistic knowledge. then you really have to be  really good at linguistics and take many linguistics classes in order to decide which are the distinctions that matter and how to separate them out, and how to identify them in a corpus  instead, we'll take a more automated I'll call it , a more algorithmic approach to do this  that we can get a lot of the same effects without needing to learn too much linguistics. Which which have effects on the word distributions, you would see. but you can add adverbs. in this vanilla Pcfg, there's no relation between any of these rules in terms of their probabilities. But that's really bad. This work that I think is really elegant and really nicely solve this problem within the this setup of probabilistic context, regrammars. really, the main problem with vanilla Pcfgs is that they make independence assumptions that are both too strong and too weak in different ways. remember what our independence assumptions is , what you choose, the relations between different random variables that you choose to model or ignore. Make this really strong independence assumption that, , you, only model some local effects, but overall, it can still be quite powerful. It makes independence assumptions that are too strong, going vertically in the tree. And  that means we need to change the independence assumptions vertically to model more of the context. On the other hand, this other problem, with  this sparsity issue with  too many rules is because we're making independence assumptions that are too weak horizontally. which is too weak of an independence assumption. Essentially, we're gonna take that what was already proposed. But do it algorithmically, which is, we're gonna split up the categories vertically and split up the rules horizontally. whereas before you might only model Np. Which might correspond to the object, position. And you're good to go. the   we're making the model more complex because we're weakening the independence assumptions. With the parent S would have its own categorical distribution at some point. But but then you still model them as a separate probability distribution. I'll copy this slide. ,  this is a quick fix for the vertical problem. Then for the horizontal problem. whereas before you have,  Vp rewrites to this entire sequence as a single unit which is atomic, which is not broken down, and you have  one parameter for its probability. And then you can solve that learning problem. , what is the star stand for? It's more about how you break down the rules. the original rule act. Times. Probability of Np. Times probability of Np. And N. Times probability of Np. you can also do a trigram model or a unigram model. you can take a linear combination of them. And then you can take  I'll weight the unigram model with  a 0 point 5, and the background model with 0 point 3, and the tracking model with  0 point 2 or something  that. this is a common noun. Np is  here the original tree structure is, you have an Np subtree, and then within the Np. and you can draw. that's what that subtree looks . Essentially. does that help? we're gonna model, Np. this process is called markovization. because we're making Markov independence assumptions. Essentially  vertically speaking, that was called that would be called vertical markovization. the vanilla Pcfg would be called a 0 order vertical markovization model because you're not considering any context. Order of , , V equals. One is no annotation. H equals one. H equals 2 is a trigram model, and then H equals. 0 is  the unigram model, and H less than or equal to do is again, they have some heuristics to select, only some things to break down and not others. because that would be infinites, Markov order horizontally and no annotations vertically. That shows the number of parameters in the model. the question is about, what does the horizontal markup order mean? If you do H equals 2, it means you look at every triple. Times, Np. Times, Np. each data set with each annotation scheme has its own characteristics and distributional patterns. I'm presenting this, not because this is state of the art by any means, but I think it's really nice in terms of how they take these intuitions from what we know about language, and transfer it into algorithms that we can. there they do need domain expert knowledge. Yes, you have the horizontal markovization. how does horizontal markup position interact with Cnf. Cnf is essentially doing 1st order horizontal markovization. this method and this procedure is about training of the model and learning the parameters of the model. But what if we started with something that was already in Cnf. Not Cnf,  it's something if you start with something that's already Cnf,  you only ever convert things to Cnf in order to run the parser,  you wouldn't  there are some linguistic theories and some approaches to syntax, where they demand everything to be binary branching. which is one of the key Cnf assumptions. in that case, then, maybe you don't really need to do mark horizontal markovization, because everything is already binaries. In other words, the process we  to convert the Cnf wouldn't apply on their probabilities, because you have to make sure the probabilities are mean. if semantics is the meaning, if it's if the study of meaning and language, then we have to 1st ask, what does meaning mean? ,  what does meaning mean? can we at least have some examples. But what does the word telephone mean. and  these are called its reference. And that's the meaning of the word telephone. suppose you had to define the word telephone to a 3 year old, or to a friendly Martian. I guess one thing you could do is you can say, telephone, telephone, not telephone, not telephone, not telephone, but that would take a really long time. ,  probably another thing you might do is if you're if the 3 year old you're talking to, or the friendly Martian you're talking to already speaks some language that you share in common. Maybe you would try to give a definition with other. for example, you might look up the dictionary definition of a telephone. ,  here's the dictionary.com definition of telephone. , an apparatus system or process for transmission of sound or speech to a distant point, especially by an electric device. You can characterize the meaning of telephone. this presupposes. And this intentional definition. Whether it's a telephone or not. It checks a bunch of conditions of ? And  then you do need to know what everything else means, and you already have to have some  language capability to do to give this definition. what happens if the dictionary definition mentions a function that needs to be fulfilled? That's fine. That's just part of the dictionary definition. The intentional definition is different to the fictionality. one of the 1st people to propose it. I guess at some point long in the past they saw this bright object in the sky this thing that looks  a star. and it appeared in the morning and in the evening. And  then they called something the Morning Star versus the Evening Star. And , it's not even a star. However, there are different senses, because the morning star might be that bright thing in the sky that appears in the morning. The evening star is that bright thing in the sky that appears in the evening? ,   they have different senses, but they have the same reference. And I guess maybe the logical thing to ask is, can you have something with different reference with the same sets. it's  higher up in some taxonomy tree. it's lower down in your taxonomy. and that there's no 2 words or expressions that truly mean exactly the same thing. you say, children in typical scenarios. Nathan Pyle. it's really hard to say what does opposite mean? ,  this, this will be a problem for us later on. , another lexical semantic relation is something called homonomy. That's why telephone means telephone, because it's tele means far away. lead versus lead. But they're written the same. Murdoch owns many newspapers. It can be very difficult to distinguish between harmonymy versus Polysemy. and there are many different senses here of position. Here is a point occupied by troops for tactical reasons. the connections between senses, but then, maybe some sentences are lost, some senses are lost over time, and those sentences are no longer used in the language, and then it really appears to be unrelated words. Then you use the same shortcuts and other people misunderstand you. You mean. It's standing for the Canadian currency. , if metony becomes  popular that it becomes  a dictionary definition, does it stop being metronomy at that point. I would say it's both a metonym and also that word is felicimous with multiple senses. or many, many offensive expressions. And then maronyms are the part. oh, thank you. here, a student is not a class, and wood is not chair and windshield. A windshield is not a car. Exactly. What relation does that exhibit? the month. cut as in hair versus cut as in bread. cut hair is, you have something that's attached to something else, and you're shortening it . Cut bread is  you have something, and you're slicing it. That's usually a clue that it's  it's  there are different senses. I don't speak French natively, but the word I would use to cut bread is  Tranche. But I don't know if you can coupe a bread. George Clooney, an actor. believe it or not. they started with English, because, . and they gave a. And then the edges in this graph correspond to lexical semantic relations between syn sets. Give me a word that I can safely type here. in Wordnet, there are 6 different synsets associated with the word table. Here is a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs. The  one is a piece of furniture with tableware for a meal laid out on it. , the  one is flat table land with steep edges. The tribe was relatively safe on the mesa, but they had to descend into the valley for water. you can see that here cable is not the most common word used to for that sense, and it's usually a mess up the  1. She sets a fine table room and board. If you click on one of these  if I click on the first, st since that it gives you its connections to other synsets  it has a direct hyponym or a full hyponym. You can have a table of contents and actuarial table, a calendar file, allocation, table, periodic table, and  on. and then there are some useful functions you can use,  you need to import it and download it. Once you have those resources, you can try to disambiguate a word into which word sense was meant by that word. which is, it's not listed here. And  one computational task that you might want to solve is called word sense disambiguation. Another question that you can ask, or maybe you should always ask, is , why do we want to solve words and disambiguation? And that often different senses of a word should be translated differently when you translate to a different language."
    ],
    "Topic 3": [
        ",  today we're going to start talking about lexical semantics. But 1st we're going to wrap up a few things from the last lecture on parsing and syntax. And, by the way, I don't know if you saw, but I posted an ed for the readings for this lecture. 3rd edition seems to have taken out this material, but I think it's  very important. I posted a previous version that still had this material. first, st here's a review. What are the components of a Pcfg. what are the components of a Pcfg. And what do they, I guess, do? Yes. But that's , once you already have the components of a Pcfg, how you use it to  parse? I forgot the word. Yes. And then you mentioned that there are rules which is  a left hand side, non terminal which rewrites to some combination of  hand sides, which are term non-terminals, or terminals with probability. Yes. this implies that terminal symbols are also part of a Pcfg. Yes. , yes, a starting symbol. And I think that this is  it. I would just add that a room  a reminder that,  each non-terminal symbol forms a categorical, you define a categorical distribution for each non-terminal symbol over all the rules with that non terminal symbol as the left hand side ? ,  I need to explain some more. ,  what this means is that ? ,  what are your non terminal symbols? Each non terminal symbol is a syntactic category. a non terminal symbol might be noun phrase, or it could be verb. what this means is that for each of these non-terminal symbols they're non-terminal, which means that they need to rewrite it into something else. That's all I'm saying. And the idea here is that you have a particular way of estimating the probabilities of rules through a maximum likelihood estimation. ,  if you have the probability of Alpha rewrites to Beta, then the way you derive the Mle estimates for that is to take the number of times you observe the rule. Alpha rewrites to beta divided by the number of times. The rules themselves are very sparse in the in the sense that there are many, many possible rules, and it's really difficult, even with a very large corpus. with noun phrases, for example, in English you can have a subject noun phrases, and you can have object noun phrases. in English all the pronouns that you have to change their form, you have to decline them according to the case, whether it's a subject or object. But what this means is that if you have a subject noun phrase versus a object noun phrase. And that can't really be captured by a standard vanilla. Pcfg. Because all you see is the Np on the left hand side. This  problem, this  issue is a less obvious one. But it's maybe even more important, which is, that certain classes of nouns are more likely to appear in the subject position versus the object position, and vice versa. in particular, in the subject position. and it's more likely cross linguistically, but in English and cross linguistically, that they appear in the subject position than the object position. Yes. Is there not a way that we could create  a subject  phrase, and then an object,  phase and have different rules for all these to account for them. the question is for is for the 1st case, is there a way to create subject, noun, phrase versus object noun phrases, and account for those that way. Yes, there is. although you should learn linguistics . they can be intransitive and take 0 arguments, in which case you might have a rule  Vp rewrites to Vbd, vbd, just means  the past tense form of a verb get in the country bank. it seems intuitive that maybe that we rather we should have,  some probability that a verb is modified by an adverb. with Pcfgs in that the vanilla version. if you talk about  subjects versus object. Nps, what you're saying is really that, , you want to model more context up and down the tree around where the Np is, because that's how you can figure out if it's a subject Np. , for example, the subject Np. think of this as essentially a new symbol, a new non-terminal symbol in your vocabulary of your Pcfg. And  you can algorithmically add all of these annotations and expand your set of non-terminal symbols. Yes, but having  many additional . I guess non terminals. Would that further decrease the individual probability? this is again, this  complexity versus data expressivity  this, this expressivity versus  a ability to estimates  sparsity trade-off. you would expect that you would need more data to learn reliable estimates of the parameters. Yes. yes, ? Oh, . maybe we should do this  bottom up. , I'm going to stop here. You get the idea ? this, oh, maybe one thing I didn't explain is this is  just a tree represented in a different way using bracket notation. , we're gonna pretend that every  hand side is a Mini Markov chain that we need to learn. for every non terminal symbol on the left hand side. It there's a on for the it's  hand side. It's  a mini little language model there, involving those symbols that would appear in the  hand side of a rule. Here there's conceptually it's the twist is that you're deciding to break down how you model this complex thing. You have , you have to solve one n-gram modeling problem for every left-hand side symbol. But it's doable. This is just multiplies. Yes. , I think I'm going to skip this example, because here it's really not about  redoing the annotation. If you're doing a diagram model. Yes. that you're  taking the in between of 2 models as your model. For example, you can do this, you can interpolate between a unigram model, a background model and a trigram model. Yes, in this example,  it's saying the line , Mp, Dta,  the 3rd bottom . Are we saying that it's  a proper noun, and then you're choosing one of those 3 brackets, or it's in that order that it's  I got lost. that line doesn't mean that we have a proper noun that is followed by one of those 3? this is not a, this is not a proper noun. and then and n. and then below that, you have an a non, exact it is. all we're saying by doing the factorization is rather than modeling the entire thing all at once, we're not gonna do that. That's that's just what we're doing. Because you're adding ancestors as contexts. And then the other thing we're doing is horizontal markovization, which is breaking down the  hand side into parts. the standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter. and then it's it was these are the results on parsing on the Wall Street Journal through doing the scheme. on one axis. And then here's the horizontal Markovizations markup order of , here's the standard Pcfg with infinite context. And then here's the background model Unigram model. No, this is the background model. How many constituents a parser trained in this way gets . And  with the standard Pcfg assumption that would be the top  of the table. For example, grandparent annotation with  a diagram model. Yes. One is 1st order, which corresponds to a diagram model. for this rule, for example. , I just want to know if from this data, if this  set oh, God. you would expect that maybe a different hyper parameter setting of , what markup order to use would result in the best performance. But the general idea, I think still is, holds across different data sets, and maybe even in other cases beyond syntactic parsing. Even in their paper they  go beyond substantially beyond 79, through additional linguistic insights. any other questions about syntax and parsing? In Cnf with just the 2  hand side values. If you think about it. And then the Cnf thing is for the parser. and then once you get to the point of  parsing. Then you can convert all the rules to Cnf. And then, since this is a Pcfg, you have to think about  how you do the conversion. oh, . Oh, the question is, when you convert things to Cnf, then maybe that wouldn't be. I would have to do some thinking about it, but I'm pretty sure there's a way to still convert things to Cnf. In with a Pcfg. I think it. I don't think it should be that hard. because if you think about it. you're creating non-terminal symbols. And  each non-terminal symbol only is involved in one rule. I think you can work it out if you spend a few minutes thinking about it. we've wrapped up structure and parsing. And this is also this is arguably super interesting for both theoretical and practical reasons. because semantics is about meaning. Very, I would say, probably superficial answers. And the 1st answer is that meaning is about the relationship of linguistic expressions to the real world, or at least to some world. And the second answer I'm going to give is that we can talk about meaning in terms of the relationship of linguistic expressions with respect to each other. And this is called lexical semantics. lexical means, it's related to words. relationship of linguistic expressions to the world and to each other. for example, I guess I need to change my slides, because, , telephone is not a word that's used very much anymore, I guess. one way you can think about the meaning of telephone is, you can think about it as you're picking out all of the objects in the world that are telephones. And then this is also called the extensional definition of a word. And note the spelling here extension with an S.  think of it as , the meaning of telephone is that it's a function. and then it gives you true or false. or if you can also, think about the meaning of words in different ways. , probably you would, . thought there was a way to follow the link. this is another way. You already know what all of these other words mean,  apparatus, or sound or speech,  that could be a problem. But , this is the general idea behind the intentional definition of a word, which is that you're  listing the necessary and sufficient conditions for something to be a telephone. I guess you can , express it using some logic, in which case you don't have to rely on other language. But your function. Are these necessary conditions satisfied? And are these sufficient conditions satisfied? But it's by a computation of  properties or in practice, we're gonna if you use a dictionary definition, then you're defining words in terms of other words. Yes, what happens if dilutionary definition? It's just expressed that function is expressed in language that's all. Yes. I think . The intentional definition is  talking about the conditions, the necessary and sufficient conditions for something to mean something. you can express that using some computation, some logic, or whatever you can express that with it, can take the form of a dictionary definition. But they're related to each other. ,  this is not a new idea. Maybe maybe he was. For example. astronomers. but But here you have an example of something where there's the same referent we  know. and that I have to think about? ,  the over, especially over time. You can express these in some logic as . For example. I assume that most of you have heard of synonyms, ? You can express that probably in a logical way as  about how things that about  it evaluates the same set of objects to true, for example, ? And this is simply an is a relationship. If you prefer to think of it that way. ,  it's something taxonomic. Oh. The the way to remember, I think is hyper is  higher up. synonym means that 2 words roughly mean the same thing. There are linguists and people who say that true synonyms don't really exist. there's some people who say that their true synonyms don't really or and true paraphrasing doesn't really exist. It would be very strange if you go up to  a friend of yours and say, Oh, Hello! See how are your offspring doing  ? There's a whole comic based on this. What's it called? It's by that guy. And it's  it's it's still useful to come up with this idea of some synonyms of words that roughly mean the same thing. but presumably there's some dichotomy, or there's some spectrum. And these words appear at opposite ends of the spectrum or opposite sides of the dichotomy in some in some way. , for example, synonym and antonym are antonyms of each other. in some way they're they're opposed to each other. we'll come back to synonyms versus Antonyms, because it turns out that it's it's  quite difficult. It's very difficult to separate synonyms from antonyms using many computational techniques. which means same form. and nim means form. It's about same and difference. Phone means sound. telephone means far away. For example, sun versus sun. whereas homograph means same written form. This one is again very interesting and comes with lots of problems for computational linguist to solve, which is called polysemy. , here poly means many and sem means meaning. the example here is the word newspaper. A newspaper can be a business firm that publishes newspapers. I think here the idea here is that the 1st one is more about the concept of that newspaper  New York Times is a newspaper in the 1st sense. I don't know. that construct of  that. which is different from the business itself, but which is also different from the printed version of that Of that construct. In the 1st sense, I guess. they use bales on newspaper every day. But we just we don't even think about this  when we process language. What the intended meaning is in which sense of the word is intended. homonym can be. one example is with a word such as position. To me it seems  they're probably related. You're drawing an analogy. But then, probably it's through this  a again, it's through this metaphorical extension of that. I would argue that all of this is probably these are instances of felicity. But sometimes it's really not obvious when you 1st think about and look at the words. and it's even possible that there's originally a chain of  of this reasoning. Here's another interesting one. there's been a substitution there. you're substituting one entity for another related one another. One is related to the example we just saw. or the last example. Dollar. and then you'll have,  some way of speaking, that's very convenient and short. it's probably both. this is called synecdokey. here the metonyms in general is just about substituting with related words, but if the relation involves whole part relations, then it has a special name, for whatever reason. yes, it has its own name, too. I think we showed here that it's probably not. oh, yes. It might be something  inheritance. There's still a hierarchy. It's still a hierarchy. Of hierarchy. , yes, these are synonyms there and there. Yes. Yes. yes, and which one is which head of the forecast? Yes, the head is the whole thing. Yes, that's . yes, Felisame, yes. I think in French, I think I have mixed. I don't know how you some. whereas to Kahair. I will use coupe. , I guess that it's also business in French. Oh, . yes. Yes. and the other one is called. the researchers spoke English. and  they came up with this resource, which is called wordnet, which is a lexical resource organized around this idea that words can have multiple senses. the idea behind Wordnet is that in the language. Maybe there still is one. Oh, , it's still there. each of this, each of these think of it as  a concept with a certain way that you can realize it in language using some words. , for example, this 1st inset can be expressed using the word table or using the phrase, tabular array. But it's  a different sense, because the 3rd one is specifically in the context of a meal. , you can see this is how it's organized. You can click on any of these and find,  other sunsets that this is related to in this network. ,  if you're using nltk, you can  interface with this. arguably, one of the most obvious things you can do. here's an example. This corresponds to a synset with this identity of hand. This represents another sense of hand. And the general idea here is that you can use the words in those contexts to help you disambiguate. For example, in the 1st context, here. one reason might be, you think it's  in inherently interesting to figure out, how do we figure out the intended sense of all of these words that have multiple senses? But if you don't think it's inherently interesting."
    ],
    "Topic 4": [
        "Shit. you can download that. Is this a table of, I guess,  derivation of the words, and then the probabilities associated with each cell that's definitely related to Pcfgs. Yesterday I said tokens, and I was wrong. Yes, those are definitely part of it. ,  we have non terminals. That's definitely one of the components. that makes sense. for  there might be, say, say, you're looking at verb phrases. ,  I hope that makes sense. You see, Alpha overall. , let me illustrate. and it's very clear that the distributions of them are not the same. then they should not follow the same probability. Again, it's not perfect that those tend to be entities that receive some action or suffer some consequences of something and  forth. Because if we take that approach. And there are also many other cases of obvious dependencies between very distant parts of the syntactic tree. and you can likewise add  adverbs and prepositional phrases, and  forth, ate a sandwich, quickly, ate a sandwich with a fork. They all have to be estimated separately, just by counting the number of times you see each rule in your tree bank. we would  to be able to factorize this probabilities in some way. the general solution for both of these problems of , not enough context and too much sparsity is  going to be the same. I'm going to present this paper. And  with Hmms. Essentially, we're modeling every single combination, and in every single ordering of  hand, side symbols, non-terminals, and terminals, as independent of each other in their generation. what are we gonna do? Rewrites to a personal pronoun node, or whatever. I'm going to use this notation where I use this carrot. Rewrites to something. Or yes, that's a great point. it's always a trade off. the idea is that we think this trade-off might be worth it because it allows the model to be a lot more expressive. this is the 1st sentence of the Penn Tree Bank. But , , but how this would work is essentially, we would annotate everything. ,  we're gonna we're gonna break down the  hand side of the rule when estimating its probabilities. , for example, whereas I'm going to add,  explicit start and end symbols to represent the beginning and the end of the  hand side. I'm going to break it down. the probability of this whole thing is  equal to the probability of Vp going to start advert phrase, and then Vp, going to advert phrase Vbd, and then Vp, going to Vbd, Np, and  on and  forth, until you get to the end again. and how would this still ensure that the sum of probabilities for the for every Vp on the left hand side is one. Great question. How do you still ensure that the sum of the probabilities for everything for one left hand side symbol still sums to one. it's through adding the start and end symbols. for maybe the interesting rule here is , Np, here rewrites to bt, jj, and n, ? here, if you look at this notation here originally, we have Np rewrites to Dt. jj, and n. ,  , what we're gonna say is that this is  going to be equal to Probability of Np. Rewrites to start Dt. Rewrites to Dtj. Rewrites to Jj. Rewrites to an end and then end. But you don't trust any single one of them very much. can you say this again? Or is it those 3 in that order,  the Gta,  the Jj, non executive. Struck Np. Here there are 3 words, and each of them has a tag. there's a determiner, an adjective, and a noun, a single, a singular common noun. and then you have a Dt, and then a jj. We're gonna model we're gonna  there's a start and end symbol. start dte 1st and then Np. and then the end symbol. And they're adjacent. V less than equals to 2 is  they have a scheme to only annotate some parents, but not others. You can do all parents you can do selected grandparents, you can do all grandparents. and you can see you can do a lot better than that. wanted to clarify about the order, because you described that,  the one we did the 1st order. Rewrites to start Dtj. Rewrites to Jjnn symbol. That's a great question. the question is about  whether this research sets some  gold standard methodology or protocol, or whether the results are specific to the Wall Street Journal. just for context, the current best parser, which is a neural parser, would be getting a score of around 95.  these are 20 year old results. They can do further category splits and merges to get to around 87 f. 1 performance. that is a great question. It's for the cky parser and what it requires. the probabilities are maintained. ,  where are we in the course? And if you're a language, technology, enthusiast, and you presumably you are. we want to be able to have some formulation and representation of meaning in the world,  that we can represent and understand things and use Nlp systems to derive these representations. and then from there you can make draw inferences, maybe through some approach that's not necessarily only specific to Nlp in order to derive new conclusions and that can help you make decisions or do whatever you want to do. That's what I'm trying to say informally. ,  then let's start off with a philosophy of  language. And here I'm going to give some. I'm gonna give at least 2 answers. And we're going to start by focusing on the meanings of words. And later on we're going to start talking about the meanings of phrases and sentences, and also how to construct these representations from the meanings of words. It's a function that takes in an object from the world. ,  here's a set of items that are telephones. And then there are many, many other  infinitely many other objects in the world which are not telephones. In other words. in the , whereas before you had,  a function that  takes in an object and returns. It's almost  , you're still taking in an object. , ,  we talked about how to relate words to each other, but also how to relate words to the world. But we can say more about how words relate to each other. And it's much more popular in the computational side  to work in this as , which is to think about how the meanings of words relate to each other. And  all of these, a lot of these. But  I'm going to define a whole bunch of terms. ,  one way in which the meanings of words can be related to each other is through hypernomy and hyponomy. For example, a monkey is a mammal. and Montreal is a city, and red wine is a beverage. these are is our relationships. such as offspring and descendant and spawn. and or happy and joyful and merry. I'm gonna close the door. , that's better. The connotations of the words . You don't say that ? But , we can pretend that synonymous exists. , you need a lot of data in order to be able to do this. Antonym, , , , it's  same. this newspaper is about the company. Montreal Gazette is a newspaper  as in  that. these are all very subtly distinct from each other. which is the cheap paper made from wood pulp used for printing newspapers. these are all very subtly distinct from each other. because it's clear to us in context. And in that way, then it makes sense that this is a metaphorical extension. ,  I would say that these are not always as clear cut as you might think. If you're trying to write clearly because I find that  If you're within a particular subgroup or subculture where you're used to talking about things in a certain way and taking shortcuts in your expressions, and it's really clear within that subgroup when you're trying to write for people from outside of that subgroup, you don't realize that you've made those shortcuts. And then  you start to. I worked for the local paper for 5 years. You don't mean that you work for the  the actual piece of paper, the print, ? You work for the organization that produces that paper. But if you start chatting with  your friends, or in a particular subgroup, or you'll you'll develop these very naturally you'll substitute words for other related words without even noticing it. Oh, that's a great question. at that point, is it a meta? And the way we impose  try to impose some order on the phenomena we observe in language. , they mean your entire person has to be on the deck. that's another  synecdoche. I don't know if that's etymologically correct. you can have groups and members  a class consists of students. Yes, all of these definitions are not necessarily mutually exclusive. are all of these definitions mutually exclusive. They're not mutually exclusive. , , that's a great question. They they are distinct from each other because hypernyms and hyponyms is about. Is our relationships ? it's about is our relationships  in programming, in object oriented programming. whereas a whole part relationships. It's  it makes ups that thing. they are still distinct from each other, even though I agree that structurally you can represent them both with,  some  tree struck  to talk about  what's what's the whole? Friend. Yes, I put him great. people have come up with the attempts to systemat systematically, annotate all of the senses of all words in the language. and  forth. They talk about these relationships, these lexical semantic relationships. They give a particular name to these senses, and they call them syn sets, which are, which is short for synonym sets. Then you have nodes, and these nodes are called SIM sets, and they can be expressed in through many different realizations  with actual words. I hope this works, there is a web interface. , great. Table table. And here's a dictionary definition of it,  a set of data arranged in rows and columns  C table one. It was a sturdy table. I reserved a table at my favorite restaurant. A company of people assembled at a table for meal or game. ,  for example, table, there are subtypes of it that you can have a correlation table. And then each of these is its own synset. You can through python. And , these are all of the synsets that these lexicographers have annotated in within this project over the past 2 decades or something. The  example sentence is due to her superior education. Her hand was flowing and graceful. But it's  the handwriting, the handwriting style. Maybe, whereas in the second context, maybe something  flowing and graceful would help inform that this hand is referring to the handwriting style. it says,  , I said before, one obvious application is in machine translation."
    ],
    "Topic 5": [
        ", I'm moving. Hold on. how do they rewrite? And that's by looking at a tree bank that has been annotated with all of these syntactic trees and looking at the structure of that and using that as a way to estimate parameters, the parameters of these categorical distributions that I just talked about. to get enough statistical to get enough data to have reliable statistical estimates of the parameters of these distributions. or generative, and all that. And this is not just in English, but it might be a cross lingual thing as , just because of, I guess how the world works and what people tend to, how people tend to describe what happens in the world. the fillers of the subject position tend to be nouns that are animate. because there's a correlation between subject position and some semantic role of  entities that tend to do things or  have actions in the world and cause changes in the world, whereas the object position, there's a correlation. And because of this, then again, you would expect the distribution of words that appear in these impacted positions to be different from each other. , here's a second problem. consider the subcategorization of verbs, but this time with modifiers as . we already talked about subcategorization of verbs, and that verbs can take different numbers of arguments. the verb eats, for example, is  unique or not unique, but it's special, because it can be intransitive, but it can also be transitive,  you can eat a sandwich,  eat a sandwich. You can even change the order. The orders around sometimes  quickly ate a sandwich with a fork. first, st that means you need a lot more data to get enough samples to estimate the parameters. And second of all, clearly, that's not really factorizing the problem in the  way. in order to both get more samples for each to estimate the parameters, and also just intuitively, it seems to make sense for this particular problem. , it's the same  thing here. Or an object Np. whereas the object Np. instead, we, we can again change our assumptions and make a stronger independence assumption,  that we can more reliably estimate those parameters. vertically, we're at, we can add more contexts by annotating parent categories. rather than Np. you have Np. and in the second case you can talk about an Np. and then otherwise everything else works the same. And it has its own distributions, which are different from the ones of just the Np. would it still work out? the Np. the Np. With the parent of S would have its own categorical distribution that sums to one. one thing you can try to combat this sparsity issue is to try to do some  interpolation or smoothing. that you still allow everything that is a empty node to share information with each other through some interpolation scheme. , time moves on for everybody. You can just annotate that its parent is the root of the sentence. and then np, here is parent would be the S. And then this Np. Its parent would be an Np,  you don't do this recursively, ,  you could do it for more than one level. But you won't , but you don't do it   Np, Np. and s, or whatever, unless you are planning to go 2 levels out. And  then you would keep annotating and hopefully, you  get the idea. But since you're a computer scientist hopefully, you can recognize this is really just a treat . where all the children are expressed within the parentheses. ,  , you just look at what the parents category main category was, and you annotate it. if I choose a Bigram model, then it would be broken down in this way it would be factorized in this way. You can show that if you add those, then you can get it to all work out  that it's still all the possibilities still sum up to one. to understand, interpretive. by interpolate. interpolate interpolating means that you have multiple models. And then, director. then it would be  Npm in , at least it would be  a non executive director. you have an Np at the root. And then, director. Dtj, and then Np. JJNN, and then Np. We're factorizing. But the order matters. and then the scheme we just described would be considered 1st order. You can have,  second order, 3rd order and  forth. The scheme we just described with the background model is called 1st order, and again, you can do any other order and can interpolate, and  forth. You have the vertical marketization. you would get  a 72.6 2 performance that way. Sure. , the one we just described is H equals. it would be Np. Rewrites to Dtjnn. in that sense it's specific. And  that we can build into the computational side, into the formalism and into how things work. I think that's really nice, plus I don't have to introduce too much extra in terms of new technical approaches. It also still works, even if we do. although maybe in a slightly different way, and the probabilities are all messed up. , to make things distinct and clear, I would keep it separate from this. That wouldn't apply here at. but we don't know how to maybe I'll assign it to you as an exercise. You can work it out. I'm pretty sure it's . Since you're in this course, you want to do things in the world. and things mean things  words mean things in the world. , semantics is super important because it's a, it's   the representation of something in the world  that you can  interact with the world with your technology. is that a stretch? , it's just a stretch. This is one answer. it's not by an enumeration anymore. they're slightly different. this distinction between sense and reference. The 1st was frege in 1892  he was one of the 1st to distinguish between the sense of a term, and its reference where the sense is more  the intentional stuff about the. the, the properties associated with a word, whereas the reference is about the objects in the world that it points to. And then eventually, somebody was smart enough to figure out it's  the same thing in the world. And it was just Venus. It's the same reference in the world. the Prime Minister of Canada. But then, at different points in time. this is the second  a main general area that people work in. Things that mean the same thing. the smaller thing,  the thing that denotes fewer things in the world is called the hyponym. and the word that denotes more things in the world is called a hypernym. , straightforward, . You just have to remember these terms. because there's always some slight differences in, if nothing else. Long time. Then antonyms and autonomy is words that roughly mean the opposite thing. but different and unrelated meaning. the way to memorize this is to understand,  how to break down these words  homo, means same. Same sound. Sound ? homophone here means same sound. they sound different. these are both subtypes of homonyms. They're both hyponyms of the hypernym homonym. polysame means the phenomenon of a word having many meanings. And in particular, polysame involves multiple related meanings. and sometimes these are  ingrained and  natural to us that we don't even realize there are slight differences in the meanings of in these different situations. It has many different related meanings. A newspaper here can be a daily or weekly publication on folded sheets containing news and articles and advertisements   that physical object that nobody  ever interacts with anymore. You see how it's different. But it's somehow different from this 3rd sense. I'm not exactly sure how  the newspaper is the physical object that is the product of a newspaper publisher. or  a haha. and then these are all different from the 4th sense of the word. , or maybe the 3rd sense, the 1st and 3rd sense are very close to me. It must seem  they're very unrelated to each other. and then the second sense. The 3rd sense is  a position in terms of a way of regarding situations or topics. and here it might not be immediately obvious whether these are related or unrelated. And then there's position with about the arrangement of the body and its limbs. Here's another one which is position to do with the relative standing of people in a society, again, is some  metaphorical extension of the physical idea of location to some abstract space. And they're they're they're related to each other, and they're  supervisors and supervisees, and  forth. metonymy. metonymy is really tricky. But  some of these are very conventionalized, and it's  used by everybody  you say we ordered many delicious dishes at the restaurant  clearly. You don't mean that you ordered the plates . In fact, you don't get to keep the plates most of the time. You you ordered the food that comes on the plates. Quebec City is cutting our budget again. , here Quebec City is not referring to the city. The city itself is not capable of cutting budgets, because it's just a it's a city. Instead, here it's referring to the governments, the provincial Government. whose capital is located in Quebec City. The loony is at an 11 year low here. This requires a lot of background knowledge and real world knowledge to fully understand  here the loony is not the physical coin. and what it means when you say it's at an 11 year low is that you're comparing it against another, its exchange rate with another currency, presumably the Us. and then the other people will understand, and then eventually there will be  a norm. But outsiders would not understand. , some of these are  ingrained that they appear in dictionaries. Is it a metonym, or is it  a polysimous word? ,  these are just our interpretations. , there's a specific  synec of metonymy, which is  funny because it usually involves things they cannot say in class. it's a specific  autonomy involving whole part relations. don't be a censored body part. the way I remember this is , Hall sounds  hope. But that's how I remember it. and there are different subtypes. You can have,  physical whole part relations  a car has a windshield. and then you can also have,  a composition,  the substance composition of something physical,  a chair, is made of wood. Something can be meton  a metonym, and it can be a polysemis or . It's more  the elements you put in a class. What's the hypernym? What's the hypernym? It's just a different style of hierarchy. It's a different style. ,   if you already have remembered all the terms yet or not. cold and freezing. homophones. And you see how these are just distinct, ? which is distinct also, if sometimes you can tell if something is polysemous, if  another language, and you try to translate it into another language, and you have to use a different word. The word I would use for cutting. let's just remember that. But it's not as extensive and not as , not as connected. here, you can see, let's just focus on the nouns. The second sense. this is funny, because,  the second and 3rd sense is , it's the same physical object. it's it's  a blissimous thing. You can look at  Member maronyms. this was just a backup in case the things didn't work online. I'll just introduce the problem. figuring out which word sense is expressed in context. His hands were tired from hours of typing. this received a lot of attention in Nlp, which is to figure out which sense of a word is met in a particular context. the fact that you see the word tired is probably informative and typing. ,  I think I'll stop there and then  time we're going to look at algorithms for word sense disambiguation."
    ]
}