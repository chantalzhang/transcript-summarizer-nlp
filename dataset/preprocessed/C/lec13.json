{
    "Topic 1": [
        "I'm I started a bit late. And what do they, , do? it's a description of transitions between tokens other than not tokens. Yes, those are definitely part of it. ,   instead of tokens those we'll call them non terminals. ,  we have non terminals. That's definitely one of the components. And then you mentioned that there are rules which is  a left hand side, non terminal which rewrites to some combination of  hand sides, which are term non-terminals, or terminals with probability. we're still missing somebody else, . , yes, a starting symbol. And I  that this is  it. a non terminal symbol might be noun phrase, or it could be verb. rules that where the verb phrase rewrites to just the verb, because it's an intransitive verb. and what this last condition property is saying is that process follows a categorical probability distribution. But what I claimed at the end, and I didn't elaborate on because we ran out of time is that even with smoothing and regularization and all that, this approach doesn't work very . I mentioned this last time, too, but I'll go over it in more detail. But what this means is that if you have a subject noun phrase versus a object noun phrase. And that can't really be captured by a standard vanilla. But it's  even more important, which is, that certain classes of nouns are more likely to appear in the subject position versus the object position, and vice versa. although you should learn linguistics . they can be intransitive and take 0 arguments, in which case  a rule  Vp rewrites to Vbd, vbd, just means  the past tense form of a verb get in the country bank. On the other hand, you can also have,  other subcategorizations. and some probability that a verb is  modified by a prepositional phrase, or it takes on some object. the general solution for both of these problems of , not enough context and too much sparsity is  going to be the same. remember what our independence assumptions is , what you choose, the relations between different random variables that you choose to model or ignore. Make this really strong independence assumption that, , you, only model some local effects, but overall, it can still be quite powerful. horizontally across the  hand side of a production. , we're modeling every single combination, and in every single ordering of  hand, side symbols, non-terminals, and terminals, as independent of each other in their generation. and then you also indicate the parents of that node. and in the second case you can talk about an Np. Yes, but having  many additional . Or yes, that's a great point. But but then you still model them as a separate probability distribution. , time moves on for everybody. we should do this  bottom up. It there's a on for the it's  hand side. It's  a mini little language model there, involving those symbols that would appear in the  hand side of a rule. , , whereas I'm going to add,  explicit start and end symbols to represent the beginning and the end of the  hand side. the probability of this whole thing is  equal to the probability of Vp going to start advert phrase, and then Vp, going to advert phrase Vbd, and then Vp, going to Vbd, Np, and  on and  forth, until you get to the end again. Here there's conceptually it's the twist is that you're deciding to break down how you model this complex thing. ,  I'm going to skip this example, because here it's really not about  redoing the annotation. here,  this notation here originally, we have Np rewrites to Dt. And N. Times probability of Np. Rewrites to an end and then end. you can take a linear combination of them. Are we saying that it's  a proper noun, and then you're choosing one of those 3 brackets, or it's in that order that it's  I got lost. Or is it those 3 in that order,  the Gta,  the Jj, non executive. this is not a, this is not a proper noun. I can also show it in. and then and n. and then below that, you have an a non, exact it is. that's what that subtree looks . and then the end symbol. and then the scheme we just described would be considered 1st order. and then you can go further. You can have,  second order, 3rd order and  forth. The scheme we just described with the background model is called 1st order, and again, you can do any other order and can interpolate, and  forth. this was proposed 20 years ago. and then it's it was these are the results on parsing on the Wall Street Journal through doing the scheme. the question is about  whether this research sets some  gold standard methodology or protocol, or whether the results are specific to the Wall Street Journal. just for context, the current best parser, which is a neural parser, would be getting a score of around 95.  these are 20 year old results. you can do this 1st and come up with  a model. and then once you get to the point of  parsing. And  , we're gonna talk about another really big topic which is semantics. It's a function that takes in an object from the world. And then there are many, many other  infinitely many other objects in the world which are not telephones. in the , whereas before you had,  a function that  takes in an object and returns. And are these sufficient conditions satisfied? And then it returns to recall? it's not by an enumeration anymore. and it appeared in the morning and in the evening. You can express that  in a logical way as  about how things that about  it evaluates the same set of objects to true, , ? And there are many others that you can do. You can express them logically as . But  I'm going to define a whole bunch of terms. and we'll just go through them. And then hypo is  lower down. and or happy and joyful and merry. There are linguists and people who say that true synonyms don't really exist. there's some people who say that their true synonyms don't really or and true paraphrasing doesn't really exist. But they sound humorous if but if you do a synonymmy replacement exchange planet or something I forget, . we'll come back to synonyms versus Antonyms, because it turns out that it's it's  quite difficult. They're both hyponyms of the hypernym homonym. polysame means the phenomenon of a word having many meanings. and sometimes these are  ingrained and  natural to us that we don't even realize there are slight differences in the meanings of in these different situations. A newspaper here can be a daily or weekly publication on folded sheets containing news and articles and advertisements   that physical object that nobody  ever interacts with anymore. or firm that publishes, that produces regular articles for consumption. Here is a point occupied by troops for tactical reasons. And they're they're they're related to each other, and they're  supervisors and supervisees, and  forth. Then you use the same shortcuts and other people misunderstand you. But  some of these are very conventionalized, and it's  used by everybody  you say we ordered many delicious dishes at the restaurant  clearly. You don't mean that you ordered the plates . You you ordered the food that comes on the plates. It's standing for the Canadian currency. But outsiders would not understand. ,  this is one of the mechanisms in which these things form. at that point, is it a meta? ,  these are just our interpretations. , there's a specific  synec of metonymy, which is  funny because it usually involves things they cannot say in class. yes, it has its own name, too. They they are distinct from each other because hypernyms and hyponyms is about. here, a student is not a class, and wood is not chair and windshield. It might be something  inheritance. It's just a different style of hierarchy. ,   if you already have remembered all the terms yet or not. , yes, these are synonyms there and there. yes, and which one is which head of the forecast? Yes, the head is the whole thing. You just said the 1st 3 letters. cut as in hair versus cut as in bread. Someone anyone who has not. cut hair is, you have something that's attached to something else, and you're shortening it . Cut bread is  you have something, and you're slicing it. Yes, I put him great. people have come up with the attempts to systemat systematically, annotate all of the senses of all words in the language. and they have a different hierarchies for different parts of speech, but the one that's by far the most  developed is the one for nouns, and then the other ones they happen to. , , this 1st inset can be expressed using the word table or using the phrase, tabular array. I reserved a table at my favorite restaurant. this is funny, because,  the second and 3rd sense is , it's the same physical object. But it's  a different sense, because the 3rd one is specifically in the context of a meal. You can click on any of these and find,  other sunsets that this is related to in this network. ,  , table, there are subtypes of it that you can have a correlation table. ,  if you're using nltk, you can  interface with this. It's not that big, but it's still takes a bit of time. I'll just introduce the problem. And we'll continue this  class. This corresponds to a synset with this identity of hand. this received a lot of attention in Nlp, which is to figure out which sense of a word is met in a particular context. , in the 1st context, here. , whereas in the second context,  something  flowing and graceful would help inform that this hand is referring to the handwriting style."
    ],
    "Topic 2": [
        "But 1st we're going to wrap up a few things from the last lecture on parsing and syntax. And, , I don't know if you saw, but I posted an ed for the readings for this lecture. What are the components of a Pcfg. what are the components of a Pcfg. Is this a table of, ,  derivation of the words, and then the probabilities associated with each cell that's definitely related to Pcfgs. But that's , once you already have the components of a Pcfg, how you use it to  parse? Yesterday I said tokens, and I was wrong. this implies that terminal symbols are also part of a Pcfg. ,  I need to explain some more. ,  what this means is that ? Each non terminal symbol is a syntactic category. what this means is that for each of these non-terminal symbols they're non-terminal, which means that they need to rewrite it into something else. ,  if you have the probability of Alpha rewrites to Beta, then the way you derive the Mle estimates for that is to take the number of times you observe the rule. we know in language that there are different syntactic positions within a sentence. We mentioned the case of pronouns. or generative, and all that. Distribution in terms of what you Gen. All the internal parts that you generate with a Pcfg. , , humans or animals, or  other moving objects or things that are perceived culturally to have some  , , intentionality or movement, or something  that. But we'll do it in a way that doesn't require linguistic knowledge. then you really have to be  really good at linguistics and take many linguistics classes in order to decide which are the distinctions that matter and how to separate them out, and how to identify them in a corpus  instead, we'll take a more automated I'll call it , a more algorithmic approach to do this  that we can get a lot of the same effects without needing to learn too much linguistics. but you can add adverbs. The orders around sometimes  quickly ate a sandwich with a fork. in this vanilla Pcfg, there's no relation between any of these rules in terms of their probabilities. This work that  is really elegant and really nicely solve this problem within the this setup of probabilistic context, regrammars. really, the main problem with vanilla Pcfgs is that they make independence assumptions that are both too strong and too weak in different ways. It makes independence assumptions that are too strong, going vertically in the tree. Nps, what you're saying is really that, , you want to model more context up and down the tree around where the Np is, because that's how you can figure out if it's a subject Np. Will tend to be closer to the top of the tree where the node above it might be an S. Node. Would be the way we've been drawing syntax trees, it'll be closer to the bottom of the tree. And  that means we need to change the independence assumptions vertically to model more of the context. Rewrites to a personal pronoun node, or whatever. think of this as  a new symbol, a new non-terminal symbol in your vocabulary of your Pcfg. And again, you don't need to learn any linguistics to do this because this can be read off from the structure of the tree in the tree bank. And you're good to go. the   we're making the model more complex because we're weakening the independence assumptions. this is the 1st sentence of the Penn Tree Bank. Very sadly Piravincin passed away, , , last year or 2 years ago. , I'm going to stop here. Then for the horizontal problem. for every non terminal symbol on the left hand side. whereas before you have,  Vp rewrites to this entire sequence as a single unit which is atomic, which is not broken down, and you have  one parameter for its probability. And  you can again look into your tree bank and you have all of those trees that are already annotated. You have , you have to solve one n-gram modeling problem for every left-hand side symbol. and how would this still ensure that the sum of probabilities for the for every Vp on the left hand side is one. How do you still ensure that the sum of the probabilities for everything for one left hand side symbol still sums to one. You can show that if you add those, then you can get it to all work out  that it's still all the possibilities still sum up to one. If you're doing a diagram model. You can interpolate, do everything, everything we talked about with N. Gram language models you can do. But you don't trust any single one of them very much. Yes, in this example,  it's saying the line , Mp, Dta,  the 3rd bottom . that line doesn't mean that we have a proper noun that is followed by one of those 3? all we're saying by doing the factorization is rather than modeling the entire thing all at once, we're not gonna do that. That's that's just what we're doing. this process is called markovization. because we're making Markov independence assumptions. vertically speaking, that was called that would be called vertical markovization. the vanilla Pcfg would be called a 0 order vertical markovization model because you're not considering any context. But of course, usually in practice, people don't go that far because of your model is  expressive and  complex that it's you don't have enough data to estimate that. And then the other thing we're doing is horizontal markovization, which is breaking down the  hand side into parts. the standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter. You have the vertical marketization. Order of , , V equals. And then here's the horizontal Markovizations markup order of , here's the standard Pcfg with infinite context. And  with the standard Pcfg assumption that would be the top  of the table. Then you get  a improvement up to  79, and your model is a little bit more expressive , and it has more parameters to learn, but not  that, much more . wanted to clarify about the order, because you described that,  the one we did the 1st order. the question is about, what does the horizontal markup order mean? , the one we just described is H equals. you look at every pair. If you do H equals 2, it means you look at every triple. I'm presenting this, not because this is state of the art by any means, but  it's really nice in terms of how they take these intuitions from what we know about language, and transfer it into algorithms that we can. that's really nice, plus I don't have to introduce too much extra in terms of new technical approaches. And Gram language models which we already know about. there they do need domain expert knowledge. Yes, you have the horizontal markovization. how does horizontal markup position interact with Cnf. Cnf is  doing 1st order horizontal markovization. If you think about it. And then, since this is a Pcfg, you have to think about  how you do the conversion. Not Cnf,  it's something if you start with something that's already Cnf,  you only ever convert things to Cnf in order to run the parser,  you wouldn't  there are some linguistic theories and some approaches to syntax, where they demand everything to be binary branching. in that case, then,  you don't really need to do mark horizontal markovization, because everything is already binaries. , the process we  to convert the Cnf wouldn't apply on their probabilities, because you have to make sure the probabilities are mean. And make sure all the probabilities of the entire tree are preserved. but we don't know how to  I'll assign it to you as an exercise. I don't think it should be that hard. because if you think about it. because semantics is about meaning. if semantics is the meaning, if it's if the study of meaning and language, then we have to 1st ask, what does meaning mean? And here I'm going to give some. Very, I would say,  superficial answers. ,  what does meaning mean? And the 1st answer is that meaning is about the relationship of linguistic expressions to the real world, or at least to some world. And the second answer I'm going to give is that we can talk about meaning in terms of the relationship of linguistic expressions with respect to each other. And this is called lexical semantics. lexical means, it's related to words. These these ideas a little bit more. relationship of linguistic expressions to the world and to each other. But what does the word telephone mean. one way you can think about the meaning of telephone is, you can think about it as you're picking out all of the objects in the world that are telephones. and  these are called its reference. And then this is also called the extensional definition of a word. And that's the meaning of the word telephone. or if you can also, think about the meaning of words in different ways. suppose you had to define the word telephone to a 3 year old, or to a friendly Martian. one thing you could do is you can say, telephone, telephone, not telephone, not telephone, not telephone, but that would take a really long time. , you might look up the dictionary definition of a telephone. ,  here's the dictionary.com definition of telephone. , an apparatus system or process for transmission of sound or speech to a distant point, especially by an electric device. You can characterize the meaning of telephone. You already know what all of these other words mean,  apparatus, or sound or speech,  that could be a problem. But , this is the general idea behind the intentional definition of a word, which is that you're  listing the necessary and sufficient conditions for something to be a telephone. you can , express it using some logic, in which case you don't have to rely on other language. Whether it's a telephone or not. And  then you do need to know what everything else means, and you already have to have some  language capability to do to give this definition. It's just expressed that function is expressed in language that's all. ,  this is not a new idea. the, the properties associated with a word, whereas the reference is about the objects in the world that it points to. It's the same reference in the world. ,   they have different senses, but they have the same reference. And   the logical thing to ask is, can you have something with different reference with the same sets. and that I have to think about? That has a certain sense. But then, at different points in time. You can evaluate that, and it points to a different referent in the real world. ,  that was our very superficial look at detour into the philosophy of language. , ,  we talked about how to relate words to each other, but also how to relate words to the world. But we can say more about how words relate to each other. And it's much more popular in the computational side  to work in this as , which is to think about how the meanings of words relate to each other. I assume that most of you have heard of synonyms, ? If you prefer to think of it that way. it's  higher up in some taxonomy tree. , then, here I assume you've heard of these ones. and that there's no 2 words or expressions that truly mean exactly the same thing. See how are your offspring doing  ? it's really hard to say what does opposite mean? but  there's some dichotomy, or there's some spectrum. It's very difficult to separate synonyms from antonyms using many computational techniques. , another lexical semantic relation is something called homonomy. That's why telephone means telephone, because it's tele means far away. whereas homograph means same written form. But they're written the same. This one is again very interesting and comes with lots of problems for computational linguist to solve, which is called polysemy. , here poly means many and sem means meaning. You see how it's different. here the idea here is that the 1st one is more about the concept of that newspaper  New York Times is a newspaper in the 1st sense. Montreal Gazette is a newspaper  as in  that. But we just we don't even think about this  when we process language. What the intended meaning is in which sense of the word is intended. But then, at other points. the 1st sense of position given here is  the particular portion of space occupied by something. You can think about jobs within an overall abstract space of  a bunch of jobs in the organization. But sometimes it's really not obvious when you 1st think about and look at the words. You can do to create this. , more lexical, semantic relations. In fact, you don't get to keep the plates most of the time. Here's 1 that's very relevant to us. whose capital is located in Quebec City. The loony is at an 11 year low here. But if you start chatting with  your friends, or in a particular subgroup, or you'll you'll develop these very naturally you'll substitute words for other related words without even noticing it. , if metony becomes  popular that it becomes  a dictionary definition, does it stop being metronomy at that point. And the way we impose  try to impose some order on the phenomena we observe in language. here the metonyms in general is just about substituting with related words, but if the relation involves whole part relations, then it has a special name, for whatever reason. or many swear words involve sensored body parts. or many, many offensive expressions. don't be a censored body part. The  relation is Hallonomy and meronomy. ,  we just mentioned whole part relations. and there are different subtypes. You can have,  physical whole part relations  a car has a windshield. these are all whole part relations of different kinds. A windshield is not a car. What relation does that exhibit? I  in French,  I have mixed. I don't know how you some. and the other one is called. and  they came up with this resource, which is called wordnet, which is a lexical resource organized around this idea that words can have multiple senses. And in fact, they take it a bit further. the primary organization is in terms of senses. the idea behind Wordnet is that in the language. in Wordnet, there are 6 different synsets associated with the word table. each of this, each of these think of it as  a concept with a certain way that you can realize it in language using some words. It was a sturdy table. The  one is a piece of furniture with tableware for a meal laid out on it. This is  a this is a metonym which has become  conventionalized that it's  listed as a separate sense. it's it's  a blissimous thing. ,  this is how it's organized. You can look at  Member maronyms. this was just a backup in case the things didn't work online. And then you can look up synsets in Wordnet. figuring out which word sense is expressed in context. And  one computational task that you might want to solve is called word sense disambiguation. one reason might be, you think it's  in inherently interesting to figure out, how do we figure out the intended sense of all of these words that have multiple senses? But if you don't think it's inherently interesting. And that often different senses of a word should be translated differently when you translate to a different language. ,   I'll stop there and then  time we're going to look at algorithms for word sense disambiguation."
    ],
    "Topic 3": [
        "The latest version of the draft of Durafsky and Martin. I posted a previous version that still had this material. I would just add that a room  a reminder that,  each non-terminal symbol forms a categorical, you define a categorical distribution for each non-terminal symbol over all the rules with that non terminal symbol as the left hand side ? ,  what are your non terminal symbols? And the idea here is that you have a particular way of estimating the probabilities of rules through a maximum likelihood estimation. in English you have differences between I and me. in English all the pronouns that you have to change their form, you have to decline them according to the case, whether it's a subject or object. This  problem, this  issue is a less obvious one. And because of this, then again, you would expect the distribution of words that appear in these impacted positions to be different from each other. Is there not a way that we could create  a subject  phrase, and then an object,  phase and have different rules for all these to account for them. the question is for is for the 1st case, is there a way to create subject, noun, phrase versus object noun phrases, and account for those that way. Because if we take that approach. , here's a second problem. and you can likewise add  adverbs and prepositional phrases, and  forth, ate a sandwich, quickly, ate a sandwich with a fork. They all have to be estimated separately, just by counting the number of times you see each rule in your tree bank. And second of all, clearly, that's not really factorizing the problem in the  way. we would  to be able to factorize this probabilities . I'm going to present this paper. if you talk about  subjects versus object. On the other hand, this other problem, with  this sparsity issue with  too many rules is because we're making independence assumptions that are too weak horizontally. which is too weak of an independence assumption. what are we gonna do? But do it algorithmically, which is, we're gonna split up the categories vertically and split up the rules horizontally. And  you can algorithmically add all of these annotations and expand your set of non-terminal symbols. it's always a trade off. this is again, this  complexity versus data expressivity  this, this expressivity versus  a ability to estimates  sparsity trade-off. you would expect that you would need more data to learn reliable estimates of the parameters. that we think this trade-off might be worth it because it allows the model to be a lot more expressive. With the parent S would have its own categorical distribution at some point. But , , but how this would work is , we would annotate everything. You can just annotate that its parent is the root of the sentence. And  then you would keep annotating and hopefully, you  get the idea. But since you're a computer scientist hopefully, you can recognize this is really just a treat . ,  , you just look at what the parents category main category was, and you annotate it. ,  this is a quick fix for the vertical problem. , we're gonna pretend that every  hand side is a Mini Markov chain that we need to learn. ,  we're gonna we're gonna break down the  hand side of the rule when estimating its probabilities. I'm going to break it down. And then you can solve that learning problem. It's more about how you break down the rules. for  the interesting rule here is , Np, here rewrites to bt, jj, and n, ? you can also do a trigram model or a unigram model. interpolate interpolating means that you have multiple models. that you're  taking the in between of 2 models as your model. , you can do this, you can interpolate between a unigram model, a background model and a trigram model. And then you can take  I'll weight the unigram model with  a 0 point 5, and the background model with 0 point 3, and the tracking model with  0 point 2 or something  that. Here there are 3 words, and each of them has a tag. then it would be  Npm in , at least it would be  a non executive director. We're gonna model we're gonna  there's a start and end symbol. V less than equals to 2 is  they have a scheme to only annotate some parents, but not others. You can do all parents you can do selected grandparents, you can do all grandparents. And then here's the background model Unigram model. No, this is the background model. H equals 2 is a trigram model, and then H equals. How many constituents a parser trained in this way gets . That shows the number of parameters in the model. This has  a real effect on Parson. you would expect that  a different hyper parameter setting of , what markup order to use would result in the best performance. But the general idea,  still is, holds across different data sets, and  even in other cases beyond syntactic parsing. any other questions about syntax and parsing? although  in a slightly different way, and the probabilities are all messed up. this method and this procedure is about training of the model and learning the parameters of the model. It's for the cky parser and what it requires. and that might require a bit of thought. And  each non-terminal symbol only is involved in one rule. we've wrapped up structure and parsing. And if you're a language, technology, enthusiast, and you  you are. That's what I'm trying to say informally. I'm gonna give at least 2 answers. and then it gives you true or false. But it's by a computation of  properties or in practice, we're gonna if you use a dictionary definition, then you're defining words in terms of other words. That's just part of the dictionary definition. you can express that using some computation, some logic, or whatever you can express that with it, can take the form of a dictionary definition. one of the 1st people to propose it. at some point long in the past they saw this bright object in the sky this thing that looks  a star. but But here you have an example of something where there's the same referent we  know. The evening star is that bright thing in the sky that appears in the evening? this is the second  a main general area that people work in. ,  one way in which the meanings of words can be related to each other is through hypernomy and hyponomy. and Montreal is a city, and red wine is a beverage. these are is our relationships. You just have to remember these terms. The the way to remember,  is hyper is  higher up. such as offspring and descendant and spawn. because there's always some slight differences in, if nothing else. The connotations of the words . And these words appear at opposite ends of the spectrum or opposite sides of the dichotomy in some . the way to memorize this is to understand,  how to break down these words  homo, means same. There's a homophone which again break it down. homophone here means same sound. If you speak a widely spoken dialect of English, . And in particular, polysame involves multiple related meanings. the example here is the word newspaper. It turns out that newspaper has many, many different senses. It has many different related meanings. A newspaper can be a business firm that publishes newspapers. this newspaper is about the company. I'm not exactly sure how  the newspaper is the physical object that is the product of a newspaper publisher. When it began to rain he covered his head with a newspaper. What other newspapers do people read? they use bales on newspaper every day. and I'm sure that the boundary is a little bit fuzzy. and I'm sure we can reconstruct some chain such that, , it seems  they're related to each other. and then the second sense. I would argue that all of this is  these are instances of felicity. If you're trying to write clearly because I find that  If you're within a particular subgroup or subculture where you're used to talking about things in a certain way and taking shortcuts in your expressions, and it's really clear within that subgroup when you're trying to write for people from outside of that subgroup, you don't realize that you've made those shortcuts. there's been a substitution there. One is related to the example we just saw. Quebec City is cutting our budget again. The city itself is not capable of cutting budgets, because it's just a it's a city. This requires a lot of background knowledge and real world knowledge to fully understand  here the loony is not the physical coin. and then you'll have,  some way of speaking, that's very convenient and short. the way I remember this is , Hall sounds  hope. But that's how I remember it. and then you can also have,  a composition,  the substance composition of something physical,  a chair, is made of wood. Something can be meton  a metonym, and it can be a polysemis or . is Hallonomy versus meronomy, the same as hipernomy and hyponomy. whereas a whole part relationships. It's more  the elements you put in a class. They talk about these relationships, these lexical semantic relationships. and they organize everything in terms of these senses rather than in terms of the words themselves. They give a particular name to these senses, and they call them syn sets, which are, which is short for synonym sets. And here's a dictionary definition of it,  a set of data arranged in rows and columns  C table one. The tribe was relatively safe on the mesa, but they had to descend into the valley for water. A company of people assembled at a table for meal or game. And then each of these is its own synset. And , these are all of the synsets that these lexicographers have annotated in within this project over the past 2 decades or something. The  example sentence is due to her superior education."
    ],
    "Topic 4": [
        ",  today we're going to start talking about lexical semantics. 3rd edition seems to have taken out this material, but  it's  very important. That that's by following a probability distribution. Otherwise, the  bit is gonna not make any sense at all. then the and at the end of the last lecture we discussed probabilistic, context-free grammars in their most basic form, which sometimes people call vanilla Pcfgs informally. And that's by looking at a tree bank that has been annotated with all of these syntactic trees and looking at the structure of that and using that as a way to estimate parameters, the parameters of these categorical distributions that I just talked about. And it's because this approach doesn't model the context very  around the rules, and it's also pretty bad in that. The rules themselves are very sparse in the in the sense that there are many, many possible rules, and it's really difficult, even with a very large corpus. with noun phrases, , in English you can have a subject noun phrases, and you can have object noun phrases. But and in other languages they have a lot more of these cases. ,  if you speak Finnish, , then there's  15 or 16, or I've lost track of how many cases they have. then they should not follow the same probability. And this is not just in English, but it might be a cross lingual thing as , just because of,  how the world works and what people tend to, how people tend to describe what happens in the world. the fillers of the subject position tend to be nouns that are animate. and it's more likely cross linguistically, but in English and cross linguistically, that they appear in the subject position than the object position. And, in fact, that's exactly the solution that we'll explore. And there are also many other cases of obvious dependencies between very distant parts of the syntactic tree. Which which have effects on the word distributions, you would see. consider the subcategorization of verbs, but this time with modifiers as . we already talked about subcategorization of verbs, and that verbs can take different numbers of arguments. You can add prepositional phrases. the verb eats, , is  unique or not unique, but it's special, because it can be intransitive, but it can also be transitive,  you can eat a sandwich,  eat a sandwich. it seems intuitive that  that we rather we should have,  some probability that a verb is modified by an adverb. We already talked about how Hmms. with Pcfgs in that the vanilla version. , we're gonna take that what was already proposed. vertically, we're at, we can add more contexts by annotating parent categories. I'm going to use this notation where I use this carrot. In your supervised learning procedure in the Treebank data, you can directly see what was the parent node. Would that further decrease the individual probability? With the parent of S would have its own categorical distribution that sums to one. this, oh,  one thing I didn't explain is this is  just a tree represented in a different way using bracket notation. where all the children are expressed within the parentheses. And then  you can run the same procedure to do supervised learning. if I choose a Bigram model, then it would be broken down in this way it would be factorized in this way. But then, the technically, we've already covered all of these, all of this technique, because this is just an end ground model. , what is the star stand for? this is a common noun. there's a determiner, an adjective, and a noun, a single, a singular common noun. , let me show it in a more easy to read tree form. the number here represents the accuracy of parse. you would get  a 72.6 2 performance that way. and  you can do a lot better than that. , grandparent annotation with  a diagram model. And  that we can build into the computational side, into the formalism and into how things work. Even in their paper they  go beyond substantially beyond 79, through additional linguistic insights. We have to reconvert it to. That wouldn't apply here at. I'm pretty sure it's . And this is also this is arguably super interesting for both theoretical and practical reasons. we want to be able to have some formulation and representation of meaning in the world,  that we can represent and understand things and use Nlp systems to derive these representations. and then from there you can make draw inferences,  through some approach that's not necessarily only specific to Nlp in order to derive new conclusions and that can help you make decisions or do whatever you want to do. ,  then let's start off with a philosophy of  language. And we're going to start by focusing on the meanings of words. And later on we're going to start talking about the meanings of phrases and sentences, and also how to construct these representations from the meanings of words. And note the spelling here extension with an S.  think of it as , the meaning of telephone is that it's a function. ,   another thing you might do is if you're if the 3 year old you're talking to, or the friendly Martian you're talking to already speaks some language that you share in common. you would try to give a definition with other. what happens if the dictionary definition mentions a function that needs to be fulfilled? The intentional definition is different to the fictionality. But they're related to each other. The 1st was frege in 1892  he was one of the 1st to distinguish between the sense of a term, and its reference where the sense is more  the intentional stuff about the. And  then they called something the Morning Star versus the Evening Star. And then eventually, somebody was smart enough to figure out it's  the same thing in the world. And it was just Venus. And , it's not even a star. ,  the over, especially over time. And  all of these, a lot of these. You can express these in some logic as . And this is simply an is a relationship. the smaller thing,  the thing that denotes fewer things in the world is called the hyponym. and the word that denotes more things in the world is called a hypernym. it's lower down in your taxonomy. I'm gonna close the door. There's a whole comic based on this. But , we can pretend that synonymous exists. they're they're opposed to each other. , you need a lot of data in order to be able to do this. ,  this, this will be a problem for us later on. but different and unrelated meaning. The 1st one is about that physical object with things printed on it. But it's somehow different from this 3rd sense. which is different from the business itself, but which is also different from the printed version of that Of that construct. these are all very subtly distinct from each other. which is the cheap paper made from wood pulp used for printing newspapers. In the 1st sense, . , or  the 3rd sense, the 1st and 3rd sense are very close to me. these are all very subtly distinct from each other. It can be very difficult to distinguish between harmonymy versus Polysemy. It must seem  they're very unrelated to each other. that clearly seems related to the 1st sense. The 3rd sense is  a position in terms of a way of regarding situations or topics. and here it might not be immediately obvious whether these are related or unrelated. To me it seems  they're  related. It's just that one is about physical location, and the other is about  a location in some more abstract sense. You're creating this abstract space, and you're  locating points of view in that abstract space. Here's another one which is position to do with the relative standing of people in a society, again, is some  metaphorical extension of the physical idea of location to some abstract space. And then this last one is really interesting of  position as  a job in an organization  at 1st glance, it would seem to me that it's just entirely distinct from  the physical location, ? One is about physical locations, the other is about your job in an organization. But then,  it's through this  a again, it's through this metaphorical extension of that. And in that way, then it makes sense that this is a metaphorical extension. the connections between senses, but then,  some sentences are lost, some senses are lost over time, and those sentences are no longer used in the language, and then it really appears to be unrelated words. I worked for the local paper for 5 years. , here Quebec City is not referring to the city. It's a concept of  some geographical location or something. Instead, here it's referring to the governments, the provincial Government. and then the other people will understand, and then eventually there will be  a norm. , some of these are  ingrained that they appear in dictionaries. Is it a metonym, or is it  a polysimous word? if you are in the situation involving sailing boats or whatever, and the captain says all hands on deck the captain doesn't mean just literally, all of your hands must be on the deck. And , with Haronomy and Morontomy, is that similar to  we saw on the 1st slide about this. while they the structure is similar, because it's it seems to be about  greater things and smaller things. they are still distinct from each other, even though I agree that structurally you can represent them both with,  some  tree struck  to talk about  what's what's the whole? And you see how these are just distinct, ? which is distinct also, if sometimes you can tell if something is polysemous, if  another language, and you try to translate it into another language, and you have to use a different word. That's usually a clue that it's  it's  there are different senses. ,  that it's also business in French. And you've got one part of the midterm down. they started with English, because, . And then the edges in this graph correspond to lexical semantic relations between syn sets. here, , let's just focus on the nouns. Here is a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs. , the  one is flat table land with steep edges. that here cable is not the most common word used to for that sense, and it's usually a mess up the  1. He entertained the whole table with his witty remarks or the arrangements. She sets a fine table room and board. You can have a table of contents and actuarial table, a calendar file, allocation, table, periodic table, and  on. arguably, one of the most obvious things you can do. Once you have those resources, you can try to disambiguate a word into which word sense was meant by that word. His hands were tired from hours of typing. which is a noun of the 1st sense, the 1st sense of that. This represents another sense of hand. which is, it's not listed here. But it's  the handwriting, the handwriting style. it says,  , I said before, one obvious application is in machine translation."
    ],
    "Topic 5": [
        "first, st here's a review. for  there might be, say, say, you're looking at verb phrases. or the verb phrase rewrites to the verb plus the direct object. If it's a transitive verb. there are a whole bunch of different options you can have for what's underneath a verb phrase, node. ,  I hope that makes sense. if you still need me to clarify, please ask. Alpha rewrites to beta divided by the number of times. to get enough statistical to get enough data to have reliable statistical estimates of the parameters of these distributions. and it's very clear that the distributions of them are not the same. Because all you see is the Np on the left hand side. in particular, in the subject position. because there's a correlation between subject position and some semantic role of  entities that tend to do things or  have actions in the world and cause changes in the world, whereas the object position, there's a correlation. Again, it's not perfect that those tend to be entities that receive some action or suffer some consequences of something and  forth. You can even change the order. first, st that means you need a lot more data to get enough samples to estimate the parameters. in order to both get more samples for each to estimate the parameters, and also just intuitively, it seems to make sense for this particular problem. , it's the same  thing here. , , the subject Np. where its parent will be a verb phrase node. instead, we, we can again change our assumptions and make a stronger independence assumption,  that we can more reliably estimate those parameters. whereas before you might only model Np. With a parent of S,  this would correspond to a subject position for a noun phrase. With a subject  with a parent of Vp. Which might correspond to the object, position. and then otherwise everything else works the same. And it has its own distributions, which are different from the ones of just the Np. would it still work out? one thing you can try to combat this sparsity issue is to try to do some  interpolation or smoothing. that you still allow everything that is a empty node to share information with each other through some interpolation scheme. here the S node at the root. and then np, here is parent would be the S. And then this Np. Its parent would be an Np,  you don't do this recursively, ,  you could do it for more than one level. But you won't , but you don't do it   Np, Np. and s, or whatever, unless you are planning to go 2 levels out. You get the idea ? it's through adding the start and end symbols. jj, and n. ,  , what we're gonna say is that this is  going to be equal to Probability of Np. can you say this again? Np is  here the original tree structure is, you have an Np subtree, and then within the Np. you have an Np at the root. and then you have a Dt, and then a jj. start dte 1st and then Np. Because you're adding ancestors as contexts. 0 is  the unigram model, and H less than or equal to do is again, they have some heuristics to select, only some things to break down and not others. and  it makes a difference in terms of the pricing performance. because that would be infinites, Markov order horizontally and no annotations vertically. One is 1st order, which corresponds to a diagram model. , I just want to know if from this data, if this  set oh, God. it is somewhat specific in that. each data set with each annotation scheme has its own characteristics and distributional patterns. in that sense it's specific. They can do further category splits and merges to get to around 87 f. 1 performance. It also still works, even if we do. In Cnf with just the 2  hand side values. that is a great question. , to make things distinct and clear, I would keep it separate from this. And then the Cnf thing is for the parser. Then you can convert all the rules to Cnf. But what if we started with something that was already in Cnf. which is one of the key Cnf assumptions. Oh, the question is, when you convert things to Cnf, then  that wouldn't be. I would have to do some thinking about it, but I'm pretty sure there's a way to still convert things to Cnf. You can work it out. you can work it out if you spend a few minutes thinking about it. ,  where are we in the course? Since you're in this course, you want to do things in the world. and things mean things  words mean things in the world. , semantics is super important because it's a, it's   the representation of something in the world  that you can  interact with the world with your technology. , it's just a stretch. but that these superficial answers will at least give us some beginnings of  guidelines for how we might want to model things computationally. can we at least have some examples. ,  I need to change my slides, because, , telephone is not a word that's used very much anymore, . ,  here's a set of items that are telephones. thought there was a way to follow the link. It's almost  , you're still taking in an object. It checks a bunch of conditions of ? Are these necessary conditions satisfied? Yes, what happens if dilutionary definition? The intentional definition is  talking about the conditions, the necessary and sufficient conditions for something to mean something. this distinction between sense and reference. However, there are different senses, because the morning star might be that bright thing in the sky that appears in the morning. the Prime Minister of Canada. Things that mean the same thing. , a monkey is a mammal. These are synonymy and autonomy. synonym means that 2 words roughly mean the same thing. It would be very strange if you go up to  a friend of yours and say, Oh, Hello! You don't say that ? you say, children in typical scenarios. I forget the name of this comic, but the whole humor from that comic comes from rephrasing common, everyday expressions, using paraphrases that are supposed to mean the same thing. And it's  it's it's still useful to come up with this idea of some synonyms of words that roughly mean the same thing. Then antonyms and autonomy is words that roughly mean the opposite thing. , , synonym and antonym are antonyms of each other. or happy and sad or descendant and ancestor. Antonym, , , , it's  same. It's about same and difference. there are different kinds of homonyms. these are both subtypes of homonyms. and then these are all different from the 4th sense of the word. because it's clear to us in context. one example is with a word such as position. and there are many different senses here of position. And then there's position with about the arrangement of the body and its limbs. and it's even possible that there's originally a chain of  of this reasoning. ,  I would say that these are not always as clear cut as you might think. And then  you start to. you're substituting one entity for another related one another. You don't mean that you work for the  the actual piece of paper, the print, ? You work for the organization that produces that paper. and what it means when you say it's at an 11 year low is that you're comparing it against another, its exchange rate with another currency,  the Us. these are relatively  standardized examples. Oh, that's a great question. I would say it's both a metonym and also that word is felicimous with multiple senses. it's a specific  autonomy involving whole part relations. , they mean your entire person has to be on the deck. And then maronyms are the part. I don't know if that's etymologically correct. you can have groups and members  a class consists of students. Yes, all of these definitions are not necessarily mutually exclusive. are all of these definitions mutually exclusive. we showed here that it's  not. , , that's a great question. it's about is our relationships  in programming, in object oriented programming. It's  it makes ups that thing. Yes,  there are, which is a some  homonyms. the head is the holyms, and the hair is the marinym enemy. And above you just said it. The word I would use for cutting. I don't speak French natively, but the word I would use to cut bread is  Tranche. But I don't know if you can coupe a bread. Then you have nodes, and these nodes are called SIM sets, and they can be expressed in through many different realizations  with actual words. But it's not as extensive and not as , not as connected. I hope this works, there is a web interface. Oh, , it's still there. Give me a word that I can safely type here. If you click on one of these  if I click on the first, st since that it gives you its connections to other synsets  it has a direct hyponym or a full hyponym. and then there are some useful functions you can use,  you need to import it and download it. and it corresponds to  your actual hand physically. Her hand was flowing and graceful. And the general idea here is that you can use the words in those contexts to help you disambiguate. the fact that you see the word tired is  informative and typing. Another question that you can ask, or  you should always ask, is , why do we want to solve words and disambiguation? You can still try to come up with application oriented reasons to work on this task."
    ]
}