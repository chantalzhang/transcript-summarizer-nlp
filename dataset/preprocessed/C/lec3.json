{
    "Topic 1": [
        "I have to speak louder. you  also did some feature selection, how to come up with features for your classifier. last week  we described classification where you have an input X.  an input X can be a vector, can be a matrix, can be anything. And  that you can split your training data into different chunks or subsets. And then you can train on the 1st 400 and evaluate on another subset. You train on K -, 1, and then you evaluate on the last one. Do you mean by a? Should it be 0.5 S? And then you can have an equal split of the range of values between 0.01 and one. And then after training your model on using this different hyperparameter, you can choose what would be the best hyperparameter in that case. every method have some theoretical back in and they have some hyperparameter that needs to be turned. In the case of neural networks, you have things  learning rates, but you don't know what should be the  value of the learning rate. OK, It's used to be very, very, very popular,   10 years ago. And then if you have very basic tasks  test classification tasks,  topic classification or sentiment classification, you can easily run this package on it. And then you can  practice some of these things you have been taught because #1 if you have a test input, you have to determine what will be the features. this  package allows you to try out different feature selection. But nowadays we don't really do a lot of feature selection because we learn a big model  neural networks that  tries to learn the features automatically given a text. It just tries to learn what are important features based on this. OK. OK. Today we'll talk about how to train a classifier on a training set. , last week  this will be another revision where we already have  feature extraction. in the 90s or 80s, people have developed ,  many methods. One of the most popular methods in the 90s rather was support vector machine and became  popular that people even didn't care about neural networks because neural networks was not making progress for  many years and they were making a lot of progress with support vector machines. And if you are mathematically inclined, you  something that you have an exact proof and  how your model will behave. I would encourage you to also try this out. OK,  I believe the first equation here you can do the math the probability of Y given X&X. Here you have to look at it as a vector or  at minimum at the vector because X can be the set of features. Let's start with the drive probability of a. here we have  see this as the. can you all see it? And this is the exact thing we did here. Probability of X is estimating the entire probability over all the set of features. The idea there are two assumptions, but the most important assumption is this independence assumption. X is a vector, you have X1X2 to SK and then a probability of XY is  a probability of Y and the product of probability X given Y. If you do the math . , because it depends on the previous thing. it's  you want, if you pick an English Dictionary, whether the one from Oxford or Cambridge, all the words there is  this is  a dictionary. OK, how do we train a naive based classifier? this is a product, but if you put log it to be a sum, you are computing this product over your entire data set. you just have to sum over the joints to get a probability of X is a vector of the features. Is the a or not a? the internal answer is this. Yes, this is my approach. P of X of the vector. And that's 1 / 3. And that's how I got the first one. It should be 1 / 3. . OK,  quickly I will rush through the remaining of the slide before we go out of time. the idea of type is that the identity of the word, which is the count of unique words and token is every single instance. for an example is below, you have YO appearing 2 times, ? There's an important distinction between that. If you can get the Theta, you already solved the problem. But of course, there are two ways of thinking about a problem. It's a big question on how do you get your features. , you can say for different tasks  nameless recognition, you can say capitalization is very important, can be an example of a feature, the length of the document can be an example of a feature, the number of stop walls can be an example of a feature, and anything can be a feature. if money is always associated with spam, then money will be a good feature for detecting spam. And if you had a log to this, you can have what is called the log likelihood. And once you have done that, you can  optimize this using gradient descent. The log of the product of probability will be the sum of the log of the probabilities. OK, the last thing I want to talk about is support vector machine, which is a very, very important algorithm. you have the blue dots and then you have the red dots. All you care about is just solving the problem. It's very simple to do this in practice. people do not believe in this. Number one, they can learn very complex functions. There is only a theoretical proof that says that they can learn any function. Given enough training data, they tend to perform very . even typically your large language models  ChatGPT are still based on neural networks. For current language models, people are trading for over six months, over a year, and if something is wrong, we have to start that over again. It's really expensive and often requires a lot of data."
    ],
    "Topic 2": [
        "who can remind me what you have been taught last week? what's the difference between classification and regression task? All ,  today we'll continue with, we'll start with linear classifiers. Output Y can be a discrete outcome or categorical variable. we discussed linear regression last week. It could be as simple as linear regression or more complicated as artificial neural networks. we already differentiated what's the difference between classification and regression. let me ask a question. I know you may know this  before, but , that's what is that validation or test set  they can generalize, not sure. All ,  quickly, let me go through cross validation. And  you can split it into  100 hundred 100. Let's make it very simple. Let's assume we have  threefold cross validation. Or  you split it into K folds. It could be you're trying to pick an hyperparameter and then by doing this K4 cross validation you'll be able to choose what will be the  hyperparameter even if you don't have a validation set. since you are not sure, you can try to tune your model to pick what is the  hyperparameter. , if you are training a simple spam classifier for SVM, we have a hyperparameter and then you can say let's set this to 0.01. And then you have the average error for that hyperparameter. OK, I hope that answers your question. support vector was very popular, but also it has limitation that it can only work very  on very basic tasks  test classification. But I will argue that even if you build a classifier for a very basic task, you should still have some of these baselines. Here's a very simple task. And then this task was not very trivial. This can also be written as this,  if you just  replace this with XY,  I believe you'll be able to see it. this is the original formulation for. , OK.  and this is what we call you assume independence between the variables. The way it's been constructed is that you have the label and then you provide an example. This is the generation process. And for the spam e-mail that has been generated, you can  say each of the words which are features are independent. It's  all the possible combination of words that you have in a language. Multiply the products you have on the individual which also consists of the products across all the features. Do you have a question? if you reuse this formulation on the board,  you will be able to do the same. For a new document you want to classify or for a new sentence you want to classify, you just reuse the base rule probability of Y given X equals the probability of the joints divided by P of X. to calculate P of X, the normal thing you are going to do is to marginalized over all the random variables. OK, I know it's a bit a lot of content but this is an exercise. I will give you  5 minutes table of whether a student will get a or not based on their habits. you have if a student reviews notes, does assignments and asks questions and this is the grade,  what is the probability that this student gets an A? Do you have a question on the task? What will be the great for this student? What the answer is the grade. OK, OK. How did you arrive at not a? OK, all , I will give you an int of the answer. A. OK, let me start from the basic. my calculation is even wrong. You only have one N out of the first 3 where you have a grade of A and that's another 1 / 3. And the last one asks question. You have Y, which is yes, ask question out of three possibilities. Let me just update the slide. The only mistake here is just where I have 2 / 3. And then you're still going to repeat it because this is computed over the probability of token, not the probability of types when we are doing this  calculation. another distinction is generative versus discriminative task. At the  class, we're going to try to do a discriminative task instead of a generative task. for the task of logistic regression. the idea again, although it has regression in the name, it is not a regression technique, it is a classification technique. And what when you want to try to solve which is also called logic? When you want to try to solve a problem using logistic regression? Here I want to clarify that features can be anything. you can design your feature. I hope this is clear. I hope I'm saying it . Do you have question on logistic regression? I was, I was a bit in a hurry because of the time, but do you have a question I can try to clarify very quickly? how can we draw this line that separates the two examples? And this is the idea of, this is the major idea of the SVM. our SVM is generative for a discriminative model. You can try logistic regression, SVM, boosting and  many techniques. OK,  the last one here is the perceptron that leads us to it's a very simple extension of the logistic regression. Because neural network is more  a generalization of this concept for artificial neural network, there are  many people that believe that it's inspired by the human brain. you can completely ignore everything I say here. Some people believe that there's some interaction about you have the neuron and the dendrites and the ASEAN in the brain. in the  class I can ask you about what  about K nearest neighbor decision trees, random forests, and  on."
    ],
    "Topic 3": [
        ", , I didn't post it this morning, but I will post it after the class. who wants to comment on why do you need things  lemmatization or stemming one beauty features? Why do you need this? We have a principle called cross validation which is also used for model selection. Is this simple enough or do you have questions? Theta is all your set of weights or your parameters you want to learn in the model. And some parameters cannot be learned automatically, you have to fix them. in the case of SVM, there are some parameters that you have to just fix and you don't know this parameter, but you  theoretically know the set of values that this parameter can take. Then you select which one has the highest accuracy on this, and  you can  pick that hyper parameter as every parameter you're going to use to train the entire model. , it's going to be different model, but you're going to just train on the same data. you trained for the experiment one, you're going to have Model 1. It will be different models. But the most important thing that you're evaluating here will be what is your validation accuracy across the different experiments which you are going to average for a particular hyperparameter. Yes, yes, you would choose the model, you would choose the hyperparameter with the best validation accuracy, yes. How many people are not familiar with SKLN? You have to use principles  TFIDF, which is a way to normalize your data  that you don't have unimportant tokens. this is a very simple way. this will give you the logic and then you can add a softmax on this to determine what will be the probability. But today we are focusing on linear models. That means you can  get the true parameter of your model and then you can prove it. And for  many years it was very difficult to beat this simple baseline of naive base. But nowadays  you can fairly beat them easily. you don't need to build an LSTM or a transform architecture to have a 90% accuracy, while if you just train naive base you can already achieve that score. You can even for using these basic methods, you can already achieve close to 98% out of 100. OK,  for naive base,  I said, when you're learning a model, you need to add  a parameter teeter. Teeter can be all the weights you want to learn. if you have a very, very simple logistic regression classifier, you just need to learn things  what is the bias of the model and things  that. even for this  basic model, you still need an, you still need a set of parameters. OK, do we have questions? OK, yes, Theta is a parameter. OK.  everyone is familiar with Bayes rule. If you are trying to do  a spam classification, every feature can be  the types. If you lemmatize then different words,  let's pick an English word, use, and then you can have different variants used using and all that will be converted to words to the root word. I'm going to give an example that is more concrete, but I want you to look at X as not just a single variable, but  a vector of the different features you want to use to learn your probability of Y given X. And then if you do the math,  the probability of a joint of a conditional probability would be probability of the joint divided by the one that is conditioned on. this is a very simple Bayes rule, and if you don't know it, just master it. It's as simple as that. Probability of Y given X is probability of XY, the value of probability of X. Then you can rearrange this using the same idea and then you can  say this is probability of Y multiplied by probability of yx given Y divided by probability of X. I can use the board if it's not clear,  it's just a way of rewriting the same thing. Rule you need to follow. Probability of AB equals probability of a, divide given B, probability of B, and then if you do the math, probability of A given B will  be probability of the joint AB divided by probability of B. probability of Y given X equals probability of XY, probability of X. Yes, they are exactly the same. And that's the same idea we used in  rearranging it here,  that you  have  saying probability of XY will not be written as probability of Y, probability of X given Y and then you still have probability of X below. But for the naive base, typically we don't estimate this probability. The reason is because this is constant when you are trying to pick what is the best, what is the best Y given X. I'm going to explain a bit further. naive base can be seen as a simple generating model and I'm going to show you the data generating process in a minute. this time we written as probability of X1X 2X3,  to XI given I.  the independent assumption says if two random variables are independent, probability of AB equals to probability of A. Probability. This is why you say. This is what it means if you say A&B are independent given C and you assume conditional independence. this would be probability of a given , given C multiplied by probability of B given C and then. This would be probability of Y, probability of X1 given Y, probability of X2 given Y, and  on the probability of XI given Y. then  you have a product between probability of XI given Y and it's a very strong assumption which doesn't hold everything is that you assume all the features are independent of the output. if you assume this independence assumption, it will fail for some applications and that's why people decide to work on better algorithms for other data sets. here for the naive based model parameters, the parameters to the model Theta consist of what we call the prior class, which is probability of Y. We call this prior class because although we want to find the probability of Y given X, we make an assumption that we are generating this data as a generative model. That first you have less as you want to train a data, , you want to construct a data. it's  you want to create an example of a spam e-mail and then you say, OK, for spam give me an example of a spam e-mail or for no spam, give me an example of no spam. that means every word in it can  lead you to predict that this is spam. This is the idea because you are saying that they are independent of each other. And because you start with Y, we call it the prior class distribution. parameters of each feature's distribution are conditioned on the class. And this is the probability of each feature XI given the class. if you have  a discrete data, we assume that a distribution of P of Y&P XI are given Y are categorical distribution. This is a very simple assumption. for categorical distribution, an example, a very good example of a categorical distribution, if you still remember your stats probability theory is Bernoulli distribution. The idea of probability is that  that the probability must sum up to 1.  for a categorical random variable, it follows this distribution if it can take one of the K outcome each with a certain probability. But typically you can also assume that this categorical distribution is a multinomial distribution because you don't have two outcomes, you can have more than two outcomes. For some applications it could be  30,000, for some we could  100,000. And this will  signify all the features that you need to estimate the probability for. that we have to compute the likelihood. For every X&Y you pick from your data, you want to estimate what is the probability of the joint probability. If you assume this independent assumption for each of your data, then you are going to have the product over all the data. OK, for a categorical distribution, if you really want to train your naive base classifier, you want to say what's the probability of AY as taking one of these values? what's the probability that Y takes a value of spam or Y taking the value of no spam for this  binary classification task and the same thing. You are going to do the same thing for this discrete probability, which is what's the probability of X taking the value of each of the features given the value of the Y you want to predict? Yes, OK. ; Is just saying that you need, you need to find this parameter of Theta to estimate this joint probability. If you can find this Theta, you can estimate the joint probability of PX, Y and that's why it's not together because if it's together it means something else. And for the training,  that for every after you have trained the model, you compute the likelihood over the entire data. And then you assume that every Y we pick a value whether spam or no spam, which is our running example. There's another rule that I have not written on the board to marginalized is you marginalized over the joint probability. That's a very simple example over the joints, OK,  and that's how you apply the base rule. Remember all the features are independent and I'm going to take you through a running example. this helps to clarify things with an example. OK. OK, this is an example, , probability of PX1, . Of XY equals to 0 plus probability of XY. I want to see if you can attempt it. Is that a yes or no? Is that a yes or no? Using the knowledge of Bayes rule, can you compute what would be the probability of Y equals grade A or not A given these three features which is review notes, does assignment and ask questions. that means you have to compute. you have to compute what you have below. What's probability of Y given X, which is the product of probability of Y and the product of the conditional. You can get a probability of X if you use the marginal, but you can ignore it. probability of X is the marginal distribution. in that case you have to compute the joint probability of every X. , OK, I get what you mean. typically for naive base, it's not that you cannot estimate it, but typically you don't need to. Because you need to compute this over every single features and the calculation for every feature is different, ? the two probabilities, if you can give me the probability of a given X and probability of not a given X, what are your probabilities? that for you have to compute what is the probability of Y, , what's the probability of grade of y = a given? And then you compute what's the probability of y = A given those assignments, probability of a given as question. And if you apply this rule,  you want to compare these two probabilities, ? Probability of y = A and probability of Y equals not a given X, ? This is what you want to compute. Then  that if you do the math, this probability of y = A equals probability of X, which is a vector given Y equals a.  that if you do the math, , what is the probability of what's probability of review notes? What's the probability of review notes given grade? What's the probability that Y is equals to A and this 3 / 5, ? And what's the probability of not a 2 / 5, ? what's the probability of review notes equals yes, given grade equals a what, 2 / 3? And then what's probability of DOS assignment? What's the probability of DOS assignment given grade equals A2 over 3? And what's the probability of ask question given probability of y = a one over 3.  we're able to compute the probability. Because once you're supposed to compute is not yes, yes, yes, all the time. What you're supposed to compute is probability of. It doesn't review notes and probability of no assignment and probability of ask question, . if you say, what is the probability of? if you say, and this is if you compute these individual probabilities, , , 2 / 3 should be 1 / 3, OK, . you're going to have 1 / 45.  what's the probability? Dozen review notes, Probability of dozen review notes is if you pick the first tree, the first tree lines, Yes, appears two times, . And if it's no, it will be one over the probability,  the number of times you have grade and that's 1 / 3, ? , the one that has the highest probability between probability of. Equals and probability of y = a or probability and probability of Y equals not a. you have OK.  the way you compute it is what's the probability of N in the first column? And if you compare all the ones that has grade of A  you have 1 / 3 and the other one, what's probability of no assignment? And then the other part, which is probability of Y equals not A, you're going to do the same. The probability of not a is 2 / 5, ? No, no, it's not Theta, but that's why I said ignore probability of X, because if you put it in inequality, you can just remove them, ? If you're comparing, you can remove them. OK. , No, it's not going to sum to one because you have not estimated the probability of X, ? Yes, yes, if you estimate the probability of X. What I'm just saying is that even without computing the probability of X, you can already decide which one is bigger. And I hope this gives you an idea of all the maps we wrote on the book. if you have completed the probability of that token, you're also going to say what's the probability of spam, probability of YO given spam. for a generative model, we learn a distribution for all of the random variables involved, which is a joint distribution probability of X, Y. And for a discriminative model, the only thing you care about is the parameter. You learn the parameter Theta. and that is the idea for most algorithm ML algorithms that have been developed, they just focus on how do you estimate the Theta such a way that the loss is low. And once you can do this, who cares about having the estimation of the  probability, because sometimes the probability of XI just showed you. Sometimes is not very easy to compute for some application. Because if you can get what is the  prior distribution, what is the  distribution that you can use to fit your data, then you can get a very accurate result. linear regression has continuous values, but for you for you to make it a probability distribution, you have to apply a simple function that will convert the output into a probability distribution  that everything can sum to 1.  the probability mass function can be written as this. OK,  this is how it is if you convert it to a probability  that  everything can go between zero and one. I'm going to show you how you will compute what is the likelihood in logistic regression in a minute. in the example I showed you here, we use a very simple example where the features are  review notes, dose assignment, and ask question. How many times does this word appear? How many times does that word appear? if the word yo that we say yes signifies is more correlated with spam, if it appears  many times in your document, it should be a good feature. for the parameters of the logistic regression here, the Theta will just be once you fit the model, you need to compute what is the parameters. , it's very simple, all the values A1A2A3 to an and of course and the intercept. You want to compute what's the probability of Y given X but given this parameter of Theta because  you want to find those parameters and those parameters in the logistic regression are A1A2A3A to an. And log likelihood is very important in NLP because it's what you use to compute things  publicity of a language model and  many things  that. and the idea here is that if you say if you have log of the products of two probability, this will give you the sum of the probabilities. There's no assumption of what is the probability distribution on this. What is the best hyper parameter? how do you decide naive based logistic regression? Naive base is your good. It's always a good bet and you can  do the math. The way they interact also  model how artificial neural network works. And this is  one of the most successful algorithm that we have currently in machine learning."
    ],
    "Topic 4": [
        "you don't have output for the unsupervised. What do you mean by non necessary features? Then you want to learn a function F that will give you the output Y. , if you want to know you want to classify a particular sentence into spam or non spam  you can learn a function F, It could be a simple function  linear regression. Or it can be a more complicated function  a deep learning algorithm. It could be a transformer architecture. you want to learn a function. Test your model or , learning function. let's assume you have a very small data and you don't have you only have training data. You don't have another split for test data, another split for validation data. And then you can, let's assume you have  500 samples and you have  5 fold validation. You can split it to generalize. when you are trying to learn a function  this, typically you have X, Theta. You try the second hyperparameter, you try the third, you try the fourth, you try the fifth, it could be 10. And if you don't know the hyperparameter, you can have a set of values. you have to give it different set of values and then try it out to determine which would be the best hyperparameter. OK,  this is a very small example which you can try. Because the best models we have, even neural networks, sometimes they still struggle on some tasks. And you could, it's a very small package which you can install. last week we also talked about things  stop words, and sometimes you can  exclude these stop words because they don't influence the performance of your model. If you install this package SQL line and then you have a very small data X here you just  have two features 2 dimensional and then you have this output which is either 0 or one and then you can fit the model and predict. You can determine the features you want to use for a particular text, and then your inputs can be a document or a sentence, and then you have a document label. OK,  if you think a little bit abstractly on this, your function can be any classifier. And that's one of the reasons why we have this  packages  SQL. Another popular technique is naive base. Although  you don't need it  much these days, but it's good to try out. Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. likelihood is  you're learning a probability, what is the best probability that can fit this data? that's the idea of likelihood and the probability, the higher the better, the closer you are to one. And then you have use and then you have different features or different words that can be categorized as your features in other tasks. I don't know if yes. for each sample, that means for each sample of Y you generate a vector X by generating each feature independently conditioned on Y.  that means every feature of X is independent. The way it's been generated is  this. And you are able to do this because they are independent. that's the data generation process. It's called Bernoulli because you have just two outcomes and then you perform this experiment once. If you perform the experiment multiple times, then you have  a binomial distribution. what is the likelihood on fitting? The idea of likelihood is that you want to fit on all your training data. And that's why we are computing this sum. You can  predict a new. And then you also have different values for the features and then you marginalized over. that's the how to marginalized. if you have if Y can take 2 values. You have a nominal data and a Bernoulli distribution. Bernoulli distribution I told you has two values. If and that's the last one, it doesn't review notes. He has not performed any assignments but always ask questions. , but I can assure you that for this task, even if you don't compute it, you will still get your answer. I don't know if anybody was able to solve it. The one that has the highest probability will be your answer. here you have, I don't think my calculation OK.   one, I have two, which should be 1 / 3, one over three, 1 / 3 and then you have 1 / 45 and then you compare to the second one. That means it doesn't review notes. And the probability of dozens review notes will be 1 / 2 because you have two appearances of not A and that's 1 / 2, the second one 1 / 2, and the third one is 1 / 2. And then if you compare these two, which one is greater 1 / 45 and 1 / 20? 1 / 20 is greater than 1 / 45. And that's how the answer is not a. All ,  the answer is not a. please take note of that. But early days of machine learning, most researchers are split into two. Some are only working on discriminative tasks because they just want to have the best model. We typically just default to what we have, which is the words we have and then use them as features because that's all we have. And then we can do counting. we typically just default to this. OK,  in practice the features depend on both the documents and the proposed class. And our idea is that we want to compute what is the likelihood, what is the condition and likelihood on our training campus, which is very similar to what we did in the naive base. I won't go too much into this because this is more of a machine learning concept, but if you do the math,  you'll be able to arrange this very quickly with this idea of the products. OK,  another technique that is very proper. And then here we can have a hard margin, which is the red 1 or a stuffed margin, which is  the green one, because you can have some dots that will be very, very close to that. SVM can work  in different tasks and settings, usually giving very little training data. What you have to do is to try different algorithms. Here we generalize the A1A2A3 that I told you into what is called the weight matrix. And here you can also stack different perceptions together. And from this is where we  move to what is called neural network. One, we have to we have some advantages of neural network. They can learn any function. And there are many different network structures that you can  come up with for neural networks. Better than Transformer are also leveraging this idea of neural networks. The disadvantage of this is that training can take a very long time."
    ],
    "Topic 5": [
        "I. Hello I'm trying to fix this thing and it's not working. Hi everyone,  for the delay in starting the lecture. last week was Jackie, I  introduced you to test classification. what's the difference between supervised and unsupervised learning? what's the difference between supervised, unsupervised? you  want to reduce the number of types that you have  that you have  more compact vocabulary. , you want to detect if a movie has a positive or a negative sentiment and for clustering you want to just give it an input. You want to cluster the data into different categories. What is the use of training set, a validation set or a test set? OK, you want to try validation set? , that's would be the test set. You can split the training data into K folds and then test. , OK,  for the first fold, you pick the first fold as your test set and then you train on fold 2 and fold three. You combine them together and train. The experiment too is that you train on fold one and fold 3 and then you evaluate on fold 2. And the last one is that you can train on fold one and fold 2 and then evaluate on fold 3. And by doing this you can  aggregate the test accuracies and then try to pick your model parameters. it could take  between  0.01 and one or not bigger than one. And then you train for the K fold. Or roughly equals size because. Every training results into a different model. OK, you have to set it as a person performing the experiment. you have to have a vocabulary. And one of the reason is because it's convex. , most test classification data set  sentiment or spam detection. If  about the Oron is it Eron data set, it's a very popular spam detection data set. the idea of training is that you want to select the hyperparameter that minimize the error on your training set or maximize the likelihood on your training data. You maximize the likelihood, but you minimize the error. And for the error, , the lowest you can get is 0.  if you continue to minimize your error, then you can get a very good model. see those two things as the rules you need to follow. we just changed the variable. that means if you have X to be 5 different features, they are not dependent on each other. This is a set of variables,  X1 to X5. Who can give me an example of where this will not hold this independence assumption time series and you appreciate? if you have, if you are flipping a coin, this is a very good example of a Bernoulli distribution. for natural language processing our outcomes, we assume that we have very, very large features, which is the size of our vocabulary and the size of our vocabulary. It's just read it multiple times. Then I will show you the my own solution. It doesn't affect your results because it's constant. , this should be 1 / 3, and then the other one should be 1 / 3, and the last one would be 1 / 3.  if you say, , OK, that's correct. in order to show, it should be 3 / 5 * 1 / 3 * 1 / 3 * 1 / 3, and then you're supposed to have 1 / 45. OK, I'm not able to edit on this laptop, but ,  1 / 45. Which one are you talking about? The notation seems correct to me. that if you already have this  this table of results, you can compute all the different probabilities and at the test, at the test time, you can estimate the probability for the new document and then decide what would be the category. there's a simple distinction between type and token. Some continue to focus on how do you estimate the  probability. But nowadays  people that focus on discriminative seems to win. Does the document contain the word money? And  that you want to project your data in a high dimensional space and then you want to draw a line that separates the different classes. Can we, if we project it in a high dimensional space, are we able to draw a line that clearly separate them? And this line can be a very hard margin or a soft margin. What is the best parameter of the model that can help you to separate your classes in a high dimensional space? And  we focus on what is the  weight matrix and the  intercept that will give me that can be used to fit my data. as you go on in your career, you can come up with another fancy architecture that will become famous. Sometimes you can train a machine translation model for a month. OK. And of course, we have different other kinds of classification algorithms that will not be covered, but you can read them up  K nearest neighbor decision trees."
    ]
}