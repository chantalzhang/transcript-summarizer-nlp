{
    "Topic 1": [
        ", . None. None, None. . No, None. None. None, . I think last week was Jackie, I probably introduced you to test classification. . All . Maybe I have to speak louder. . what's the difference between classification and regression task? . All . who wants to comment on why do you need things  lemmatization or stemming one beauty features? . What do you mean by non necessary features? you  want to reduce the number of types that you have  that you have  more compact vocabulary. . Then you want to learn a function F that will give you the output Y. For example, if you want to know you want to classify a particular sentence into spam or non spam  you can learn a function F, It could be a simple function  linear regression. you want to learn a function. I think we already differentiated what's the difference between classification and regression. What can you do? Do you mean by a? when you are trying to learn a function  this, typically you have X, Theta. Theta is all your set of weights or your parameters you want to learn in the model. in the case of SVM, there are some parameters that you have to just fix and you don't know this parameter, but you  theoretically know the set of values that this parameter can take. Should it be 1? Should it be 0.5 S? You try the second hyperparameter, you try the third, you try the fourth, you try the fifth, it could be 10. Is that correct? . Every training results into a different model. But the most important thing that you're evaluating here will be what is your validation accuracy across the different experiments which you are going to average for a particular hyperparameter. And if you don't know the hyperparameter, you can have a set of values. All . OK, It's used to be very, very, very popular,  let's say 10 years ago. . For. And then you can  practice some of these things you have been taught because #1 if you have a test input, you have to determine what will be the features. you have to have a vocabulary. I think last week we also talked about things  stop words, and sometimes you can  exclude these stop words because they don't influence the performance of your model. It just tries to learn what are important features based on this. If you install this package SQL line and then you have a very small data X here you just probably have two features 2 dimensional and then you have this output which is either 0 or one and then you can fit the model and predict. You can determine the features you want to use for a particular text, and then your inputs can be a document or a sentence, and then you have a document label. That means you can  get the true parameter of your model and then you can prove it. And that's one of the reasons why we have this  packages  SQL. But nowadays I think you can fairly beat them easily. Here's a very simple task. And then this task was not very trivial. Teeter can be all the weights you want to learn. Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. if you have a very, very simple logistic regression classifier, you just need to learn things  what is the bias of the model and things  that. even for this  basic model, you still need an, you still need a set of parameters. You maximize the likelihood, but you minimize the error. And for the error, , the lowest you can get is 0.  if you continue to minimize your error, then you can get a very good model. And then you have use and then you have different features or different words that can be categorized as your features in other tasks. Let's say I'm going to give an example that is more concrete, but I want you to look at X as not just a single variable, but  a vector of the different features you want to use to learn your probability of Y given X. here we have  see this as the. can you all see it? that means if you have X to be 5 different features, they are not dependent on each other. Of B. because they are related. if you have, if you are flipping a coin, this is a very good example of a Bernoulli distribution. for natural language processing our outcomes, we assume that we have very, very large features, which is the size of our vocabulary and the size of our vocabulary. For some applications it could be  30,000, for some we could  100,000. And that's why we are computing this sum. this is a product, but if you put log it to be a sum, you are computing this product over your entire data set. Multiply the products you have on the individual which also consists of the products across all the features. It's just read it multiple times. If you can find this Theta, you can estimate the joint probability of PX, Y and that's why it's not together because if it's together it means something else. . And then you also have different values for the features and then you marginalized over. that's the how to marginalized. Maybe this helps to clarify things with an example. if you have if Y can take 2 values. that means you have to compute. you have to compute what you have below. You can get a probability of X if you use the marginal, but you can ignore it. . Why? Because you need to compute this over every single features and the calculation for every feature is different, ? , but I can assure you that for this task, even if you don't compute it, you will still get your answer. All . I think I don't know if anybody was able to solve it. . Is the a or not a? And if you apply this rule,  you want to compare these two probabilities, ? This is what you want to compute. . . , . What you're supposed to compute is probability of. . if you say, and this is if you compute these individual probabilities, , , 2 / 3 should be 1 / 3, OK, I think. , this should be 1 / 3, and then the other one should be 1 / 3, and the last one would be 1 / 3.  if you say, , OK, that's correct. Of. Dozen review notes, Probability of dozen review notes is if you pick the first tree, the first tree lines, Yes, appears two times, . And if it's no, it will be one over the probability,  the number of times you have grade and that's 1 / 3, ? Y. Yes, this is my approach. . here. And that's 1 / 3. And the probability of dozens review notes will be 1 / 2 because you have two appearances of not A and that's 1 / 2, the second one 1 / 2, and the third one is 1 / 2. . The notation. The notation seems correct to me. . It should be 1 / 3. . for an example is below, you have YO appearing 2 times, ? You learn the parameter Theta. If you can get the Theta, you already solved the problem. Sometimes is not very easy to compute for some application. for the task of logistic regression. And what when you want to try to solve which is also called logic? When you want to try to solve a problem using logistic regression? Here I want to clarify that features can be anything. We typically just default to what we have, which is the words we have and then use them as features because that's all we have. And then we can do counting. if the word yo that we say yes signifies is more correlated with spam, if it appears  many times in your document, it should be a good feature. we typically just default to this. you can design your feature. For example, you can say for different tasks  nameless recognition, you can say capitalization is very important, can be an example of a feature, the length of the document can be an example of a feature, the number of stop walls can be an example of a feature, and anything can be a feature. if money is always associated with spam, then money will be a good feature for detecting spam. for the parameters of the logistic regression here, the Theta will just be once you fit the model, you need to compute what is the parameters. You want to compute what's the probability of Y given X but given this parameter of Theta because  you want to find those parameters and those parameters in the logistic regression are A1A2A3A to an. And if you had a log to this, you can have what is called the log likelihood. And log likelihood is very important in NLP because it's what you use to compute things  publicity of a language model and  many things  that. and the idea here is that if you say if you have log of the products of two probability, this will give you the sum of the probabilities. All. . The log of the product of probability will be the sum of the log of the probabilities. Do you have question on logistic regression? And the idea is that you want to project your data in a high dimensional space and then you want to draw a line that separates the different classes. Can we, if we project it in a high dimensional space, are we able to draw a line that clearly separate them? Why? All you care about is just solving the problem. Because neural network is more  a generalization of this concept for artificial neural network, there are  many people that believe that it's inspired by the human brain. you can completely ignore everything I say here. And this is probably one of the most successful algorithm that we have currently in machine learning. There is only a theoretical proof that says that they can learn any function. They can learn any function. as you go on in your career, you can come up with another fancy architecture that will become famous. ."
    ],
    "Topic 2": [
        "I. Hello I'm trying to fix this thing and it's not working. Yes. Yes. Yes, OK. All ,  today we'll continue with, we'll start with linear classifiers. Linear classifiers. Output Y can be a discrete outcome or categorical variable. Or it can be a more complicated function  a deep learning algorithm. For example, you want to detect if a movie has a positive or a negative sentiment and for clustering you want to just give it an input. You want to cluster the data into different categories. This is statistical. , yes. let's assume you have a very small data and you don't have you only have training data. You don't have another split for test data, another split for validation data. And  you can split it into  100 hundred 100. Let's make it very simple. Let's assume we have  threefold cross validation. You can split the training data into K folds and then test. Or let's say you split it into K folds. You combine them together and train. Is that clear? Is this simple enough or do you have questions? Yes, what? , yes. How many people are not familiar with SKLN? And you could, it's a very small package which you can install. this is a very simple way. But today we are focusing on linear models. One of the most popular methods in the 90s rather was support vector machine and became  popular that people even didn't care about neural networks because neural networks was not making progress for  many years and they were making a lot of progress with support vector machines. And one of the reason is because it's convex. support vector was very popular, but also it has limitation that it can only work very  on very basic tasks  test classification. If  about the Oron is it Eron data set, it's a very popular spam detection data set. You can even for using these basic methods, you can already achieve close to 98% out of 100. this is a very simple Bayes rule, and if you don't know it, just master it. It's as simple as that. Let's start with the drive probability of a. I don't know if yes. we just changed the variable. Probability of X is estimating the entire probability over all the set of features. The way it's been generated is  this. Just give me one. this is the original formulation for. Is it clear? , because it depends on the previous thing. if you assume this independence assumption, it will fail for some applications and that's why people decide to work on better algorithms for other data sets. We call this prior class because although we want to find the probability of Y given X, we make an assumption that we are generating this data as a generative model. That first you have less as you want to train a data, , you want to construct a data. parameters of each feature's distribution are conditioned on the class. if you have  a discrete data, we assume that a distribution of P of Y&P XI are given Y are categorical distribution. This is a very simple assumption. for categorical distribution, an example, a very good example of a categorical distribution, if you still remember your stats probability theory is Bernoulli distribution. That's a Bernoulli distribution. But typically you can also assume that this categorical distribution is a multinomial distribution because you don't have two outcomes, you can have more than two outcomes. It's  all the possible combination of words that you have in a language. And this will  signify all the features that you need to estimate the probability for. I want to see if you can attempt it. I will give you  5 minutes table of whether a student will get a or not based on their habits. You have a nominal data and a Bernoulli distribution. Is that a yes or no? Is that a yes or no? you have if a student reviews notes, does assignments and asks questions and this is the grade,  what is the probability that this student gets an A? He has not performed any assignments but always ask questions. Using the knowledge of Bayes rule, can you compute what would be the probability of Y equals grade A or not A given these three features which is review notes, does assignment and ask questions. What will be the great for this student? probability of X is the marginal distribution. What the answer is the grade. what's the grade? the internal answer is this. What's the probability of review notes given grade? A. OK, let me start from the basic. what's the probability of review notes equals yes, given grade equals a what, 2 / 3? And then what's probability of DOS assignment? What's the probability of DOS assignment given grade equals A2 over 3? Because once you're supposed to compute is not yes, yes, yes, all the time. It doesn't review notes and probability of no assignment and probability of ask question, . , the one that has the highest probability between probability of. The one that has the highest probability will be your answer. You only have one N out of the first 3 where you have a grade of A and that's another 1 / 3. Is that clear? Let me just update the slide. The only mistake here is just where I have 2 / 3. , yes, . Yes, yes, if you estimate the probability of X. What I'm just saying is that even without computing the probability of X, you can already decide which one is bigger. there's a simple distinction between type and token. please take note of that. Because if you can get what is the  prior distribution, what is the  distribution that you can use to fit your data, then you can get a very accurate result. linear regression has continuous values, but for you for you to make it a probability distribution, you have to apply a simple function that will convert the output into a probability distribution  that everything can sum to 1.  the probability mass function can be written as this. in the example I showed you here, we use a very simple example where the features are  review notes, dose assignment, and ask question. In natural language processing. I hope this is clear. I hope I'm saying it . Because you don't need. There's no assumption of what is the probability distribution on this. SVM can work  in different tasks and settings, usually giving very little training data."
    ],
    "Topic 3": [
        "what's the difference between supervised and unsupervised learning? , , I didn't post it this morning, but I will post it after the class. what's the difference between supervised, unsupervised? Thank you. Thank you. It's continuous. But the idea is very simple. What is the use of training set, a validation set or a test set? I know you may know this probably before, but , that's what is that validation or test set  they can generalize, not sure. OK, you want to try validation set? , that's would be the test set. Test your model or , learning function. , thank you. All ,  quickly, let me go through cross validation. And the idea is that you can split your training data into different chunks or subsets. And then you can, let's assume you have  500 samples and you have  5 fold validation. You can split it to generalize. the idea is, OK, let's say for the first fold, you pick the first fold as your test set and then you train on fold 2 and fold three. And by doing this you can  aggregate the test accuracies and then try to pick your model parameters. It could be you're trying to pick an hyperparameter and then by doing this K4 cross validation you'll be able to choose what will be the  hyperparameter even if you don't have a validation set. Hyperparameter. And some parameters cannot be learned automatically, you have to fix them. since you are not sure, you can try to tune your model to pick what is the  hyperparameter. And then you can have an equal split of the range of values between 0.01 and one. And then after training your model on using this different hyperparameter, you can choose what would be the best hyperparameter in that case. For example, if you are training a simple spam classifier for SVM, we have a hyperparameter and then you can say let's set this to 0.01. And then you have the average error for that hyperparameter. , it's going to be different model, but you're going to just train on the same data. Let's say you trained for the experiment one, you're going to have Model 1. you have to give it different set of values and then try it out to determine which would be the best hyperparameter. Yes, yes, you would choose the model, you would choose the hyperparameter with the best validation accuracy, yes. OK,  this is a very small example which you can try. And then if you have very basic tasks  test classification tasks, for example topic classification or sentiment classification, you can easily run this package on it. this will give you the logic and then you can add a softmax on this to determine what will be the probability. OK. OK. Today we'll talk about how to train a classifier on a training set. And if you are mathematically inclined, you  something that you have an exact proof and  how your model will behave. For example, most test classification data set  sentiment or spam detection. the idea of training is that you want to select the hyperparameter that minimize the error on your training set or maximize the likelihood on your training data. likelihood is  you're learning a probability, what is the best probability that can fit this data? that's the idea of likelihood and the probability, the higher the better, the closer you are to one. OK,  I believe the first equation here you can do the math the probability of Y given X&X. And then if you do the math,  the probability of a joint of a conditional probability would be probability of the joint divided by the one that is conditioned on. Probability of Y given X is probability of XY, the value of probability of X. Then you can rearrange this using the same idea and then you can  say this is probability of Y multiplied by probability of yx given Y divided by probability of X. Is this equation clear? This can also be written as this,  if you just  replace this with XY,  I believe you'll be able to see it. Probability of AB equals probability of a, divide given B, probability of B, and then if you do the math, probability of A given B will  be probability of the joint AB divided by probability of B. probability of Y given X equals probability of XY, probability of X. And that's the same idea we used in  rearranging it here,  that you  have  saying probability of XY will not be written as probability of Y, probability of X given Y and then you still have probability of X below. The idea there are two assumptions, but the most important assumption is this independence assumption. X is a vector, you have X1X2 to SK and then a probability of XY is  a probability of Y and the product of probability X given Y. This is a set of variables, let's say X1 to X5. this time we written as probability of X1X 2X3, let's say to XI given I.  the independent assumption says if two random variables are independent, probability of AB equals to probability of A. Probability. This is why you say. This is what it means if you say A&B are independent given C and you assume conditional independence. this would be probability of a given , given C multiplied by probability of B given C and then. This would be probability of Y, probability of X1 given Y, probability of X2 given Y, and  on the probability of XI given Y. , OK.  and this is what we call you assume independence between the variables. then  you have a product between probability of XI given Y and it's a very strong assumption which doesn't hold everything is that you assume all the features are independent of the output. Who can give me an example of where this will not hold this independence assumption time series and you appreciate? Another example, . it's  you want to create an example of a spam e-mail and then you say, OK, for spam give me an example of a spam e-mail or for no spam, give me an example of no spam. And for the spam e-mail that has been generated, you can  say each of the words which are features are independent. And this is the probability of each feature XI given the class. The idea of probability is that  that the probability must sum up to 1.  for a categorical random variable, it follows this distribution if it can take one of the K outcome each with a certain probability. the idea is that we have to compute the likelihood. The idea of likelihood is that you want to fit on all your training data. For every X&Y you pick from your data, you want to estimate what is the probability of the joint probability. That is the likelihood. If you assume this independent assumption for each of your data, then you are going to have the product over all the data. You are going to do the same thing for this discrete probability, which is what's the probability of X taking the value of each of the features given the value of the Y you want to predict? And for the training, the idea is that for every after you have trained the model, you compute the likelihood over the entire data. You can  predict a new. For a new document you want to classify or for a new sentence you want to classify, you just reuse the base rule probability of Y given X equals the probability of the joints divided by P of X. And then you assume that every Y we pick a value whether spam or no spam, which is our running example. That's a very simple example over the joints, OK,  and that's how you apply the base rule. Remember all the features are independent and I'm going to take you through a running example. OK. OK, this is an example, , probability of PX1, . This could be probability. Of XY equals to 0 plus probability of XY. Equals to Y. . Then I will show you the my own solution. What's probability of Y given X, which is the product of probability of Y and the product of the conditional. you just have to sum over the joints to get a probability of X is a vector of the features. in that case you have to compute the joint probability of every X. , OK, I get what you mean. OK, OK. How did you arrive at not a? the two probabilities, if you can give me the probability of a given X and probability of not a given X, what are your probabilities? the idea is that for you have to compute what is the probability of Y, , what's the probability of grade of y = a given? And then you compute what's the probability of y = A given those assignments, probability of a given as question. Probability of y = A and probability of Y equals not a given X, ? Then  that if you do the math, this probability of y = A equals probability of X, which is a vector given Y equals a. The idea is that if you do the math, for example, what is the probability of what's probability of review notes? What's the probability that Y is equals to A and this 3 / 5, ? And what's the probability of not a 2 / 5, ? And what's the probability of ask question given probability of y = a one over 3.  we're able to compute the probability. if you say, what is the probability of? you're going to have 1 / 45.  what's the probability? The probability? Equals and probability of y = a or probability and probability of Y equals not a. , of course it's. you have OK.  the way you compute it is what's the probability of N in the first column? And if you compare all the ones that has grade of A  you have 1 / 3 and the other one, what's probability of no assignment? And that's how I got the first one. And then the other part, which is probability of Y equals not A, you're going to do the same. The probability of not a is 2 / 5, ? Probability of X? the idea is that if you already have this  this table of results, you can compute all the different probabilities and at the test, at the test time, you can estimate the probability for the new document and then decide what would be the category. In this Case, No, it's not going to sum to one because you have not estimated the probability of X, ? OK,  quickly I will rush through the remaining of the slide before we go out of time. if you have completed the probability of that token, you're also going to say what's the probability of spam, probability of YO given spam. and that is the idea for most algorithm ML algorithms that have been developed, they just focus on how do you estimate the Theta such a way that the loss is low. And once you can do this, who cares about having the estimation of the  probability, because sometimes the probability of XI just showed you. But early days of machine learning, most researchers are split into two. Some continue to focus on how do you estimate the  probability. I'm going to show you how you will compute what is the likelihood in logistic regression in a minute. It's a big question on how do you get your features. Does the document contain the word money? in this case, it's very simple, all the values A1A2A3 to an and of course and the intercept. And once you have done that, you can  optimize this using gradient descent. I won't go too much into this because this is more of a machine learning concept, but if you do the math, I think you'll be able to arrange this very quickly with this idea of the products. I was, I was a bit in a hurry because of the time, but do you have a question I can try to clarify very quickly? you have the blue dots and then you have the red dots. And this is the idea of, this is the major idea of the SVM. It's very simple to do this in practice. What you have to do is to try different algorithms. OK,  the last one here is the perceptron that leads us to it's a very simple extension of the logistic regression. Here we generalize the A1A2A3 that I told you into what is called the weight matrix. And  we focus on what is the  weight matrix and the  intercept that will give me that can be used to fit my data. Thank you."
    ],
    "Topic 4": [
        "Hi everyone,  for the delay in starting the lecture. Is it clearer? OK, . , exactly. it could be  LSTM. other examples. let me ask a question. Why do you need this? We have a principle called cross validation which is also used for model selection. OK, great question. Should it be 0.01? Then you select which one has the highest accuracy on this, and  you can  pick that hyper parameter as every parameter you're going to use to train the entire model. Any further question. OK, you have to set it as a person performing the experiment. every method have some theoretical back in and they have some hyperparameter that needs to be turned. OK, all . in the 90s or 80s, people have developed ,  many methods. I would encourage you to also try this out. Although maybe you don't need it  much these days, but it's good to try out. pay attention to this. OK, do we have questions? OK, yes, Theta is a parameter. OK.  everyone is familiar with Bayes rule. If you are trying to do  a spam classification, every feature can be  the types. Rule you need to follow. OK, . see those two things as the rules you need to follow. Yes, they are exactly the same. And you are able to do this because they are independent. They are independent. A&B are independent. If you do the math . OK, all . The way it's been constructed is that you have the label and then you provide an example. that means every word in it can  lead you to predict that this is spam. This is the idea because you are saying that they are independent of each other. And because you start with Y, we call it the prior class distribution. It's still prior. It's called Bernoulli because you have just two outcomes and then you perform this experiment once. If you perform the experiment multiple times, then you have  a binomial distribution. it's  you want, if you pick an English Dictionary, whether the one from Oxford or Cambridge, all the words there is  this is  a dictionary. Do you have a question? Let's say what's the probability that Y takes a value of spam or Y taking the value of no spam for this  binary classification task and the same thing. Yes, OK. ; Is just saying that you need, you need to find this parameter of Theta to estimate this joint probability. Bernoulli distribution I told you has two values. Do you have a question on the task? It doesn't affect your results because it's constant. OK, all , I will give you an int of the answer. And the last one asks question. You have Y, which is yes, ask question out of three possibilities. OK, I'm not able to edit on this laptop, but ,  1 / 45. And that's how the answer is not a. Do you have question? No, no, it's not Theta, but that's why I said ignore probability of X, because if you put it in inequality, you can just remove them, ? If you're comparing, you can remove them. OK. All ,  the answer is not a. the idea of type is that the identity of the word, which is the count of unique words and token is every single instance. At the  class, we're going to try to do a discriminative task instead of a generative task. for a generative model, we learn a distribution for all of the random variables involved, which is a joint distribution probability of X, Y. How many times does this word appear? How many times does that word appear? OK,  in practice the features depend on both the documents and the proposed class. OK, the last thing I want to talk about is support vector machine, which is a very, very important algorithm. And this line can be a very hard margin or a soft margin. how can we draw this line that separates the two examples? And then here we can have a hard margin, which is the red 1 or a stuffed margin, which is  the green one, because you can have some dots that will be very, very close to that. our SVM is generative for a discriminative model. What is the best hyper parameter? It's always a good bet and you can  do the math. You can try logistic regression, SVM, boosting and  many techniques. people do not believe in this. Some people believe that there's some interaction about you have the neuron and the dendrites and the ASEAN in the brain. Given enough training data, they tend to perform very . The disadvantage of this is that training can take a very long time. Sometimes you can train a machine translation model for a month. For current language models, people are trading for over six months, over a year, and if something is wrong, we have to start that over again. It's really expensive and often requires a lot of data. OK. And of course, we have different other kinds of classification algorithms that will not be covered, but you can read them up  K nearest neighbor decision trees. in the  class I can ask you about what  about K nearest neighbor decision trees, random forests, and  on."
    ],
    "Topic 5": [
        "who can remind me what you have been taught last week? Can you hear me? you don't have output for the unsupervised. I think you probably also did some feature selection, how to come up with features for your classifier. last week I think we described classification where you have an input X.  an input X can be a vector, can be a matrix, can be anything. I think we discussed linear regression last week. It could be a transformer architecture. It could be as simple as linear regression or more complicated as artificial neural networks. And then you can train on the 1st 400 and evaluate on another subset. You train on K -, 1, and then you evaluate on the last one. The experiment too is that you train on fold one and fold 3 and then you evaluate on fold 2. And the last one is that you can train on fold one and fold 2 and then evaluate on fold 3. it could take  between maybe 0.01 and one or not bigger than one. And then you train for the K fold. Or roughly equals size because. It will be different models. OK, I hope that answers your question. In the case of neural networks, you have things  learning rates, but you don't know what should be the  value of the learning rate. Because the best models we have, even neural networks, sometimes they still struggle on some tasks. You have to use principles  TFIDF, which is a way to normalize your data  that you don't have unimportant tokens. this  package allows you to try out different feature selection. But nowadays we don't really do a lot of feature selection because we learn a big model  neural networks that  tries to learn the features automatically given a text. , last week probably this will be another revision where we already have  feature extraction. OK,  if you think a little bit abstractly on this, your function can be any classifier. Another popular technique is naive base. And for  many years it was very difficult to beat this simple baseline of naive base. But I will argue that even if you build a classifier for a very basic task, you should still have some of these baselines. Maybe you don't need to build an LSTM or a transform architecture to have a 90% accuracy, while if you just train naive base you can already achieve that score. OK,  for naive base,  I said, when you're learning a model, you need to add  a parameter teeter. Here you have to look at it as a vector or  at minimum at the vector because X can be the set of features. If you lemmatize then different words,  let's pick an English word, use, and then you can have different variants used using and all that will be converted to words to the root word. I can use the board if it's not clear,  it's just a way of rewriting the same thing. And this is the exact thing we did here. But for the naive base, typically we don't estimate this probability. The reason is because this is constant when you are trying to pick what is the best, what is the best Y given X. I'm going to explain a bit further. naive base can be seen as a simple generating model and I'm going to show you the data generating process in a minute. for each sample, that means for each sample of Y you generate a vector X by generating each feature independently conditioned on Y.  that means every feature of X is independent. here for the naive based model parameters, the parameters to the model Theta consist of what we call the prior class, which is probability of Y. This is the generation process. that's the data generation process. OK, how do we train a naive based classifier? what is the likelihood on fitting? I think if you reuse this formulation on the board, I think you will be able to do the same. OK, for a categorical distribution, if you really want to train your naive base classifier, you want to say what's the probability of AY as taking one of these values? to calculate P of X, the normal thing you are going to do is to marginalized over all the random variables. There's another rule that I have not written on the board to marginalized is you marginalized over the joint probability. OK, I know it's a bit a lot of content but this is an exercise. If and that's the last one, it doesn't review notes. typically for naive base, it's not that you cannot estimate it, but typically you don't need to. Review notes, ? P of X of the vector. Maybe my calculation is even wrong. I think here you have, I don't think my calculation OK. I think one, I have two, which should be 1 / 3, one over three, 1 / 3 and then you have 1 / 45 and then you compare to the second one. That means it doesn't review notes. in order to show, it should be 3 / 5 * 1 / 3 * 1 / 3 * 1 / 3, and then you're supposed to have 1 / 45. I think. And then if you compare these two, which one is greater 1 / 45 and 1 / 20? 1 / 20 is greater than 1 / 45. Which one are you talking about? And I hope this gives you an idea of all the maps we wrote on the book. And then you're still going to repeat it because this is computed over the probability of token, not the probability of types when we are doing this  calculation. There's an important distinction between that. another distinction is generative versus discriminative task. And for a discriminative model, the only thing you care about is the parameter. Some are only working on discriminative tasks because they just want to have the best model. But nowadays I think people that focus on discriminative seems to win. But of course, there are two ways of thinking about a problem. the idea again, although it has regression in the name, it is not a regression technique, it is a classification technique. OK, I think this is how it is if you convert it to a probability  that  everything can go between zero and one. And our idea is that we want to compute what is the likelihood, what is the condition and likelihood on our training campus, which is very similar to what we did in the naive base. OK,  another technique that is very proper. They are discriminative. What is the best parameter of the model that can help you to separate your classes in a high dimensional space? how do you decide naive based logistic regression? Naive base is your good. And here you can also stack different perceptions together. And from this is where we  move to what is called neural network. The way they interact also  model how artificial neural network works. One, we have to we have some advantages of neural network. Number one, they can learn very complex functions. And there are many different network structures that you can  come up with for neural networks. Better than Transformer are also leveraging this idea of neural networks. even typically your large language models  ChatGPT are still based on neural networks."
    ]
}