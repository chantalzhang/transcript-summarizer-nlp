{
    "Topic 1": [
        "This is Comp. and this is David Adelani, we're going to introduce ourselves. Do you prefer the captions, or are they distracting? They're a bit distracting. I'm . I'll hide them for . . we're going to talk all about what this course is about the topics and  forth. But 1st I thought we thought we would introduce ourselves. And I've been at Mcgill for a while  since 2,015. ,  that's me. And then I'll turn it over to David. You  . David. I need my patients at Zailand and a postdoc at Ucr, London. of course, you already know your instructors. And also  the building. Very , I'm seeing. for the evaluation we have 2 programming assignments, which will be 20%. And we have. And then we have a group project which could be a group of up to 3. ,  the textbook will be using. , . There's no one, is there? No. it should be. hey? , . We have some prerequisite for this call. you should take it very serious. . And of course, we're going to give you more details as we approach it. for the final project. , you have to. it's very important. We. which we've been nothing. . paper project proposal, and then you have the progress update and the final submission. We are going to announce the due date. Later. I'm sure you all know the university rules on this. hey? Yes, this is a very important one. I know you have the question, can I use Chat Gpt, or any other language model for the exercise? If it helps you to understand course materials to summarize it. But please do acknowledge it if you use this technology. If you only generate all your solutions. Also don't use it to generate your entire report. The meet I'm also will be there. Yes, that's important. . we're using zoom today as  a backup lecture recording, although it's supposed to be automatically done. in general, we're we do not encourage online attendance. , thank you. . you can also see some balance in the lecture. , before that. We all know about Llms Chat gpt, and  on. is it? ? I've gotten our lstn very difficult. . in 2020. . if I say, Mary had a little lamp. Very. And up. What's a good prediction? And then you can rank the prediction and say, , which world is more likely to be predicted? I think in this case you have 2 good choices. That would be a good completion but these days it's more, of course. you could take the entire text in the on the web. And nowadays we even use a bigger, bigger purpose for that. , what this course is not about. Why did people try methods? . I think, what this course is about other. Why do people try the methods that they did? Hey, our! This is not the course. Of course we all use machine learning in some ways or the other. and of course. Covering different applications and has a lot of importance both for a daily task and in different industries. I try to. or that scales to other languages. close to half of the world, languages are  endangered. ? . And in language, one thing that is important is how it is highly expressive and productive. . We do evaluation. Also a lot of statistical methods and machine learning. and sometimes we also have rule based methods. here we have. But in generation you really want to produce text but also to be traditionally referred to as semantic formalism to text. But more recently, it's more about you're giving a tax, and you want to generate something it doesn't have to be your provided input and text. the original input might be images. But the output will always be text. And the output is text. ? it could be. What? ? and another one is, what is the weather forecast for tomorrow? . . . , . maybe 1st language accusation you want to wear cum skip. and  on and  forth. . the last slide here that will cover the mathematical foundations of sale, mathematical properties of formal system and algorithm. Thank you. . Thank you, David. . ? ? . . And and we've also seen some of the goals of the field. ,  in this course, we're primarily going to be focusing on text. And  we're primarily going to be working with text. ? ? And Canada, Uk, and  forth. And  on. ,  broken lines of thought and trains of thought, and  forth, and that you resume   then there are different techniques to try to deal with, that there are also non standard language, which is extremely interesting. We've also had tremendous progress in that by using deep learning techniques there sometimes is a perception that is a solved problem. But it's by no means a solved problem. . It's not the only thing. That's a lot. ? more concretely. we come to the interactive part of this lecture. ? ? ? , you say, Peach, you can feel it. . If you say speech, you should not be able to feel it again. ? . they're the same abstract. They belong to the same abstract category. trivia fact. They. you can. even sounds that appear to be the same sounds across languages there, there might be slight phonetic differences between them. ? There's anti. This  that you're, I guess I don't even know anymore. And then you add an anti. And then you're against that. Another. I guess we still have an English bias, but in other languages, for example,  Finnish or Czech or Swahili, many other languages have very complex morphologies, where one word has many, many parts, a lot of them involve conjugations to express meanings that in English we would use a separate word or an adverb. It's not a string in the language and grammatical means is . Why is that ? ,  here is a comic. if you I just ran my 1st Marathon in 6 years. That's pretty slow dude. . , that's . It's the last time you ran, it was 6 years ago. You have some. Typically, they're called multiple senses. Here's another one. That's . . ? ? ? we come to another web comic. ? If you're accusing me of such a deed, I have nothing more to say. or even some. Would you  any dessert would be delicious? This one is  something that you one might say , the dessert would be delicious. You're not  saying yes. in fact, you could say something , Oh, if someone asks you, would you  any dessert? You can say, Oh, dessert would be delicious, but not today for me. ? ? But if you have to come up with formal analyses of these things. ? how? on that. , and then discourse. this discourse is the study of the structure of larger spans of language beyond individual clauses or sentences. why do we say multiple things? and , then that means that quite naturally, most things, most utterances, and most sentences that you say one after the other, or you put in text one after the other, there'll be some logical relation between them. And, in fact, there's a whole branch of comedy  absurdist comedy, is based on putting things that are unrelated  to each other for a comedic effect. ? , in this course, we're going to cover many of these different areas, maybe 2 different levels of depth. Not all Nlp methods necessarily have to be machine learning based or machine learning methods, but still some of the ways in which we investigate these issues might include thinking about. And if you do both of these things. and also through, say, linguistic knowledge, through linguistic analyses of languages, and we combine that with learning from data, from websites, news articles, whatever's on the Internet. ? there are a lot of, say, meteorological observation stations. . is this email, spam or not spam. ? But that's a that's that's an Nlp problem, email, spam classification or another one is sequence models, which is where you're not just making one single decision , is this one email is this email, spam or non spam, which is one decision. ? that's how do you? ? And we're going to look at machine learning techniques for it within the classification paradigm. And  on and  forth. we'll have semantics and reference resolution, which is at the meeting level. Then please email us. If you're if you send us suggestions or recommendations. . . Yes. ? And that's a controversial topic I might give you one view of it. And and why is that? ?"
    ],
    "Topic 2": [
        "I don't know if it's because they want to  you're saying in the last week 10 response for exactly , is it? Hi, everybody! Is the microphone working? Oh, , great Hi,   natural language processing. And in my lab, we do research on obviously topics in natural language processing. and that includes natural language, generation, automatic summarization. If you don't already. I work on topics around multilingual Nlp machine translation, representation Lane. Speech processing. preliminaries. if you want to see Jackie. On a Wednesday. You can also come see me. And also we are gonna have,  4 reading assignments, 5% each. it's 25% is. Oh, . This is an Nlp class. you are supposed to come up with a new idea it could be. If you're in this class, you already know some of the odd topics in Nlp. we will decide on what to do of plagiarism. Please don't do it. We're in Quebec,  I think you could use English or French to write your essay are susceptible. or you write an essay. Or maybe you're  or make your essay better. it's not  to use this as a primary means to complete your task. I will release your grades in my courses. , administrative things are over. Because, , if you're not doing , don't come in and spread it to everyone else ? But  otherwise the, we won't make the zoom link just publicly available. we want to make a distinction between computational linguistics and natural language processing. But we try to make a distinction here. and I'm happy. We want to clarify what this lecture is not about. Even for difficult tasks. when I was doing my Phd question, answering is a very, very difficult task. that many labs just focus on this also, we have code generation that is not very prominent in social engineering, essay. Also, back in the days was a very big topic and very difficult task. and our older models, or which I will call inferior models. And sometimes it's even used in setting disputes. 2020, when the charge pity come out who remembers 22? , in 2023. language model is a very, very simple concept that has been a statistical Nlp for many, many years. and we even have statistical language models before the neural language models where you can pick a small chunk of text. the idea is, predict the  one. It's as simple as sentence complexion. Accident? It's not as simple as just. we started from statistical language model and statistical Nlp before moving into the new architectures. What are some of the common tasks and paradigms involving natural language, including very basic classification tasks and linear models will be covered in this course. In my view, that covers some of this deep learning machine learning as a primary focus. But this is not a focus of the cost languages everywhere. and I want to also emphasize that language is not all only working on English language. One of my research area is multilingual nlp, we, because we have  many languages in the world. The word language can be translated or can be expressed in different languages in the world. is French. But we also have  many languages in the world. and then in the Americans and European languages which. many of these languages are underrepresented,  we can only say only 7% of the languages are can be categorized at institutional that are used for many, many different things, including education and business, and  on. What's language? But we can also have other  language. whether voice or text, or  on. And it is nearly universal. We want to say, this course, is focus on natural language, but we also have other forms of languages  programming languages. But of course, we would be focusing on the natural language, not program languages. The domains of natural language can be acoustic signal for names, words, signal syntax, semantics. it involves natural language, understanding, or comprehension or natural language generation, I think. There's a big distinction about whether you're working on Nlu task or Nlg. But nowadays we tend to be the model. Oh,  Nlu and Nlg. Task and in speech processing they also have this distinction for Nlu and Nlg task. a new language! Oh. I don't be focuses on more practical technologies. in terms of natural language, understanding. natural language, understanding. And Esr, the input is voice or audio. most work in Nlp is an Nlu for even very basic tasks,  tagging battle speed, tagging name, density, recognition. You have a translation of I  natural language processing in German. Automatic sprouts. Besides new language technologies, there are other reasons to study Co and Nlp as . Here we have the nature of language. Propose a universal grammar for language on are here. And this is an interesting research which you can study even from a baby acquiring a new language. the nature of language processing some sentences are supposed to be gammatical, correct. but are difficult to process. And here, in incl you want to have a formal mathematical model to account for this. The cats caught escaped. in language processing, , some sentences are supposed to be what grammatically correct. But if they're not grammatical, grammatically correct, it's very difficult for us to process. if you have a structure for a sentence, you can know what is broken in the sentence. because every language has a structure some languages the verb has to be in the middle, some languages the verb is in the front. And the question is, can they be efficiently learned from data or efficiently recovered from a sentence? I gather it's cool these days to , hold the microphone in your hand. by , then, you have at least some basic sense of what the terms computational linguistics and natural language processing mean. , for example, in terms of the type of language. And then a little bit after that, we're going to talk about the different ways in which you can break down the phenomenon of language and potentially look at these phenomena separately, or to look at the interactions between these phenomena. And and it's it's  it's discrete. However, you do lose some information by working with text rather than working with speech data. But that's very limited, and the things have changed quite a bit in the past few years, and that  we work on many other languages as  as David also does. working with speech is also a research area, very important. much , for example, speech has disfluencies. but it means that you might. and some of the tasks there in speech processing include automatic speech, recognition, and text to speech generation among others. ASR,  automatic speech recognition is we. you might still be interested in analyzing these large language models in terms of, ,  you train this model. How do you characterize which aspects of language it seems to be handling  in terms of its behavior, and maybe which aspects in which it's not handling  . We don't. let's take a look at this. phonetics is the sub area of linguistics that studies speech sounds the speech sounds that make up language, or, in the case of sign languages, is  the way which she articulates with parts of her body to produce sign language outputs. Suppose we have the word peach. You can have a relatively low, level transcription of the speech sounds that make up the word peach. There's a p sound. P. There's a puff of air, there's  a long I sound e, and then there is this something called an African, which is . , for the p sound that involves closing of your lips. And  vowels are more continuous. But it's a different area of study that studies the rules regarding how sounds are patterned in a language and how they're organized with each other. specifically, the pea and peach is supposed to be aspirated. If you're not a native speaker of English. If you speak French, if you're a native French speaker, your B's and your P's are  different from the B's and P's in English. whether someone grew up speaking a language in that in French, the B's. we have morphology, which is about the study of word formation and meaning. ,  there's but the point here is that there's some regular structure. I guess linguistic trivia point is that English  has a very simple, relatively simple system of word formation. English morphology is  relatively simple compared to many other languages. syntax is the study of the structure of language. ungrammatical means. You can study it. The same sentence to have multiple interpretations. The other person says, 6 years. And then the 1st person says, No, no,  I don't run them very often. And then the second one says, , that's fairly obvious. ,  , where does the ambiguity here come from? It's  some structural analysis of sentences. Then, , we have semantics, which is the study of the meaning of language. it can mean various different things. And there are sub areas of semantics as ,  a big one that we've done, we do a lot of work, or at least we used to do a lot of work on and still do in computational linguistics is something called lexical semantics, which is the study of word meaning. Individual words themselves can also have multiple meanings, multiple interpretations. that's lexical semantics. This is this one is less obvious. I claim that there are at least 2 interpretations of the sentence. and somebody figure out, give me  one of the interpretations. you can see there's a there's a difference, ? And , it's caused by different senses or interpretations of this article of a . don't dismiss short common words, because those are sometimes the hardest to model in terms of their meanings. And then , another area which I  to study a lot in my lab is pragmatics is the study of meaning of language in context, and in particular, there's often a difference between the literal meaning which has been put in the semantics bucket versus the meaning in context, which is often put in the Pragmatics context. this one is Pinocchio was cursed  that his nose would grow whenever he lied. ,  it's a it's a semantic curse. It's not a pragmatic curse. the curse is only in terms of literal meaning. ,  this curse is pretty easily managed. And Pinocchio says there are people who would dispute that perspective. or Pinocchio, did you egg my door? This one is more innocuous, sir. you can see that in terms of the literal meaning none of these statements  evaluate to false. But it's clear that there is a interpretation that any reasonable speaker of the language would give. And  and this one is  pretty common, ? Thanks. that illustrates that there's a difference between the literal meaning and the meaning in context and intended meaning in context. This is pretty obvious, ? the entity referred to, the antecedent depends on who is saying the sentence. and that's also an area of study. And  that should be reflected by the text that she generates as , or that attribute generates, or in terms of how they interpret it. There's no relation between them, and  it doesn't make any sense, and this is not coherent. it's sometimes you can violate some of these common properties in order to make some point, but by default we assume that there should be some logical structure within our discourse. And and  then that means that we're gonna talk about the technological perspective. But also the maybe it's linguistic or scientific perspective of all of these phenomena. And what form and representation the data should have as  as the algorithms and methods and models that you construct and train on top of this data. And  what we do in Nlp is, we do a lot of work on problem specification, on thinking about machine learning algorithms, on eliciting annotations from humans, especially when we have a particular  representation that we want to parse it to, for example. For knowledge about language,  in French, translates to happy, maybe some of the time most of the time machine learning , giving examples and letting some statistical method figure out the associations between those examples. But you might have to make a sequence of decisions  about every word that you have in your document, for example, and there are many other paradigms, and deep learning is also one of the algorithms that fit within this top high level topic of machine learning. also knowledge, representation. in the middle of the course, we'll talk about using logic to represent the meaning of sentences. Those are continuous valued numbers. then, ,  the high level, then structure of the course will be that there will be some correspondence between the Nlp topic with the linguistic layer, with some of the techniques. , for example, we'll start with text classification where we mostly are concerned at the granularity of individual words. And then the linguistic layer is also words, but potentially also syntactic structures. And then where the linguistic layer is the syntactic structure, and then the techniques involved would involve structure, prediction algorithms and also various dynamic programming algorithms that can help us more efficiently explore all possible parses of the sentence. And then and then we're going to look at techniques for representation of meaning,  logic and also machine learning techniques  semi-supervised learning and neural models. And you're closer to the end of the course. you have any preferences. And we can see if we can incorporate that, because the last few lectures of the year they change every year, and we're able to tailor it. cool. 5, 50 is to help you understand the broad topics and applications and common terminology of the field is to help prepare you for either research or employment, including internships in computational linguistics or Nlp. And then, some people are , Oh, it's AI gonna take over the world. hope you have a good week."
    ],
    "Topic 3": [
        "Jackie Cheung, Professor: It's the same. I am an associate professor in the School of computer science and I'm also affiliated with Mila. But by the end of today's lectures, at least, you should be able to know what these terms mean,  semantics and pragmatics. and I resume as a assistant professor at the School of Computer Science this fall. If you're here today, you I'm I'm very sure  the time. you should come on Monday from 2 to 3 30 pm. There's no fine for this one. You hadn't the assignment online through my courses. How many people know my courses? I believe many people here should be familiar with python programming. for the midterm this will cover 25%. a quarter of your grid. if you work on all the images except as multimodal. We need to summarize the and review relevant papers, report on the experiment and must be done in a team of 3. It doesn't have to be completely November. , but it should be at least a project that could extend an existing model to solve a particular task and work on a relevant topic of interest. if you are late to submit your assignment, you have a grace period of 24 h, which I think is very generous. Fine to search for information or brainstorm, I guess. I think that should be fine. and there's a procedure on how to go about this if you are caught. Some of  my courses. But for the assignment and project submissions will be on my courses. I'll I'll just take few slides before handing it over to Jackie. if you have confused, you're not in . And they have impressive performance. I'm not sure. this man, Tom Scott. Then you can tell me if this prediction is  or wrong, artificial number processing remains 10 years away. our artificial Lambo process still remains 10 years away. he's not sure. 3 years after what happened during this time? you have a probability distribution that . for many page generation capacity. , for example, all the text of Wikipedia, English, Wikipedia, and training language model. This is not a cost about it. Longer models dominating the end of your research. We started from somewhere, ? What are the metrics that are used. including very basic metrics  accuracy. The properties are much no longer reflected in an open search. I'm sure there will be other courses. The largest proportion of the languages in the world are in Asia, followed by Africa, and then the Pacific. I would encourage you to also have projects that extends more. arbitrary part pairing diff between form and meaning. including speech. Nlp, on the other hand. But there's a slight difference. This is one example the rat escaped, and then you have the rats. I'll continue to do that rather than to  work. And that's also how the field is, it's the vast majority of the work is on textual data. in some sense it's an idealization of the spoken language. For example, one thing you might lose is  tone of voice, and it might be more difficult to convey,  your attitude. It's just that there are additional issues there or different issues there that we won't cover in this course. we have phonetics, phonology, morphology, syntax, semantics, pramatics, discourse. And and they're roughly organized by the smallest units going on to bigger and bigger chunks of units. Here's an example. sure, this, this sound, and very specifically what goes on in your mouth when you say the word peach, ? you can have these very detailed studies of speech, sounds. Phonology is something that's often confused with phonetics. in the spelling, at least peach and speech are both spelled with P.  I'm guessing that for the vast majority of you think of these as  the same ? , , all 3 of these sounds are pronounced differently. And there's a phonological rule that tells that in, if you speak a. a native version of English, then there's a phonological rule that changes the way that the P sounds in different environments. what I'd  you to do is to say peach and speech, but put , put your hand in front of your mouth  you can feel the puff of air. it's not universal. then we can say that the P. And teach and speech are the same phoneme. But they're  phonetically distinct from each other. Here's another fun. This is one of the ways you can tell. You can check. You can check that at home and ask your friends or something, whereas, and also with the peas in French, the puff. The little puff I talked about tends to be much weaker or non-existent compared to a native English speaker. and  , we won't spend that much more time on morphology in this course. For , then is syntax. if you say IA woman saw a park Indie, that is not an English sentence, that's not a grammatical sentence of English. , for example. traditionally, then, people have drawn tree structures to represent relations between different parts of a sentence and then put those together. you can analyze this syntactically. ,  that was syntax. And  what does that mean? just  a sentence, can have multiple interpretations and be ambiguous. Here we have 2 senses of the word bank. It can either be the bank of a river or the bank, as in the financial institution. However, this curse, apparently this curse is, it was cast by a wizard or witch which does semantics. If you just phrase every sentence  broadly that they can't be false. one prominent example is pronouns. But if you say it, if you say I think the cilantro taste great, then I then points to you. , for example. Because usually because these multiple things have some relation to each other and they help us complete some communicative goal. At the end. The rabbit jumped and ate 2 carrots. And you can analyze that as . a lot of data is in some non human readable form. A a lot of the activations within,  a neural network model. And the machine learning techniques involved would be models that are designed specifically to handle sequence data. and at the end, hopefully, you can answer questions , is it easier or hard to do something? ,  if not, then, , we'll end up early today."
    ],
    "Topic 4": [
        "10 spots wrong? Can you hear me? Hopefully, we're all in the  place. I think there's a way to keep them, but not show them on screen. we have the following office hours. we have the following, Tas Shira Zilling, Guaraff Shi, jun. I'm  if I didn't pronounce it. But it's more consistent with Juraski test book on speech and language processing, and it's publicly available. you can check the new chapters that have been added any questions  far. Is there no violence? The the Gopaja will be the final exam exactly. I'm programming needs to be done in python. or a little bit of advanced materials that we are not able to cover. this gives us the opportunity to really test what you have been taught in class. and it covers, . the tentative time, I think, is finer. I think this is also acceptable. I think there will be a template. If it's more than 24 h. . Don't copy your colleague. also, in terms of the language policy. but it's not that you copy the entire code for your project from chat to pick. A complete paragraph using Chan Gpt. I think we'll also be using the ad platform because he has some advantages. and if you're sick, then you can ask us for the zoom link. even for big profs and  established researchers, thirsty arguments on what is this? Cloudy and Gemini. whatever is your favorite? Summarization. I'll find the task of summarization. And also we have commercial uses use cases from customer service, personal assistant and healthcare, and also for entertainment. What did he say that this new technology, the thing that was going to change everything was starting to  change everything. ,  how do language model works? whether a storybook and you can train a language model to just predict what is the  work. And the key insight is that you learn correlations between words. In context. given, a context predicts the  one. Is it a law or accident? For you to predict the  word you have to decide using different algorithms to say, what  word to predict? Should I predict, Lamb. or should I predict accidents? And you can have more than 5 different words. Training language model on storybook. How did we get to land number models? How did we get to land? That is more than one language that works on more than one language. That's are not vocal. Of course, language is primarily vocal, but some are not vocal,  the sign languages. Task. the goal of Cl is language technology applications and also scientifically understanding how language works. How a baby leg. try to understand the language  it could be how language works in general. although they are used inter interchangeably. You want to know, really understand how language works? How they can be usable for machines and humans for different tasks. They can be considered as primarily nlu task. Or, , , I think a few more stuff. one example of understand is call a taxi to take me to the airport in 30 min in any of your task. You want to know what is the intent of the user. If I ask you, what is the intent of the user here, call a taxi to take me to the airport in 30 min. and then you can have different categorization of the intent of user. It could be current or can be an intent it could be transport to the airport can be an intent it would be. Wake me up  alarm can be an intent wake up time, and  on. This is talking about another domain, but has another index, but generation is different. that you all agree that the second sentence is not very grammatically correct, and then you have the rats, the cat, the dog chased, caught escape. And also we do have some complexity analysis. I think I will stop here and you can pass. I'll talk a little bit more about what we're analyzing. how many people have had a misunderstanding because, somebody you were texting with didn't understand. I think this is really important and useful to know. At least, that's the core. Here we are, maybe more neutral. At least I won't take any stand about whether these divisions are  there, but at least they're still useful to it's still useful to have these divisions for us to be able to more easily talk about different phenomena in language. the 1st division here is phonetics. And  there's issues to do with articulation and transmission and perception that you might be interested in and . , your lips have to come together. There's usually this a 2D diagram of , and you can talk about vowel heights, and whether the vowels near the front of your mouth in the back of or the back of your mouth, and  forth. here we have 3 related, similarly sounding words, peach and speech and beach. Whereas Beach is different because it's a b sound. and the B and beach is just a different sound, and there your vocal cords will vibrate. This might not work   about that, and if you say beach there should be no puff of air, but your vocal cords should vibrate a little bit earlier. whenever P appears after an S. Its pronunciation changes in English. You start by rating your vocal cords a tiny little bit earlier than in the be than for the B's in English. here's a really long word in English anti disestablishmentarianism, and you can break that down into these different chunks which we can call morphemes. And then this, and then establishment Aryanism. You can even analyze it, starting from the stem, and you keep adding prefixes and suffixes, and each time you change some property of the word,  you have established, which is a verb. Then you have a establishment which is, I guess, turns it into some noun related to something that's being established. Then this establishmentarian, I guess, is somebody who is pro establishment, perhaps. And then, if you added this, then it changes the changes. And how they affect the meanings of words. and how you put words together to form sentences, and how those sentences end up being end up, being interpreted as being a string of that language  grammatical or not a string of that language  ungrammatical. in English, we have strict rules about the orderings of words, and how you're allowed to put them together to form a sentence. And if you're able to put them together according to some grammar, then it's a valid sentence. Another interesting thing is that these phenomena and language there are aspects of them that cause. Do do you see it in 6 years. if you draw some tree structure, the ambiguity comes from the fact that this prepositional phrase, in 6 years it might modify different things. It might modify that the marathon is the 1st one in 6 years,  it attaches to the noun, which is the incorrect interpretation, or that you, you ran it in 6 years  that it was. a very popular task, at least last decade in Nlp has been something called word sense, disambiguation. you see all of these words. how do you figure out which sense of the word is the intended sense in that particular context. And people have devised algorithms for that and come with data sets and evaluations. I wonder if you can figure this puzzle out. Ross wants to marry a Swedish woman. Yes, the specific woman that wants Ross wants to marry is Swedish and Ross wants to marry any Swedish woman. here one interpretation, which is hopefully the more common one is that there's a specific woman that Ross wants to marry who happens to be a Swedish woman. And then the other interpretation is that Ross really has a thing for Swedish women and really wants to marry one of them. Pinocchio, did you bully that boy at school? This one is called Dixis, which is that the interpretation of expressions can depend on extra linguistic context. If I say I think cilantro tastes great, then the I there is pointing to me as the speaker. She lost my cell phone. There's a relationship between these 2 sentences. in modern Nlp, there's often we often think about modern Nlp as some combination of some pre specified knowledge, and also machine learning from data. and traditionally, they have been hand engineered systems. It's  one of the poor parts of our modern existence. Think about. And then we might look at syntactic parsing. then, in terms of the learning outcomes and course objectives, then the goal of Comp. You can feel free to agree or disagree, but at least by the end of the course you'll be able, you should be able to better argue that for your point of view,  yes, I think AI will take over the world because or no, it's overhyped ?"
    ],
    "Topic 5": [
        "And then there's  interesting that certainly doesn't. 5, 50. I am Jackie Chung. I am Jackie Chung. And then some terms that you may or may not know  computational semantics and computational pragmatics as  as applications. I'll take few slides before adding over to Jackie. We're going to have  4 reading assignments, 5% each and then at the in the middle of the semester, we are going to have a midterm which will be 25% of the average. this guy, the slice may deviate some sometimes a bit. I'm not Charlie Drasky. Released an updated version this August. on the assignments we have  2 programming assignments. which will be very similar to some of the things you have been taught in class. November 6, th 2024. you need to work on a language data. There's language data. a topic  that and consult a list of suggested projects to be posted. You can pick from this, or you can also deviate from this a bit. for your project steps. for the general policies. I think you can use it if it assist you. Reassignment from this we're going to detect. we are going to be using different platforms. And then we'll be releasing most of the details on these platforms. Do you have questions? There will be lecture recordings. let's go into the lecture. If you attended the Acl Conference, or you have read about what happened there. People still do not agree on what is competition, linguist linguistics? And what is Nlp. You're in the same boat with many other people. My colleague here has more background in computational linguistics than me. I'm more from the Nlp side. Writing. I don't think it's the best way to set a dispute. just as it has for the last few decades. that you probably know. Another definition could be, what is the probability of the entire text, and also be referred to as the language model? This is not about large language models. What was the progress of the field of Nlp. what was the progress of the field of Nlp. How do we evaluate and analyze Nlp systems? f, 1 score that some of you may be already familiar with  more application specific ones  Broscore or Krf. What this cost is not about the latest techniques in large longer. for your projects, you will probably also use machine learning. German. in Chinese, and  on. dominates most of the data on the web. just  286. Nlp should be beyond English. It can be referred to as a form of communication. You are producing some form of data. which is what you use to code vocalization by your favorite animal,  a dog or cat or written English. ,  computational linguistics to distinguish between Cl and Nlp. Modeling natural language with computational models and techniques. That's a simple definition of computational linguistics. Early on. That  work for both Nlp and Nl. And this distinction is quite interesting that in both core Nlp. It could be as how people develop a language it could even be! the methodology and techniques, of course, is very standard, which might also be similar to Nlp. We often gather data which we call language resources. Cl, can we can refer you to as more science or Nlp is more engineering because you just want to make things work for a large number of people or for practical use cases. image captioning can be also referred to as text generation task, even though the infuses image. is this what I'm supposed to stop? For think about a dialogue system. Where you have to produce a text, it could be in the same language, or it could be in another language , Here you are. Which is, I hope I still remember my German very . computational linguistics. And probably all of , chomsky. we are trying to understand what is the innate knowledge. That's what init knowledge most children already have in order to learn their mother tongue. And also there is a way you can detect this by having a structure for each sentence. Why design designing the algorithm. But, as David pointed out earlier, this is different from the perception, at least by traditionally in linguistics, that the spoken form is the primary form of language. However, Texas is just  convenient. It's it's really simple to have come up with a scheme to store text, and then we can grab all of it from online, and we don't have to do any  messy signal processing to convert acoustic or speech signals into  a into a symbolic form. You were being sarcastic. , probably many people, ? And in fact, that's there are theories that  this  loss is why emojis and  Smileys and things became be became popular, became used  in terms of informal a textual communication, and that we're losing something which is  important for communication, and  we add it back in  in nlp, then, a lot of the older work has been on clean, formal standard English especially English, because a lot of the researchers back then were from, say, the Us. And also  we work on a more diverse set of text, not just formal text, but also these informal texts that I just talked about  text messages and online communications. There might be more diversity, and you might have to account for that, and people also speak with different accents. Just try to do use, do ASR in  a noisy environment where there are multiple streams of speech, and you'll quickly realize that. No, there's still a lot of work to be done. ,   I'm going to talk about how linguists, at least, have traditionally divided up language, the phenomenon of language, into these different subdomains. even though large language models work by just doing  word prediction. And  then we can turn to these divisions from linguistics. in linguistics. I think some linguists really believe that these divisions are cognitively, somehow real in our minds, in our model of language. You build up some pressure in your oral cavity, and then you release it with a puff of air or aspiration. and vowels can also similarly be described in terms of formants,  the e can be represented in terms of the formants that make up the vowel sound. ,  this is phonetics. They're just peas ? There's this puff of air, the pea and speech is unaspirated. There's no puff of air after that. ,  then, you can  study this, there's a rule here. And this is , this is a rule in English. In a very subtle way. then, we had phonetics and phonology. and then you have Establishmentarianism, which is the philosophy or belief related to us, being establishmentarian. , there's some correspondence between how you word pieces are built up. You have to arrange them in a particular way,  I saw a woman in the park. and  these are technical terms, grammatical or ungrammatical. There are other issues in pragmatics which are quite interesting. In fact, it's quite likely that Hgbt and large language models don't understand these distinctions. I would claim, although you can dispute me. if you say something , I am angry at her. This is coherent because it's clear that there are. If you say I am angry at her. But we're going to try our best to cover some of the basic distinctions and also computational models and algorithms related to all of these areas. although, to be clear. say, how you collect the data, how you work with the data itself. Very , this is how you can get very high levels of performance and potentially useful or harmful technology that results from this. also structured kinds of data. And they collect a bunch of data there. And that's also useful data. And we might want to interface with that data or work with that data in some way. And that's also part of Nlp, is that interfacing? some of the major paradigms that have been pursued in Nlp, which are not necessarily mutually exclusive include rule based systems. And  that's what we're gonna talk about first.st ,  classification. how do you formally encode information in a way that's useful or interpretable in some way? there might be formal structures that you can posit for this and you can use some  logic. And and we can talk about pros and cons at that point as , or with, say, neural networks. How does that correspond or map to meaning, that's pretty interesting. Then the  topic will be something  language modeling and part of speech tagging. Then we'll talk about machine translation, summarization and potentially other applications potentially other issues with evaluation and ethics of Nlp. Or if you have any topics you would  to explore that are more in that area that may be more applied or advanced. , any questions? I think I already talked about this. we're going to learn some basic linguistics along the way, we're going to learn some basic algorithms. And hopefully, you'll be able to read an Nlp paper and understand the challenges that are in computational linguistics and analy. I think this is a really important skill to have, especially because anything related to AI these days is  prominently in the news, and everyone's talking about it  my parents know what Chat Gp is  they asked me about it. , any last questions comments. the  lecture is  Wednesday, because  Monday is Labor Day."
    ]
}