{
    "Topic 1": [
        "because  my goal in this course is that you don't go away thinking that Nlp is just about hacking things with  prompts and Llms, that would be  really sad. you have a thousand product reviews on a popular product on Amazon. there's under specification, and that,  the meaning representation should be under specified with respect to multiple possible interpretations of the input. , , here it's , take. And then there you go. my students, my grad students are wonderful. he came back to me after a few years. I can't make any predictions 5 or 10 years from . , but ,  we're all here for the more, . Interesting, exciting kinds of Nlg. Abstractly speaking, then, here are the steps in a potential energy system. we talked quite a bit about this last class, ? this step is often application specific. , we talked about how there might just be some topics that people find inherently interesting and important, but we also talked about signals of importance in text. I mentioned at least 3 possible sources of importance in Texas. But but  I mentioned at least 2 other ones. And that's the location is a specific instantiation of this broader theme of discourse structure. And what was the last thing we talked about? A lot of that really depends on the specific application. a structure  there may be tomorrow. and then the second Mike presents the details of the forecasts also. Part of micro planning is to generate referring expressions. And then finally, in later mentions,  it's  the pronoun. But again, these all tend to be contact, specific and application specific. And then the task is to  generate a linguistic expression that picks out a subset of those objects. Or if you say,  the green thing, that's also not very discriminative. They're usually under specified . and then you order the components with respect to each other, fill in inflections. Blah, blah, linearize the tree into the final string. figure out to turn the semantic representation into some  syntactic form which allows you to then convert that syntactic form with the syntax tree into  a linear sequence of tokens. it was quite tricky to  create a system  this, because you have to hand. And it's all very precise. and then you use that to predict the  token and  around 10 years ago this was implemented, as, say, an Lstm model that's ,  you initialize it with some initial word embeddings. But then it's fine tuned on the specific problem and task you're interested in. the second way of reconciling it is that if you have a model that can generate some outputs. You can just rewrite it and have that form. There's no separate modeling of each of those steps, at least in the most extreme form of that. that's that's that's a good point. there's no guarantee that it'll generate something that's either semantically coherent or syntactically  formed. , it's just empirical that it happens to often work  for many of the tasks that we try. Once you have a set of rules to translate the meaning representation to the language outputs, then it should work regardless of what generation task you are pursuing. But this is cheating a bit, because then there's a step of translating from the task specific inputs to the meaning representation. And that part is doesn't come for free . Then you have to do that for every task. and then add mechanisms in the decoder that are appropriate for that specific generation task  a an attention or copy mechanism, or things  that, , fine tuning, and or  few shot learning whatever. The idea here is just to , illustrate the point that the input here is a structured formulation of something. But then you can reformat into reformat, that input into a sequence of tokens that you pass into a sequence to sequence model. and then 1, 2 is  the token position, and  forth or we can look at  18 April 1352. these  this work was  already, a bit old is already a bit old,  8 years ago these days you could even do something even more lightweight, and you can just  pass in the entire . You can really just , pass this in into  some form of  open bracket born, and then 21 November, 1914. you have,  born time and then born location, open bracket, Newington, Yorkshire, and whatever. There must be something about the input that we'd  to change and depending on what? There are also semantic constraints in that. This is how you ensure syntactic correctness. what we would really  is to have a method that allows us to write down all of these hard constraints and soft constraints. This is how we're gonna interpret the meaning of that particular variable. here, this is either 0 or one. And also you need the second to ensure that the selected nodes  form a connected tree. the outputs of the , ,  this formulation. you need a separate step to linearize it, to linearize the tree into the sequence of words. Because then you have . , it could , at least according to these scores. that's  the point of the. because this tree, I know I didn't give you very many examples. Is that what you're saying? ,  the downside is, you have no guarantee the constraints would be respected. But in practice it might work better. there are things you can try and experiment with here. In fact, here they're binary. In practice, you can still solve reasonable sized problems with, especially with  industrial strength, implementations of these solvers that are highly optimized and very, very fast. And you can interpret the output back into your application contact, setting for your task. It's not just for Nlp. If you need  guarantees that you're respecting certain constraints. here you can really make things into a hard constraint  you must satisfy this. Otherwise you just, or you refuse to output something. this is a problem that I've worked on in my lab. You have no guarantee that the outputs will respect semantic correctness or some other constraint that   a safety constraint or something, and this  happens in practice. And this is  a this could be a really bad thing, ? which is to ensure that the output has a certain style or formality, or some  certain semantic content, or avoid saying something bad. And then they within 24 h they've managed to get the Chatbot to further fine tune and adapt, and to say all sorts of horrible things, with lots of  racist and misogynistic content, and  forth. , they're  this, this sounds. You can do this with all sorts of properties. And ,  that's the current approach which is really popular. , people are just putting in whatever,  it could be  some instructions. But this is  what it is. It's just very hard to specify that. , by using sentiment scores to figure out there's some outlier in sentiment or polarity, or in sentiment, polarity or something else. and the characteristics of it. there's work on that in specific application context. Was 3 to 5 min."
    ],
    "Topic 2": [
        "Yes, settings up all . we are going to look at how natural language generation has been approached with different methodologies and different approaches in the literature in the past. And we'll look at each of those. We were looking at automatic summarization focusing on extractive summarization. But if you want to describe the overall distribution of opinions, , of that product, and synthesize and the thoughts that people have on specific aspects, then you're going to have to do some  abstraction and abstractive summarization. there's all that good stuff. one  natural language generation which is always available and always there and  requires very minimal amounts of Nlp is just canned text. But then we can start to get into use cases that are  a little bit more flexible and more adaptive. And arguably, that's a good thing. And then he went into industry, and then he found that a lot of the work is about  writing rules and writing out templates and try to fix things at the last step  that they can release a product. And it was , Oh, , a lot of industry. and then, appropriately, the thing you put in is called the slot filler. You decide to engineer your system  abstractly. And then there's something called micro planning. last class, we talked about multi document summarization. I'll give you one of them. we were talking about  news summarization and looking at the opening paragraphs. in order to decide how to structure the contents of the output. There's  the same argument in the opposite direction, just as  in naturally occurring text. You may want to respect those conventions and formats,  that other people reading that document would be able to do the same thing. there are some guidelines here for that as  that you can try to make use of. And then , as another part of micro planning is to decide how all of these words fit together in clauses and sentences, and this is called sentence planning or aggregation. We talked about this last week when we talked about co-reference resolution. if you have an entity. All the stuff we mentioned. with from parse trees of some kind, or from some semantically annotated structure meaning representation. you can call it the blackboard. If you twist your mind a bit. the top of the tree is  on the left, the outside. and then the tree has,  a tight edges,  one called cat, one called process one called partick here, and then the stuff on the  of that. And  every time you have,  another square bracket pair, that's   a subtree. And and this is  a representation of the semantics of a sentence to be generated. The cat here stands for category. , and on the outside. And then there's  a category, for  noun phrase, and  forth. This is this should be expressed as the subject in the sentence. Singular, then it should be a she. and if you're able to understand the whole process, you can understand exactly what's going on and what's happening and intervene that way. But of course there are Major Downsides, and that's why it's not  popular. And  here you just train a conditional language model. this could be a sequence to sequence model. And then here we're talking about a conditional language model, where it looks  you have a conditional probability distribution. sometimes it's just in the model itself. and sometimes you have to do a little bit of work, because it could be that the fact that it can generate output means that you can derive the implicit, generative, joint, generative model that is implied by that generation process. Also the fact that it's written here as a conditional , it's just how you use the model  the, it's just how you. And that's a good question. ,  let's also talk a little bit about. everything just gets stuck into this conditional language model ? it be air propagation, if there's  one. Yes, , that's a -known problem as . you  need compute resources and all that. then you can also explore something in between the 2 extremes you can. You decide which steps of the analogy pipeline should be in their own modules with a separate planning step, . ,  let's take a look at that. It's formatted in a pretty way. And then you can then apply neuro language model training to this as a sequence of sequence task and generate the outputs. That means there may be. That can be called something different. ,  again, of course, you can just always choose to just throw everything into  a neural language, model a pre-trained model and see what you get. But today I'd  to talk about a particular approach to a particular problem, which  is a bit interesting, because I can then introduce another technical approach that you can add to your toolkit. And  this technique is called integer linear programming. How many people have heard of linear programming. And then there are different reasons. in this example, I have Bohr studied at the University of Copenhagen and got his Phd. After graduating, he studied physics and mathematics at the University of Copenhagen. and what you get is something , after graduating Bohr studied physics and mathematics at the University of Copenhagen, and got his Phd. You want the output sentence to be  formed and grammatically correct. The output sentence is still faithful to the inputs material. both sentences before, if you have  2 sentences  he studied sciences with pleasure, or he studied math and physics with Bohr. Then you can merge those 2 nodes  that it's a single node for studied, and then all of the dependency, relations  subject or prepositional phrases or object, and  forth. And then you're merging them together. And and this is why we would approach this with a type of solution such as linear programming. ,  you have this optimization problem with all of these constraints. Here's another question, how many people learned prologue? in the case of prologue, it might be something  in a simplified or restricted  1st order logic. where you write down every all your constraints and everything  about the problem, and then you prologue finds a solution for you here. All of the optimization problem looks  this. you're either maximizing or minimizing something. Should be pretty simple to do. What would that look ? if the order of the words is determined by the . after this, you have a tree structure, and then you can just do some  simple rescoring with a language model, even with  a very bad language model to linearize it, and you'll get something reasonable. ,   one thing that's more interesting to talk about is , how can you get this approach to work with  a neural model, is it still? Do you 1st try solve this optimization problem and then pass the output to a neural model. the input 1st 3 as something that will be passed into the neural model, and the output of the neural model would be whether or not to select the edge and the model,  by training for that objective would  automatically learn the constraints I see. you're saying to bypass the this optimization thing. and use that as some   goodness score. also I  glossed over the fact that why is this called an integer linear program? And  if you've done linear programming, you  have learned that in linear programming. 1st of all, it's called linear because all of the constraints and the optimization objective, they're linear functions over the variables. for real valued linear programming. They're  in the worst case, complexity is  worse than that. If it's not continuous, you cannot  do some   local search about a good solution. But still the solvers are really good these days,  you can have very good ilp solvers and also  stat solvers. If you've heard of that problem where everything is expressed as  and logic is  true or false statements connected by logical connectives. optimization, convex optimization stuff is its own field. where you specify these diverse objectives and constraints of your problem. and these also introduce interesting additional techniques which are  a bit more familiar. But , that's what it's called these days. That's a news event that can be very terrible. And then you can apply it to some to the outputs of  existing summarization models. It could be a transformer model and some  neural model. As long as it's you can cast this as a sequence to sequence problem. this was a while ago , but Microsoft released a chatbot called Tay. But , they tried back then, and then they released it on Twitter. that if you train a large language model on the contents of the Internet, then it'll ingest all this data, and it will affect the quality and the behaviors of the large language model. you also have additional techniques and tools that you apply to try to even after you've trained the first, st underlying model,  the pre-trained model. Then you might want to do some adaptation to change the behaviors of the model after training. And  there are ways to do that. one simple way in the that people have tried in the past is to add something called a control token. Just do this  you could prepend special tokens  polite or impolite in front of the sequences, and then train a language model with that with those annotations. And you can do this is a very flexible paradigm. Again, assuming that you're able to derive this set of negative contrast words. The last term here, which is, predict the  word given all of the previous contexts. And then, in unlikelihood training, you also have an additional term. here it's the opposite sign is one minus something which is  the C is the contrast words. and then you throw this into your objective, and then you have some coefficient to weight it a certain way, and then off you go. you can do this, but the results may be mixed. Oh, the model shouldn't generate because it's not the correct answer. there's a stronger pressure here with this approach to push these things down. is, is there a possibility additional numbers to get the calls? ,  I'll try to see if I understand the question. , that's an interesting approach. Is it a widespread approach? there's 1 line of work which is  related to that, I would say, at a high level which is  this, chain of thoughts, work where you have a model, generate outputs, and then you condition on the model generator output to generate something else. And  it also allows it to inspect and choose,  to correct something and fix something. , that's not a question of adversarial environment. , in the previous slide, when you're conditioning the model with some  token or some  prompting context. ,  the question is, if the model is conditioning on a token to generate its outputs. Or oh, I see that. Pre-trained language model is trained on some data. And  the other major trend   is you try to fix these issues with human feedback where you sample a collection of model outputs, and you get people to decide which one they prefer, and which one is better, and that becomes a reward signal, and that reward signal can be used within this other machine, learning paradigm that we haven't talked about called reinforcement, learning to try to affect the model's behaviors and its outputs,  that the model tends to generate things that give it a higher reward. you can reduce the reward constructed loss for the model that can be used to backrot it to update the parameters. The reward signal is used within some objective function and with back propagation in order to change the model parameters. I'll talk about a little bit more about evaluation of Nlp systems."
    ],
    "Topic 3": [
        "we're people online can hear us. We're going to talk about natural language generation. but it's good to learn about them to expand your toolkit. one way that we can categorize different Nlg approaches and  the field, is to think about the  generation you're doing , what is the format of the source material from which you're doing generation. And  I'm going to divide that into something I call data to text generation versus text to text generation. And you don't have to do. Natural language generation and abstraction has many use cases. And  that's why we need to talk about natural language generation overall. Again,  this was  a slide. This is  a similar to a slide from the very 1st lecture. in the 1st lecture we define natural language understanding where the issue is, you have ambiguity, and you want to do disambiguation. whereas in natural language generation the concerns are a bit different, because it's about selecting appropriate content which we covered last class, and also selecting the appropriate form in which to express that content. then we can divide up Nlg into data to text versus text to text, depending on the source, the  input. Almost all of the text is scripted ? That's that's natural language generation, ? It's it's not very interesting from our point of view, but it is a kind  natural language generation. and there's natural language generation here as . you don't think about it as such, but it's there. because, , I'll take this street and 2 words, or until Pine Avenue, and then follow Pine Avenue until some other place, and  on, and  forth. This is also a type of natural language generation. if you think about it, how this works is that there are templates that are formulaic patterns that you fill in and you get the to get the final sentence ? You do not want diversity. Oh, here's a joke, or here's a poem for you to figure out the directions that you need to take  that does. Is that gonna be true in the future? the abstract  thing that you need to fill in. you implement with some machine learning system or a neural network or something or a large language model. And and , and then in practice, how you  design it for a particular system. You might , some of these steps might be combined together as part of some module, and some of them might be separated out, depending on the specific ways. You need to do content selection. You need to do document structuring. ,  content, selection is about deciding what to say. and what is the relevant knowledge about the world that will inform how we do content selection. And how do we approach content selection there? Anyone remember those, any one of them? ,  that goes to show that content selection and determining what's important content. When you're doing generation, you want to respect those. The 3rd step is to select the specific lexical items that you might want to use to generate the final output. Suppose , you have a data to text generation setting where the input is say, the results of some weather forecasting system here are represented in some  a made up a formal. , you have to select the specific words in the language you're trying to generate this into, to represent those concepts. again,  all of those things we talked about on the side of a natural language understanding. we have to do the opposite, the reverse when we're doing natural language generation. And you have, , audio output, etc, etc. the thing with the Nlg with the literature on Nlg is that up until large language models the field was very diverse and not unified, because the in the form of the input was  diverse. They have all different sorts of fields and different types of data and values. and then others define Nlg by as generation from some structured linguistic representation of the types that we've seen in class. And there's no uniformity there and then for the text to text people working on summarization and translation, the input is itself text. the input types were all different from each other, all different. And  the field was not unified, and that made it  difficult to work in this area for a long time. one of them is called referring expression generation, which is you come up with  a scene  you can imagine,  coming up with,  a picture with a bunch of different objects, with different properties. You could call it  the freestanding blackboard, or the blackboard with wheels or the blackboard on the left. Then, there's some standardization there. And  this goes to show the state of the field back, then. this type of structure you see here on the  is called an attribute value matrix. and then the syntactic role would be. and then it handles closed class words , if you have this thing  a personal pronoun, that's  a feminine gender in the 3rd person. that you start off with this complex structure, and then through a series of deterministic transformations, you gradually make decisions to . And each of these steps involves rules. the advantage of this type of system is that you have a lot of control over what happens. this is a lot more familiar to us in,  the recent literature. ,  you have your source text , if you're doing summarization or translation. and then you have all of the tokens you've generated  far. This is  a super silly question. One way is that to say that there are 2 different senses of the word generative. There's 1 sense which is  purely about the type of machine learning model. And one sense which is about, do you  generate some language output,  you can reconcile it that way by not reconciling it. it's   this alone isn't enough to tell you whether the model is generative or not generative. and all of those steps in this in the Nlg architecture of  content selection, and then  not content selection. But after content selection  with  the document structuring and micro planning surface realization. They're all clearly separated step and distinct steps in this rule, based approach. you can do it with a lot of data. You can just feed it some data and , get it to run and adapt. , you don't need to handcraft anything. This one is  arguable in both directions. But another difference between the 2 is that in the rule based formulation it's in the ideal case is task independent. If you're doing any  fine tuning or adaptation. You can have some  neural data to text generation as . and then come up with a way to embed the input data structures into a format that can be ingested by the neural model. And  this can you can have a combination approach as . This is a data to text generation task. yes, this is artificial and contrived. And then the output is a natural language sentence. That depends on the word's presence in the info box table. this is called an info box on the left here. and  then it's represented in this way,  the field, name the index from the start, and then the index from the end. here it's  name 1, 2.  name is the field name, because the field is here. And then the span  it's it goes from  a 1, 3 and  forth. then,  far, we've covered data to text generation. We can also talk about text to text generation, such as summarization where the input is other texts. And then here, the thing to keep in mind is that when you're doing text to text generation because, you're trying to generate some new text. , if you're changing the language, then it's a machine translation task. If you're changing the complexity of the language, then it's some  text simplification task, or something that I'll introduce called sentence fusion. it's used in Nlp as , or it can be used in Nlp as . This is a task that I've worked on myself during my Phd. and then in the outputs you can combine parts of the 2 sentences. And  ,  there's a lot of constraints here. the idea, the general approach that one line of work has taken is that you represent each of the individual sentences by some structured representation  their dependency parse tree. And then you can create a sentence graph sentence graph by merging the input sentences, dependency trees at the nodes with the same words. then, assuming this made sense, then the  part is , you, then need to create an optimization problem to extract a new sentence from your larger graph structure. The issue is that there are many things that you want from the selections, and there are also many constraints. You don't want a sentence that's too long. and then to perform the selection with respect to these constraints. And you write down an optimization problem where you explicitly write out all of these constraints. again, this is, this represents a very different approach to thinking about AI problems. 2. , , that's more than 0.  this rep,  back in  the late eighties and early nineties that the wave of AI back then and the hype back then, was based on prologue. which is that you don't tell the model how to solve the problem. and you don't give it examples and ask it to , figure out the function. And we want to select nodes within this graph which correspond to words that will appear in the output sentence. that's 1st we need to define the nodes. We select this edge in the sentence graph and 0 means we don't select this edge. And ,  this is one variable in our optimization problem. and then everything else you can write out as constraints. then, in this type of approach. ensures that each word has at most one head which you need because you want this, the overall selection to correspond to a tree. you don't just select  individual nodes. ,  you've spotted something which is good. take the inspiration from this, that,  there are constraints. and then feed it into a neural model, and then ask the neural model to do its thing and predict some outputs, and then hopefully, through training, it will learn to respect the constraints. That's definitely a good approach to try. and then you feed that in as coefficients to some formulation where you have a setup  here, where you then solve this constraint optimization problem. There exists polynomial time algorithms for that although the most efficient ones are not. And  that is its own field. and then you have general purpose off the shelf solvers that you can run to solve those optimization problems. if you're working on  other kinds of data where you don't have a large pre-trained model. That's just doing a lot of things for you. one trend is to think about correctness again. you've  all seen examples of this in  of  hallucinations and errors of large language models. Another trend which is still ongoing, because it's not a solved problem by any means is to do controllable text generation. And we can't just release them into the wild and have them adapt to that. and a big part of  training of large language models these days is to figure out how to find good content source, appropriate contents, and  to filter out the inappropriate content that exists on the Internet for training and then also through later through in throughout the development pipeline. The idea here is that you just prepend the special token that indicates the type of content you want to generate. and then at test time, you can control the output indirectly by prepending the token corresponding to the target property as . which is to tell them decoder what not to generate. That's the that's the standard log likelihood objective. It's  the set of words that you don't want the model to generate . Doesn't that mean the model is  quite responsive to certain tokens. They call themselves   they call it jailbreaking. which is  usually the setup is you have a large language model. that's another way to try to control the generation."
    ],
    "Topic 4": [
        "Here is the plan for today. And we are going to look at,  techniques that are not currently, , the trendiest or the most popular. instead, we're going to look at a broad range of approaches. And also we're going to look at an approach to thinking about problems which  used to be really popular, and it still has its merits. And we can talk about that which is a more declarative approach to optimization. And we contrasted extractive summarization, which is where you take snippets of the source text and concatenate them together versus abstractive summarization, which is where you compose novel text not found in the source. But then, if we want to do abstraction, then we have to do. again, think about the case of, say, product reviews. Sure, you can extract specific top reviews and put those comments together. We can also compare and contrast Nlu and Nlg. Hey, if you play a video game up until very recent times. You reach a certain point in the game, and then a message pops up, and then and then you have , and there you go. Here is another one here I searched up the driving directions from Mcgill to Mila. The Research Institute in the little Italy neighborhood. It will say, these driving directions to you as you reach the appropriate location. this is very temp, templatic and formulaic. , this is , I would say that,  for something  a GPS system or some other very routine formulaic things. you want things that are highly structured and expected of a certain form. , depending on the application and depending on the usage, you might just want to handwrite some rules and handwrite these templates and then fill those templates in with the appropriate information. , if you go into industry, here are more stories from my students. And then sometimes, I teach them unpopular things. And they're , why are we learning this? And I got the last laugh. usually in a template, you have these slots where you need to put stuff in the slot. in this example, if the template, if the overall structure of the template is a take the Slot Street name to the Slot Street name. The slot fillers are Rue University and Avenue des Penaux. where you need to do you might want to think at a more high level about all of the steps in the architecture of an Nlg system, and then think about which ones you want to architect more, and which ones . we can talk at a high level about these steps. One of them was centrality,  repetition of a word or concept within source documents. using the discourse structure of a document to find out where the important things are. And then some of the general trends in the context of summarization still apply to this step. this is different from  using the discourse structure to find what's important. this is  going in the reverse in the opposite direction. Then  to consider some of these factors  the importance of the concepts, because this is the OP. important information tends to be found in certain locations. and then you might want to generate discourse relations and think about the coherence of the passage that you're generating overall. one specific instance of this is something called argumentation theory, which provides some guidelines on how to arrange information. , the 1st sentence, in your output might present the location and time that the weather forecast pertains. remember, this term comes from. And then, in other context,  you just need the 1st and last name, and in other contacts you might want to refer to that entity with their title, etc. there are different possible levels of input specifications. nevertheless, there have been a few standard tools and task definitions in Nlg. , you have various options, you can just call it. which is they assume a highly detailed semantic structure. then the idea behind this generation task is that you assume that, , you're given this type of highly detailed semantic structure. in that system at a high level. hopefully, you get the idea. Engineer all of these rules, and you have to figure out the order in which they apply and check it against these structures. And you have to be an expert on the input structure. You have to be an expert on this meeting representation and on the rules of the language that you're engineering this for. And also you have to  , how realistic is it to assume that you  have a semantic structure  this meaning representation  this in the application that you're concerned with. that's that's a major reason why this approach did not take off. at the opposite end of the spectrum, I would say, would be something  a neural Nlg model. But then, these days it's  a pre-trained language model with, based on a transformer architecture. Oh, regenerative models that my understanding was that they join learned a joint probability distribution. But then here we're learning a conditional probability distribution over generating tokens. And how do you reconcile these 2. , there, that's an excellent question. There are 2 ways you can reconcile it. what are the positives and negatives of things just  thinking about things this way. In this second approach of  a neural nlg. You just assume that the pre trained language model, , can figure all of that out. what are some pros and cons of the second approach? If there's 1 poorly generated token, will it affect future? This model is very accessible. that's a positive which is with the neural approach is very accessible. , everyone can just use a pre trained language model these days,  fine tuning. It is not  accessible, because, . it may be different levels of accessibility requiring different things. But if you ignore that, then it's a task independent where it was the neural energy approach. And  then what you could do is you could formulate the inputs. each word is associated with a word embedding and an embedding. if this is your input table it gets formulated, reformatted in this way, ,  it's , . at a high level, then, just  somehow converts your structured input into a format that you believe contains the relevant information for the neural language model to process and then on the output side, then just get it to generate text. And it may be successful or not. And it's really one instance of a broader class of methods and a more general approach, which is to think about things declaratively. a fair number of you great. One would be that you want to present a union of the information in the input sentences. There, and the different colors here represents where you got the information from the 2 input sentences. ,  what's interesting about this is that if you think about this problem there are many possible. The idea here is that  you write down. And then this becomes an optimization problem that has a certain form that you can then pass on to a solver that does that figures out the solution for you. It was based on this idea that we can have general purpose algorithms that solve any problem as long as you can write down your problem in a form that the solver recognizes. This is a similar idea. this idea, this approach, is called a declarative way of thinking about problems , say, declarative programming or declarative method to solving problems. It's about just writing down all of the constraints and everything  about the problem into a form such that you then ask a general solver to figure it out. And  this will say that it has to be 0, or one where one is. and then an important score, which is  how important is the word W in this context. It doesn't really matter, because you can always just take the negation to flip the min to the Max and whatever. we're maximizing this objective function. The 1st constraint, it turns out. the 1st constraint ensures that each word has at most one head, and the second ensures that it's connected. Here's a question we can  write another constraints to constrain the number of words in the output as . This formulation does not specify the order of the words directly. it's because you believe that there are these other concerns with that,  you, you want the output to be a  formed tree. it turns out that you can write out a constraint to we. indirectly, by counting the number of edges, I'm thinking, just taking the sum of all. You need to sum up over all of your x's and make sure the sum of all of them is less than your budget, which is a constant in the optimization problem. there'd be some way of passing the inputs in where the , the output of the model is whether or not to select this edge and then training for that objective? what is the order in which you do things? It's because all of the variables in this setting. and then it turns out there. Then it becomes a Np-hard problem. and we can leverage those tools and those findings to apply it to natural language in certain natural language generation tasks or other Nlp tasks if we'd . just , there are declarative. This, the general idea I want to convey here is that there's a whole other approach to solving problems which is not currently that popular but has existed and was influential, was the dominant paradigm for a while. Which is this idea of doing things in a declarative way. Then then you can consider approaches  this as . that's also another reason to do this. ,  I'll end with a few more trends in the energy literature in the past few years. They're closer to  the neural settings. correctness and factuality sometimes these days. It's called the hallucination problem, although, to be clear, this terminology implies that language. , , one problem we talked about just  is that if you do everything in a neural way. If you have an automated news summarizer, and it gives,  the wrong basic information about an event. And  we train a system that it doesn't fully fix the problem, but it partially fixes the problem, and that it's a post editing model to detect these factual inconsistencies and then to correct them. And we did that through artificially perturbing news articles to come up with  an automatic set of training data with factual inconsistencies and then training a model on that artificial data. It's , predict, the corrected summary based on the potentially incorrect or corrupted inputs plus the source documents and how this is implemented, I'm sure by  you can , imagine this for yourself. and they just released it into the wild on the Internet. these days  that would be considered very naive. And they also got the model. They had this vision that people would interact with it, and the model would get gradually get better and better through its interactions from users on the Internet. It was  just  a matter of  one day before Microsoft had to take down this Chatbot. It's funny, and it is funny, but it's still a concern, ? , it's just that this these days is happening at a larger scale, and  with a slower timeframe, and that people could be writing things on the Internet and that things that could itself be automatically generated  to try to influence the overall contents of the Internet. You can put in other things  desired words or contents, or you can put out. People have also tried putting  a plan of what you want to generate overall into the prefix, the prompt,  of what you're gonna of the language model and then start the generation process condition on that. You put it in the prompt and then condition on that generates another approach that people have tried is to do unlikelihood training. this is interesting because it's  the opposite of  this standard approach to neural training. the standard approach to neural training. this is the objective function of unlikelihood training, and the standard approach only has the second term here. really, what we want is when we want to tell the model what not to generate at some semantic level. oh, , what's the difference between this and contrastive learning? the difference is that in contrastive learning your contrastive set, the negative set. The interpretation of the negative set is that it's just the things that are not the correct answer. in contrastive learning you have, this is the correct answer. And then we're going to  sample from the everything else to come up with a contrast set to separate the 2 here with unlikelihood training. It's  similar, but the difference is that here the contrast set, the negative set are things you. It's not just things that you think. then, how you draw this set of  Ct is different. The set Ct here is  specifically in the negative things. , in terms of the general idea of using the models, outputs. And  this is more general in that the model can choose to continue what it's generating in the same directions. But , that's what I can think of   in terms of a general approach. and  some of it contains sensitive information, and  you have some mitigation techniques to  through adaptation, model adaptation with  reinforcement, learning or something to get it to not reveal sensitive information. The main assumption is that since the pre trained language model has seen the sensitive information in training, it's there somewhere in the model parameters. And if you search for the  sequence of ,  fake tokens artificial tokens, or  just something on the on the inputs to prepend, you can get the model to reveal the information that it's seen that it's not supposed to. I didn't talk about in the slides is  reinforcement learning. , at a high level, it's the same. The difference is that in supervised learning you have a loss that is defined in a very structured way, that each sample is associated with a label, a label or a correct label or not. whereas in reinforcement learning it's just some scalar. It's just some scalar of  good versus bad,  a positive signal versus a negative signal, and that gets propagated down to  the sequence of decisions that the language model has to make as it's doing. reinforcement learning methods typically are harder to get working and to train. that's all I want to say."
    ],
    "Topic 5": [
        "Jackie Cheung, Professor: , Hi! And we're going to look at rule-based systems. We're going to look at shared machine learning systems. ,  then, here's the outline. And  we focus on extraction because it was easier to think about. We can focus on these interesting issues to do with modeling, content, and modeling the relationship of words with respect to each other, and that how that helps us inform what is important. , one key one is that it lets you aggregate information from multiple inputs. and it doesn't really make sense, it makes less sense to do extraction. And  the  we know that  sometimes. And if you use your phone's GPS system, then this is also  it will  spit out. And then there's a slot here,  take and then street name to street name. And then  you have follow street name to street name. and  at some point, it'll say, turn left at street, name, or turn  at street name. You do not want creativity. ,  that helps us with,  our mental processing. And your GPS system  is telling you. That doesn't make any sense ? And they do all this cool work. And then I have one student who, when he was here, he was very pro neural networks, deep learning, all that stuff, all the old work is useless. Nlp is still based on rule writing rules and  thinking about grammars and stuff. But at least   you still need to know this stuff you still need to know,  about templates and rules, and  forth. And then  just a bit of terminology for you don't have to remember this, , but  it might help you to if I say it to you at least once. in terms of the templates. when I say , take street name to street, name the street name that's called a slot. And finally, something called surface realization. And if you want to take a very principled approach to this, then you should think about what is the communicative goal of the energy system. It's more , how do you structure the outputs of the natural language generation system  that it's the most effective at accomplishing the communicative goal that you have in mind. there's some work at in at the intersection of, say,  psychology, of reading and Nlp and computer science that looks into things  this. some of this you also find in  writing advice,  for students. , , you can talk about, or you can structure your documents  that you present the main claims first, st and then you can try to arrange and discuss the supporting evidence, and then present and debate opposing evidence, and  on, and  forth. There's gonna be a blizzard with a high of minus 5 and a low of minus 10 and 30 kilometer per hour winds, and the location is Montreal. Then,  the Prime Minister, then depending on   the 1st time you introduce the entity in your document, you present the full name or something. this might be what you find on Wikipedia or something. and then the last step in this abstract description of an Nlg system would be surface realization, which is to fully convert all of those the specified discourse plans, and the outputs and decisions of the micro planner into the output form which takes the form of these sentences that you see or you can run a text to speech or something. it could be some highly detailed semantic structure where all of these decisions are already made,  with lexical items and tense and aspect and mood of the verbs and referring expressions. or it could be shallower in that the inputs to the surface realization step could be  a dependency tree. Some people working on Nlg were interested in  generation from structured databases, but each structured database has a different format. Suppose our scene was just what you see in the classroom  , and the task is to generate that thing. But then there are multiple blackboards in the room,   that's not a good choice. , and then surface realization. there are some tasks about generation from dependency parses to , say, output sentences or something. But  one from the early literature called FUF. and then they apply a cascade of deterministic rules to convert that structure into a string. This is  a tree structure. Here this is meant to be a clause, and then within the on the inside there are,  smaller and smaller units with their own category. and the output sentence that this structure is meant to correspond with is she hands the draft to the editor. and you want to generate the output sentence. What it does is, it maps it 1st maps these thematic structures or semantic roles into syntactic roles. a semantic rule will be something  this is the agent of this predicate. and then that handles syntactic alternations , are you using the active form or the passive form, or some dative alternation? for all of the missing information, you fill in some default features with agreement, features, and  forth. and then you make sure that the subject and the verb agree in number, because, remember, we have to do that in English. it's easy to make a mistake in engineering this. It's it's not  easy to get these structures. It doesn't make sense  it's a lot of work even to convert whatever input you happen to have into this structure. And your source text might also contain instructions  prompts, and then you have all the tokens you generated  far, and you continue to generate. in the course we've defined a generative model as a model that gives you a joint probability distribution over everything of interest, over  your inputs and your labels. Usually it does turn out that the you can write it as a in the form of a generative model with the joint distribution. if you have the joint probability distribution. you can always , write out this conditional form from the joint distribution. the 2 approaches we've seen  far is one is highly structured, detailed, rule-based. , I see a con that  this can generate the non automatically. It's the type of redirects in terms of. But assuming you have that, then, at least from a technical perspective, it's more accessible because you don't really need to know the details of how the rule based structure works. That's a big thing, ? , , here, let's consider the task. which is a generation of Wikipedia biographies. the task is to generate the 1st sentence of the Wikipedia biography article from its info box. th, this, this is just  a pretty ui. But this is really  something from  a database, ? And  in this work, this is how they chose to do it. And then the output candidates is that you generate something similar. If you're changing the length, then it's a summarization task. Or if you're changing the style of it, then you're changing, then it's then it's called the A style transfer problem. ,  let me 1st introduce a sentence fusion task. Here the task is to combine information from multiple sentences in order to generate an output sentence. You want to do this one, Major. There are many constraints about the form of the output sentence that you should generate. They all exist in that sentence, graph as such. this, this part makes sense. It's  you have,  each of the parses. And then you get this graph structure. the idea here is that you select a subset of nodes in the sentence graph that will form a new dependency tree from which a new sentence can be generated here. , 1st of all, one constraint might be that the nodes must form a tree. but you also want to make sure that the selected nodes contain important words, and the selected nodes should make sense with respect to each other, while also, respecting  some output length. in this particular setting in this particular generation setting, how that's gonna work. then the nodes we can call it  Xl. for each edge in the sentence, graph from word h to word, w with label L. Oh,   ! This this node  corresponds not to a word, but  an edge relation between where it's. And and then we optimize an objective where? this is for all of the edges. You sum over all the edges, and then you have a grammaticality score which tells you how often this head word is generated generates a dependent with this label. which is about , , which edges do you choose to select. and then how good is each of those edge! it only specifies the tree that you select in the sentence graph. But that's relatively easy to do from the dependency graph. And  the constraint of selecting things that are interconnected tree doesn't limit the output, and , remove good potential output sentences. the constraint that you have to select a tree it very  could it could be reduce the quality of these selected outputs. this is all very abstract. But this tree,  each selected edge, corresponds to a word, because it's a dependency tree. It might work very . you could also do the opposite, which is  you have a neural model, and if you have access to the internals of the neural model,  you can derive  scores, or you can take the logits or the posterior distributions from the neural model. And that matters, and also the fact that they are real valued matters in terms of making sure it's  efficient to solve. But , but once you get to  the integer case, it turns out that it makes the math a lot harder because you cannot do . , even though in the worst case it  has,  very poor complexity, results. And this could be applicable even in other domains. Models have mental states which we don't really know if they do. , we looked into this in my lab as  before it was cool. here we have,  some, the original output of  an abstractive, summarized neuro abstractive summarization system which identified the wrong victim of a terrorist attack. And here's the form of that. This could be an Lstm model. And of course, people are not nice, necessarily. And  that shows that ,  we need to have controllability of our AI systems. suppose you have a corpus of text labeled with a property you care about   polite versus impolite , could you please do something versus? rather than single tokens, it started off being single tokens with special meanings. It could be some  overall larger plan for what you're going to generate. it's expressed in a slightly different form, in terms of  some  a cross entropy loss or whatever. it turns out this unlikelihood training can be a little bit tricky to get it to work , and it  makes sense, because,  telling the model what not to generate at the token level isn't exactly what we want, ? And , turn that into  a form that the language model understands. You want the model to actively, not generate. These are things that the model should actively not generate. the question is ,  there's this work in adversarial learning, and then you're interested in figuring out if there are ways for the model to itself, automatically identify things it should not generate. To determine whether it has generated something very strange is a good one. And therefore you can  search and generate adversarial tokens that impact the models. Doesn't that mean it's conditioning on it's paying too much attention to that token, and it might influence the generation . Can you generate an adversarial token that would impact the model . there's a line of work on that as . And this line of work in jailbreaking. there's a line of work there. And then  the other major trend that I haven't. , because it makes looser assumptions about  the structure of what's happening. week there are 2 lectures. One lecture will be on evaluation. and I'll also advertise the course I'm teaching  term a seminar course on evaluation, and then the other lecture will be a guest lecture by one of my postdocs, or Ernst on a topic that still to be determined  to do with natural language generation."
    ]
}