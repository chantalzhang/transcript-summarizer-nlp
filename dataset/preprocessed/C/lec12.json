{
    "Topic 1": [
        "Hello, can you is the microphone working OK? But last class we talked about syntax and structure and last class we talked about syntax and structure and hierarchical structure in natural language. hierarchical structure in natural language. It's a, it's a model of formal grammar that we can apply to model the syntax of natural languages. It's a it's a model of formal grammar that we can apply to model the syntax of natural languages. , that's not the bottom. OK, forget it, I'll just leave it. you can have time to focus on that. there's some in, in our case there's going to in our case. And for us they correspond to words in a sentence. where we left off last time OK,  where we left off last time we asked we asked, or I brought up this question of. we're using formal languages or I brought up this question of  we're using formal languages to model natural languages. we're saying that a particular natural language  English. OK.  we're saying that a particular natural language  English we'll model it as a formal language with the Cfg. And, in fact, even broader question is , instead of for each individual language,  English, French, or German, is, does there exist a Cfp for it? Can we show or ask And in fact, that even broader question is  instead of for each individual language  English, French or German is, does there exist the CFP for it? Can we show or ask whether for any natural language whether for any natural language that we currently that we currently know about on Earth, can all of know about on earth? And it's 2 particular languages. it's only Swiss, German and Babarra. It's not even standard German. , it's just Swiss, German and Babarra And it's two particular languages. it's only Swiss German and Bambara. It's not even standard German, OK, it's just Swiss German and Bambara and all the other languages. and all the other languages. The basic idea is that in these languages And the basic property, the basic idea is that in these languages there are certain dependencies that take the there are certain dependencies that take the form of A to the MB, to the NC to the MB to the N, form of A to the MB to the NC to the MBD to the north. And then, Bambara,  the argument goes along with something to do with morphology of Bambara. And in Bambara or ,  the argument goes along with something to do with morphology of Bambara, . in fact, in all of these arguments about what in all of these arguments about what are natural languages, context-free grammars, and  forth. And the other major assumption here is this assumption that's in common with everything we're discussing today, which is that strings are either in a natural language, or they're not in a natural language. And the other major assumption here is this assumption that's in common with everything we're discussing today, which is that strings are either in the natural language or they're not in the natural language. you really have to buy into a particular view a particular worldview about how language works in order for these arguments to make sense. you really have to buy into a particular view A. About how language works in order for these arguments to make sense. which is , what  formal models and formal What  formal models and formal mechanisms you might need mechanisms you might need to account for the phenomena you to account for the phenomena you see in natural language see in natural language. It's not a new word, because  you've seen it in  programming languages or something. But in natural language natural language has different characteristics compared to programming languages. And in fact, it's  exactly the same thing, but in natural language. Natural language has different characteristics compared to programming languages. You want to come up with a tree structure, with all the constituents that describe that particular sentence. And you want to come up with a tree structure with all the constituents that describe that particular sentence. and you can contrast this. last time we also talked briefly about dependency representations,  and dependency structures to characterize the syntax in a different way. And you can contrast this. last time we also talked briefly about dependency representations, ? And dependency structures to characterize the syntax in a different way. And  this is one key difference between natural language and programming languages. And  this is one key difference between natural language and programming languages. when you're parsing into a programming language. when you're parsing into a programming language, you would you would expect there to be just one parse  of a programming language. expect there to be just one parse, , of a programming language. But assuming that, , we're working with natural languages. And that's because natural languages are OK, But assuming that, , we're working with natural languages, the main difference is we want to recover all possible parses, and that's because natural languages are ambiguous. the natural languages are ambiguous in many, many different ways across many different levels,  I presented some examples of that in the 1st lecture. ,  the natural languages are ambiguous in many, many different ways across many different levels. There are usually multiple parses that can correspond to any sentence of a language. There are usually multiple parses that can correspond to any sentence of a language, and they usually correspond to different and they usually correspond to different meanings. And the sentence is, I shot the elephant in my pajamas. OK, And the sentence is I shot the elephant in my pajamas. I shot the elephant in my pajamas, where there's a prepositional phrase that attaches to shot  there are two parses here for that sentence. OK, I shot the elephant in my pajamas where there's a prepositional phrase that attaches to shot in the first in the 1st parse, and in the second parse in my pajamas is a prepositional phrase that attaches to the elephants. And in the second parse, in my pajamas is a prepositional phrase that attaches to the elephants. Can anyone see how they might have different meetings? Can anyone see how they might have different meanings? in one of them, I'm wearing the pajamas, and in the second one the elephant is wearing the pajamas. , the full joke is, I shot the elephant in my pajamas. in one of them, I'm wearing the pajamas, and in the second one, the elephant is wearing the pajamas. the full joke is I shot the elephant in my pajamas. , that's the full joke. OK, that's the full joke. that would correspond to  that would correspond to the second parse, ? the second to parse, ? Because it's the elephant in my pajamas. Because it's the elephant in my pajamas. what are the different kinds of parsing algorithms that exist? If you think about it? There are 2 general strategies. what are the different kinds of parsing algorithms that exists? If you think about it, there are two general strategies. There are even strategies for parsing that goes both up and down. There are even strategies for parsing that goes both up and down. , these there are these different kinds of parsers and they have different properties, and with certain kinds of grammars, one tends to be more efficient than the other, and  forth. It only works with a particular  Cfg. It only works with a particular  CFG. creating a new non-terminal symbol. you just take your terminal symbol and you create a new non-terminal specifically for that case. you just take your terminal symbol and you create a new non terminal specifically for that case. here S is a terminal symbol,  just turn it into  a new non-terminal, and then add an extra rule of x 2 SX. just turn it into  a new non terminal and then add an extra rule of X2SX2 rewrites to S and both of these  fit CNF. And  you have this particular case, where for every rule in the grammar where B is the left hand side. this one, how it works OK  this one is the hardest 1.  this one, how it works is if you go is if you go back to read the this for every rule in the grammar we're back to read the this, for every rule in the grammar where B is the left hand side, you copy B is the left hand side. This means in our case. Every time This means in our case, every time we see a we see a rule where N is on the left hand side. rule where N is on the left hand side, we and then we have to. ,  this is how you would do it. you have two copies of the rules of generating I and elephant in pajamas, one with N on I and elephant in pajamas, one with N on the left hand side and one with Np. the left hand side and one with NP. There's the tree in the second rule. , the first rule would be  X1. then it would become VP. Vp goes to x, 1. Because if I can get through. OK, And  we have P goes to N. and  we have key goes to N, , almost there. I deleted the rule that I needed, and I didn't put in the Oh, I, I messed up. I deleted the rule that I needed and I didn't put in the. That's all , OK,  let a sentence have ,  let a sentence have n words. We're going to create a table such that cell a cell in row. I column J corresponds to the span from We're going to create a table such that cell, a cell and row I column J corresponds to the span WI. All it's saying is that cell stores all of the constituents that spans a certain subspan within your sentence of the words within the sentence. All it's saying is that cell stores all of the constituents that spans a certain subspan within your sentence of the words within the sentence. and also because And also because time works in flows in one because time works in flows in one particular order. ,  intuitively, the entry at each cell is the list of non-terminals that can span those words according to the grammar. OK,  intuitively, the entry at each cell is the list of non terminals that can span those words according to the grammar. OK,  here we have the sentence we want to we have the sentence, we want to parse. I shot the elephant in my pajamas, and here we have the updated Cnf grammar from before. I shot the elephant in the pyjamas. and each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. And each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. this cell corresponds to the word I,  this cell corresponds to the word I, and this cell of three to four corresponds to the word elephant. and this cell of 3 to 4 corresponds to the word elephant, and  forth. and the cell that corresponds to the entire sentence is here. This cell corresponds to 0 to 7.  it's all 6 words. And the cell that corresponds to the entire sentence is here this cell corresponds to zero to seven. it's all six words where I say this cell whereas, say, this cell of 2 to 5 corresponds to the words, the elephant in of two to five corresponds to the words the elephant in OK I'm using Python style indexing where the last, , I'm using python style, indexing where the last, the second number is . the general idea is that we're going to go from small chunks to big chunks. the general idea is that we're going to go from small chunks to big chunks. those also involve the base case and the recurrent step. it's exactly the same, except the structure is different because we are assuming a different  structure. those also involve the base case and the recurrent steps. it's exactly the same except the, the, the structure is different because we are assuming a different  structure topologically. ,  for us, the base case corresponds to individual words. OK,  for us, the base case corresponds to individual words. That that makes sense, . OK,  , here are the first 3 words. And in our grammar, it's going to be NP and N for shot it would be V and for the For shot. , , the span for the cell for the span I shot D would be from  a  , the span for the cell for the span I shot B would be from  a position 0 to 3 exclusive. all we need to do is check all of the possible break points in between the start and the and see if you can build a constituent end and see if you can build a constituents with the rule of the form A from. we care about position, I to J.  whether we can break it up into I to M, and then M. To J. the rule of the form A from . we, we care about position I to J.  whether we can break it up into I to M and then M to J.  for a particular breakpoint, M.  for a particular break point M, you see whether You see whether you can build a rule with the constituents that you've already found there? ,  here's an example of a recurrent step. OK,  here's an example of a recurrent step. here I shot is  a 0 to one ? here I shot is  a 0 to one, ? that means there's only one possible break point, which is you break after the word of position 0.  that means we have to check for rules that are either Npv. this cell is corresponds to the elephants. this cell is corresponds to the elephants. Again, there's only one possible break point. that's just a different way of late. It would be first saying you would say X2 and then that 2 for 3:00 and 3:00 to. then the second, the second constituent you've built here would be X2 that 2:00 to 3:00 and 3:00 to 4:00. , you can store multiple constituents in each cell. ,  the last part is that you have to. if you think about it. if you think about it, you have to fill you have to fill in the table in the table when you're processing a cell, you have when you're processing a cell, you have to make sure that you filled out all the cells to the left and below it to make sure that you filled out all the cells to the left and below it, everything  to the everything  to the bottom left of it from my perspective. or you can do  the you can do it  one column at a time, bottom up. Or you can do  the you can do it  a one column at a time bottom up. as long as you filled out everything As long as you filled out everything to the bottom to the bottom left of a cell when you're processing that cell. bottom to top for each Bottom to top for each column,  that also works. We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect. we're going to see whether we can recover the two parse trees that we expect. For shot it would be V. For a shot it would be V for the For V it would be debt. Or N, N. or in it would be P. Or N it would be P, for my it would For my. And  I'm gonna go And  I'm going to go bottom up and column bottom up and by column, left to . column by column, left to . for shot, we already checked that ? there's nothing we can build there,  that cell is empty OK,  for shot, we already checked that ? that cell is empty for  this is for   this is, for I shot . from zero to two,  we have 1:00 to  we have one to 3,  this corresponds to shot B, 3:00.  this corresponds to shot B.  do we see any V debt in our grammar. this cell is also empty. this cell is also empty. I shot B.  here we have I shot B.  we have 2 possible breakpoints. we can have I, and then shot B,  we have two possible break points. we can have I and then shot B, or we can have I shot, and then B we can have I shot and then B, ? I and then Shati would correspond to 0 to one, and then one to 3,  I and then shot D would correspond to 0 to one and then 1:00 to 3:00.  we can't build anything there, because there's nothing that spans one to 3,  we can't build anything there because there's nothing that spans one to three. and then for I shot, and then B, that would correspond to 0 to 2, and then 2 to 3. And then for I shot and then B that would correspond to zero to 2 and then two to three. this cell is also empty. this cell is also empty. Question, could we have,  generalized that, based on the fact that the cell to the left and to the bottom were already in. Could we have  generalized that based on the fact that the cell to the left and to the bottom were? We have generalized that to based on the fact that this out of the bottom and the left already empty. Could we have generalized that to based on the fact that this out of the bottom and the left are already empty? You cannot take that as a heuristic, because in this particular case it's 0 to one and then one to 3, and then 0 to 2, and then 2 to 3. You cannot take that as a heuristic because in this particular case, it's zero to 1 and then one to three and then zero to 2 and then two to three. ,   we are at 2 to 4. OK,   we are at 2:00 to 4:00.  there's only one possible breakpoint. there's only one possible break point. , I'm going to give up on drawing the arrowheads. OK, I'm going to keep up on drawing the arrowheads. , we have shot the elephants. OK.  we have shot the elephants. We can go, one to 2, and then 2 to 4, We can go one to two and then 2:00 to 4:00 or 1:00 to 3:00 and then three to four, or one to 3, and then 3 to 4, ? with one to 2, and then 2 to 4. there's Vp goes to Vnp that works. there's VP goes to VNP that works. That should be a Vp, Bp goes to B VP goes to V&P and X1 goes to. ,  how about one to 3 to 3 to 4? OK,  how about 123 to 3:00 to 4:00? We have 0 to one and one to four. That column  we're done that column  column. Is there any rule for n, and then P. No. Is there any rule for N and then P? the cell is empty 2:00 to 5:00. we have to check 2:00 to 3:00 and then three to five, which is not possible because three to because 3 to 5 is empty. 2 to 4, and then 4 to 5. Is there any rule of Np. And then P  2:00 to 4:00 and then four to five. Is there any rule of NP and then P? Nope, is there any rule for x. 2, and then P Nope. one to 5,  one to five. Vpp, is there any rule where that's the  hand side? VPP, is there any rule where that's the  hand side? and also x 1 p Nope,  that cell is also empty. if I'm going too fast, just . if I'm going too fast, just  raise your raise your hand to get me to explain some more. hand to get me to explain some more. But the basic idea is, if a cell is empty, then But the basic idea is if a cell's empty, then that's not then that's not possible to build something there. , and then 0 to 5. 0 to 4, and then 4 to 5. ,  column 4 to 6. , ,  this cell is empty,  we can't go 3 to 4, and then 4 to 6. This cell is empty,  we can't go 3 to 5, and then 5 to 6. this cell is empty,  we can't go 3:00 to 4:00 and then four to six this cell is empty,  we can't go 3:00 to 5:00 and then 5:00 to 6:00. , OK, two to six is I don't think anything is 2 to 6 is, I don't think anything is possible there, either. The last thing you said, we should check 1:00 to 4:00, then 4:00 to 6:00. And this all makes intuitive sense, because it's  I shot the elephant in my . And this all makes intuitive sense because it's  I shot the elephant in my . Yes,  is there a heuristic rule? No, the rule is just. Is there a heuristic rule ? The rule is just you're just checking that break point. then try, OK,  the last column will be ,  the last column will be  interesting again. OK.  here we have 4:00 to 7:00.  the possible. Because 4 to 6 is empty. the first possibility is NPPP. do we have any rule of the form. Do we have any rule of the form NPPP? What about 3 to 5, 5 to 7. 3 to 6, 6 to 7. What about 3 to 5? 2 to 7.  we have 2 to 3, 3 to 7. 2 to 4, 4 to 7. ,  shot the elephant in my pajamas. OK,  shot the elephant in my pajamas. , let's use blue this time. OK, let's use blue this time. How about one to 4, 4 to 7. How about one to four? Do we have any rule? Do we have any rule which is BPPP? How about x, 2, or ? we have a different vp. we have a different VP, OK. ,  one to 5, 5 to 7 is not possible. we only have one cell left  one to five, five to seven is not possible. we only have one cell left. And if things worked out properly, there should be a And if things worked out properly, there should be a bunch of S's here. the other S The other rule that works is the other rule that works is Npvp, but with the other vp. NPVP, but with the other VP. 0 to 4, 4 to 7. Yes, when there are 2 Vps in one cell with the notation where you refer to  Vp, and then the cell number, how do they distinguish? When there are two VPS in one cell with the notation where you refer to  VP and then the cell number. in the other notation, you keep a list of you built a Vp OK,  in the other notation, you keep a list of you built AVP from 1:00 to 2:00 and from one to 2 and then 2 to 7, using the V and the Mps there. You built another VP which is one to four with which is one to 4 with the x 1 there, and then 4 to 7 with the PP. Oh,  to be consistent here, this should be 2 different. S's Oh,  to be consistent here, this should be two different s because we built two different s because we built 2 different. And then there you go. find is not just the number of s, s you find in the top cell. , every time there's a decision point  every time there's a decision point where you create where you create the same non terminal constituent,  in somewhere in a smaller span. Because if you if suppose that we had found different ways to build the Vp Because if you if suppose that we had found different ways to build the VP in  a in  in  a in,  one of the sub cells, and you do it multiple. And then that's used in a parse somewhere else. 1 of the sub cells and you do it multiple and then that's used in a parse somewhere else. the total number of ambiguous  the total number of ambiguous parse trees for the parse trees for the sentence will be the you have to multiply everywhere you have a decision point. , what about the s, that's not in the top, ? What about the S that's not in the top ? What about the s, that's not in the top ? That's not in the top . But it's not the sentence that we're currently interested in. but it's not the sentence that we're currently interested in. All this tells you is that I shot the elephant is also a sentence in this grammar. I shot the elephant is also a sentence in this grammar. OK,  although there was that joke by Martial Grouse  although there was that joke by Marshall grougs. And  what we're going to do is we're going to associate each rule with a probability. And  what we're going to do is we're going to associate each rule with a probability. I made this roll up. NPV writes to NPPPI made this roll up,  it has a 0.2 probability and  on and  forth. and then the probability of a parse tree is  going to be the product of the probabilities of all of the rules in that parse tree. And then the probability of a parse tree is  going to be the product of the probabilities of all of the rules in that parse tree. that forms our probability distribution. symbol A on the left hand side. , , you have a probability distribution associated with  , you have a probability distribution associated with all of the noun phrases,  all the noun phrase all of the noun phrases,  all the noun phrase rules. On the left hand side must sum up to one. And  then we can define a new parsing problem which is to recover the most probable parse tree which is to find the Argmax among all the trees And  then we can define a new parsing problem, which is to recover the most probable parse tree, which is to find the Arg Max among all the trees that you can  recover that you can  recover using your PCFG for that using your Pcfg. ,  probabilistic parsing recover the best possible parse for a sentence along with its probability. OK  probabilistic parsing recover the best possible parse for a sentence along with its probability. and we want to find the Argmax of the probability of And we want to find the ART Max of the probability of T of  the of the sentence. , we're gonna augment that. our table with the list of entries for all of the ways we could build things, you include the best you include the best probability of generating the constituent with that particular non-terminal. probability of generating the constituent with that particular non terminal, . we're going to augment that with the probability  we're going to augment that with the probability of that constituent as  that constituent as . ,  the table OK,  the table at for the span two to four with a particular non terminal NP as probability of for the span 2 to 4, with a particular non-terminal Np whatever that we've created by using some substructures, some Subs as probability of whatever that we've created by using some substructure, some sub constituents from 2 to 3, and then 3 to 4. constituents from 2:00 to 3:00 and then three to four. There has some probability And then there, these are some other possible entries where  and you got this number because  at 3:00 to 4:00 with the NP, there has some probability, no, no, that. the left hand side with that break point  at each particular break point. You also have to compute the new probability of that constituents. and you do that by taking the existing probabilities from each of the subconstituents, and multiplying with it the probability you get from your Pcfg. For that additional new rule that you're adding to that subtree. It's the probability you get from your PCFG for that additional new rule that you're adding to that subtree. In particular, there might be multiple ways. You might find multiple breakpoints to form  a Vp or something or an Np. cable IJA in particular, there might be multiple ways you might find multiple break points to form  AVP or something or an MP but for the same triple But for the same triple of the span, left hand side and  hand side and non-terminal symbol. You only need to keep the Max probability and the Max back pointers of the span, left hand side and  hand side and non terminal symbol. You only need to keep the Max probability and the Max back pointers for that constituent. You never need to keep the suboptimal one, because that suboptimal one will never be used in the construct in the best possible parse tree. You never need to keep the suboptimal one because that suboptimal one will never be used in the construct in the best possible parse tree if you think about it, If you think about it, because all of these things factor out  locally, there are no long range dependencies between,  the top of the tree and the bottom of the tree. If you create a different non terminal symbol for the a different non-terminal symbol for the same span. same span then you have to also store that separately because the probabilities there is there's this for a different because the probabilities there is, there's it's for a different constituent. you look at the entries here that correspond to S and you find the entry with the highest and you find the entry with the highest probability. where you throw out everything that is Where you throw out everything that is suboptimal locally. My understanding of this rule is that if they're  My understanding of this rule is that if they're  2 possible ways of creating. The same token we throw out. We throw out this about new one. 2 possible ways of creating the same token we throw out, we throw out the suboptimal one. But if they're two possible tokens we can create, and one of them has a higher probability than the other. one of them has a higher probability than the other, then we keep both. just to rephrase, if you have two possible ways if you have 2 possible ways or multiple ways to create the same constituent or multiple ways to create the same constituent with the with the same non-terminal symbol that spans same non terminal symbol that spans a particular span, then a particular span. Then you throw out all but the best. But then when we're recovering, we and we take the and we take the constituents with the highest probability and ignore other possibilities. constituents with the highest probability and ignore other bus. Is it possible to get the same probability. Is it possible to get the same probability? Is it possible to get the same probability? Is it possible to get the same probability? how do you  train a Pcfg,  I talked about the parsing algorithm first.st  I talked about the parsing algorithm first, but I didn't talk about where the probabilities come from. But I didn't talk about where the probabilities come from. , for the most, for the longest time from  19 Exciting stuff. And  that's the most famous Treebank. In order to estimate these we discussed to do with estimating values of parameters with  HMMS and Ngram models and  forth, in order to estimate these parameters of these new categorical distributions these parameters of these new categorical distributions we're creating in the Pcfg we're creating in the PCFG by using counts. And this is the mle estimate OK. And this is the MLE estimate for that rule, for that rule, for the probability of that rule. the probability of that rule. I don't think I have time to explain the solution. Really, the reason is because,  I don't think I have time to explain the solution, but I'll I'll talk about  why this is . There are many different ways. There are many different ways you could build that verb phrase and it seems  a different ways you could and it seems  the different ways you could use to build that verb phrase they might be. ,  a verb phrase with  an object noun. a verb phrase with  an object noun. And there are many other situations where this the and there are many other situations where the vanilla Pcfg assumption, where you just have a rule, locally of left hand side rewrites to -hand side is too restrictive. vanilla PCFG assumption where you just have a rule locally of left hand side rewrites to  hand side is too restrictive. And another obvious example would be  the difference between a syntactic subject versus a syntactic object. Another obvious example would be  the difference between a syntactic subject versus a syntactic object. is a subject pronoun in English , whereas me would be the object version. Whereas with if it's in  the object position, syntactically, it's much more likely to be me than I.  this, the standard assumption of the Pcfgs does a very poor job of modeling all of these situations. this, the standard assumption of the PCFGS, does a very poor job of modeling all of these situations."
    ],
    "Topic 2": [
        "we are going to continue our discussion about syntax. we are going to continue our discussion about syntax. We talked about these things called constituents. try to recall, we talked about these things called constituents, which are groups of words that acts together as which are groups of words that act together as a unit in a sentence. And we also talked about tests for constituency. And we also talked about tests for constituency, if you Hey? Do you all remember that all remember that. the other thing we talked about is we talked about context-free grammars? The other thing we talked about is we talked about context free grammars. ,   seen on Ed that I   seen on Ed that I posted information about both the midterm that's happening as  as the final project description. please do take a look at those and start working on it with your project partners posted information about both the midterm that's happening as  as the final project description. please do take a look at those and start working on it with your project partners for the for the for the final project. there's lots of things coming up, but I'll try to make sure the deadlines are not most of the Most of the deadlines will not be before the midterm. deadlines will not be before the midterm,  you can have time to focus on that. remember, this is what a Cfg tree  looks . remember, this is what acfg tree  looks . And then, based on that, you use these rewrite rules to rewrite it into non-terminal symbols. And then based on that you use these rewrite rules to rewrite it into non terminal symbols. And remember by convention in natural language, we tend to and remember by convention. In natural language we tend to use capital letters for that. will model it as a formal language with the CFG, but why are we using context free grammars, . But why are we using context-free grammars? Can all of them be modeled as some context free ground? free grammars with the associated context free language. our natural languages context-free grammars. it turns out the answer. throughout the years there have been 2 demonstrations that there are phenomena in natural language in some natural languages  throughout the years there have been two demonstrations that there are phenomena in natural in some natural languages which cannot be modeled by context-free grammars. which cannot be modeled by context free grammars, OK. ? As far as we know, all of the phenomena, you can describe it with some context grammar, OK. , and the basic property. and this is known as a cross serial dependency. And this is known as a cross serial dependency. what this means is that there have. There has to be  M.  what this means is that there have there has to be  M things of one type, followed by Things of one type, followed by N things of one type of a second type, followed by M things of a 3rd type, and then n, things of a 4th type. N things of one of a second type, followed by M things of 1/3 type and then N things of a fourth type. And because there's this notion, there's this, you have to remember what the M's and N's are as you're generating and accepting, you can prove using  a pumping lemma, or something that there does not exist a context, free grammar. You have to remember what the Ms. and NS are as you're generating and accepting. You can prove using  a pumping lemma or something that there does not exist a context free grammar that can accept all, and only the strings that are of this form. That are of this form. , in Swiss German, if  any German,  that in Swiss German there are some verbs that require accusative case, and some verbs that require dative case, and because of the way that it's ordered in Swiss German, it creates  in Swiss German, if  any German,  that in Swiss German there are some verbs that require accusative case and some verbs that require dative case. And because of the way that it's ordered in Swiss German, it creates this type of cross serial dependency. this type of cross serial dependency. And in fact, it's  exactly the same thing. and what we'd  to do is to generate the output parse tree associated with And what we'd  to do is to generate the output parse tree associated with that sentence according to the that sentence according to the rules of that grammar. won't present them explicitly in the lecture. This might be violated with Perl, which is why Perl is the worst programming language, and you should not write in Perl. I presented some examples of that in the first lecture. the S, One general strategy is you start with the starting symbol,  the S and then you search through all of and then you search through all of the possible rewrite rules, and you try to find a way to rewrite the non-terminal symbols, to get finer and finer grained until you get to rules that let you generate the actual words that you see in the sentence you're trying to. the possible rewrite rules and you try to find a way to rewrite the non terminal symbols to get finer and finer grained until you get to rules that let you generate the actual words that you see in the sentence you're trying to. using the rules by matching the words 1st to non-terminals and then the non-terminals to  bigger non-terminals. rules by matching the words first to non terminals and then the non terminals to  bigger non terminals. But we're just going to cover one, because these days you're lucky in that,  parsing is no longer as popular. does anybody  does anybody have any thoughts that have popped into have any thoughts that have popped into their mind. ,  it's a cock, younger Kasami or cock Kasami younger. OK,  it's a cock younger kasami or a cock kasami younger. and at a high level. And at a high level, here are the steps of the CYK algorithm. we're because, first, st we're going to convert the Cfg into an appropriate form, such that Cyk will work with it. we're because first we're going to convert the CFG into an appropriate form such that CYK will work with it. But we are, we are going to go through the details and make sure that we understand this is not  easy the 1st time you see it. But we are, we are going to go through the details and make sure that we understand because it's not  easy the first time you see it. First, st we're going to convert everything to something called Chomsky normal form. OK,  to make things easier later, first we're going to convert everything to something called Chomsky normal Form. And in Chomsky normal Form, this is a sub. and in chomsky normal form. Either one non terminal symbol rewrites to exactly 2 non one non-terminal symbol rewrites to exactly 2 non-terminal symbols terminal symbols, or one non terminal symbol rewrites to exactly or one non-terminal symbol rewrites to exactly one terminal symbol. this seems  it might be restrictive. But , it's not because we can deterministically convert any Cfg to be of the this type of Cnf form. this seems  it might be restrictive, but  it's not because we can deterministically convert any CFG to be of the this type of CNF form. Here are the rules about that. We're gonna apply in order to do the conversion. Here are the rules about that we're going to apply in order to do the conversion. there are 3 possible cases of Cfg rules that do not conform to Cnf form  chomsky, normal form. those rules and apply these procedures to turn it into rules that do fit the CNF assumptions. one possibility of how things might not fit Cnf is that  one non terminal rewrite to 3 or more  one possibility of how things might not fit CNF is that  one non terminal rewrite to three or more  hand side rules or symbols, either  hand side rules or symbols, either terminal or not terminal. just give it whatever name you'd ,  x 1.  just give it whatever name you'd ,  X1, and then we're going to say that A rewrites the And then we're going to say that a rewrites the x 1 X1. , , if originally your rule had,  3 -hand side symbols   if originally your rule had  3  hand side symbols, after the rewrite it will only after the rewrite, it will only have 2, and then  it will fit Cnf. have two and then  it will fit CNF if If it had  a 50  has 49, because the 1st 2 got put into this it had  50  has 49 because the first two got put into this new non terminal. that's the 1st rewrite strategy. The second rewrite strategy is if you have rules where rules where you have a mix of terminals and non-terminals that's really easy to deal with. you have a mix of terminals and non terminals, that's really easy to deal with. 2, rewrites to S,  here S is a terminal symbol. and both of these  fit Cnf. You have a rule of the type. One non-terminal rewrites to exactly one non-terminal. In the third one, you have a rule of the type 1 non terminal rewrites to exactly 1 non terminal. But after you do this, then you're allowed to delete a rewrites to B. But after you do this, then you're allowed to delete A rewrites to B.  the idea here is, if you had a tree where you had a rewrites to B, and then B rewrites to something else. You want to remove that intermediate level,  that  you have a rewrites to the thing directly. the idea here is if you had a tree where you had A rewrites to B and then B rewrites to something else, you want to remove that intermediate level  that  you have AV rights to the thing directly. These are called unary rules, because it involves exactly one  hand side. These are called junior rules because it involves exactly 1  hand side. I shot the elephant in my pyjamas and you can and  that it has some things that don't fit. see that it has some things that don't fit CNF. It has some rules that have three  hand sides, and then it has some rules that mix, say, and then it has some rules that mix, say terminals and non-terminals. And it has some rules that are unary. And it has some rules that are unary. st , can you guys help me? The Np to N, yes. one important thing to note. one important thing to note,  a beef pure  it'll be clear if I just delete it. if I just delete it. one important thing to note is that you keep the original rules, too. you have 2 copies of the rules of generating  one important thing to note is that you keep the original rules too. The reason you need to keep it is that your tree could have gone to I an elephant in pajamas through N. Through some other way. The reason you need to keep it is that your tree could have gone to I and Elephant in pajamas through N through some other way, not through the NP not through the Np. it'll be clear VP goes to X1PP. Is this, you replace the 1st 2? Is this you replace the 1st 2 ? OK,  this becomes X2 and then X2 rewrites to  this becomes x 2, and then x. Cnf, because all rules have either exactly 2  hand side non terminals. Because all rules have either exactly 2  hand side non terminals or exactly 1 terminal. And remember the vertical bar notation. This just means n rewrites to. IN rewrites to elephant and rewrites to pajamas. And remember the vertical bar notation? This just means N rewrites to eye, N rewrites to elephant, and rewrites to pajamas. that was, step one step 2 is to set up our data structure. Step 2 is to set up our data structure. from WI to J plus 10 indexed. We want I to be less than J.  we only really need half the table. that means our table will look  an upper triangular particular order, we want I to be less than J.  we only really need half the table. and we want to fill in this table. And we want to fill in this table. , any questions  far about the meaning of the table? OK any questions  far about the meaning of the table? and that's easy, because you can just check in your Cfg and find out all the rules that And that's easy because you can just check in your CFG and find out all the rules that are of are of the form one non-terminal rewrites to one terminal. In fact, you can just build an index and do  a reverse lookup to look up all of  the form 1 non terminal rewrites to 1 terminal. In fact, you can just build an index and do  a reverse look up to look up all of the rules that generate those non-terminals. the rules that generate those non terminals. here are the 1st 3 words. for the word, I,  for the word I, you just look up all you just look up all of the rules that can generate the word I. And P. And N. of the rules that can generate the word I. And you can do this for everything on the diagonal. And you can do this for everything on the diagonal. And  the key idea here is to take advantage again of chomsky, normal form in that all rules that produce phrases are of the form, a rewrites to B and C. And  the key idea here is to take advantage again of Chomsky normal form. And not all rules that produce phrases are of the form A rewrites to B&C. Because, remember, we're going from smaller chunks to bigger chunks. That means we have already found the smaller chunks B and C, and we can combine them to form the bigger chunk A, Because remember, we're going from smaller chunks to bigger chunks. that means we have to check for rules that are either NPV or envy. Do we have any rules of the type? Do we have any rules of the type NPV or envy? we can check are there any rules of the Are there any rules of the form, debt, Np. form Det NP or Det N? is missed here which is that you can create X2 which is  X2 goes to debt. And  this is , the pointer way is more human, interpretable. And  this is  the pointer way is more human interpretable. from your perspective, , OK, because those correspond because those correspond to all of the smaller constituents and smaller chunks that you might need in order to build to all of the smaller constituents and smaller chunks that you might need in order to build the constituent that the constituent that spans the current cell. what you can do is you can do  one diagonal at a time,  do the diagonal, and then to do the things  to the diagonals and  forth. what you can do is you can do  1 diagonal at a time. do the diagonal and then do the things  to the diagonals and  forth. You can do any combination of them that you  that you would  You can do any combination of them that you  that you would . here,  that here the order that I'm proposing is, you fill out the diagonal entries, and then you go  Here. the here the order that I'm proposing is you fill out the diagonal entries and then you go . for I, we already said, this could be an Np. OK,  for I we already said this could be an NP or an N, ? N. And we already said that this can be either an Np. it's either Det NP or Det N and we already said that this can be either an NP or Or an ex. Do we see any rules? with one to two and then two to four, do we see any rules? it looks  it's we can build a Vp. OK,  it looks  it's we can build AVP also x, 1. also X1, also X. and P. And x, 1 goes to the The NP,  the, the blue and the red is and P. just to help you distinguish the two, ? the blue and the red is just to help you distinguish the 2 . But this set of red edges has nothing to do with this set of red edges just to be clear. But this, this set of red edges has nothing to do with this set of red edges, just to be clear, ? is a thing and NX 1 is not a thing. and NX, 1 is not a thing. We have to check is there any rule for NP and then P? Is there any rule for X2 and then P? Oh, NP goes to PNP, yes, that exists, but it's yes, that exists, but it's in the wrong order, because here we have a Np, and then p, not p, and then np. We have to check for rules of the form. Check for rules of the. the  hand side of any rules. if there are rules of the form, Vp. if there are rules of the form VP debt or x. 1 debt, then or X1 debt, then that would be relevant. The last thing you said we should check one to 4, then 4 to 6. is there a little bit heuristic role based on the empty slots around us? 5 to 6, 6 to 7, 7. ,  we have a debt. we have the same rules that apply. do we have any rules of the form. P. And P. We do  do we have any rules of the form PNP? And then And then how about PX2? Are there any rules of the form. Are there any rules of the form NPPP? Nope, but  there is one for x 2 PP, No, but  there is one for X2 PP  that creates an NP. because it's this V with this Np, which gives us. Because it's this V with this NP which gives us VP. I'll use yet another color. OK,  I'll use yet another color. Oh, no, it looks too much  red Oh no, it looks too much  red. Yes, we have a Vp from x 1 PP. How about X2 or , X1PP? Yes, we have AVP from X1PP. Which song was I on? Oh , it's still, it's still, , it's still the song. Do we have any Mvps Do we have any NVPS or NX-1 or NPX one? How do they distinguish them? then two to seven using the V and the NPS there. And then every time you see a starting symbol, you can just use the back pointers And then every time you see a starting symbol, you can just use the back pointers to trace through all to trace through all of the possible paths. If you tilt your head  45 degrees this way, you can even  see the parse directly in the chart, ? Rewrites to I, and then Vp. Rewrites to whatever,  on and  forth. If you tilt your head  45° this way, you can even  see the parts directly in the chart, ? S rewrites to NPVPNP rewrites to I, and then VP rewrites to whatever,  on and  forth. for our purposes, it's not relevance. for our purposes, it's not relevance. All this tells you is that. And  And  the simplest way to do this is to the simplest way to do this say that each non terminal symbol that rewrites to something is to say that each non-terminal symbol that rewrites to something else that forms are probability distribution. ,  when you have a non-terminal, say A in your non-terminal set. OK.  when you have a non terminal, say A in your non terminal set, you create categorical probability distribution you create categorical probability, distribution for all the rules that have that same non terminal for all the rules that have that same non-terminal symbol a on the left hand side. The sum of all of the rules involving that Np. NP rewrites to anything else. The sum of all of the rules involving that NP on the left hand side must sum up to one. and then, formally speaking, the probability of a tree is going to be the product of all of the rules that make up that tree. And then formally speaking, the probability of a tree is going to be the product of all of the rules that make up that tree. T of,  the of the sentences. That would be  a obvious way to do this. That would be  a obvious way to do this, and it would be easy, and then you don't have and it would be easy. And then to learn anything else for your midterm. you don't have to learn anything else for your midterm. However, unfortunately for you, there's a more efficient way to do this. recall that we had this notation for the back pointers that we could have with , just indicating which are the subconstituents we use to build a bigger constituents  recall that we had this notation for the back pointers that we could have with  just indicating which are the sub constituents. or equivalently, if you don't  this notation, you can try this notation. Or equivalently, if you don't  this notation, you can try this notation which is you're going to write this which is, you're going to write this as a three-dimensional array. previously in the recursive step of the algorithm you just have to check recursive step of the algorithm. previously in the recursive step of the algorithm, you just have to check was that combination of  hand was that combination of  hand side symbols? note that it might be possible that there are multiple rules that form the same constituents with the same non-terminal symbol for a span. notes that it might be possible that there are multiple rules that form the same constituents with the same non terminal symbol for a span. because all of these things factor out  locally there are no long range dependencies between  the top of the tree and the bottom of the tree. But note that it really matters that it has to be the same span with the same non terminal symbol. If you create But note that it really matters that it has to be the same span with the same non terminal symbol. just as an example here is one of the recursive steps again. OK  just as an example, here is one of the recursive steps again. for each sale of the there you go. in each cell we're storing all the constituents we build and we're storing, in a probability associated with each of them  in each cell we're storing all the constituents we build, and we're storing in a probability associated with each of them. This is called a tree bank. if we have this is called a tree bank. A tree bank is a collection of trees A tree bank is a collection of trees in a in a bank, with , a with sentences. bank with, with  a, with sentences. OK,  , for the most, for the longest time from  19, from the in the 1990s and in the 19 nineties and in the 2 thousands, people worked on a particular Corpus, which is  the most famous corpus in Nlp. Which is the Wall Street Journal Corpus in the 2000s, people worked on a particular corpus, which is  the most famous corpus in NLP, which is the Wall Street Journal corpus. , no, it's called the Pantry Bank, and it involves Wall Street, mostly Wall Street Journal data, but also some other stuff. , no, it's called the Penn Treebank and it involves Wall Street, mostly Wall Street Journal data, but also some other stuff. And  that's the most famous Tree Bank. And then if you have a collection of trees. that by the total number of times that you see the non terminal symbol alpha. And you can definitely do good turn smoothing or add and you can definitely do good turning, smoothing, or add one smoothing, or whatever everything we talked about in that module of the course to do with smoothing still applies here, because these are also categorical distributions. Everything we talked about in that module of the course to do with smoothing still applies here, because these are also categorical distributions. , , it with pronouns  I,   it with pronouns,  I is a subject pronoun in English ?"
    ],
    "Topic 3": [
        "a unit in a sentence. Oh no, it won't go away. use capital letters for that. and that they rewrite into terminal symbols, which are the leaf nodes, and for us they correspond to words in a sentence. And then they rewrite into terminal symbols, which are the leaf nodes. to the rules of that CFG for that sentence. this is just a very brief reminder of Cfgs from last class. this is just a very brief reminder of CFGS from last class. And why not something more expressive,  a context, sensitive grammar, or the most expressive one, which is some  recursively enumerable grammar? And why not something more expressive  a context sensitive grammar or the most expressive one, which is some  recursively innumerable grammar. And because there's this notion, there's this. can accept all and only the strings. ,  things can be arbitrarily use recur  you can use this. These constructions recursively as many times as you'd . OK,  things can be arbitrarily used  you can use these constructions recursively as many times as you'd . ,  in practice for all of these cross serial  in practice, for all of these cross serial dependencies, the M's and the n's are never greater than  2 dependencies, the Ms. and the NS are never greater than  two or 3 at most. and anything else is  very arbitrary and contrived and artificial, and you never see it attested in corpora. And anything else is  very arbitrary and contrived and artificial and you never see it attested in corpora. This is  a fun aside, it's not super central to the course. But This is  a fun aside. It's not super central to the course, but OK. . There are other kinds of parsing. There are other kinds of parsing. given a particular CFG and given a sentence made-up And given a sentence made up of words that are in the terminal vocabulary of the Cfg. of words that are in the terminal vocabulary of the CFG, the goal that we have is to recover all The goal that we have is to recover all possible parses of the sentence. his name is Groucho Marx. The other strategy that you can have is a bottom up strategy, which is, you start from the input words. back in the day when parsing was really popular, an Nlp course would be  1 3rd about parsing, and we would go over all sorts of parsing algorithms. it's  a complex search strategy where you have 2 ends and you start to  gradually get them to meet together. back in the day when parsing was really popular, an NLP course would be  1/3 about parsing and we would go over all sorts of parsing algorithm. it's  as complex search strategy where you have two ends and you start to  gradually get them to meet together. , these there are these different kinds of parsers. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. Sometimes the cyk, sometimes the cky. Sometimes it's CYK, sometimes it's CKY. This is a restriction on the on the Cfg. There's a restriction on the on the CFG such that Such that all of the rules must take a particular format. , all of the rules have to be in one of these 2 types, either OK, All of the rules have to be in one of these two types. and  by iteratively doing this, you can gradually turn all the rules into rules that have exactly 2  hand side symbols. And  by iteratively doing this, you can gradually turn all the rules into rules that have exactly 2  hand side symbols. The second rewrite strategy is if you have OK,  that's the first rewrite strategy. And finally, the 3rd one is  the trickiest to deal with. And finally the third one is  the trickiest to deal with. And this helps you get rid of  these. And this helps you get rid of  these. here I've written out a tiny little grammar  here I've written out a tiny little grammar that will allow us to handle the sentence of. It has some rules that have 3  hand sides. You place all NS in the rest of the rules in the rest of the rules with. We have to copy it. and  you need to keep it to not mess up the strings that your grammar will accept or reject. And  you need to keep it to not mess up the strings that your grammar will accept or reject. this one needs to be changed  this one needs to be changed to. I'm gonna use P for preposition. I'm going to use P for preposition. everything is Great,   everything is CNF. This is just a shorthand. This is just a shorthand, ? what  by an acceptable change to the grammar is that it doesn't change the set of strings that are accepted or rejected by the grammar. what  by an acceptable change to the grammar is that it doesn't change the set of strings that are accepted or rejected by the grammar. when you convert something to Cnf, you won't change the set of strings that are accepted or reject. And  when you convert something to CNF, you won't change the set of strings that are accepted or rejected. we're going to set up a 2 dimensional table that will store all of the constituents that can be built from contiguous spans within the sentence. by contiguous, that just means that all of the things within the span are  to each other. we're going to set up a 2 dimensional table that will store all of the constituents that can be built from contiguous spans within the sentence. by contiguous, that just means that all of the things within the span are  to each other. the second number is  exclusive. we have to fill in the table itself. 0 1 and 2. zero to three exclusive,  01 and two. that means there's only one possible breakpoint, which is, you break after the worded position 0. the possibilities. Again, there's only one possible breakpoint,  we can check. But then, when you  store it in a data structure, then you have to store that this was the debt from this cell. But then when you  store it in a data structure, then you have to store that this was the debt from the cell and then this was the end from the cell and that's how you created this NP. Yes, there and here you have to store both. Then here you have to store both . then the second, the second constituent you've built here would be x 2, that 2 to 3 and 3 to 4. . And it turns out that we don't mind, and we're very comfortable dealing with ambiguity in communication. And it turns out that we don't mind and we're very comfortable dealing with ambiguity and communication in the in the context of this algorithm. what you mean is that this is how we, the algorithm allows us to deal with ambiguity because you can store multiple things. what you mean is that this is how we, the algorithm allows us to deal with ambiguity because you can store multiple things. , you can store multiple constituents in each cell and then you can use all of them to build bigger. you do catch my mistakes. The other possibility is  a The other possibility is  AX2 goes to dead end. We already know that's impossible, because one to 3 is empty. We already know that's impossible because 123 is empty. in the wrong order because here we have ANP and then P, not P and then NP. I highly doubt any of this is possible, either. I highly doubt any of this is possible either. But there are not wait. It's  any, anything that ends with my , it's unlikely you'll find a constituent if your grammar is reasonable. You're just checking that breakpoint. when you're checking these pairs of cells, you're checking a possible breakpoint. If you want to run the Cyk algorithm exactly. when you're checking these pairs of cells, you're checking a possible break point, ? If you want to run the CYK algorithm exactly, there are no heuristics. 2 s. We don't have any PX twos. the 1st possibility is Npp. no, , what about Npp? OK. What about NPP also? Wait , which song was that on? from 0 to 1 to 7, we have Sb rights to Npp. Are there any rules that are SPPI don't see any. the same non terminal constituent  in somewhere in a smaller span, you have to  copy  the structure You have to  copy  the structure over ? Then you have to multiply the number of possibilities. Then you have to  multiply the number of possibilities. You have to multiply everywhere. one of these  groucho bookings in practice I knew I said, something wrong in practice. One of these, , I Groucho muffins in practice, I knew I said something wrong. No idea  you have to. We have to change something about the Cfg or something. you have to we have to change something about the CFG or something. we are given the sentence sent. we are given the sentence sent and then Tau of sent is the set of possible parse trees and then Tau of sent is the set of possible parse trees for it. The simplest way to do this with what we've already The simplest way to do this with what we've already talked about in class today is to just run the Cyk algorithm talked about in class today, is to just run the CYK algorithm as is, ignoring the probabilities, and then you as is ignoring the probabilities. and evaluate the probabilities for all of them. that's the idea we're going to extend the Cyk algorithm to keep track of probabilities in the table itself. We're going to extend the CYK algorithm to keep track of probabilities in the table itself. , , previously we might have had an entry, which is, we created a new Np for the span, 2 to 4 from the debt at the span, 2 to 3, and the end of the span 3 to 4. OK.  , previously we might have had an entry which is we created a new NP for the span two to four from the debt at the span two to three and the end of the span three to four. side symbols, were you able to combine them to form Were you able to combine them to form the left hand side with that breakpoint  at each particular breakpoint. And you do that by taking the existing probabilities from each of the sub constituents and multiplying with. And  then you multiply them all together, and then you report it in your table. And  then you multiply them all together and then you report it in your table. constituent  it's not the same. If you create an Np constituent here. We already know that we won't use it. We already know that we won't use it,  we're throwing that out. But if there are 2 possible tokens we can create. just to re, I would just not use the word token, I would use the word constituent. just to read I would just not use the word token. I would use the word constituent. Yes, yes, ,  OK,  we are getting the, it won't grow. Then you can use what we already know And then if you have a collection of trees, then you can use what we already know from everything that from everything that we discussed to do with estimating values of parameters, with  Hmms. And Ngram models, and  forth. it's the exact same ideas as you've already looked at. it's the exact same ideas as you've already looked at. , however, this scheme of these Pcfgs turns out to not work very  in practice. OK,  however, this scheme of these PCF GS turns out to not work very  in practice. The probabilities are not all independent of each other. They might be, they might  the probabilities are not all independent of each other. But  you add some adverbs   we talked about kick the ball versus kick the ball softly or quickly, or something. we talked about kick the ball versus kick the ball softly or, or, or quickly or something. Whereas me would be the object version."
    ],
    "Topic 4": [
        "I hope you all had a good week reading week. I hope you all had a good week reading week and you're enjoying the unseasonably nice weather. and you're enjoying the unseasonably nice weather. I know it's been over a week, and  that's an eternity, and we've all forgotten everything already. But  I know it's been over a week and  that's an eternity and we've all forgotten everything already. ,  OK,   , we'll get into all that again  , we'll get into all that again today. I'll also post the reading assignments soon, and I'll also post the programming assignment soon,  there's lots of things coming up. I'll also post the reading assignments soon and, and I'll also post the programming assignment soon. ,  we're gonna talk about Zky parsing or cyk parsing. OK.  we're going to talk about CKY parsing or CYK parsing and then we're going to motivate why we need And then we're going to motivate why we need that. and we say that a sentence forms some valid sentence of a language. And we say that a sentence forms some valid sentence of a language if you can find this tree according If you can find this tree according to the rules of that Cfg for that sentence. in particular, there are other potential choices we could have made instead of context-free grammars. OK.  in particular, there are other potential choices we could have made instead of context free grammars. According to this, this view of the world is no. it turns out the answer according to this, this view of the world is no, because there exists. As far as we know, all of the phenomena, you can describe it with some context, Gram. However, personally, I don't find this type of argument or this type of way of thinking about natural language to be particularly useful. However, personally I don't find this type of argument or this type of way of thinking about natural language to be particularly useful. all of the arguments, even for,  is natural language, a context-free grammar, and their demonstrations that there are some phenomena that you cannot express with a regular grammar they rely on  unlimited processing power. all of the arguments even for  is natural language, a context free grammar and they're demonstrations that there are some phenomena that you cannot express with a regular grammar. They rely on  unlimited processing power. here, , the M and the ends have can be arbitrarily large values. here , the M and the NS have to can be arbitrarily large values. Personally, I don't think it's the most useful question to ask, but other people will disagree. personally, I don't think it's the most useful question to ask, but other people would disagree. ,  back to something more OK,  back to something more algorithmic. ,  today, we're going to talk about parsing. OK,  today we're going to talk about parsing. what is parsing  what is parsing? And we're going to look at that. parsing in natural languages can be a little bit trickier and we're going to look at that. the main idea in parsing is that you have a sentence, an input sentence. And then you have a grammar  a context, free grammar associated with that sentence. OK,  the main idea in parsing is that you have a sentence, an input sentence, and then you have a grammar,  a context free grammar associated with that sentence. And then there, you can talk about dependency parsing. And then there you can talk about dependency parsing. there are algorithms that  look a lot  the algorithms I'm gonna talk about. But  there are algorithms that  look a lot  the algorithms I'm going to talk about, but I but I won't present them explicitly in the lecture. one of the main difficulties in parsing is to find an efficient way to search through all of the plausible parse trees  one of the main difficulties in parsing is to find an efficient way to search through all of the plausible parse trees for some input sentence. And  we're going to talk about an algorithm. And  we're gonna talk about an algorithm to do that. then here is OK,  then here is more specifically the problem we're more specifically the problem we're going to solve for Cfg. going to solve for CFG. possible parses of the sentence. Another fun fact, this might be violated with Pearl, which is why Pearl is the worst programming language and you should not write in Pearl. The main difference is, we want to recover all possible Parses. Why did you prop up? Why did you prop up? here is a joke from the early 20th century, and it's by a comedian. here is a joke from the early 20th century and the sentence by a comedian,  his name is Groucho Marx. there are 2 parses here for that sentence. what is the difference in the meaning between implied by these 2 parses? what is the difference in the meaning between implied by these two parses? How he got into my pajamas I'll never know. How he got into my pajamas, I'll never know. But for us, we're going to talk about an algorithm to recover all parses according to a Cfg. OK, But for us, we're going to talk about an algorithm to recover all parses according to ACFG. And that's called a top down strategy. and that's called a top-down strategy. S, that spans the entire sentence, and that's called a bottom up strategy. And that's called a bottom up strategy. there's an early parser that goes top down. We're going to talk about one called Cyk. there's an early parser that goes top down. We're going to talk about one called CYK that goes bottom up. There's a shift reduce strategy. But you still should know about it, because it's very good pedagogically. And  we're just going to cover one parsing algorithm, we're going to cover the Cyk algorithm, which is a bottom up algorithm. But we're just going to cover one because these days you're lucky in that  parsing is no longer as popular, but you still should know about it because it's very good pedagogically. And  we're just going to cover 1 parsing algorithm. We're going to cover the CYK algorithm, which is a bottom up algorithm. OK,   we're going to cover the CYK algorithm,   we're going to cover the Cyk algorithm, which is going to be an example of a dynamic programming algorithm. which is going to be an example of a dynamic programming algorithm, OK? Sometimes it's  different sources order the authors differently. , again, it's a dynamic programming algorithm, which means that partial solutions are stored and they're efficiently reused  that we can recover all possible parses for the entire sentence  , again, it's a dynamic programming algorithm, which means that partial solutions are stored and they're efficiently reused  that we can recover all possible parses for the entire sentence. Here are the steps of the Cyk algorithm. first, st it turns out that Cyk doesn't work with all Cfgs. first, it turns out that CYK doesn't work with all CF GS. but is this is not  a real problem. It's not a big problem. But is this is not  a real problem, it's not a big problem. And for us, we're going to create a table to store all of these possible constituents. And for us, we're going to create a table to store all of these possible constituents. 3, rd we're going to fill in the table in an efficient manner. Third, we're going to fill in the table in an efficient manner. and finally, we'll read the table to recover all of the possible parses of the sentence. And finally, we'll read the table to recover all of the possible parses of the sentence. once you're used to dynamic programming algorithms, this  thing becomes  once you're used to dynamic programming algorithms, this  thing becomes pretty standard and formulaic. to make things easier later. ,  how are we gonna do that? OK,  how are we going to do that? any ,  there are three possible cases of CFG rules that do not conform to CNF form. And  you have this particular case where for every rule and  grammar where B is the left hand side, you copy that rule, but you replace B with You copy that rule, but you replace B with A, A.  this is  a  this is  a bit more involved. , we're going to do an example. OK, we're going to do an example. that will allow us to handle the sentence of I shot the elephant in my pajamas. Replace all ends The NP to N. Yes. You copy it and replace it with A. it and replace it with A. have to copy it and then we have to replace Replace N with np. I'm gonna put at the end. , that's no, I'm going to put at the end. And then x, 1 And then X1 goes to VNP. we could replace it in. One more OK, almost there. Did I put in the new one? The new did I put in the new one. Oh, no, I did ? Oh no, I did . here we have 2 rules with the same  hand side sequence of symbols, and that's totally fine. Rules with the same  hand side sequence of symbols, and that's totally fine. There's nothing to stop the grammar from doing that. There's nothing to stop the grammar from doing that. once you see the example. that means our table will look  an upper triangular table. Again, you can reread this sentence later on, after we do the example, and it'll make sense afterwards. Again, you can reread this sentence later on after we do the example and it'll make sense afterwards. , but this is what it's gonna look . ,  here OK, but this is what it's going to look . OK.  , we have to fill in the table itself. And in fact, when you did. Hmms, all of those algorithms  forward algorithm backward algorithm viterbi. Those are also dynamic programming algorithms. And in fact, the when you did HMMS, all of those algorithms  forward algorithm, backward algorithm, Viterbi, those are also dynamic programming algorithms, ? And in our grammar it's going to be Np. you can build a rule with the constituents that you've already found there. if we're able to build this bigger chunk A. if we're able to build this bigger chunk A, that means we have already found the smaller chunks B&C and we can combine them to form the bigger chunk A. and we just need to check all possible breakpoints to make sure that we cover all of the possible And we just need to check all possible break points to make sure that we cover all of the possible ways to build that bigger chunk. ways to build that bigger chunk. then, again, you can build an index to this. then again, you can build an index to this if you have big grammar. I don't see any,  that means we cannot build any constituents there. that means we cannot build any constituents there. that means we can build 2 possible constituents there. that means we can build 2 possible constituents there. We can build either an Np there. We can build either an NP there or we can or we can build an X 2 there. one thing which is also  one thing which is also familiar to you from familiar to you from the viterbi algorithm is you should store back pointers. the Viterbi algorithm is you should store back pointers. here we're going to create this constituent Np goes to debt. N,  here we're going to create this constituent NP goes to dead end and also there's another one which and also there's another one which is missed here, which is that you can create x 2, which is  x, 2 goes to dead end there as . And then this was the end from this cell. And then you can use all of them to build bigger chunks, bigger constituents. Chunks bigger constituents  , in that sense, yes, OK, ,  we're gonna work through this . We're going to work through this example in total. , the second last part is you have to , fill in the table in the correct order. OK,  the last part is that you have to, , the second last part is you have to  fill in the table in the correct order. bottom left of it from my perspective. Then you're good left of a cell when you're processing that cell, then you're good. And then we're gonna OK,  let's finish the example. Oh, , I need to. it would be debt for a determiner For a determiner or elephants it would be NP or elephants. and for pajamas it would be Np or N, And for pyjamas, it would be NP or N OK,  those were our base cases and we're done our . those were our base cases and we're done our base cases. there's nothing we can build there. do we see any V debt in our grammar? again, we can't build anything that way, because 0 to do has no. again, we can't build anything that way because zero to do has no nothing spans that. we have Oh , question. But later on, with bigger spans. But later on with bigger spans, you just have to, you have to make sure you check the  two cells. It's not always going to be the cells   to you that matter. It's not always going to be the cells   to you that matter. , you have to really reason through it and or and work through the algorithm ,  , you have to really reason through it and or, and work through the algorithm robotically at the robotically at the start. Wish me luck because I don't have a pointer up because I don't have a pointer too thick. ,  one One that end . x 2 goes to dead end. See, I said, I wouldn't miss things See, I said I wouldn't miss things,  oh shoot, that should be a VP. And  we can build an S there, s, goes to Npvp. And  we can build an S there. That's not possible, because 0 to 2 is empty. OK.  we're done with zero to 1 and 1:00 to 4:00.  we have to check zero to two and two to four. That's not possible because zero to two is empty. 0 to 3 and 3 to 4 is also not possible for the same reason, because 0 to 3 is not possible. Zero to three and three to four is also not possible for the same reason, because zero to 3 is not possible. oh, Np goes to Pnp. in Cfg is the order, matter. in the CF GS the order matter. ,  one to 2, and then OK,  one to two and then two to five 2 to 5 is not possible. One to three and the three to five is not possible. possible to build something there. OK. And then zero to five. 0 to one and one to 5 is not possible. 0 to 2 and 2 to 5 is not possible. 0 to 3 and 3 to 5 is not possible. 0 to one and one to five is not possible. Zero to two and two to five is not possible. Zero to three and three to five is not possible. Zero to four and then four to five. P. Debt, is that possible? OK.  column four to six,  P debt, is that possible? possible there either because of all the empty cells one because of all the empty cells. And one to 5, 5 to 6 is all worthy. And one to five, five to six is all OK. . And then here, 0 to 6.  it's also not possible. And then here is zero to 6.  it's also not possible because of all of because of all of the empty cells there. any anything that ends with my . you should just  do every possible pair and do every possible pair and try. ,  OK,  five to six, six to seven, seven. OK.  we have that end . OK. , , here we have 4 to 7. The 1st possible breakpoint is 4 to 5, 5 to 7.  the possible, the first possible break point is 4 to 5, five to seven. There's a PP rule there. OK. And then 4 to 6, 6 to 7 is not possible, ? And then four to six, six to seven is not possible, , Because four to six is empty. , we have to sell 3 to 7. OK,   we have the cell three to seven. there's  that salad doesn't work. that's how it doesn't work. Five to seven, that doesn't work. Three to six, six to seven, that doesn't work. we have two to three, three to seven, that's not possible. , I feel  I might be missing something. I feel  I might be missing. 2 to 5, 5 to 7,  2 to 5, five to seven, not possible. 2 to 6, 6 to 7, not possible. Two to six, six to seven, not possible. ,  first, st we have a Vnp,  that's definitely possible. OK.  first we have a VNP,  that's definitely possible. Are there any other Vnp rules? Oh, , there's an x 1. Are there any other VNP rules? Oh , there's an X1. OK,  there's BNP  1-2. , 1, 2, 3, 3 to 7, Three, three to seven? Oh, , it's still, it's still , it's still the cell. One to 6, 6 to 7 is not possible. One to six, six to seven is not possible. from zero to one, one to seven, we have S rewrites to NPVP. ,  that's 0 1, 1, 2, 7, OK.  that's zero to one, one to seven and then and then 0 to 2, 2 to 7 is not possible. 0 to 3, 3 to 7 is not possible. zero to two, two to seven is not possible. Zero to three, three to seven is not possible. Zero to four, four to seven. I don't see any 0 to 5, 5 to 7, not possible. 0 to 6, Zero to 5, five to seven, not possible. Zero to six, six to seven also not possible. 6 to 7 also not possible. the X1 there and then four to seven with the PP there. you'd index into the cell. you index into the cell the second NP. You have all of the parses that you can recover And then there you go, you have all of the parses that you can recover, ? and with the same with the other S. And with the same with the other S And this And this is how you can recover all of the parses. is how you can recover all of the parses. , there might be  a You'll see  the depth ? The depth, that's a good idea, . But we're going to try some another strategy, which is that we're going to add probabilities to everything But we're going to try some another strategy, which is that we're going to add probabilities to everything  that  that we can say that's the we can say that the, our goal  is not our goal  is not to just recover all possible parses of a sentence, but  to recover the most to, to recover all possible parses of a sentence, but  to recover the, the most probable parse of a probable parse of a sentence. I don't technically need to introduce any new algorithms for this. OK,  I don't technically need to introduce any new algorithms for this. And then you just score all of the possible trees that we cover . just score all of the possible trees that we cover, ? We used to build a bigger constituents, ? we're going to augment that  that in  that in our table with the list of entries for all of the ways we could build things. And then there, these are some other possible entries where , and you got this number, because  at 3 to 4 with the Np. that no, this should be OK. No, this should be a . these are just some other possible entries. these are just some other possible entries, . you do that, and if you're able to create that match  that you can create the bigger constituents. you do that and if you're able to create that match  that you can create the bigger constituents, you also have to compute the new probability of that constituents. Np rewrites to dead End, . NP rewrites to dead end . And then, finally, once you've parsed the whole And then finally, once you've parsed the whole sentence, then sentence, then you just look for the starting symbol. you look at the entries here that correspond to S, you just look for the starting symbol. or are we still computing all possible parses. we are no longer computing all possible parses. Are we still computing all possible parses? we are no longer computing all possible parses   because of because of this. a real PCFG situation, OK. ? But  there's no, the theory doesn't give you any reason to prefer one parts over the other in that situation. But , there's no the theory doesn't give you any reason to prefer one marks over the other in that situation. we are getting the, we're getting the cell. , I said before, there are other parsing algorithms. if you're interested, you can look them up. The the one that I would look at , if you're interested is to do top down parsing with the early algorithm. I said before, there are other parsing algorithms,  if you're interested you can look them up. The the one that I would look at  if you're interested, is to do top down parsing with the early algorithm. Hence the extra E, This is a name and state extra E and . , , here are the maximum likelihood estimates  , here are the maximum likelihood estimates for the probability of the probability of a rule of alpha rewrites to beta, rule of Alpha rewrites to beta, which is you count the number of times that you see the rule. But I'll I'll talk about , why this is . as we writes to it ,  suppose you have something  a  suppose you have something  a sentence, or  even better would be  a verb phrase. a sentence, or  even better, would be  a verb phrase. You could build that verb phrase. use to build that verb phrase. at the beginning of a sentence, you have an Np. ,   class, we're going to talk about one particular scheme that people have proposed OK,   class we're going to talk about one particular scheme that people have proposed to try to fix to try to fix this issue a little bit, and then we're going to get started on semantics. this issue a little bit, and then we're going to get started on semantics after that."
    ],
    "Topic 5": [
        "I. Jackie Cheung, Professor: I'm . OK,  context free grammars, I remember is formalism. A remember, is a formalism. And how can I get rid of this thing? And how can I get rid of this thing? The little arrow, the green arrow on the left, the Green Arrow. The little arrow, the green arrow on the left. The green arrow, OK, that's not the bottom. But I'll try to make sure the deadlines are not. there's 1 topic from last class we didn't quite manage to cover, which  would be interesting to talk about that. there's one topic from last class we didn't quite manage to cover, which  would be interesting to talk about. And it's more of  a linguistics flavored  sub topic. we'll get into that And it's more of  a linguistics flavored  sub topic, but  it's interesting. , here's just a  about Cfgs and constituent trees. there's some  , here's just a  about CF, GS and constituent trees. There's gonna be an S node which is the starting symbol. be an S node which is the starting symbol. them be modeled as some context free gram or do we need some other schmutz? Or do we need some other Schwartz? in particular, here are some other possible possibilities, and  heard about some of these in other computer science classes. in particular, here are some other possible possibilities, and  heard about some of these in other computer science classes. why is it that we chose context-free grammars and not say a regular grammar with  a regular language. why is it that we chose context free grammars and not say a regular grammar with  a regular language? you need to take some automa theory or theory of computation if you want to learn more about these other classes. you need to take some automata theory or theory of computation if you want to learn more about these other classes. But  the ones that I'm pretty sure you've seen are regular languages,  regular expressions But  the ones that I'm pretty sure you've seen are  regular languages,  regular expressions and context and context, free grammars with the associated context, free language. then, the question that we're gonna briefly discuss   is OK,  then the question that we're going to briefly discuss   is, are natural languages context free grammars? are natural languages, context free grammars and  forth, they rely on some assumptions which you can disagree with. they rely on some assumptions which you can disagree with. And still, that's an interesting  point to note, And still, that's an interesting  point to note, which is a . hopefully it's not a new word because  you've seen it in  programming languages or something. parsing in natural languages can be a little bit trickier. if we're doing parsing with a context-free grammar, then we're doing something that in Nlp we would call constituent parsing. if we're doing a parsing with a context free grammar, then we're doing something that in NLP we would call constituent parsing. ,  OK,  how do you get rid of this thing? how do you get rid of this thing. but here the for us, the most important  ambiguity is syntactic ambiguity. But here the for us, the most important  ambiguity is syntactic ambiguity. One, general strategy is, you start with the starting symbol. The other strategy that you can have is a bottom up strategy, which is you start from the input words and then you build ever bigger sub trees using the and then you build ever bigger subtrees. you're creating bigger and bigger subtrees corresponding to bigger and bigger constituents until eventually you find the starting symbol. you're creating bigger and bigger subtrees corresponding to bigger and bigger constituents until eventually you find the starting symbol S that spans the entire sentence. And with certain kinds of grammars, 1 tends to be more efficient than the other and  forth. And when you hear these words about avoiding redundant computation, there should be a general And when you hear these words about avoiding redundant computation, there should be a general class of algorithms that this class of algorithms that this that springs to mind. , as is common with many dynamic programming algorithms, you need to create some data structure to store these partial solutions. , as is common with many dynamic programming algorithms, you need to create some data structure to store these partial solutions. all of the rules must take a particular format. and  we just need to take all of and  we just need to take all of those rules and apply these procedures to turn it into rules that do fit the Cnf assumptions. to deal with this, we're going to follow this we're going to follow this strategy of strategy of creating a new non terminal symbol. and then D, and then x, 1 rewrites to BC, And then D and then X1 rewrites to BC and by doing this we shorten the length of what's and by doing this we shorten the length of what's gonna be there. ,  let's convert this grammar fragment into Cnf. OK,  let's convert this grammar fragment into CNF. Can you guys help me? what's 1 thing I should do first? what's one thing I should do first? ,  this one is the hardest one. And  you can get rid of this rule. OK,  this is how you would do it and  you can get rid of this rule. I'll just keep it here. I'll just keep it here. And I didn't  formally define what counts as a , a  question. OK. And I didn't  formally define what counts as a  a  question. I never formally defined what it means to be  an acceptable change to the grammar. I, I never formally defined what it means to be  an acceptable change to the grammar. OK,  that was step one. And this sounds more complicated than what it is. And this sounds more complicated than what it is,  once you see the example, this will make sense. And here we have the updated CNF grammar from before. , along the diagonals the diagonals just corresponds to single words. The diagonals just corresponds to single words. ,  2 to 5 means the words to at positions 2, 3, and 4. OK,  two to five means the words to at positions 2-3 and four. we're 1st going to fill out the constituents corresponding to single words, and then we're going to do the ones corresponding to say 2 words, and the 3 words and 4 words, and  forth. we're first going to fill out the constituents corresponding to single words, and then we're going to do the ones corresponding to say 2 words and the three words and four words and  forth. again, if you've seen dynamic programming algorithms before,  that we need a base case and we need a recurrence step. again, if you've seen dynamic programming algorithms before,  that we need a base case and we need a recurrence step. It would be V, and for the, it would be depth and  forth. IT would be Det and  forth. the harder step is the recursive step, because this step corresponds to multiple words. OK,  the harder step is the recursive step because this step corresponds to multiple words. all we need to do is check all of the possible breakpoints in between the start and the end. for I shot, we need to check all of we need to check all of the possibilities. Those are the 2 possibilities for how to Those are the two possibilities for how to break that break that up. If you have big grammar, we have a tiny grammar,  we can just check by hand. We have a tiny grammar,  we can just check by hand. N. There are 2 possibilities ? Det N there are two possibilities, ? you need to point out how you created that constituent. If you  care about recovering the parse tree at the end. you need to point out how you created that constituent if you  care about recovering the parse tree at the end. And that's how you created this. that's just this, a different way of  representing the same the same. if you care about recovering all possible parses, then you have to store both. if you care about recovering all possible parses, then you have to store both and, yes. Yes, this is where, I'm , ambiguity comes from. this is where  ambiguity comes from. you have to define what comes from means You have to define what comes from means. ambiguity comes from the language    ambiguity comes from the language, ? ambiguity comes from how natural languages work. Ambiguity comes from how natural languages work. That, that's, , the, the, the words that spans The the words that spans the words corresponding to the current cell. the words corresponding to the current cell. you just need to pick an order to fill out the table. you just need to pick in order to fill out the table that respects that. We're gonna do it together for practice. OK, And you need to help me because I am And you need to help me, because I am sure that I will  overlook things. sure that I will  overlook things. you need to catch my mistakes. What I need to do is create a bunch of text boxes. Oh, OK, I need to what I need to do is create a bunch of text boxes. you have to be very careful. You have to make sure you check the  2 cells. you have to be very careful. 2.  we need to  we need to do that and I'm going to do that. and I'm going to attempt to draw. OK,   we have how do I insert text how do I insert textbooks? ,  here we have to check the possibilities. OK.  here we have to check the possibilities. They're they're just They're, they're just there to help you not get tripped there to help you not get tripped up. finally, for this column, , we have 0 to 4.  we have to check all the possibilities we have 0 to one and one to 4.  let's check that first.st OK,  finally, for this column , we have 0 to 4:00,  we have to check all the possibilities. Npx, one is not a thing, and NPX 1 is not a thing and NVPI don't think Mvp. I don't think is a thing. we're done with 0 to one and one to 4.  we have to check 0 to 2 and 2 to 4. we have 3 to 5.   we have 3 to 5 S elephant in it seems  it's not a constituent, but we have  elephant in it seems  it's not a constituent, but we have to check formally. Is there any rule for Np, and then P. to check formally. 2 to 5.  we have to check 2 to 3, and then 3 to 5, which is not possible. one to 3, and the 3 to 5 is not possible, and then one to 4 and 4 to 5 we have to check. And then one to four and four to five, we have to check. Form SP on the  and there's no SP on and there's no Sp on the  hand side of any rule. but to check it's  one to 2, 2 to 6, 1 to 3, 3 to 6, and  forth. we should check one to 4, 4 to 6, because that might be possible. But to check is  one to two, two to six, one to three, three to six and  forth. we should check one to four, four to six because that might be possible. If your grammar is reasonable, yes. you should not, you shouldn't. Let's check that Two to four, four to seven, let's check that. But I hope I'm not. Something, but I hope I'm not. ,  there's 1 s. bunch of s here. Are there any roles that are Spp. OK,  in the other notation, just keep a list. and then you just add another entry to the list. And then you just add another entry to the list. in the back pointers, you just keep a list of everything you build, and you make sure you store the  index of how you created that. in the back pointers you just keep a list of everything you build and you make sure you store the  index of how you created that. the last step of the Cyk algorithm is to check OK,  the last step of the CYK algorithm is to check the top  cell and then see the top  cell if you find the starting symbol. and then see if you find the starting symbol. then you just check the top  cell . then you just check the top  cell and you try to find your starting symbol. and you try to find your starting symbol. The thing, though, is that The thing though, is that the number of parses you the number of parses you find is not just the number of s's you find in the top cell. You have a decision point, , . that is a sentence that's recognized by the grammar, that is a sentence that's recognized by the grammar. One of these parses is much more likely than the other. In practice, one of these parses is much more likely than the other. any ideas about how we can computationally  any ideas about how we can computationally decide which beside one is more likely to be the correct interpretation? which one is more likely to be the correct interpretation. There might be  a ways to think about  ways to think about ,  if you have too much complexity as determined by, say, depth or something,  it's less likely as a parse. if you have too much complexity as determined by say depth or something,  it's less likely as a parse. ,  we're gonna make OK,  we're going to make CF GS become PCF Cfgs become Pcfgs. it has a 0 point 2 probability, and  on, and  forth. And  then this is how you might disambiguate. this is how you might disambiguate. You can decide to pick the tree with the most probable parse. You can decide to pick the tree with the most probable parse. , but we need to change things a little bit, because ,  Cfgs did not have a concept of probability built in. And since we're adding that into the formalism we have to change the formalism to make it a Pcfg. But we need to change things a little bit because ,  CF GS did not have a concept of probability built in. And since we're adding that into the formalism, we have to change the formalism to make it a PCFG. If you only care about the most probable tree, which is that we modify the Cyk algorithm directly to incorporate probabilities as we go along,  that by the end you'll just end up with the most probable parse tree as is, and that'll be more efficient, because then you don't have to  explicitly, create all of the ambiguous trees and However, unfortunately for you, there's a more efficient way to do this if you only care about the most probable tree, which is that we modify the CYK algorithm directly to incorporate probabilities as we go along,  that by the end you'll just end up with the most probable parse tree as is. And that'll be more efficient because then you don't have to , explicitly create all of the ambiguous trees and evaluate the probabilities for all. This is just another way of rewriting that I don't know what's you prefer in terms of being more human, interpretable. But This is just another way of rewriting that. I don't know what's you prefer in terms of being more human interpretable, but it's just the same thing. it's just the same thing. ,  ,  , ,  and  we need to change the and  we need to change the recursive step of the algorithm. you only need to keep the Max the most probable structure at each point. you only need to keep the Max the most probable structure at each point. Then you have to also store that separately. suppose that the that writes to the probability is 0 point 6, and the N. Writes to elephants, probability is, say, point 2 5.  suppose that the debt rights to the probability is .6 and the N rights to elephants probability is 8.25. If you create an NP constituent here, then you have then, you have to also figure out the new probability of the elephant, which will be 0 point 6 times 0 point 2 5 times the probability of the rule. to also figure out the new probability of the elephant, which will be .6 * .25 times the probability of the rule. probability and then you can use the back pointers from and then you can use the back pointers from there to recover the most probable parse tree. there to recover the most probable parse tree. and in this way you don't have to recover all the parse trees, you can just recover the most probable one. And in this way you don't have to recover all the parse trees, you can just recover the most probable. Are you still computing, partner. But then, when we're recovering, we That's . the second part of this question is, when you're reconstructing the most probable parse tree. Once you get to that point, then you'll just need you only need the most probable way of constructing that constituent. the second part of this question is when you're reconstructing the most probable parse tree. Once you get to that point, then you'll just need. You only need the most probable way of. Then you have to keep both, because even the probability,  the probabilistic account, doesn't help you disambiguity there. In that case,  then you have to keep both because even the probability  the probabilistic account doesn't help you disambiguate there. But  in practice, that is But  in practice, that is very rare in  very rare in  a real Pcfg situation. And , , since in practice is  rare you might decide to just And , . And since in practice is  rare, you might decide to just do some approximation and you only store  do some approximation, and you only store  one of them arbitrary. When you have the back pointers from the most probable S. Node. At the end, when you have the back pointers from the most probable S node, the back pointers already tell the back pointers already tell you what to do. you what to do,  you just follow the back  you just follow the back pointers. At that point you no longer need to care about the probabilities of each of the subconstituents, because the back pointers already took that into account when you were creating. At that point you no longer need to care about the probabilities of each of the sub constituents because the back pointers already took that into account when you were creating them. ,  OK,  how do you  train APCFG? just  with the HMMS and everything else, you And everything else, you can train it if you have annotations. can train it if you have annotations. which is you count the number of times that you see the rule alpha rewrites to beta and you divide and you divide that by the total number of times that you see the non-terminal symbol. Really the reason is because  ESPY writes to . But  you add some adverbs, ? at the beginning of a sentence, you have an NP, but if it's at the beginning, it's much more But if it's at the beginning, it's much more likely to be I than me, whereas with, if it's in  the object position syntactically, it's much more likely to be me than I. likely to be I than me."
    ]
}