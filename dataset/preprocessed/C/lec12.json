{
    "Topic 1": [
        "? hierarchical structure in natural language. the other thing we talked about is we talked about context-free grammars? The other thing we talked about is we talked about context free grammars. ? context-free grammars. OK,  context free grammars, I remember is formalism. A remember, is a formalism. I'm . I'm . , that's not the bottom. , . . there's some  , here's just a quick reminder about CF, GS and constituent trees. ? ? But why are we using context-free grammars? ? ? in particular, there are other potential choices we could have made instead of context-free grammars. OK.  in particular, there are other potential choices we could have made instead of context free grammars. And, in fact, even broader question is , instead of for each individual language,  English, French, or German, is, does there exist a Cfp for it? Can we show or ask And in fact, that even broader question is  instead of for each individual language  English, French or German is, does there exist the CFP for it? Can all of them be modeled as some context free ground? them be modeled as some context free gram or do we need some other schmutz? why is it that we chose context-free grammars and not say a regular grammar with  a regular language. why is it that we chose context free grammars and not say a regular grammar with  a regular language? And why not something more expressive,  a context, sensitive grammar, or the most expressive one, which is some  recursively enumerable grammar? And why not something more expressive  a context sensitive grammar or the most expressive one, which is some  recursively innumerable grammar. But I guess the ones that I'm pretty sure you've seen are regular languages,  regular expressions But I guess the ones that I'm pretty sure you've seen are  regular languages,  regular expressions and context and context, free grammars with the associated context, free language. free grammars with the associated context free language. ? then, the question that we're gonna briefly discuss   is OK,  then the question that we're going to briefly discuss   is, are natural languages context free grammars? our natural languages context-free grammars. it turns out the answer. it turns out the answer according to this, this view of the world is no, because there exists. because there exists. throughout the years there have been 2 demonstrations that there are phenomena in natural language in some natural languages  throughout the years there have been two demonstrations that there are phenomena in natural in some natural languages which cannot be modeled by context-free grammars. which cannot be modeled by context free grammars, OK. ? As far as we know, all of the phenomena, you can describe it with some context, Gram. As far as we know, all of the phenomena, you can describe it with some context grammar, OK. , and the basic property. and this is known as a cross serial dependency. And this is known as a cross serial dependency. what this means is that there have. And because of the way that it's ordered in Swiss German, it creates this type of cross serial dependency. this type of cross serial dependency. However, personally, I don't find this type of argument or this type of way of thinking about natural language to be particularly useful. However, personally I don't find this type of argument or this type of way of thinking about natural language to be particularly useful. in fact, in all of these arguments about what in all of these arguments about what are natural languages, context-free grammars, and  forth. are natural languages, context free grammars and  forth, they rely on some assumptions which you can disagree with. all of the arguments, even for,  is natural language, a context-free grammar, and their demonstrations that there are some phenomena that you cannot express with a regular grammar they rely on  unlimited processing power. all of the arguments even for  is natural language, a context free grammar and they're demonstrations that there are some phenomena that you cannot express with a regular grammar. here, in this case, the M and the ends have can be arbitrarily large values. here in this case, the M and the NS have to can be arbitrarily large values. three at most. . Personally, I don't think it's the most useful question to ask, but other people will disagree. personally, I don't think it's the most useful question to ask, but other people would disagree. It's not super central to the course, but OK. . ? And then you have a grammar  a context, free grammar associated with that sentence. and what we'd  to do is to generate the output parse tree associated with And what we'd  to do is to generate the output parse tree associated with that sentence according to the that sentence according to the rules of that grammar. if we're doing parsing with a context-free grammar, then we're doing something that in Nlp we would call constituent parsing. if we're doing a parsing with a context free grammar, then we're doing something that in NLP we would call constituent parsing. There are other kinds of parsing. last time we also talked briefly about dependency representations,  and dependency structures to characterize the syntax in a different way. There are other kinds of parsing. And dependency structures to characterize the syntax in a different way. To do that. ? . . OK, OK, thank you. , thank you. ? I shot the elephant in my pajamas, where there's a prepositional phrase that attaches to shot  there are two parses here for that sentence. OK, I shot the elephant in my pajamas where there's a prepositional phrase that attaches to shot in the first in the 1st parse, and in the second parse in my pajamas is a prepositional phrase that attaches to the elephants. parse. And in the second parse, in my pajamas is a prepositional phrase that attaches to the elephants. . , yes, that's . yes, that's . How he got into my pajamas I'll never know. How he got into my pajamas, I'll never know. the second to parse, ? ? OK, But for us, we're going to talk about an algorithm to recover all parses according to ACFG. what are the different kinds of parsing algorithms that exist? If you think about it? what are the different kinds of parsing algorithms that exists? There are others. There are others. , these there are these different kinds of parsers. , these there are these different kinds of parsers and they have different properties, and with certain kinds of grammars, one tends to be more efficient than the other, and  forth. they have different properties. And with certain kinds of grammars, 1 tends to be more efficient than the other and  forth. ? , yes. ? first, st it turns out that Cyk doesn't work with all Cfgs. first, it turns out that CYK doesn't work with all CF GS. . any. ? , can. Yes. Yes. The Np to N, yes. with. ,  this is how you would do it. ? Yes. Yes. yes, that's . then it would become. Yes, that's . , that's not. ? . . . Yes. What's ? What's ? . , yes, that's . Because if I can get through. yes, that's ? . One more. . , yes, that's . Yes, that's . , I confused myself. . OK,  this becomes X2 and then X2 rewrites to  this becomes x 2, and then x. ? ? And I didn't  formally define what counts as a , a  question. OK. And I didn't  formally define what counts as a  a  question. . I see. , yes. Yes. here we have two. There's nothing to stop the grammar from doing that. There's nothing to stop the grammar from doing that. . . . . . That's all. . To J. from WI to J plus 10 indexed. parse. And here we have the updated CNF grammar from before. For example, along the diagonals the diagonals just corresponds to single words. For example, along the diagonals. The diagonals just corresponds to single words. And  forth. . ? ? ? ? . And you can do this for everything on the diagonal. IT would be Det and  forth. And you can do this for everything on the diagonal. ? all we need to do is check all of the possible break points in between the start and the and see if you can build a constituent end and see if you can build a constituents with the rule of the form A from. we care about position, I to J.  whether we can break it up into I to M, and then M. To J. the rule of the form A from . we, we care about position I to J.  whether we can break it up into I to M and then M to J.  for a particular breakpoint, M.  for a particular break point M, you see whether You see whether you can build a rule with the constituents that you've already found there? that means there's only one possible breakpoint, which is, you break after the worded position 0. the possibilities. that means there's only one possible break point, which is you break after the word of position 0.  that means we have to check for rules that are either Npv. Those are the 2 possibilities for how to Those are the two possibilities for how to break that break that up. up. I don't see any,  that means we cannot build any constituents there. that means we cannot build any constituents there. Again, there's only one possible breakpoint,  we can check. Again, there's only one possible break point. N. There are 2 possibilities ? form Det NP or Det N? Det N there are two possibilities, ? that means we can build 2 possible constituents there. that means we can build 2 possible constituents there. build an X2 there. you need to point out how you created that constituent. If you  care about recovering the parse tree at the end. you need to point out how you created that constituent if you  care about recovering the parse tree at the end. is missed here which is that you can create X2 which is  X2 goes to debt. There as . And  this is , the pointer way is more human, interpretable. And  this is  the pointer way is more human interpretable. And that's how you created this. that's just this, a different way of  representing the same the same. Yes, there and here you have to store both. if you care about recovering all possible parses, then you have to store both. if you care about recovering all possible parses, then you have to store both and, yes. And yes, that's . . Four, yes, that's . . Yes. Yes, this is where, I'm , ambiguity comes from. this is where  ambiguity comes from. you have to define what comes from means You have to define what comes from means. I guess ambiguity comes from the language   I guess ambiguity comes from the language, ? ambiguity comes from how natural languages work. And it turns out that we don't mind, and we're very comfortable dealing with ambiguity in communication. Ambiguity comes from how natural languages work. And it turns out that we don't mind and we're very comfortable dealing with ambiguity and communication in the in the context of this algorithm. context of this algorithm. Yes. ,  the last part is that you have to. , the second last part is you have to , fill in the table in the correct order. OK,  the last part is that you have to, , the second last part is you have to  fill in the table in the correct order. if you think about it. . That's . That respects that. what you can do is you can do  one diagonal at a time,  do the diagonal, and then to do the things  to the diagonals and  forth. And that's fine. what you can do is you can do  1 diagonal at a time. do the diagonal and then do the things  to the diagonals and  forth. And that's fine. here, I think that here the order that I'm proposing is, you fill out the diagonal entries, and then you go  Here. I think the here the order that I'm proposing is you fill out the diagonal entries and then you go . . let's finish the example. . ? Or N, N. or in it would be P. Or N it would be P, for my it would For my. ? there's nothing we can build there. from 0 to 2. , here we have. we can have I, and then shot B,  we have two possible break points. ? again, we can't build anything that way, because 0 to do has no. we have. we have Oh , question. Already empty. . . ,   we are at 2 to 4. there's only one possible break point. it's either Det NP or Det N and we already said that this can be either an NP or Or an ex. an X2. Wish me luck because I don't have a pointer up because I don't have a pointer too thick. good. far  good. ,   we have. ,  what? What was that? What was that? ? ,  here we have to check the possibilities. OK.  here we have to check the possibilities. with one to 2, and then 2 to 4. ? I see one, ? I see one, ? Are there any others? Are there any others? ? Also x, 1. Thank you. One thank you. . ? up. ,  how about one to 3 to 3 to 4? We already know that's impossible, because one to 3 is empty. ? We have 0 to one and one to four. And I think we can build an S there. . anything else? Anything else? . That's not possible, because 0 to 2 is empty. 0 to 3 and 3 to 4 is also not possible for the same reason, because 0 to 3 is not possible. we're done. No. Also? No. Also no. five is empty. 2 to 4, and then 4 to 5. Is there any rule for X2 and then P? also? No. that's also empty. Also? No. that's also empty. there is. . Oh, NP goes to PNP, yes, that exists, but it's yes, that exists, but it's in the wrong order, because here we have a Np, and then p, not p, and then np. in the CF GS the order matter. . one to 5,  one to five. ,  one to 2, and then OK,  one to two and then two to five 2 to 5 is not possible. is not possible. One to three and the three to five is not possible. possible to build something there. , and then 0 to 5. 0 to one and one to 5 is not possible. 0 to 2 and 2 to 5 is not possible. 0 to 3 and 3 to 5 is not possible. 0 to 4, and then 4 to 5. 0 to one and one to five is not possible. We have to. One to 6. to six. But there are not. . . 0, , . You're . I got confused. that's empty. You're . That's not possible, . . , the last. You're . , . I got confused. that's empty. You're . That's not possible, . And then here, 0 to 6. I think it's also not possible. I think it's also not possible because of all of because of all of the empty cells there. you'll find a constituent. If your grammar is reasonable, yes. You're just checking that breakpoint. when you're checking these pairs of cells, you're checking a possible breakpoint. ? you should not. The rule is just you're just checking that break point. when you're checking these pairs of cells, you're checking a possible break point, ? you should just  do every possible pair and do every possible pair and try. N, ? we have the same. . The 1st possible breakpoint is 4 to 5, 5 to 7.  the possible, the first possible break point is 4 to 5, five to seven. We do. . how about px. 2. Px. . OK. And then 4 to 6, 6 to 7 is not possible, ? Because 4 to 6 is empty. ? , we have to sell 3 to 7. And Ppp. No. Also? No. . there's  that salad doesn't work. No. that's how it doesn't work. What about 3 to 5, 5 to 7. That doesn't work. 3 to 6, 6 to 7. That doesn't work. it's . Nothing. There. What about 3 to 5? Five to seven, that doesn't work. Three to six, six to seven, that doesn't work. , what about this? 2 to 7.  we have 2 to 3, 3 to 7. That's not possible. 2 to 4, 4 to 7. we have. Nope, but I think there is one for x 2 PP, No, but I think there is one for X2 PP  that creates an NP. that creates an np. . not possible. 2 to 6, 6 to 7, not possible. ,  first, st we have a Vnp,  that's definitely possible. OK.  first we have a VNP,  that's definitely possible. See? fine. Fine. not possible. Not possible. How about one to 4, 4 to 7. How about one to four? Which is Vpp. Do we have any rule which is BPPP? No. How about x, 2, or ? No. How about X2 or , X1PP? . Which song was I on? Wait , which song was that on? Oh , it's still, it's still, , it's still the song. One to 6, 6 to 7 is not possible. , I think we're good. OK, I think we're good. ? . One. no. the other S, No. 0 to 3, 3 to 7 is not possible. 0 to 4, 4 to 7. I don't see any 0 to 5, 5 to 7, not possible. 6 to 7 also not possible. ? Yes. ? There. . you would. . you would keep. . you would. ,  you would keep. ? s there, ? . And then there you go. . If you tilt your head  45 degrees this way, you can even  see the parse directly in the chart, ? If you tilt your head  45° this way, you can even  see the parts directly in the chart, ? it's  multiplicative. over . it's  multiplicative. And then that's used in a parse somewhere else. Then you have to multiply the number of possibilities. Then you have to  multiply the number of possibilities. . You have to multiply everywhere. , what about the s, that's not in the top, ? What about the S that's not in the top ? ? What about the s, that's not in the top ? . What about the S? That's not in the top . ? ,  we're gonna make OK,  we're going to make CF GS become PCF Cfgs become Pcfgs. GS. Maybe it has a 0 point 2 probability, and  on, and  forth. And  then. You can decide to pick the tree with the most probable parse. You can decide to pick the tree with the most probable parse. And since we're adding that into the formalism we have to change the formalism to make it a Pcfg. And since we're adding that into the formalism, we have to change the formalism to make it a PCFG. that forms our probability distribution. , for example, you have a probability distribution associated with  for example, you have a probability distribution associated with all of the noun phrases,  all the noun phrase all of the noun phrases,  all the noun phrase rules. . The sum of all of the rules involving that Np. The sum of all of the rules involving that NP on the left hand side must sum up to one. And  then we can define a new parsing problem which is to recover the most probable parse tree which is to find the Argmax among all the trees And  then we can define a new parsing problem, which is to recover the most probable parse tree, which is to find the Arg Max among all the trees that you can  recover that you can  recover using your PCFG for that using your Pcfg. ,  probabilistic parsing recover the best possible parse for a sentence along with its probability. OK  probabilistic parsing recover the best possible parse for a sentence along with its probability. for it. ? ? We used to build a bigger constituents, ? ? This is just another way of rewriting that I don't know what's you prefer in terms of being more human, interpretable. I don't know what's you prefer in terms of being more human interpretable, but it's just the same thing. And then there, these are some other possible entries where , and you got this number, because maybe at 3 to 4 with the Np. There has some probability And then there, these are some other possible entries where  and you got this number because maybe at 3:00 to 4:00 with the NP, there has some probability, no, no, that. these are just some other possible entries. these are just some other possible entries, I guess. the left hand side with that break point  at each particular break point. You also have to compute the new probability of that constituents. table IJA. for that constituent. You never need to keep the suboptimal one, because that suboptimal one will never be used in the construct in the best possible parse tree. You never need to keep the suboptimal one because that suboptimal one will never be used in the construct in the best possible parse tree if you think about it, If you think about it, because all of these things factor out  locally, there are no long range dependencies between,  the top of the tree and the bottom of the tree. it's not the same. constituent  it's not the same. ? If you create an Np constituent here. you look at the entries here that correspond to S and you find the entry with the highest and you find the entry with the highest probability. probability and then you can use the back pointers from and then you can use the back pointers from there to recover the most probable parse tree. there to recover the most probable parse tree. and in this way you don't have to recover all the parse trees, you can just recover the most probable one. And in this way you don't have to recover all the parse trees, you can just recover the most probable. 1. Yes. Yes. or are we still computing all possible parses. we are no longer computing all possible parses. Are we still computing all possible parses? we are no longer computing all possible parses   because of because of this. where you throw out everything that is Where you throw out everything that is suboptimal locally. suboptimal locally. . , yes. yes. My understanding of this rule is that if they're  My understanding of this rule is that if they're  2 possible ways of creating. The same token we throw out. We throw out this about new one. But if there are 2 possible tokens we can create. 2 possible ways of creating the same token we throw out, we throw out the suboptimal one. But if they're two possible tokens we can create, and one of them has a higher probability than the other. Then we keep both. one of them has a higher probability than the other, then we keep both. That's . just to rephrase. That's . Then you throw out all but the best. you throw out. All but the best. That's . But then, when we're recovering, we That's . But then when we're recovering, we and we take the and we take the constituents with the highest probability and ignore other possibilities. constituents with the highest probability and ignore other bus. That's . the second part of this question is, when you're reconstructing the most probable parse tree. That's . the second part of this question is when you're reconstructing the most probable parse tree. Constructing that constituent, yes. Yes. Is it possible to get the same probability. Is it possible to get the same probability? Yes, it's possible. Is it possible to get the same probability? Is it possible to get the same probability? Yes, it's possible. But  there's no, the theory doesn't give you any reason to prefer one parts over the other in that situation. But , there's no the theory doesn't give you any reason to prefer one marks over the other in that situation. Yes. Yes. Yes. Yes. . in each cell we're storing all the constituents we build and we're storing, in a probability associated with each of them  in each cell we're storing all the constituents we build, and we're storing in a probability associated with each of them. ? . this is a name. Hence the extra E, This is a name and state extra E and . and . if you have. . In order to estimate these we discussed to do with estimating values of parameters with  HMMS and Ngram models and  forth, in order to estimate these parameters of these new categorical distributions these parameters of these new categorical distributions we're creating in the Pcfg we're creating in the PCFG by using counts. by using counts. . And you can definitely do good turn smoothing or add and you can definitely do good turning, smoothing, or add one smoothing, or whatever everything we talked about in that module of the course to do with smoothing still applies here, because these are also categorical distributions. 1 smoothing or whatever. Everything we talked about in that module of the course to do with smoothing still applies here, because these are also categorical distributions. . , however, this scheme of these Pcfgs turns out to not work very  in practice. OK,  however, this scheme of these PCF GS turns out to not work very  in practice. You could build that verb phrase. There are many different ways you could build that verb phrase and it seems  a different ways you could and it seems  the different ways you could use to build that verb phrase they might be. They might . use to build that verb phrase. you might have,  a verb phrase with  an object noun. you might have  a verb phrase with  an object noun. And another obvious example would be  the difference between a syntactic subject versus a syntactic object. Another obvious example would be  the difference between a syntactic subject versus a syntactic object. , for example, it with pronouns  I,  for example it with pronouns,  I is a subject pronoun in English ? is a subject pronoun in English , whereas me would be the object version. Whereas me would be the object version. at the beginning of a sentence, you have an NP, but if it's at the beginning, it's much more But if it's at the beginning, it's much more likely to be I than me, whereas with, if it's in  the object position syntactically, it's much more likely to be me than I. likely to be I than me. Whereas with if it's in  the object position, syntactically, it's much more likely to be me than I.  this, the standard assumption of the Pcfgs does a very poor job of modeling all of these situations. this, the standard assumption of the PCFGS, does a very poor job of modeling all of these situations. After that. all . Thank you. All . Thank you."
    ],
    "Topic 2": [
        "Hello, can you? Hello, can you is the microphone working OK? Great. I hope you all had a good week reading week. I hope you all had a good week reading week and you're enjoying the unseasonably nice weather. ,  OK,   , we'll get into all that again  , we'll get into all that again today. today. , forget it. OK, forget it, I'll just leave it. But I'll try to make sure the deadlines are not. there's lots of things coming up, but I'll try to make sure the deadlines are not most of the Most of the deadlines will not be before the midterm. ,  we're gonna talk about Zky parsing or cyk parsing. OK.  we're going to talk about CKY parsing or CYK parsing and then we're going to motivate why we need And then we're going to motivate why we need that. we'll get into that. And it's more of  a linguistics flavored  sub topic. But I think it's interesting. we'll get into that And it's more of  a linguistics flavored  sub topic, but I think it's interesting. there's some in, in our case there's going to in our case. and that they rewrite into terminal symbols, which are the leaf nodes, and for us they correspond to words in a sentence. And then they rewrite into terminal symbols, which are the leaf nodes. And for us they correspond to words in a sentence. where we left off last time OK,  where we left off last time we asked we asked, or I brought up this question of. you need to take some automa theory or theory of computation if you want to learn more about these other classes. you need to take some automata theory or theory of computation if you want to learn more about these other classes. can accept all and only the strings. I think, in Swiss German, if  any German, the idea is that in Swiss German there are some verbs that require accusative case, and some verbs that require dative case, and because of the way that it's ordered in Swiss German, it creates I think in Swiss German, if  any German, the idea is that in Swiss German there are some verbs that require accusative case and some verbs that require dative case. OK. They rely on  unlimited processing power. These constructions recursively as many times as you'd . OK,  things can be arbitrarily used  you can use these constructions recursively as many times as you'd . and anything else is  very arbitrary and contrived and artificial, and you never see it attested in corpora. And anything else is  very arbitrary and contrived and artificial and you never see it attested in corpora. And still, that's an interesting  point to note, And still, that's an interesting  point to note, which is a . ,  back to something more OK,  back to something more algorithmic. algorithmic. ,  today, we're going to talk about parsing. OK,  today we're going to talk about parsing. what is parsing  what is parsing? It's not a new word, because maybe you've seen it in  programming languages or something. hopefully it's not a new word because maybe you've seen it in  programming languages or something. parsing in natural languages can be a little bit trickier. parsing in natural languages can be a little bit trickier and we're going to look at that. And then there, you can talk about dependency parsing. last time we also talked briefly about dependency representations, ? And then there you can talk about dependency parsing. there are algorithms that  look a lot  the algorithms I'm gonna talk about. one of the main difficulties in parsing is to find an efficient way to search through all of the plausible parse trees  one of the main difficulties in parsing is to find an efficient way to search through all of the plausible parse trees for some input sentence. And  we're going to talk about an algorithm. And  we're gonna talk about an algorithm to do that. going to solve for CFG. when you're parsing into a programming language. when you're parsing into a programming language, you would you would expect there to be just one parse  of a programming language. expect there to be just one parse, , of a programming language. But for us, we're going to talk about an algorithm to recover all parses according to a Cfg. We're going to talk about one called Cyk. ,  chart parsing. OK,  chart parsing. , yes, dynamic programming. Dynamic programming. OK,   we're going to cover the CYK algorithm,   we're going to cover the Cyk algorithm, which is going to be an example of a dynamic programming algorithm. which is going to be an example of a dynamic programming algorithm, OK? Sometimes the cyk, sometimes the cky. Sometimes it's CYK, sometimes it's CKY. ,  it's a cock, younger Kasami or cock Kasami younger. OK,  it's a cock younger kasami or a cock kasami younger. , again, it's a dynamic programming algorithm, which means that partial solutions are stored and they're efficiently reused  that we can recover all possible parses for the entire sentence  , again, it's a dynamic programming algorithm, which means that partial solutions are stored and they're efficiently reused  that we can recover all possible parses for the entire sentence. , as is common with many dynamic programming algorithms, you need to create some data structure to store these partial solutions. , as is common with many dynamic programming algorithms, you need to create some data structure to store these partial solutions. once you're used to dynamic programming algorithms, this  thing becomes  once you're used to dynamic programming algorithms, this  thing becomes pretty standard and formulaic. But we are, we are going to go through the details and make sure that we understand this is not  easy the 1st time you see it. But we are, we are going to go through the details and make sure that we understand because it's not  easy the first time you see it. OK,  how are we going to do that? and then D, and then x, 1 rewrites to BC, And then D and then X1 rewrites to BC and by doing this we shorten the length of what's and by doing this we shorten the length of what's gonna be there. going to be there. , we're going to do an example. OK, we're going to do an example. here I've written out a tiny little grammar  here I've written out a tiny little grammar that will allow us to handle the sentence of. This means in our case. Every time This means in our case, every time we see a we see a rule where N is on the left hand side. The reason you need to keep it is that your tree could have gone to I an elephant in pajamas through N. Through some other way. The reason you need to keep it is that your tree could have gone to I and Elephant in pajamas through N through some other way, not through the NP not through the Np. and  you need to keep it to not mess up the strings that your grammar will accept or reject. And  you need to keep it to not mess up the strings that your grammar will accept or reject. Great. OK, great. Maybe it'll be clear. yes, great. Great. Maybe I'll just keep it here. Maybe I'll just keep it here. One more OK, almost there. the new. Did I put in the new one? The new did I put in the new one. to that n great. everything is Great,   everything is CNF. This is just a shorthand. This is just a shorthand, ? here we have 2 rules with the same  hand side sequence of symbols, and that's totally fine. Rules with the same  hand side sequence of symbols, and that's totally fine. what  by an acceptable change to the grammar is that it doesn't change the set of strings that are accepted or rejected by the grammar. what  by an acceptable change to the grammar is that it doesn't change the set of strings that are accepted or rejected by the grammar. when you convert something to Cnf, you won't change the set of strings that are accepted or reject. And  when you convert something to CNF, you won't change the set of strings that are accepted or rejected. N words,  0 indexed. 0 indexed. Plus 1 0 indexed. And this sounds more complicated than what it is. once you see the example. it's all six words where I say this cell whereas, say, this cell of 2 to 5 corresponds to the words, the elephant in of two to five corresponds to the words the elephant in OK I'm using Python style indexing where the last, , I'm using python style, indexing where the last, the second number is . the second number is  exclusive. exclusive. ,  2 to 5 means the words to at positions 2, 3, and 4. OK,  two to five means the words to at positions 2-3 and four. we're 1st going to fill out the constituents corresponding to single words, and then we're going to do the ones corresponding to say 2 words, and the 3 words and 4 words, and  forth. we're first going to fill out the constituents corresponding to single words, and then we're going to do the ones corresponding to say 2 words and the three words and four words and  forth. the base case. again, if you've seen dynamic programming algorithms before,  that we need a base case and we need a recurrence step. the base case. again, if you've seen dynamic programming algorithms before,  that we need a base case and we need a recurrence step. And in fact, when you did. Hmms, all of those algorithms  forward algorithm backward algorithm viterbi. Those are also dynamic programming algorithms. Topologically. And in fact, the when you did HMMS, all of those algorithms  forward algorithm, backward algorithm, Viterbi, those are also dynamic programming algorithms, ? ,  for us, the base case corresponds to individual words. OK,  for us, the base case corresponds to individual words. , for example. OK,  for example, here are the first 3 words. here are the 1st 3 words. 0 1 and 2. zero to three exclusive,  01 and two. What about this case? What about this case? We're going to work through this example in total. example in total. We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect. we're going to see whether we can recover the two parse trees that we expect. you do catch my mistakes. you need to catch my mistakes. What I need to do is create a bunch of text boxes. Oh, OK, I need to what I need to do is create a bunch of text boxes. for I, we already said, this could be an Np. OK,  for I we already said this could be an NP or an N, ? and for pajamas it would be Np or N, And for pyjamas, it would be NP or N OK,  those were our base cases and we're done our . those were our base cases and we're done our base cases. base cases. again, we can't build anything that way because zero to do has no nothing spans that. , pretty easy. OK, pretty easy  far. You cannot take that as a heuristic because in this particular case, it's zero to 1 and then one to three and then zero to 2 and then two to three. N. And we already said that this can be either an Np. 2.  we need to  we need to do that and I'm going to do that. to thick OK . The other possibility is  a The other possibility is  AX2 goes to dead end. , I'm going to give up on drawing the arrowheads. OK, I'm going to keep up on drawing the arrowheads. box? OK  what? That's not possible because zero to two is empty. Zero to three and three to four is also not possible for the same reason, because zero to 3 is not possible. OK. And then zero to five. Zero to two and two to five is not possible. Zero to three and three to five is not possible. Zero to four and then four to five. OK, . that would be relevant. And one to 5, 5 to 6 is all worthy. And one to five, five to six is all OK. . And then here is zero to 6. You shouldn't. you should not, you shouldn't. interesting again. OK. , , here we have 4 to 7. the 1st possibility is Npp. the first possibility is NPPP. OK,  there's. it's OK, nothing there. OK, what about this? we have NPPP. , I feel  I might be missing something. But I hope I'm not. OK. I feel  I might be missing. Something, but I hope I'm not. because it's this V with this Np, which gives us. Because it's this V with this NP which gives us VP. Maybe I'll use yet another color. OK, maybe I'll use yet another color. I use purple. I'll use purple. And if things worked out properly, there should be a And if things worked out properly, there should be a bunch of S's here. ,  there's 1 s. bunch of s here. OK.  there's 1S. Do we have any Mvps Do we have any NVPS or NX-1 or NPX one? And then every time you see a starting symbol, you can just use the back pointers And then every time you see a starting symbol, you can just use the back pointers to trace through all to trace through all of the possible paths. the total number of ambiguous  the total number of ambiguous parse trees for the parse trees for the sentence will be the you have to multiply everywhere you have a decision point. You have a decision point, , . that is a sentence that's recognized by the grammar, that is a sentence that's recognized by the grammar. But it's not the sentence that we're currently interested in. for our purposes, it's not relevance. but it's not the sentence that we're currently interested in. for our purposes, it's not relevance. any ideas about how we can computationally  any ideas about how we can computationally decide which beside one is more likely to be the correct interpretation? which one is more likely to be the correct interpretation. We have to change something about the Cfg or something. you have to we have to change something about the CFG or something. I'll I'll go see? , there might be  a You'll see  the depth maybe? There might be  a ways to think about  ways to think about , maybe if you have too much complexity as determined by, say, depth or something, maybe it's less likely as a parse. maybe if you have too much complexity as determined by say depth or something, maybe it's less likely as a parse. And  then this is how you might disambiguate. this is how you might disambiguate. , but we need to change things a little bit, because ,  Cfgs did not have a concept of probability built in. OK. But we need to change things a little bit because ,  CF GS did not have a concept of probability built in. I don't technically need to introduce any new algorithms for this. OK,  I don't technically need to introduce any new algorithms for this. The simplest way to do this with what we've already The simplest way to do this with what we've already talked about in class today is to just run the Cyk algorithm talked about in class today, is to just run the CYK algorithm as is, ignoring the probabilities, and then you as is ignoring the probabilities. That would be  a obvious way to do this. That would be  a obvious way to do this, and it would be easy, and then you don't have and it would be easy. However, unfortunately for you, there's a more efficient way to do this. If you only care about the most probable tree, which is that we modify the Cyk algorithm directly to incorporate probabilities as we go along,  that by the end you'll just end up with the most probable parse tree as is, and that'll be more efficient, because then you don't have to  explicitly, create all of the ambiguous trees and However, unfortunately for you, there's a more efficient way to do this if you only care about the most probable tree, which is that we modify the CYK algorithm directly to incorporate probabilities as we go along,  that by the end you'll just end up with the most probable parse tree as is. But This is just another way of rewriting that. that no, this should be OK. No, this should be a . ,  ,  , ,  and  we need to change the and  we need to change the recursive step of the algorithm. Once you get to that point, then you'll just need you only need the most probable way of constructing that constituent. Once you get to that point, then you'll just need. You only need the most probable way of. When you have the back pointers from the most probable S. Node. OK. . , I said before, there are other parsing algorithms. The the one that I would look at , if you're interested is to do top down parsing with the early algorithm. I said before, there are other parsing algorithms,  if you're interested you can look them up. The the one that I would look at  if you're interested, is to do top down parsing with the early algorithm. ,  OK,  how do you  train APCFG? how do you  train a Pcfg,  I talked about the parsing algorithm first.st  I talked about the parsing algorithm first, but I didn't talk about where the probabilities come from. just  with Hmms. just  with the HMMS and everything else, you And everything else, you can train it if you have annotations. can train it if you have annotations. OK,  for example, for the most, for the longest time from  19, from the in the 1990s and in the 19 nineties and in the 2 thousands, people worked on a particular Corpus, which is probably the most famous corpus in Nlp. Which is the Wall Street Journal Corpus in the 2000s, people worked on a particular corpus, which is probably the most famous corpus in NLP, which is the Wall Street Journal corpus. , no, it's called the Pantry Bank, and it involves Wall Street, mostly Wall Street Journal data, but also some other stuff. , no, it's called the Penn Treebank and it involves Wall Street, mostly Wall Street Journal data, but also some other stuff. And then if you have a collection of trees. that by the total number of times that you see the non terminal symbol alpha. But I'll I'll talk about , why this is . Really, the reason is because,  I don't think I have time to explain the solution, but I'll I'll talk about  why this is . But maybe you add some adverbs, ? at the beginning of a sentence, you have an Np. ,   class, we're going to talk about one particular scheme that people have proposed OK,   class we're going to talk about one particular scheme that people have proposed to try to fix to try to fix this issue a little bit, and then we're going to get started on semantics. this issue a little bit, and then we're going to get started on semantics after that."
    ],
    "Topic 3": [
        "we are going to continue our discussion about syntax. we are going to continue our discussion about syntax. But last class we talked about syntax and structure and last class we talked about syntax and structure and hierarchical structure in natural language. Oh, no. Oh no, it won't go away. it won't go away. The little arrow, the green arrow on the left, the Green Arrow. The little arrow, the green arrow on the left. The green arrow, OK, that's not the bottom. I guess there's 1 topic from last class we didn't quite manage to cover, which I think would be interesting to talk about that. I guess there's one topic from last class we didn't quite manage to cover, which I think would be interesting to talk about. Can we show or ask whether for any natural language whether for any natural language that we currently that we currently know about on Earth, can all of know about on earth? Or do we need some other Schwartz? There has to be  M.  what this means is that there have there has to be  M things of one type, followed by Things of one type, followed by N things of one type of a second type, followed by M things of a 3rd type, and then n, things of a 4th type. N things of one of a second type, followed by M things of 1/3 type and then N things of a fourth type. And then, Bambara, I think the argument goes along with something to do with morphology of Bambara. And in Bambara or I think, I think the argument goes along with something to do with morphology of Bambara, . ,  things can be arbitrarily use recur  you can use this. About how language works in order for these arguments to make sense. hopefully? the main idea in parsing is that you have a sentence, an input sentence. OK,  the main idea in parsing is that you have a sentence, an input sentence, and then you have a grammar,  a context free grammar associated with that sentence. You want to come up with a tree structure, with all the constituents that describe that particular sentence. And you want to come up with a tree structure with all the constituents that describe that particular sentence. and you can contrast this. And you can contrast this. But  there are algorithms that  look a lot  the algorithms I'm going to talk about, but I but I won't present them explicitly in the lecture. won't present them explicitly in the lecture. for some input sentence. ambiguous. Why did you prop up? Why did you prop up? here is a joke from the early 20th century, and it's by a comedian. here is a joke from the early 20th century and the sentence by a comedian, I think his name is Groucho Marx. , that's the full joke. OK, that's the full joke. The other strategy that you can have is a bottom up strategy, which is, you start from the input words. The other strategy that you can have is a bottom up strategy, which is you start from the input words and then you build ever bigger sub trees using the and then you build ever bigger subtrees. you're creating bigger and bigger subtrees corresponding to bigger and bigger constituents until eventually you find the starting symbol. S, that spans the entire sentence, and that's called a bottom up strategy. you're creating bigger and bigger subtrees corresponding to bigger and bigger constituents until eventually you find the starting symbol S that spans the entire sentence. back in the day when parsing was really popular, an Nlp course would be  1 3rd about parsing, and we would go over all sorts of parsing algorithms. Reduce strategy. There are even strategies for parsing that goes both up and down. it's  a complex search strategy where you have 2 ends and you start to  gradually get them to meet together. back in the day when parsing was really popular, an NLP course would be  1/3 about parsing and we would go over all sorts of parsing algorithm. There are even strategies for parsing that goes both up and down. it's  as complex search strategy where you have two ends and you start to  gradually get them to meet together. But we're just going to cover one, because these days you're lucky in that,  parsing is no longer as popular. But you still should know about it, because it's very good pedagogically. And  we're just going to cover one parsing algorithm, we're going to cover the Cyk algorithm, which is a bottom up algorithm. But we're just going to cover one because these days you're lucky in that  parsing is no longer as popular, but you still should know about it because it's very good pedagogically. And  we're just going to cover 1 parsing algorithm. We're going to cover the CYK algorithm, which is a bottom up algorithm. Sometimes it's  different sources. Order the authors differently. Sometimes it's  different sources order the authors differently. Here are the steps of the Cyk algorithm. And for us, we're going to create a table to store all of these possible constituents. And for us, we're going to create a table to store all of these possible constituents. to make things easier later. You have a rule of the type. And  you have this particular case where for every rule and  grammar where B is the left hand side, you copy that rule, but you replace B with You copy that rule, but you replace B with A, A.  this is  a  this is  a bit more involved. a bit more involved. that will allow us to handle the sentence of I shot the elephant in my pajamas. And it has some rules that are unary. And it has some rules that are unary. Replace all ends The NP to N. Yes. ,  this one is the hardest one. You copy it and replace it with A. it and replace it with A. We have to copy it. have to copy it and then we have to replace Replace N with np. if I just delete it. Vp. then it would become VP. Is this, you replace the 1st 2? Is this you replace the 1st 2 ? we could replace it in. to the digital. Oh, I messed up. Oh, no, I did ? Oh no, I did . OK,  that was step one. that was, step one step 2 is to set up our data structure. Step 2 is to set up our data structure. we're going to set up a 2 dimensional table that will store all of the constituents that can be built from contiguous spans within the sentence. by contiguous, that just means that all of the things within the span are  to each other. we're going to set up a 2 dimensional table that will store all of the constituents that can be built from contiguous spans within the sentence. by contiguous, that just means that all of the things within the span are  to each other. We're going to create a table such that cell a cell in row. I column J corresponds to the span from We're going to create a table such that cell, a cell and row I column J corresponds to the span WI. This will make sense. All it's saying is that cell stores all of the constituents that spans a certain subspan within your sentence of the words within the sentence. And this sounds more complicated than what it is,  once you see the example, this will make sense. All it's saying is that cell stores all of the constituents that spans a certain subspan within your sentence of the words within the sentence. We want I to be less than J.  we only really need half the table. that means our table will look  an upper triangular particular order, we want I to be less than J.  we only really need half the table. that means our table will look  an upper triangular table. table. Again, you can reread this sentence later on, after we do the example, and it'll make sense afterwards. Again, you can reread this sentence later on after we do the example and it'll make sense afterwards. OK,  here we have the sentence we want to we have the sentence, we want to parse. and we want to fill in this table. And we want to fill in this table. and each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. And each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. this cell corresponds to the word I,  this cell corresponds to the word I, and this cell of three to four corresponds to the word elephant. and this cell of 3 to 4 corresponds to the word elephant, and  forth. and the cell that corresponds to the entire sentence is here. This cell corresponds to 0 to 7.  it's all 6 words. And the cell that corresponds to the entire sentence is here this cell corresponds to zero to seven. OK.  , we have to fill in the table itself. we have to fill in the table itself. those also involve the base case and the recurrent step. it's exactly the same, except the structure is different because we are assuming a different  structure. those also involve the base case and the recurrent steps. it's exactly the same except the, the, the structure is different because we are assuming a different  structure topologically. That that makes sense. That that makes sense, . for the word, I,  for the word I, you just look up all you just look up all of the rules that can generate the word I. And P. And N. of the rules that can generate the word I. the harder step is the recursive step, because this step corresponds to multiple words. OK,  the harder step is the recursive step because this step corresponds to multiple words. , for example, the span for the cell for the span I shot D would be from  a  for example, the span for the cell for the span I shot B would be from  a position 0 to 3 exclusive. all we need to do is check all of the possible breakpoints in between the start and the end. Because, remember, we're going from smaller chunks to bigger chunks. if we're able to build this bigger chunk A. That means we have already found the smaller chunks B and C, and we can combine them to form the bigger chunk A, Because remember, we're going from smaller chunks to bigger chunks. if we're able to build this bigger chunk A, that means we have already found the smaller chunks B&C and we can combine them to form the bigger chunk A. and we just need to check all possible breakpoints to make sure that we cover all of the possible And we just need to check all possible break points to make sure that we cover all of the possible ways to build that bigger chunk. ways to build that bigger chunk. ,  here's an example of a recurrent step. OK,  here's an example of a recurrent step. for I shot, we need to check all of we need to check all of the possibilities. Or Nv. that means we have to check for rules that are either NPV or envy. Do we have any rules of the type? Npv or nv. Do we have any rules of the type NPV or envy? this cell is corresponds to the elephants. this cell is corresponds to the elephants. Or debt? N,  debt? the Viterbi algorithm is you should store back pointers. here we're going to create this constituent Np goes to debt. But then, when you  store it in a data structure, then you have to store that this was the debt from this cell. But then when you  store it in a data structure, then you have to store that this was the debt from the cell and then this was the end from the cell and that's how you created this NP. that's just a different way of late. Then here you have to store both . , you can store multiple constituents in each cell. And then you can use all of them to build bigger chunks, bigger constituents. , in that sense. , you can store multiple constituents in each cell and then you can use all of them to build bigger. Chunks bigger constituents  , in that sense, yes, OK, ,  we're gonna work through this . if you think about it, you have to fill you have to fill in the table in the table when you're processing a cell, you have when you're processing a cell, you have to make sure that you filled out all the cells to the left and below it to make sure that you filled out all the cells to the left and below it, everything  to the everything  to the bottom left of it from my perspective. from your perspective, I guess. bottom left of it from my perspective. from your perspective, I guess, OK, because those correspond because those correspond to all of the smaller constituents and smaller chunks that you might need in order to build to all of the smaller constituents and smaller chunks that you might need in order to build the constituent that the constituent that spans the current cell. spans the current cell. That, that's, , the, the, the words that spans The the words that spans the words corresponding to the current cell. the words corresponding to the current cell. you just need to pick an order to fill out the table. you just need to pick in order to fill out the table that respects that. or you can do  the you can do it  one column at a time, bottom up. Or you can do  the you can do it  a one column at a time bottom up. as long as you filled out everything As long as you filled out everything to the bottom to the bottom left of a cell when you're processing that cell. Then you're good left of a cell when you're processing that cell, then you're good. bottom to top for each Bottom to top for each column,  that also works. column. Oh, , I need to. be debt. it would be debt. And  I'm gonna go And  I'm going to go bottom up and column bottom up and by column, left to . column by column, left to . do we see any V debt in our grammar? this cell is also empty. this cell is also empty. nothing spans that. this cell is also empty. this cell is also empty. Oh, . But later on, with bigger spans. You just have to. You have to make sure you check the  2 cells. But later on with bigger spans, you just have to, you have to make sure you check the  two cells. , you have to really reason through it and or and work through the algorithm ,  , you have to really reason through it and or, and work through the algorithm robotically at the robotically at the start. start. it's either debt, Np. Or debt. attempt. Wish me luck. See, I said, I wouldn't miss things See, I said I wouldn't miss things,  oh shoot, that should be a VP. Oh, shoot! That should be a Vp, Bp goes to B VP goes to V&P and X1 goes to. and P. And x, 1 goes to the The NP,  the, the blue and the red is and P. just to help you distinguish the two, ? the blue and the red is just to help you distinguish the 2 . But this set of red edges has nothing to do with this set of red edges just to be clear. But this, this set of red edges has nothing to do with this set of red edges, just to be clear, ? finally, for this column, , we have 0 to 4.  we have to check all the possibilities we have 0 to one and one to 4.  let's check that first.st OK,  finally, for this column , we have 0 to 4:00,  we have to check all the possibilities. we're done with 0 to one and one to 4.  we have to check 0 to 2 and 2 to 4. That column  we're done that column  column. column. We have to check. the cell is empty. 2 to 5.  we have to check 2 to 3, and then 3 to 5, which is not possible. Nope, is there any rule for x. 2, and then P Nope. OK,  cell. cell. one to 3, and the 3 to 5 is not possible, and then one to 4 and 4 to 5 we have to check. And then one to four and four to five, we have to check. Nope. Nope. and also x 1 p Nope,  that cell is also empty. And also X1P Nope. that cell's also empty. we check. we check. Check for rules of the. ,  column 4 to 6. P. Debt, is that possible? OK.  column four to six,  P debt, is that possible? , ,  this cell is empty,  we can't go 3 to 4, and then 4 to 6. This cell is empty,  we can't go 3 to 5, and then 5 to 6. possible there either because of all the empty cells one because of all the empty cells. but to check it's  one to 2, 2 to 6, 1 to 3, 3 to 6, and  forth. I guess we should check one to 4, 4 to 6, because that might be possible. Debt. But to check is  one to two, two to six, one to three, three to six and  forth. I guess we should check one to four, four to six because that might be possible. 1 debt, then or X1 debt, then that would be relevant. But there are not wait. Wait. Oh, , . the empty cells there. And this all makes intuitive sense, because it's  I shot the elephant in my . And this all makes intuitive sense because it's  I shot the elephant in my . any anything that ends with my . If you want to run the Cyk algorithm exactly. you should just . If you want to run the CYK algorithm exactly, there are no heuristics. then try, OK, I think the last column will be , I think the last column will be  interesting again. 5 to 6, 6 to 7, 7. ,  we have a debt. And then And then how about PX2? no, , what about Npp? OK. What about NPP also? Npp. Npp. Vp. Oh, , there's an x 1. Oh , there's an X1. Is orange. This is orange. Oh, no, it looks too much  red Oh no, it looks too much  red. wait. Oh, , it's still, it's still , it's still the cell. we have a different vp. we have a different VP, OK. ,  one to 5, 5 to 7 is not possible. we only have one cell left. exciting. exciting. One or Npx. Are there any roles that are Spp. Yes, when there are 2 Vps in one cell with the notation where you refer to  Vp, and then the cell number, how do they distinguish? When there are two VPS in one cell with the notation where you refer to  VP and then the cell number. How do they distinguish them? You built another Vp. You built another VP which is one to four with which is one to 4 with the x 1 there, and then 4 to 7 with the PP. the last step of the Cyk algorithm is to check OK,  the last step of the CYK algorithm is to check the top  cell and then see the top  cell if you find the starting symbol. Oh, I guess to be consistent here, this should be 2 different. S's Oh, I guess to be consistent here, this should be two different s because we built two different s because we built 2 different. With two different. then you just check the top  cell . The thing, though, is that The thing though, is that the number of parses you the number of parses you find is not just the number of s's you find in the top cell. find is not just the number of s, s you find in the top cell. the same non terminal constituent  in somewhere in a smaller span, you have to  copy  the structure You have to  copy  the structure over ? sentence will be the. sentence. For that sentence. sentence. and we want to find the Argmax of the probability of And we want to find the ART Max of the probability of T of  the of the sentence. And then you just score all of the possible trees that we cover . just score all of the possible trees that we cover, ? that's the idea we're going to extend the Cyk algorithm to keep track of probabilities in the table itself. We're going to extend the CYK algorithm to keep track of probabilities in the table itself. recall that we had this notation for the back pointers that we could have with , just indicating which are the subconstituents we use to build a bigger constituents  recall that we had this notation for the back pointers that we could have with  just indicating which are the sub constituents. , we're gonna augment that. we're going to augment that  that in  that in our table with the list of entries for all of the ways we could build things. , for example, previously we might have had an entry, which is, we created a new Np for the span, 2 to 4 from the debt at the span, 2 to 3, and the end of the span 3 to 4. OK.  for example, previously we might have had an entry which is we created a new NP for the span two to four from the debt at the span two to three and the end of the span three to four. we're going to augment that with the probability  we're going to augment that with the probability of that constituent as  that constituent as . I guess. previously in the recursive step of the algorithm you just have to check recursive step of the algorithm. side symbols, were you able to combine them to form Were you able to combine them to form the left hand side with that breakpoint  at each particular breakpoint. you do that, and if you're able to create that match  that you can create the bigger constituents. you do that and if you're able to create that match  that you can create the bigger constituents, you also have to compute the new probability of that constituents. And  then you multiply them all together, and then you report it in your table. And  then you multiply them all together and then you report it in your table. You only need to keep the Max probability and the Max back pointers of the span, left hand side and  hand side and non terminal symbol. You only need to keep the Max probability and the Max back pointers for that constituent. you only need to keep the Max the most probable structure at each point. you only need to keep the Max the most probable structure at each point. Then you have to also store that separately. same span then you have to also store that separately because the probabilities there is there's this for a different because the probabilities there is, there's it's for a different constituent. just as an example here is one of the recursive steps again. OK  just as an example, here is one of the recursive steps again. We already know that we won't use it. We already know that we won't use it,  we're throwing that out. just to re, I would just not use the word token, I would use the word constituent. just to read I would just not use the word token. I would use the word constituent. In that case, I guess. Then you have to keep both, because even the probability,  the probabilistic account, doesn't help you disambiguity there. In that case, I guess then you have to keep both because even the probability  the probabilistic account doesn't help you disambiguate there. Yes, yes, ,  OK,  we are getting the, it won't grow. we are getting the, we're getting the cell. we're getting the cell. At the end, when you have the back pointers from the most probable S node, the back pointers already tell the back pointers already tell you what to do. you what to do,  you just follow the back  you just follow the back pointers. pointers. Exciting stuff. for example, for the most, for the longest time from  19 Exciting stuff. Then you can use what we already know And then if you have a collection of trees, then you can use what we already know from everything that from everything that we discussed to do with estimating values of parameters, with  Hmms. Oh. But maybe you add some adverbs   we talked about kick the ball versus kick the ball softly or quickly, or something. we talked about kick the ball versus kick the ball softly or, or, or quickly or something."
    ],
    "Topic 4": [
        "I. Jackie Cheung, Professor: I'm . Is the microphone working? and you're enjoying the unseasonably nice weather. try to recall. We talked about these things called constituents. try to recall, we talked about these things called constituents, which are groups of words that acts together as which are groups of words that act together as a unit in a sentence. And we also talked about tests for constituency. a unit in a sentence. And we also talked about tests for constituency, if you Hey? This is worse. This is worse. ,  you might have seen on Ed that I  you might have seen on Ed that I posted information about both the midterm that's happening as  as the final project description. please do take a look at those and start working on it with your project partners posted information about both the midterm that's happening as  as the final project description. please do take a look at those and start working on it with your project partners for the for the for the final project. I'll also post the reading assignments soon, and I'll also post the programming assignment soon,  there's lots of things coming up. the final project. I'll also post the reading assignments soon and, and I'll also post the programming assignment soon. you can have time to focus on that. deadlines will not be before the midterm,  you can have time to focus on that. , here's just a quick reminder about Cfgs and constituent trees. There's gonna be an S node which is the starting symbol. be an S node which is the starting symbol. And then, based on that, you use these rewrite rules to rewrite it into non-terminal symbols. And then based on that you use these rewrite rules to rewrite it into non terminal symbols. to the rules of that CFG for that sentence. this is just a very brief reminder of Cfgs from last class. this is just a very brief reminder of CFGS from last class. in particular, here are some other possible possibilities, and you might have heard about some of these in other computer science classes. in particular, here are some other possible possibilities, and you might have heard about some of these in other computer science classes. And it's 2 particular languages. and all the other languages. And because there's this notion, there's this. That are of this form. , in fact. you really have to buy into a particular view a particular worldview about how language works in order for these arguments to make sense. you really have to buy into a particular view A. any questions about this. Any questions about this? This is  a fun aside, it's not super central to the course. But This is  a fun aside. But in natural language natural language has different characteristics compared to programming languages. Natural language has different characteristics compared to programming languages. And we're going to look at that. then here is OK,  then here is more specifically the problem we're more specifically the problem we're going to solve for Cfg. given a particular Cfg. given a particular CFG and given a sentence made-up And given a sentence made up of words that are in the terminal vocabulary of the Cfg. of words that are in the terminal vocabulary of the CFG, the goal that we have is to recover all The goal that we have is to recover all possible parses of the sentence. possible parses of the sentence. And  this is one key difference between natural language and programming languages. And  this is one key difference between natural language and programming languages. another fun fact. This might be violated with Perl, which is why Perl is the worst programming language, and you should not write in Perl. Another fun fact, this might be violated with Pearl, which is why Pearl is the worst programming language and you should not write in Pearl. But assuming that, , we're working with natural languages. The main difference is, we want to recover all possible Parses. And that's because natural languages are OK, But assuming that, , we're working with natural languages, the main difference is we want to recover all possible parses, and that's because natural languages are ambiguous. I think I presented some examples of that in the first lecture. but here the for us, the most important  ambiguity is syntactic ambiguity. But here the for us, the most important  ambiguity is syntactic ambiguity. There are usually multiple parses that can correspond to any sentence of a language. There are usually multiple parses that can correspond to any sentence of a language, and they usually correspond to different and they usually correspond to different meanings. meanings. I think his name is Groucho Marx. there are 2 parses here for that sentence. what is the difference in the meaning between implied by these 2 parses? what is the difference in the meaning between implied by these two parses? Can anyone see how they might have different meanings? One, general strategy is, you start with the starting symbol. the S, One general strategy is you start with the starting symbol,  the S and then you search through all of and then you search through all of the possible rewrite rules, and you try to find a way to rewrite the non-terminal symbols, to get finer and finer grained until you get to rules that let you generate the actual words that you see in the sentence you're trying to. the possible rewrite rules and you try to find a way to rewrite the non terminal symbols to get finer and finer grained until you get to rules that let you generate the actual words that you see in the sentence you're trying to. there's an early parser that goes top down. That goes bottom up. there's an early parser that goes top down. We're going to talk about one called CYK that goes bottom up. And it's called cyk. And it's called CYK. and at a high level. And at a high level, here are the steps of the CYK algorithm. It only works with a particular  Cfg. It only works with a particular  CFG. we're because, first, st we're going to convert the Cfg into an appropriate form, such that Cyk will work with it. we're because first we're going to convert the CFG into an appropriate form such that CYK will work with it. Third, we're going to fill in the table in an efficient manner. and finally, we'll read the table to recover all of the possible parses of the sentence. And finally, we'll read the table to recover all of the possible parses of the sentence. pretty standard and formulaic. First, st we're going to convert everything to something called Chomsky normal form. OK,  to make things easier later, first we're going to convert everything to something called Chomsky normal Form. And in Chomsky normal Form, this is a sub. and in chomsky normal form. This is a restriction on the on the Cfg. There's a restriction on the on the CFG such that Such that all of the rules must take a particular format. all of the rules must take a particular format. Either one non terminal symbol rewrites to exactly 2 non one non-terminal symbol rewrites to exactly 2 non-terminal symbols terminal symbols, or one non terminal symbol rewrites to exactly or one non-terminal symbol rewrites to exactly one terminal symbol. 1 terminal symbol. But , it's not because we can deterministically convert any Cfg to be of the this type of Cnf form. this seems  it might be restrictive, but  it's not because we can deterministically convert any CFG to be of the this type of CNF form. ,  how are we gonna do that? there are 3 possible cases of Cfg rules that do not conform to Cnf form  chomsky, normal form. any ,  there are three possible cases of CFG rules that do not conform to CNF form. Chomsky normal form. one possibility of how things might not fit Cnf is that you might have one non terminal rewrite to 3 or more  one possibility of how things might not fit CNF is that you might have one non terminal rewrite to three or more  hand side rules or symbols, either  hand side rules or symbols, either terminal or not terminal. terminal or not terminal. creating a new non-terminal symbol. have two and then maybe it will fit CNF if If it had  a 50  has 49, because the 1st 2 got put into this it had  50  has 49 because the first two got put into this new non terminal. you just take your terminal symbol and you create a new non-terminal specifically for that case. you just take your terminal symbol and you create a new non terminal specifically for that case. 2, rewrites to S,  here S is a terminal symbol. and both of these  fit Cnf. In the 3rd one. But after you do this, then you're allowed to delete a rewrites to B. But after you do this, then you're allowed to delete A rewrites to B.  the idea here is, if you had a tree where you had a rewrites to B, and then B rewrites to something else. I shot the elephant in my pyjamas and you can and you can see that it has some things that don't fit. Cnf. see that it has some things that don't fit CNF. ,  let's convert this grammar fragment into Cnf. OK,  let's convert this grammar fragment into CNF. one important thing to note. one important thing to note, maybe a beef pure maybe it'll be clear if I just delete it. one important thing to note is that you keep the original rules, too. you have 2 copies of the rules of generating  one important thing to note is that you keep the original rules too. I'm gonna put at the end. , that's no, I'm going to put at the end. Maybe it'll be clear VP goes to X1PP. Vp goes to x, 1. PP, ? And then x, 1 And then X1 goes to VNP. goes to Vnp. to PP goes to. PP goes to. I'm gonna use P for preposition. I'm going to use P for preposition. OK, And  we have P goes to N. and  we have key goes to N, , almost there. I deleted the rule that I needed, and I didn't put in the Oh, I, I messed up. I deleted the rule that I needed and I didn't put in the. 2 rewrites that end. It's just syntactic sugar. It's just syntactic sugar. Any questions  far. Any questions  far? we're supposed to. That's all , OK,  let a sentence have ,  let a sentence have n words. and also because And also because time works in flows in one because time works in flows in one particular order. , but this is what it's gonna look . ,  here OK, but this is what it's going to look . , any questions  far about the meaning of the table? OK any questions  far about the meaning of the table? In fact, you can just build an index and do  a reverse lookup to look up all of  the form 1 non terminal rewrites to 1 terminal. In fact, you can just build an index and do  a reverse look up to look up all of the rules that generate those non-terminals. And  the key idea here is to take advantage again of chomsky, normal form in that all rules that produce phrases are of the form, a rewrites to B and C. And  the key idea here is to take advantage again of Chomsky normal form. And not all rules that produce phrases are of the form A rewrites to B&C. I don't see any. N,  here we're going to create this constituent NP goes to dead end and also there's another one which and also there's another one which is missed here, which is that you can create x 2, which is  x, 2 goes to dead end there as . And then this was the end from this cell. We're gonna do it together for practice. And then we're gonna OK,  let's finish the example. I don't see any. I don't see any. far. Question, could we have,  generalized that, based on the fact that the cell to the left and to the bottom were already in. Could we have  generalized that based on the fact that the cell to the left and to the bottom were? We have generalized that to based on the fact that this out of the bottom and the left already empty. Could we have generalized that to based on the fact that this out of the bottom and the left are already empty? you have to be very careful. You cannot take that as a heuristic, because in this particular case it's 0 to one and then one to 3, and then 0 to 2, and then 2 to 3. you have to be very careful. and I'm going to attempt to draw. To draw. ,  one One that end . that end . x 2 goes to dead end. far. OK,   we have how do I insert text how do I insert textbooks? there's Vp goes to Vnp that works. there's VP goes to VNP that works. let's check that first. And I think we can build an S there, s, goes to Npvp. S goes to NPVP. let's put that . let's put that. Npx, one is not a thing, and NPX 1 is not a thing and NVPI don't think Mvp. I don't think is a thing. is a thing and NX 1 is not a thing. and NX, 1 is not a thing. We have to check for rules of the form. Sp on the . Form SP on the  and there's no SP on and there's no Sp on the  hand side of any rule. that's awesome. that's awesome. I don't think . I don't think . I highly doubt any of this is possible, either. I highly doubt any of this is possible either. if there are rules of the form, Vp. if there are rules of the form VP debt or x. Yes,  is there a heuristic rule? There are no heuristics. Is there a heuristic rule ? OK.  we have that end . do we have any rules of the form. There's a PP rule there. No laughter, please. No laughter please. We don't have any. 2 s. We don't have any PX twos. Let's check that Two to four, four to seven, let's check that. Are there any rules of the form. Let's keep going. OK, let's keep going. , let's use blue this time. OK, let's use blue this time. Are there any other Vnp rules? Are there any other VNP rules? ,  there's Bnp. OK,  there's BNP  1-2. , 1, 2, 3, 3 to 7, Three, three to seven? x, 1 PP. Yes, we have a Vp from x 1 PP. Yes, we have AVP from X1PP. I got lost. I got lost. from 0 to 1 to 7, we have Sb rights to Npp. or Nx. the other S The other rule that works is the other rule that works is Npvp, but with the other vp. NPVP, but with the other VP. Are there any rules that are SPPI don't see any. and then see if you find the starting symbol. then you just check the top  cell and you try to find your starting symbol. and you try to find your starting symbol. You have all of the parses that you can recover And then there you go, you have all of the parses that you can recover, ? S. Rewrites to Npvp. Rewrites to I, and then Vp. and with the same with the other S. And with the same with the other S And this And this is how you can recover all of the parses. is how you can recover all of the parses. , every time there's a decision point  every time there's a decision point where you create where you create the same non terminal constituent,  in somewhere in a smaller span. OK,  although there was that joke by Martial Grouse  although there was that joke by Marshall grougs. One of these parses is much more likely than the other. One of these, , I Groucho muffins in practice, I knew I said something wrong. In practice, one of these parses is much more likely than the other. , go ahead. , go ahead. But we're going to try some another strategy, which is that we're going to add probabilities to everything But we're going to try some another strategy, which is that we're going to add probabilities to everything  that  that we can say that's the we can say that the, our goal  is not our goal  is not to just recover all possible parses of a sentence, but maybe to recover the most to, to recover all possible parses of a sentence, but maybe to recover the, the most probable parse of a probable parse of a sentence. Npv writes to Mpp. we are given the sentence sent. we are given the sentence sent and then Tau of sent is the set of possible parse trees and then Tau of sent is the set of possible parse trees for it. T of,  the of the sentences. And then to learn anything else for your midterm. you don't have to learn anything else for your midterm. And that'll be more efficient because then you don't have to , explicitly create all of the ambiguous trees and evaluate the probabilities for all. or equivalently, if you don't  this notation, you can try this notation. Or equivalently, if you don't  this notation, you can try this notation which is you're going to write this which is, you're going to write this as a three-dimensional array. as a three-dimensional array. and you do that by taking the existing probabilities from each of the subconstituents, and multiplying with it the probability you get from your Pcfg. And you do that by taking the existing probabilities from each of the sub constituents and multiplying with. note that it might be possible that there are multiple rules that form the same constituents with the same non-terminal symbol for a span. notes that it might be possible that there are multiple rules that form the same constituents with the same non terminal symbol for a span. You might find multiple breakpoints to form  a Vp or something or an Np. But note that it really matters that it has to be the same span with the same non terminal symbol. If you create But note that it really matters that it has to be the same span with the same non terminal symbol. If you create a different non terminal symbol for the a different non-terminal symbol for the same span. Np rewrites to dead End, . NP rewrites to dead end . you look at the entries here that correspond to S, you just look for the starting symbol. for each sale of the there you go. at the end. if you're interested, you can look them up. This is called a tree bank. if we have this is called a tree bank. A tree bank is a collection of trees A tree bank is a collection of trees in a in a bank, with , a with sentences. bank with, with  a, with sentences. And  that's the most famous Tree Bank. And this is the mle estimate OK. And this is the MLE estimate for that rule, for that rule, for the probability of that rule. Really the reason is because  ESPY writes to ."
    ],
    "Topic 5": [
        "welcome back, everybody. Welcome back, everybody. I know it's been over a week, and  that's an eternity, and we've all forgotten everything already. But  I know it's been over a week and  that's an eternity and we've all forgotten everything already. Do you all remember that all remember that. It's a, it's a model of formal grammar that we can apply to model the syntax of natural languages. It's a it's a model of formal grammar that we can apply to model the syntax of natural languages. And how can I get rid of this thing? And how can I get rid of this thing? I'll just leave it. remember, this is what a Cfg tree  looks . remember, this is what acfg tree  looks . And remember by convention in natural language, we tend to and remember by convention. In natural language we tend to use capital letters for that. use capital letters for that. and we say that a sentence forms some valid sentence of a language. And we say that a sentence forms some valid sentence of a language if you can find this tree according If you can find this tree according to the rules of that Cfg for that sentence. we're using formal languages or I brought up this question of  we're using formal languages to model natural languages. to model natural languages. we're saying that a particular natural language  English. OK.  we're saying that a particular natural language  English we'll model it as a formal language with the Cfg. will model it as a formal language with the CFG, but why are we using context free grammars, . According to this, this view of the world is no. it's only Swiss, German and Babarra. It's not even standard German. , it's just Swiss, German and Babarra And it's two particular languages. it's only Swiss German and Bambara. It's not even standard German, OK, it's just Swiss German and Bambara and all the other languages. The basic idea is that in these languages And the basic property, the basic idea is that in these languages there are certain dependencies that take the there are certain dependencies that take the form of A to the MB, to the NC to the MB to the N, form of A to the MB to the NC to the MBD to the north. And because there's this notion, there's this, you have to remember what the M's and N's are as you're generating and accepting, you can prove using  a pumping lemma, or something that there does not exist a context, free grammar. You have to remember what the Ms. and NS are as you're generating and accepting. You can prove using  a pumping lemma or something that there does not exist a context free grammar that can accept all, and only the strings that are of this form. they rely on some assumptions which you can disagree with. And the other major assumption here is this assumption that's in common with everything we're discussing today, which is that strings are either in a natural language, or they're not in a natural language. And the other major assumption here is this assumption that's in common with everything we're discussing today, which is that strings are either in the natural language or they're not in the natural language. it's a binary decision. it's a binary decision. ,  in practice for all of these cross serial  in practice, for all of these cross serial dependencies, the M's and the n's are never greater than  2 dependencies, the Ms. and the NS are never greater than  two or 3 at most. Particular world view. which is , what  formal models and formal What  formal models and formal mechanisms you might need mechanisms you might need to account for the phenomena you to account for the phenomena you see in natural language see in natural language. And in fact, it's  exactly the same thing. And in fact, it's  exactly the same thing, but in natural language. rules of that grammar. ,  OK,  how do you get rid of this thing? how do you get rid of this thing. the natural languages are ambiguous in many, many different ways across many different levels, I think I presented some examples of that in the 1st lecture. ,  the natural languages are ambiguous in many, many different ways across many different levels. And the sentence is, I shot the elephant in my pajamas. OK, And the sentence is I shot the elephant in my pajamas. Can anyone see how they might have different meetings? in one of them, I'm wearing the pajamas, and in the second one the elephant is wearing the pajamas. , the full joke is, I shot the elephant in my pajamas. in one of them, I'm wearing the pajamas, and in the second one, the elephant is wearing the pajamas. the full joke is I shot the elephant in my pajamas. that would correspond to  that would correspond to the second parse, ? Because it's the elephant in my pajamas. Because it's the elephant in my pajamas. There are 2 general strategies. If you think about it, there are two general strategies. And that's called a top down strategy. and that's called a top-down strategy. using the rules by matching the words 1st to non-terminals and then the non-terminals to  bigger non-terminals. rules by matching the words first to non terminals and then the non terminals to  bigger non terminals. And that's called a bottom up strategy. There's a shift. There's a shift reduce strategy. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. And when you hear these words about avoiding redundant computation, there should be a general And when you hear these words about avoiding redundant computation, there should be a general class of algorithms that this class of algorithms that this that springs to mind. that springs to mind. does anybody  does anybody have any thoughts that have popped into have any thoughts that have popped into their mind. Sure. their mind? Sure. but is this is not  a real problem. It's not a big problem. But is this is not  a real problem, it's not a big problem. 3, rd we're going to fill in the table in an efficient manner. This is a sub. , all of the rules have to be in one of these 2 types, either OK, All of the rules have to be in one of these two types. this seems  it might be restrictive. Here are the rules about that. We're gonna apply in order to do the conversion. Here are the rules about that we're going to apply in order to do the conversion. and  we just need to take all of and  we just need to take all of those rules and apply these procedures to turn it into rules that do fit the Cnf assumptions. those rules and apply these procedures to turn it into rules that do fit the CNF assumptions. to deal with this. to deal with this, we're going to follow this we're going to follow this strategy of strategy of creating a new non terminal symbol. just give it whatever name you'd ,  x 1.  just give it whatever name you'd ,  X1, and then we're going to say that A rewrites the And then we're going to say that a rewrites the x 1 X1. , for example, if originally your rule had,  3 -hand side symbols  for example if originally your rule had  3  hand side symbols, after the rewrite it will only after the rewrite, it will only have 2, and then maybe it will fit Cnf. new non-terminal rule. Rule. and  by iteratively doing this, you can gradually turn all the rules into rules that have exactly 2  hand side symbols. And  by iteratively doing this, you can gradually turn all the rules into rules that have exactly 2  hand side symbols. that's the 1st rewrite strategy. The second rewrite strategy is if you have OK,  that's the first rewrite strategy. The second rewrite strategy is if you have rules where rules where you have a mix of terminals and non-terminals that's really easy to deal with. you have a mix of terminals and non terminals, that's really easy to deal with. here S is a terminal symbol,  just turn it into  a new non-terminal, and then add an extra rule of x 2 SX. just turn it into  a new non terminal and then add an extra rule of X2SX2 rewrites to S and both of these  fit CNF. And finally, the 3rd one is  the trickiest to deal with. And finally the third one is  the trickiest to deal with. One non-terminal rewrites to exactly one non-terminal. In the third one, you have a rule of the type 1 non terminal rewrites to exactly 1 non terminal. And  you have this particular case, where for every rule in the grammar where B is the left hand side. You want to remove that intermediate level,  that  you have a rewrites to the thing directly. the idea here is if you had a tree where you had A rewrites to B and then B rewrites to something else, you want to remove that intermediate level  that  you have AV rights to the thing directly. And this helps you get rid of  these. These are called unary rules, because it involves exactly one  hand side. And this helps you get rid of  these. These are called junior rules because it involves exactly 1  hand side. It has some rules that have 3  hand sides. It has some rules that have three  hand sides, and then it has some rules that mix, say, and then it has some rules that mix, say terminals and non-terminals. and non terminals. Can you guys help me? what's 1 thing I should do first? st , can you guys help me? what's one thing I should do first? You place all NS in the rest of the rules in the rest of the rules with. this one, how it works OK  this one is the hardest 1.  this one, how it works is if you go is if you go back to read the this for every rule in the grammar we're back to read the this, for every rule in the grammar where B is the left hand side, you copy B is the left hand side. rule where N is on the left hand side, we and then we have to. N with NP. And  you can get rid of this rule. OK,  this is how you would do it and  you can get rid of this rule. you have two copies of the rules of generating I and elephant in pajamas, one with N on I and elephant in pajamas, one with N on the left hand side and one with Np. the left hand side and one with NP. To N rule. to N rule. What's another thing? What's another thing? There's the tree in the second rule. , the first rule would be  X1. this one needs to be changed  this one needs to be changed to. , I confuse myself. Cnf, because all rules have either exactly 2  hand side non terminals. Because all rules have either exactly 2  hand side non terminals or exactly 1 terminal. or exactly one terminal. And remember the vertical bar notation. This just means n rewrites to. IN rewrites to elephant and rewrites to pajamas. And remember the vertical bar notation? This just means N rewrites to eye, N rewrites to elephant, and rewrites to pajamas. Yep. I never formally defined what it means to be  an acceptable change to the grammar. I, I never formally defined what it means to be  an acceptable change to the grammar. ,  intuitively, the entry at each cell is the list of non-terminals that can span those words according to the grammar. OK,  intuitively, the entry at each cell is the list of non terminals that can span those words according to the grammar. I shot the elephant in my pajamas, and here we have the updated Cnf grammar from before. I shot the elephant in the pyjamas. the general idea is that we're going to go from small chunks to big chunks. the general idea is that we're going to go from small chunks to big chunks. and that's easy, because you can just check in your Cfg and find out all the rules that And that's easy because you can just check in your CFG and find out all the rules that are of are of the form one non-terminal rewrites to one terminal. the rules that generate those non terminals. And in our grammar it's going to be Np. And in our grammar, it's going to be NP and N for shot it would be V and for the For shot. It would be V, and for the, it would be depth and  forth. you can build a rule with the constituents that you've already found there. for Ishot. here I shot is  a 0 to one ? here I shot is  a 0 to one, ? then, again, you can build an index to this. If you have big grammar, we have a tiny grammar,  we can just check by hand. then again, you can build an index to this if you have big grammar. We have a tiny grammar,  we can just check by hand. we can check are there any rules of the Are there any rules of the form, debt, Np. We can build either an Np there. We can build either an NP there or we can or we can build an X 2 there. one thing which is also  one thing which is also familiar to you from familiar to you from the viterbi algorithm is you should store back pointers. Np. thing. Thing yes, there. then the second, the second constituent you've built here would be x 2, that 2 to 3 and 3 to 4. . It would be first saying you would say X2 and then that 2 for 3:00 and 3:00 to. then the second, the second constituent you've built here would be X2 that 2:00 to 3:00 and 3:00 to 4:00. Maybe what you mean is that this is how we, the algorithm allows us to deal with ambiguity because you can store multiple things. Maybe what you mean is that this is how we, the algorithm allows us to deal with ambiguity because you can store multiple things. That works, too. That works too. You can do any combination of them that you  that you would  You can do any combination of them that you  that you would . that also works. OK, And you need to help me because I am And you need to help me, because I am sure that I will  overlook things. sure that I will  overlook things. Or an Ed ? For shot it would be V. For a shot it would be V for the For V it would be debt. it would be debt for a determiner For a determiner or elephants it would be NP or elephants. It would be Np. for shot, we already checked that ? there's nothing we can build there,  that cell is empty OK,  for shot, we already checked that ? that cell is empty for  this is for   this is, for I shot . I shot, . from zero to two,  we have 1:00 to  we have one to 3,  this corresponds to shot B, 3:00.  this corresponds to shot B.  do we see any V debt in our grammar. I shot B.  here we have I shot B.  we have 2 possible breakpoints. we can have I and then shot B, or we can have I shot, and then B we can have I shot and then B, ? I and then Shati would correspond to 0 to one, and then one to 3,  I and then shot D would correspond to 0 to one and then 1:00 to 3:00.  we can't build anything there, because there's nothing that spans one to 3,  we can't build anything there because there's nothing that spans one to three. and then for I shot, and then B, that would correspond to 0 to 2, and then 2 to 3. And then for I shot and then B that would correspond to zero to 2 and then two to three. It's not always going to be the cells   to you that matter. It's not always going to be the cells   to you that matter. OK,   we are at 2:00 to 4:00.  there's only one possible breakpoint. , we have shot the elephants. OK.  we have shot the elephants. We can go, one to 2, and then 2 to 4, We can go one to two and then 2:00 to 4:00 or 1:00 to 3:00 and then three to four, or one to 3, and then 3 to 4, ? Do we see any rules? with one to two and then two to four, do we see any rules? it looks  it's we can build a Vp. OK,  it looks  it's we can build AVP also x, 1. also X1, also X. They're they're just They're, they're just there to help you not get tripped there to help you not get tripped up. OK,  how about 123 to 3:00 to 4:00? We already know that's impossible because 123 is empty. OK.  we're done with zero to 1 and 1:00 to 4:00.  we have to check zero to two and two to four. we have 3 to 5.   we have 3 to 5 S elephant in it seems  it's not a constituent, but we have  elephant in it seems  it's not a constituent, but we have to check formally. Is there any rule for Np, and then P. to check formally. We have to check is there any rule for NP and then P? Is there any rule for n, and then P. No. Is there any rule for N and then P? the cell is empty 2:00 to 5:00. we have to check 2:00 to 3:00 and then three to five, which is not possible because three to because 3 to 5 is empty. Is there any rule of Np. And then P  2:00 to 4:00 and then four to five. Is there any rule of NP and then P? Np. There is NP. oh, Np goes to Pnp. in the wrong order because here we have ANP and then P, not P and then NP. in Cfg is the order, matter. Vpp, is there any rule where that's the  hand side? VPP, is there any rule where that's the  hand side? I'm going fast. if I'm going too fast, just . I'm going fast. if I'm going too fast, just  raise your raise your hand to get me to explain some more. hand to get me to explain some more. But the basic idea is, if a cell is empty, then But the basic idea is if a cell's empty, then that's not then that's not possible to build something there. the  hand side of any rules. this cell is empty,  we can't go 3:00 to 4:00 and then four to six this cell is empty,  we can't go 3:00 to 5:00 and then 5:00 to 6:00. , OK, two to six is I don't think anything is 2 to 6 is, I don't think anything is possible there, either. The last thing you said we should check one to 4, then 4 to 6. The last thing you said, we should check 1:00 to 4:00, then 4:00 to 6:00. It's  any, anything that ends with my , it's unlikely you'll find a constituent if your grammar is reasonable. No, the rule is just. is there a little bit heuristic role based on the empty slots around us? ,  OK,  five to six, six to seven, seven. we have the same rules that apply. Rules that apply. OK.  here we have 4:00 to 7:00.  the possible. P. And P. We do  do we have any rules of the form PNP? there's APP rule there. And then four to six, six to seven is not possible, , Because four to six is empty. OK,   we have the cell three to seven. do we have any rule of the form. Do we have any rule of the form NPPP? Two to seven? we have two to three, three to seven, that's not possible. Are there any rules of the form NPPP? 2 to 5, 5 to 7,  2 to 5, five to seven, not possible. Two to six, six to seven, not possible. ,  shot the elephant in my pajamas. OK,  shot the elephant in my pajamas. Four to seven? Do we have any rule? we only have one cell left  one to five, five to seven is not possible. One to six, six to seven is not possible. from zero to one, one to seven, we have S rewrites to NPVP. ,  that's 0 1, 1, 2, 7, OK.  that's zero to one, one to seven and then and then 0 to 2, 2 to 7 is not possible. zero to two, two to seven is not possible. Zero to three, three to seven is not possible. Zero to four, four to seven. 0 to 6, Zero to 5, five to seven, not possible. Zero to six, six to seven also not possible. in the other notation. Just keep a list. OK,  in the other notation, just keep a list. in the other notation, you keep a list of you built a Vp OK,  in the other notation, you keep a list of you built AVP from 1:00 to 2:00 and from one to 2 and then 2 to 7, using the V and the Mps there. then two to seven using the V and the NPS there. and then you just add another entry to the list. And then you just add another entry to the list. the X1 there and then four to seven with the PP there. you'd index into the cell. in the back pointers, you just keep a list of everything you build, and you make sure you store the  index of how you created that. you index into the cell the second NP. in the back pointers you just keep a list of everything you build and you make sure you store the  index of how you created that. S says, there, ? With 2 different rules. Rules, . of the possible paths. Np. Rewrites to whatever,  on and  forth. S rewrites to NPVPNP rewrites to I, and then VP rewrites to whatever,  on and  forth. Because if you if suppose that we had found different ways to build the Vp Because if you if suppose that we had found different ways to build the VP in  a in  in  a in,  one of the sub cells, and you do it multiple. 1 of the sub cells and you do it multiple and then that's used in a parse somewhere else. All this tells you is that I shot the elephant is also a sentence in this grammar. All this tells you is that. I shot the elephant is also a sentence in this grammar. in practice. In practice. one of these  groucho bookings in practice I knew I said, something wrong in practice. No idea  you have to. No idea. the depth, maybe the depth. That's a good idea. The depth, that's a good idea, . And  what we're going to do is we're going to associate each rule with a probability. And  what we're going to do is we're going to associate each rule with a probability. I made this roll up. NPV writes to NPPPI made this roll up, maybe it has a 0.2 probability and  on and  forth. and then the probability of a parse tree is  going to be the product of the probabilities of all of the rules in that parse tree. And then the probability of a parse tree is  going to be the product of the probabilities of all of the rules in that parse tree. And  And  the simplest way to do this is to the simplest way to do this say that each non terminal symbol that rewrites to something is to say that each non-terminal symbol that rewrites to something else that forms are probability distribution. ,  when you have a non-terminal, say A in your non-terminal set. OK.  when you have a non terminal, say A in your non terminal set, you create categorical probability distribution you create categorical probability, distribution for all the rules that have that same non terminal for all the rules that have that same non-terminal symbol a on the left hand side. symbol A on the left hand side. rules. Np. Rewrites to anything else. On the left hand side must sum up to one. NP rewrites to anything else. and then, formally speaking, the probability of a tree is going to be the product of all of the rules that make up that tree. And then formally speaking, the probability of a tree is going to be the product of all of the rules that make up that tree. and evaluate the probabilities for all of them. OK,  that's the idea. our table with the list of entries for all of the ways we could build things, you include the best you include the best probability of generating the constituent with that particular non-terminal. probability of generating the constituent with that particular non terminal, . ,  the table OK,  the table at for the span two to four with a particular non terminal NP as probability of for the span 2 to 4, with a particular non-terminal Np whatever that we've created by using some substructures, some Subs as probability of whatever that we've created by using some substructure, some sub constituents from 2 to 3, and then 3 to 4. constituents from 2:00 to 3:00 and then three to four. it's just the same thing. previously in the recursive step of the algorithm, you just have to check was that combination of  hand was that combination of  hand side symbols? For that additional new rule that you're adding to that subtree. It's the probability you get from your PCFG for that additional new rule that you're adding to that subtree. In particular, there might be multiple ways. cable IJA in particular, there might be multiple ways you might find multiple break points to form  AVP or something or an MP but for the same triple But for the same triple of the span, left hand side and  hand side and non-terminal symbol. because all of these things factor out  locally there are no long range dependencies between  the top of the tree and the bottom of the tree. suppose that the that writes to the probability is 0 point 6, and the N. Writes to elephants, probability is, say, point 2 5.  suppose that the debt rights to the probability is .6 and the N rights to elephants probability is 8.25. If you create an NP constituent here, then you have then, you have to also figure out the new probability of the elephant, which will be 0 point 6 times 0 point 2 5 times the probability of the rule. to also figure out the new probability of the elephant, which will be .6 * .25 times the probability of the rule. And then, finally, once you've parsed the whole And then finally, once you've parsed the whole sentence, then sentence, then you just look for the starting symbol. Are you still computing, partner. this rule. Rule. we're throwing that out. just to rephrase, if you have two possible ways if you have 2 possible ways or multiple ways to create the same constituent or multiple ways to create the same constituent with the with the same non-terminal symbol that spans same non terminal symbol that spans a particular span, then a particular span. But  in practice, that is But  in practice, that is very rare in  very rare in  a real Pcfg situation. a real PCFG situation, OK. ? And , , since in practice is  rare you might decide to just And , . And since in practice is  rare, you might decide to just do some approximation and you only store  do some approximation, and you only store  one of them arbitrary. one of them arbitrary. At that point you no longer need to care about the probabilities of each of the subconstituents, because the back pointers already took that into account when you were creating. At that point you no longer need to care about the probabilities of each of the sub constituents because the back pointers already took that into account when you were creating them. But I didn't talk about where the probabilities come from. And  that's the most famous Treebank. And Ngram models, and  forth. it's the exact same ideas as you've already looked at. it's the exact same ideas as you've already looked at. , for example, here are the maximum likelihood estimates  for example, here are the maximum likelihood estimates for the probability of the probability of a rule of alpha rewrites to beta, rule of Alpha rewrites to beta, which is you count the number of times that you see the rule. Alpha rewrites to Beta. which is you count the number of times that you see the rule alpha rewrites to beta and you divide and you divide that by the total number of times that you see the non-terminal symbol. Alpha. the probability of that rule. I don't think I have time to explain the solution. as we writes to it ,  suppose you have something  a  suppose you have something  a sentence, or maybe even better would be  a verb phrase. a sentence, or maybe even better, would be  a verb phrase. There are many different ways. The probabilities are not all independent of each other. They might be, they might  the probabilities are not all independent of each other. And there are many other situations where this the and there are many other situations where the vanilla Pcfg assumption, where you just have a rule, locally of left hand side rewrites to -hand side is too restrictive. vanilla PCFG assumption where you just have a rule locally of left hand side rewrites to  hand side is too restrictive."
    ]
}