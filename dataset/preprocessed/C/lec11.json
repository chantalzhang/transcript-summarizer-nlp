{
    "Topic 1": [
        ", we're talking about universities that you're you're pumped and that you're open. and I'll try to play around that question. is it doing the 11.th , when I checked and my courses was the top. And I remember giving a lecture on that a long time ago in September or something. And then with David, you should have talked about text as sequences and looked at sequence, labeling problems. for the  part of the course, we're going to look at hierarchical structures. And first, st we're going to look at syntax. We're going to look at some particular characteristics or properties of English syntax themselves. And then we're also gonna look at a formal system for describing structure called context-free grammars. And if,  many of them, you're reading and following a lot of machine learning blogs and  forth, you might be under the assumption that language is a sequence of tokens. while language could be described as a sequence of tokens. we might think that language is a sequence of tokens. And that means that there  are a whole bunch of other phenomena where the most natural way to think about them and characterize them and describe them and model them, is not to just think about them as a sequence of tokens. 1st of all, language is not just a linear sequence of tokens. You never notice how the pronunciations of words bleed into each other themselves. , , you don't pronounce r's at the end of syllables , instead of saying car, you might say car or  there, you would say that. not everyone from there has it. You tend to talk about utterances rather than sentences. , and that seems  it. It cuts across multiple tokens in the sequence. But for today's lecture, the most salient phenomenon here is that we can think about and analyze the internal parts of sentences and find patterns that we can better explain through positing hierarchical structures. and also to come up with a formal computational model of this structure. One key concern is what makes up a valid sentence of a language. and this is a notion called grammaticality. This means this arrangement of words don't form a sentence of that language  one goal that we could have with syntax, and by looking at internal structure. I will say, though, that there's another usage of grammar which is, which is not the one that I'm gonna be concerned with in this class. And it's it can be even captured systematically by books. No, which is good because this is not the type of grammar we're going to talk about. And that's simply not true. I said earlier this lecture, that the main goal I have for this lecture is to convince you that there exists hierarchical structures in language. Somehow they form some unit, and then there are properties that you associate with a unit rather than with each of the words that make up that unit? And   I'll go through a bunch of tests to check whether something a group of words is a constituent or not. And and it can obscure things. But  you should try multiple tests. But I'm gonna give you some tests. this is your syntactic environment. that's the 1st test . , here's the second test. Constituents can be placed in different positions or re replaced in the sentence as a unit. you can move these around. because it's singular 3rd person masculine. And I already said this orally. But  the type of constituent that you have is going to be called its syntactic category. There's not really a universal list of syntactic categories, because. Is there  a definition, or is it defined through the tests? for us, a particular syntatic category will be defined. these tests seem to be qualitative and also are there other tests for other types of constituents? yes, that these tests are qualitative and also yes, these there you have to define different tests for other syntactic types, syntactic categories. There are also replacements for verbs. There might be also tests in involving a phenomenon called elision, where you . remove things and you skip over them. here are some that are as very widely known, and  you've heard of them, and they're related to verbs in particular. you can talk about what are called arguments of verbs,  verbs can have subjects, and they can have objects, and they can have direct objects and indirect objects. note here that subjects an object here. We're defining them purely, syntactically. The  most common one is direct object. The ball is the direct object. and sometimes you also get sentences where you have multiple objects, and one of them is the indirect object. she gave him a good beating him. Here is the indirect object. verbs and usually predicates in general. Your shoulders to do this exercise properly, or something  that. And there are some regular processes that exist in languages that can systematically change the categorization. ,  there, there's both a subject and a direct object. I should have explained the number. that's why they're they're called arguments in both cases. , in natural language, analogously, you can have, you can think of , there are things  this as  where you have predicates which are  the verbs or other things, but mostly verb, for , and they take on arguments, and then those arguments are. and you can call them subject and direct object positions and  forth. And in fact, we'll make this analogy quite explicit in a model later on in the course. , there are many other subcategorization possibilities  with the verb wants. And then this is called an infinite table clause, because it's to learn. and,  before, it's not only verbs, but other things can do this  difference, depending on your dialect. I prefer from or to. , that's what this is all about. 1st of all, what is a preposition? Can somebody identify what the preposition is in this sentence? Go back to those tests and think about it . No, you cannot do that . we have pretty strong evidence that this is a constituent of some kind. And , we're going to talk about the formal computational model that we're going to use to describe these hierarchical structures. And 1st of all,  I should say, what makes something formal. ,  there's a whole field of research in formal language, theory, formal grammars. And  some of you are taking that course or another course in formal grammars and formal language theory. You can be working with arbitrary symbols. For  I don't know if I said that before. But we're gonna , go with it. For  to say that we're gonna pretend that we can model English or any natural language as sets of a set of string. , a formal understanding helps us to develop appropriate algorithms for handling and dealing with hierarchical structures and syntax plus? Fsa's our finance data are particular ways to talk about and describe regular grammar. and fsas correspond to a class of formal grammars called regular grammars. it's more natural and useful to use a more powerful class of formal grammars. And these are in particular context, free grammars. ,  that's what we're gonna talk about . Somebody is really upset that I'm talking about context-free grammars. To keep rewriting until you end up with a sequence of words. Yes, what makes what about this makes them context free? What about this makes them context-free? you should just try to ignore context, free the meaning of context, free. here's a,  this is . formally speaking, this is what makes it a context, free grammar. There's 1 called context Sensitive grammar. The formal definition is what we should go by. , and this is the formal definition of a context-free grammar. or from  on I'm gonna start saying Cfg, to be shorter. But that's a that's an aside. All of the constituent, the syntactic categories. ,  what was it we wanted? And , I'm going to put them in a 3rd person just to make things simpler. these become 3 separate symbols in our system. and we can do that. if you had this, this deals, you would not be able to find a derivation  this, where at the top. 1st of all, is under generation  it misses a whole bunch of valid English sentences. , there's a whole bunch of things to do with  nouns, and that and it does that. , then we can have overgeneration. at least not in  the standardized formal version. You can say I was a grammar engineer. I'm also going to make the Np structure more interesting. how will we do that? if you're thinking about this formally in the from the Cfg perspective, yes, you're allowed to do it by my definition of Cfg, because here this is 0 or more , you can have a non-terminal rewriting into an empty SIM  into nothing from the perspective of linguistics. , I'm just talking about  Cfg and what they do and how you use them to describe syntax. Turns out to be  everything there are complicated answers. There can only be one. I know that, or he knows that, or she knows that or something, somebody knows that ? , we have a limited amount of  brainpower and memory, and  forth. And and this model suggests that you can have sentences that are arbitrarily long, infinitely long potentially. They they make vocalizations, and you can teach them to sign. you can in principle have sentences that are very, very long. And suppose we also want to model , I, . we have both 1st person and 3rd person. we have versions with S and versions without S. Oh, oops! Oh, no, these are all. , this is going to be . But I, , would not. And there are 2 different kinds of noun phrases, the ones that are 3rd person singular, and the ones that are not 3rd person singular. And then all of the not 3rd person. Singular nouns would be  balls. I'm gonna say, 3 s. Rather than 3rd person's. 3 s. What do you prefer? What  by S, here is 3rd person singular. the  nouns that are not 3rd person singular will be handled by this rule. nouns, and that our 3rd person singular would be handled by this rule. And then you have to ensure that the 3rd person, singular nouns agree with the 3rd person. But but yes, hopefully, I didn't make any mistakes. How do we incorporate that into the grammar? Usually it means that you have to split up a category into multiple categories. And then you have to think about where that information needs to go, because if it if it affects something that's very far away in the syntax tree. A noun can be 1st or second or 3rd person. And that's really cool because it's a computational model. far, then, we've talked about. And then here is one formal computational model to describe said structures. and we have the student studied for the oh, I should go back to full screen mode. Grammars are just a different view of the hierarchical structure of language. But it shares  this basic property and assumption that it that it's hierarchical. you can think about . that's 1 aspect that helps, . Again, there's a lot of complexities and different theories that do different things. How do you tell that something is a direct object. The only way to do that is to describe some here, at this portion of the tree you might find the subject as an Np. if the dependency edges don't cross each other. You start from the leaf, nodes, and you see what is associated with it. ,  that you've merged those nodes, then you can check. you don't get exactly the same thing as  the  constituent grammars that we talked about before. But at least there's some mechanism to convert a dependency tree structurally into something that which is a constituent tree. And it turns out you can do things the other way around, too. Does anybody speak, Farsi, and they're really into Persian poetry. you can have dependencies, graphs where the edges cross each other . These are both sentences of German. And  in the 1st version there's no crossing dependencies. And in the second version, there's a bunch of crossing dependencies,  that he has me tried to reach  has a crossing dependency because it's reached me. we'll end there for . time we're going to talk about  our natural languages, context-free grammars."
    ],
    "Topic 2": [
        "want to switch about this one second. reminder that  week is Thanksgiving, plus the fall reading break. please be sure to get on top of that. Also, we're gonna be releasing the final project description hopefully by the end of the week. from my understanding of what we've done  far in the course we talked about text classification where we treated passages as samples, ? And then, most recently, Lstms, ? And crfs,  crfs and Lcms,  that's great. if,  many students in the course you were, you got interested in Nlp because of all the recent excitement with e-learning. And my goal in this lecture is to give you evidence to disabuse you of that notion. how many people here speak a dialect of English? one way in which the pronunciations of words affect each other adjacently, and those dialects of English, , would be that in most situations they don't pronounce the r at the ends of words and syllables, but they would pronounce it if the following word starts with a vowel. A a common example of this is, if you have , if you want to say, I have an idea of what is happening in those dialects, you might say an idea of what's happening. , just the articles that  the. A  a masculine, singular version of a How do you say it? But you think there is, because it's spelled UN. But it's really a nasalized valve. However, again, if the following syllable starts with a vowel. and the vowel is no longer nasalized. Whereas if it's something else that's masculine, singular. Then you don't pronounce the n. but the vowels neither . that's another example of pronunciations bleeding across words also intonation patterns. If it's a question, , there are particular intonation patterns for questions in English, and it depends on your dialect. That's not at the level of individual sequences of tokens. And  you don't always have to go through linear sequences of tokens, as you might with the Lstms. And you can do this recursively  you can start off with  the smallest possible units. There are some strings that are accepted by the language, say English, and they're part of the set of valid English sentences. And then there are other strings which are not accepted, and  they don't form valid English sentences. this is a valid sentence is a valid sentence, a sentence, this valid is, is not a valid sentence of English, and  you can say that the 1st sentence is grammatical, and the second one is ungrammatical. is to specify an all and exactly those sentences of a language which are grammatical. Panini, which I'm not pronouncing  . And he wrote this whole grammar of it, and you can translate it. People are just as smart as today. And if you don't write in some particular way you get yelled at. , there's this famous book on grammar called the Elements of Style by Strunk and white. these are style guides , and they're prescriptive. The informal version is not even necessarily simpler or worse, or lazier, or anything  that. there's no relation at all between the level of formality versus the complexity of a system. There are many different kinds of constituents. then the  question is , How is it that what does it mean for these words to form a unit? I saw and then blank. , you can say I saw it. That's that's a grammatical sentence of English. You can say I saw 3 people on the bus. You can't say I saw Ondi. that , tests for different constituents. That is translated by Namda solution process. as you might expect, linguists don't agree, and there are different formalisms and different languages might  have different syntac categories that seem to make sense for them. Another question supposed to be applied. ,  it can get very complicated. far, we've covered syntactic categories and constituents. , we can talk about the relationships between the different constituents, because the it's not just that they there exist constituents in a sentence. And  we can talk about grammatical relations between constituents. But in general there are many more grammatical relations than this. or the wallet was stolen by a thief. Here the verb is, was, or was stolen. and then the subject of it is the wallet. ,  structurally, the subject is still the wallet. subject is the most common one, the , most at least in English. The boy, keep the ball here. and,  before, there are many other grammatical relations. but, ,  adjectives, can also require certain arguments, and you can talk about the relations between a determiner  the versus its noun,   the ball or something. there are many other relations. they tend to impose some  constraints on other syntactic elements that must appear to form a valid sentence. and that's called subcategorization,  subcategorizing verbs into different kinds of verbs, depending on what constraints they add to the other elements in the sentence. heard of terms  intransitive verb or transitive verb or ditransitive verb. if you've heard of those terms, those are  different subcategorizations of verbs. , different subcategories of verbs. , an intransitive verb is one that only requires a subject. relax is an example of that. relax also happens to be a transitive verb. ,  that in that case it would be a transitive verb that takes on both the subject and a direct object. There are 3 things there  often those require 3 elements, and then they're called ditransitive verbs. , , in English, there's this passive structure which changes a transitive verb into one where it only requires the subject, and it changes the mapping between the syntactic and the semantic roles of the arguments of that. Other languages have different role, different operations and things  that as . When you pacifize it, it becomes the wallet was stolen. And then the other the prepositional phrase, you can add it on, but it doesn't count as an argument. and then the list here is the relation to  what else it requires,  the list of argument type types. a function that is , multiply XY, and it takes in 2 numbers x and y, and it multiplies them together. I want to learn about computational linguistics. I don't think that sounds weird to me. I'm not sure which one I prefer. I'm not sure exactly, and  in your dialect it's a different preposition. prepositions are these words that help indicate relations between things in a sentence, and they appear before a  phase. But in English, it's overwhelmingly propositions. and compare that against our alternatives. Because remember, we're computational linguists. We're not just linguists, we're computational. , who has heard of automata theory? where we assume that our goal is again to characterize exactly the set of strings that form valid sentences of English. ,  there are other formal grammars  heard of and encountered, or use finest state machines  finest state automata and regular grammars. here, then, formal grammars, you can also talk about them as  possible sets of strings that you could describe with that formalism. I'm going to start by giving you  the form of it, because I'm guessing that  when you're thinking about describing programming languages,  seen something similar before. But , they talk about what? they talk about our rewrite rules. Oh, we might have a sentence which we'll denote with some symbol  S. And a sentence should be rewritten into a noun phrase, plus a verb phrase. We write to something  a verb, and there are some options for that as . It means, or ,  a verb can be  to is a verb. Can we write to rocks? this is already a very simple Cfg,  this is, I'm starting off with an example. It's a very simple grammar. This rules which is accepted by this grammar. You just use a different rule there. then, you have non-terminals, which by convention are written with capital letters. You can be , I'm allowed to rewrite an S. To an Mpp. Can be rewritten to something in the context of something else. and the star here means you can have 0 or more of them. technically, you can rewrite to an empty symbol. we're just pretend it's  one or more, but  it's 0 more also works. And also, if you're allowed empty things, then it causes parsing to be much harder. That are larger than single words. 1st of all, everything requires a subject. the thing we need to change is we need to change what can be a verb phrase. this means we need to change this rule from Vp rewrites to V, to Vp. We need to come with subcategories of the verbs, because there are going to be some verbs that are that take one that are intransitive,  they only take the subject some verbs that take 2 arguments and some verbs that take 3 arguments. And  we can change the subcategory,  that not all of them are simply these. For  any questions  far about the example. Or is this all great question? It describes the possible sentences that can be accepted. this describes all of the possible sentences that could be accepted. I'm going to attempt to use the drawing mechanism. Don't want it to be red. , we can draw lines to indicate that they correspond to rules in the grammar  . But clearly that grammar is not the grammar of English, because we say other things too. you might say that relax and relaxes they should be. They're they're both intransitive verbs, ? However, there's something annoying called subject-verb agreement. you're not allowed to say this relax in English. This relax, even though it should not be accepted. it becomes an engineering project, ? it becomes a grammar engineering project. I'm going to say that a noun phrase can also rewrite into a determiner plus a noun and a determiner can be something  V, or  a and a noun can be something  a foul or boy or girl, or something else. , , or wallet, . we can do softly or quickly or slowly. and it seems that the most obvious way is to put it in with the verb phrase. this grammar will accept a sentence  the girl kicks the balls softly or quickly, or whatever . question can you have  an empty element inside? From the good question, can you have an empty elements inside. , in the  linguistics you're likely to encounter in linguistics courses here. They  will teach you that there are empty elements, and they give evidence for that. The the empty elements are  to do, parsing properly with empty elements, becomes  intractable because you have to posit all possible combinations of up to infinite number of empty elements everywhere in your sentence. in practice you  have to limit yourself to . At most, I will have,  this, many empty elements in a row, or something  that. In fact, this is just a shorthand. This is  6 different rules. It's these are, think of these as 6 different rules. and assuming that you have no empty elements, and,  you, have actual words there. There might be a fixed set and fixed inventory of symbols and signs that can be produced, but they don't have the same properties that,  Of recursion, where you can have, . Relax the boy or the boy relax the boy, kick the ball. We can have the ball the boy. , split them into different categories  that they absolutely great. ,  we have to split them into different categories and rewrite the rules to make it all fit. We'll have B 2 with an S. And v, 2 without an XS. and we'll have 3 v. 3 with an s. and then v. 3 without an s. And that means we have to duplicate all of these rules. Versions with S versions without. And not only that we also have to pass that information up to the level of the Vp. if you think about it, the agreement works between the subject and the and the verb , and the subject is  all the way over here on the left. and then the verb is inside the Vp, which means that we have to pass that information along somehow. ,  that means that we also have to indicate that this is a verb phrase, where there's an S on the verb. And that means,  we need 2 versions of this rule. We need one rule, which is  Mps Vps, and one role, which is Mpvp. we're going to have 2 types of nps, those kinds of envy Nps where you they have to take an S on the verb, and the  Nps where they have to not take an S on the verb, ? all the 3rd person singular nouns would be  a ball boy girl wallet. then the mechanism we have for making sure that the other part of the sentence knows about this is to propagate that information up and down through the syntax tree and through the rules described by the syntax tree. and we want to be able to pass that feature information up and down the tree. then, although we don't talk about it much in this course, people have developed theories of grammar that incorporate that. And you can specify things much more plainly and elegantly than we're doing here. And they should take she this they should teach this in linguistics, but I don't think they do. It's just a different view of the syntactic structures within a sentence  grammatical relations, they induce a dependency relation between the words that are involved. some of it is through which elements specify the properties for the whole phrase, the whole constituent. It's the student that lets  that the entire phrase of the student is singular. What's the what's the head word more generally, it's the fact that the student is a noun that causes the student as a whole to be a noun phrase. It's not V that causes the whole thing to be a determiner phrase, although there are theories that do that. But but ,  for our purposes, it's the student that causes the student to be a noun phrase. And there's a preposition, prepositional phrase, argument, relation with this park. , you're solving some information extraction task. The way to do the conversion is to check. And , dependencies can cross in English. They cross much more frequently. He tried to reach me. It's  he has me tried to reach, or he has tried mean to reach. and you can draw the arrows using how the syntax of German works. not tried me, but reach me. you will get discontinuous constituents. And then we're going to talk about parsing algorithms."
    ],
    "Topic 3": [
        ", we'll get started in a few minutes we should leave time for people to get here, because the main entrance was blocked. And in the meantime, if you have lighting preferences, let me know. , I'm gonna stick with this setting unless someone tells me they prefer something else. it's just some announcements and reminders. , no classes but that the reading assignment is due later this week. please watch out for that. , my slides could be wrong. I'm gonna summarize where  we're at in the course, and then you can  I'm if I'm off base. And smoothing hidden Markov models. , and Texas seems to naturally come as a sequence of discrete tokens ? , chances are quite high. You speak that a version of English  that. ,  and sometimes there's even a phenomenon called intrusive R, where there originally wasn't an r in the word, but then you add the extra r in to smooth that out. you might notice this if you if you   time you, you chat with someone from the Uk, you can try to pay attention to the speech and see if they have . Or another really obvious example is in French. how many people here speak French? French pronunciation is  famous for having these effects. And it's a really difficult thing for learners of French to grasp. if you have,  a in French, . one simplification and formalization here is, you can assume that for every single string of tokens. It's not even an idea from last millennium. Ago some of the earliest people to have thought about this that we are still aware of are grammarians from, say, from South Asia. developed a grammar for Sanskrit, which is a classical language of South Asia. and it's very interesting, because even back then. And and they come up with, and they produce descriptions of the languages that they speak . There's some option where you can add certain things, but other things are not allowed, and there's a prohibition, and they talk about affixes, say, and then they always receive some augment. And also this book is just bad because it contains lots of linguistically inaccurate descriptions. I that here's another common misconception, which is that somehow things that are non standard or informal or casual somehow have no grammar. It's quite possible that the informal or casual version of a language has a different grammar compared to the formal version, the written version and the prestigious version. have heard of some of these terms,  a noun phrase and an adjective phrase. , a noun phrase, something  computational linguistics is a noun phrase, the word the single word. It is a noun phrase. Justin Trudeau is a noun phrase. 3 people on the bus is a noun phrase. These are all noun phrases. and you don't just have noun phrases. You can have adjective phrases  very good, is an adjective phrase. ridiculously annoying and tame, is an adjective phrase. and the more tests that a group of word pass, the more likely it is that it's  a synaptic constituent. Suppose you have a context. You can say I saw Jean-claude Van Dam. this is also a noun phrase. That's also a noun phrase. You cannot say I saw a van from Jean. You can't just say I saw a van. is this a test for a noun phrase constituent. technically, this is a test for a noun phrase constituents. Suppose you have Jean-claude Van Dam. The muscles from Brussels meet me up. You can also say it was Jean-claude Van Dam, the muscles from Brussels who beat me up  it's the same group of words, and you can put it in different environments. You can say I was beaten up by Jean-claude, madame, the muscles from Brussels. But but the type of constituent stays the same. , it's just  as an example of something that fails. If someone asks who beat you up, you cannot answer the muscles from that's not a constituent. all of these examples are with working with noun phrases. But you can have verb phrases,  phrases, prepositional phrases, clauses, sentences, and  on and  forth. Question when you define a constituent  noun phrases, is there a definition, or is it defined by its tests? When you define a constituent such as a noun phrase? I will say it's useful to have it if it forms some coherent group where all of these things that we call say that they belong to the same category, they all behave distributionally. ,  Jean-claude Van Damme relaxed. Here the verb is relaxed, and the subject is this noun phrase constituent of Jean-claude Van Damme. Oh, it was the thief who stole the wallet,  shouldn't the thief be the subject? No, here the thief is part of a prepositional phrase. These are just some of the common ones related to verbs that  heard about before. and here, from the examples, before steel is also transitive, and kink is also transitive. But and then something that takes  2 objects would be something , give usually verbs that involve the change or transfer of something. I didn't understand for steel, . for steel it would be something , . and technically by a thief. the in terms of what's required by the verb, it's just the subject. Here is just the number of arguments that it takes. there's an analogy to be drawn there. You can have something  a prize which takes a subject, an object, and also a prepositional object. The minister apprised him of the new developments. and  the minister is the subject. Him is, , an object, and then prepositional phrase, object of, and it has to be with of you can't say the minister apprised him in something, and you can't simply say the minister apprised him. But but what I expected. a subcategorization is a slightly different in that. Identify the prepositional phrase in the following sentence, and give arguments for why? we can do this together because we're running behind time. Why, it's just on, not a constituent. The whole thing is a prepositional phrases. But it's a it's a different constituent. And you cannot answer just on , when is the  assignment due on. You can be working with,  the language of arithmetic. That's a correct arithmetic expression. there's an exact correspondence between them. And they are  used in Nlp as . they're very practical because they tend to be much faster to , compile and run and process , but  just to use the same terminology again. They can be used for tasks such as stemming and lemmatization and morphological analysis stuff that we talked about at the beginning of the course. And before we continue, I'm gonna close the door. It's made up of a noun phrase plus a verb phrase. and then  a noun phrase. here a vertical bar is just a shorthand. Rewrites to this, and then Vp rewrites to V, and then V rewrites to some of one of these sperm. for B rewrites to rocks. In the context of something else or Np. a Cfg is a four-tuple. because we  have 3 options. Rewrites to V, or Vpv. this is a noun phrase, ? This is also an Np. and then B 2 Np. And you can check that. But it's not accepted currently by the grammar. I used a different, a more complex example when I was thinking about this. This is not a very interesting example. suppose you add, , relax here. ,   time someone asks you, what did you do in class? to make things slightly more interesting. And then parse with respect to that. But , that's a really interesting question. yes, in our example we would have deleted the other Vp mapping ? It's just one mapping, that's all extended. What do you mean by mapping? It's the wrong word, but  Vp rights to 2 different things. But it's  just one big rule. ,  this is , vp, rights to v, 1 Vp rewrites to v, 2 nt, Vp rights to that. these are 6 different rules just for the sake of saving space and using the vertical bar, that's all. But literally, it's just a safe space. And  the question also asks for prepositional phrases. It's the same idea you have to say, what is a prepositional phrase, come up with the internal structure of a prepositional phrase, and then integrate that into your grammar. , here's an interesting phenomenon. and then the Np is  the subject,  it can be  any noun phrase. The Vp is the verb phrase, and then it gives you . But the really interesting thing here is that you can have it. And  this is an interesting property, and which is suggested by Cfgs, which is that in natural language there seems to be no fixed upper bound in terms of the length of a sentence. And some people think this is a big deal. Have you heard,  bees, dance in a particular way to signal to their colony about the location of  food or shelter and stuff. And they have properties and  whales, clicks. then the verb also has an affix of s or es, depending on the verb. otherwise is just the base form of the verb. That's currently accepted by this grammar. and then through the Vp branch you can go to Vp, v. 2 np. and I  realize that I hate my naming terminology. And for I,  you can have debt n and also I. Do you see why I hate my terminology? , this is just a noun. and, as you can tell, this becomes very hairy very quickly. we have a feature, which is that a noun can be singular or plural. There's an alternative which is also very popular in computational linguistics in Nlp called dependency grammar. and it's a 4 that causes this whole thing to be a prepositional phrase. you can directly tell here that there's a subject relation with this arc. To the left of some Vp or something, and that's quite complex. Also, the other thing is that you can convert between them. You can merge it with 4 . And finally, you have studied, which takes in 2 arguments with  students and for the exam, and  that you can merge those 3 together, and you get a ternary note there. , , we're out of luck. You just have to trust me. And this is an example from German."
    ],
    "Topic 4": [
        "Jackie Cheung, Professor: It sounds  nothing. that means there are no classes or office hours or lectures. The reading assignments are not meant to take a long time there, but they are there, those to help complement the material that we discuss in the course. And also and if you need any help in finding final project members, if you haven't started thinking about that yet we will provide help for that as . that was  the 1st content topic. , , he should have taught you about n-gram models, ? it's  a lot more than that. Often we interact with each other through textual means. We even have spaces between the each word. But it's important to remember that text is just an approximation of language. It's 1 view of language. but it's a discrete approximation of language. The pronunciation of words that are  to each other will often affect each other some of the most of the time. It's in these very small ways that it's really hard to notice, especially if you're used to speaking a language. if you're from the Uk. to add a consonant between 2 words. At least that's my approximation of how you say it. And if you think about it, there's  no consonant there. that doesn't start with a vowel. It depends on whether it's a yes, no question or whether it's a question with a question word, and  on and  forth. and by convention, you mark, and grammaticality with an asterisk. The second key concern that we might have, and why we might want to work with internal structures and syntax is, we might want to use the syntactic structure to help us infer the semantic structure to help us come up with a meaning representation of that sentence. and that comes to  very tricky and philosophical questions about what is meaning and what is a meaning structure have to do and what counts as a meaning structure. From the 4th century BC. Some people say, even smarter than today, because they don't have  technology or whatever. Has anybody heard of this book? They tell you how you should write. , it doesn't tell you how a language is as and how it occurs. It tells you that it's some normative standard about how they think you should write in order to have something be correct. All varieties of language have some grammar. But they can all be described using rule, based systems. Sometimes it's even more complex. however, it's good to try to separate the 2. ,  for our purposes here, we want to build computational models of language as it is. And  we're more interested in descriptive graphics. And in the domain of syntax. You can have adverb phrases. you can put you can put anything that is a noun phrase,  and syntactically, it would be correct even if semantically, it might not be . You can say things that are syntactically correct, but semantically meaningless,  you say, I saw computational linguistics. ,  Jean-claude, madame, the muscle from Brussels would be replaced by the pronoun he. That's not a  face constituent. the question is, there's not people online. you might need to do something , ask questions , you did. but they also relates to each other and systematic and regular ways. You can say I relaxed after a long day at work. You can say I relaxed my muscle. Usually you need,  the transfer, the recipient, and the thing being transferred . Do you mind to give an example of that? the active voice one would be a thief stole the wallet. You can add it if you want, but it's not obligatory. here it means that you need to include a subject. and you need to include a direct object, and with give, you need to include an indirect object as . you can think of a parallel to say programming languages if it helps where you can talk about functions. And then one of these prepositions. It's talking about what certain predicates expect. And that's part of what helps you figure out if this the sentence is grammatical or not. Do you want to see? Did you want to say something else? Yes, on Saturday, October 12? ,  why is it on Saturday, October 12.th   give me some arguments for why, this is a constituent as opposed to just on. Why is it not  on Saturday or on Saturday, October. it would still retain its meaning. Yes, you can move the whole thing to the beginning of a sentence. on Saturday, October 12.th The  assignment is due. On Saturday the  assignment is due October 12.th  you could, but it would be very strange, and it means something very different which doesn't make any sense, , and you cannot say on the  assignment is due. Saturday, October 12.th Great more tests. You can ask, when is the  assignment due? On Saturday, October 12.th I suppose you can also just say Saturday, October 12.th When is the  time to do? Saturday, October 12.th You can do that, too. that's evidence that Saturday, October 12th is  also a constituent. You could say  is due then? The  assignment is due, then, is where then? Replaces on Saturday, October 12.th , great. here we're going to take that machinery and that theory and apply it to natural language. and in this context language simply refers to the set of strings that you want to accept. where you have symbols involving plus and minus and multiply and divide and numbers, and you want to figure out what forms . In our case, we're going to take that form of machinery, and we're going to apply it to natural language. Again, there are good reasons to question this assumption, but we're gonna run with it. ,  there are good reasons to question that assumption. ,  why do we want to do this? there are implications for cognitive sciences and language learning. in natural language you can talk about. Can we write to  a word  this? Can you write to kicks? However, can we write to jumps and over? You start off with a starting symbol which for us will be the S. The sentence. and you apply the rules of a Csg. that's part of the language described by this grammar another one is  this, rocks. Again, this is just a convention, and the terminals or leaf nodes or words in our case, which are the , which correspond to words, because we're applying it to natural language. If you want the intuition, the reason that is called a context, free grammar is because all of these rules you can always apply them. But that's not super important for our purpose. We love those they have a N. And Sigma and R. And S. Where N is a set of non-terminal symbols. Sigma is a set of terminal symbols. In this particular form of something from the non-terminal set rewriting into some combination of things in the non-terminal and terminal sets. But that's not very useful most of the time in natural language. And also you need a designated start symbol, which is an element of the non-terminal symbols. You have a derivation to where it's a sentence accepted by this grammar. If you can start from the starting symbol, and we write to a bunch of terminal symbols. ,  I'm gonna start off by copying the initial grammar from before. We want to account for? we're good, because,  this 1st rule of Sv. The subject is there in  this 1st element of np. We can have the intransitive option, the transitive option, or the ditransitive option. Rewrites to Vnp and P. And not only that, here comes a subcategorization part we need to subcategorize. v, 1 would handle relaxes. and then v. 2 handles, steals and kicks. and v. 3 handles gives. , all this grammar can do is say, this relaxes, this steals, this, this kicks this. We also have to draw a tree. Do we also have to draw a tree? you have to draw a tree anytime you  work? this is a grammar ? But it doesn't work with any particular sentence. if you want to work with a particular sentence  this, this steals this, then you would have to draw the tree. this is an Np steals is v, 2. Each of these is a rule in the grammar, and since the top node is an S, which is a starting symbol, this is a sentence accepted by the grammar and everything that doesn't work . It doesn't know about ,  noun phrases  boys and thieves and balls and wallets, and  forth. that's something in the actual language that should be accepted. here the you can even argue  this steals. And  then this grammar would overgenerate because it generates it accepts the string. If you want to account for what's  in Ingram. we can work on this, we can gradually start to modify our grammar to account for more and more phenomena, and to fix issues with over and under generation. ,  let's extend our grammar to account for say. ,  we want to account for, say, prepositional phrases. Or, , let's do adverbs first.st ? How do we add adverbs to our grammar? to modify this grammar, to add adverbs. , first, st you need to add the adverbs. And , we need to integrate this into our grammar somehow. and  we can add it . There are some other people who say that there's there's something wrong with that approach, and that there may not be empty elements. But they're they're useful to account for many phenomena from the perspective of parsing. , we're going to start talking about how to systematically recover these trees. suppose, given a grammar, given a sentence, give me the parse of that sentence from the perspective of parsing empty elements cause a big headache because you don't know where by definition, you don't see where the empty things are. we need to keep the original rules. You can write it  that knows that rewrites to S  your initial starting symbol. It's because we have an instance of recursion where you have a non-terminal symbol rewriting to itself eventually. what this would say would be that it's possible to have sentences of that are arbitrarily long, arbitrarily long. if it fits within this structure of  this, this described by this grammar. It's arbitrary, and really the main constraints are due to our processing power. animal community again, super controversial, but animal communication,  with  bee dances, . fortunately, English has a very limited amount of subject-verb agreements in the present tense, . the only thing we really have to fix is, we need to ensure that if you have a singular 3rd person. ,  let's fix that part of the grammar. ,  let's start with this grammar and then fix it. ,  what is our general strategy gonna be any ideas? For with an S and v 1 without an s. and likewise. is that we want to. Every time we create a distinction we have to think about , , is it? And we want to have rules that check and ensure for correspondences between the features and different parts of your sentence. through a feature-based account of a language and properties of words and grammar rules, and  forth. But we won't have time to go into that in depth. if you're interested in that ask me, and I can point you to references. But , that's a rant for some other time and for us, we're just sticking with standard context-free grammars, where your only option is to create these very complex symbols. Here's evidence for hierarchy and structure in sentences. if you have the student study for the exam you can have, you can talk about how each phrase has a head word, and you can connect the head words to each other. , , for the entire sentence, the head word is said to be studied within the constituent of the student. It's student within for the exam. It's 4, , and within the exam. here this is a subject relation, and here this is a prepositional phrase, argument, relation, and from student you can draw an arrow to V, and from 4 you draw to exam, and from exam to B.  dependency. , how do you identify headwords? whereas if you draw things out in a tree. How do you tell that something is a subject? ,  the students study for the exam. And then you turn that into a constituent. in the student study for the exam here, the leaf is B, and there's a student there. Because  the exam is and is  a leaf, a new leaf note, because you've merged them  you can. then it becomes for the exam. you might need to do a little bit more work to if you want to ensure that   a binary branching trees with internal structure that we talked about before. And also note that there are no labels here. You need to figure out the label you. You have to define additional rules to convert between from the these, these grammatical role labels into the non-terminal labels in the constituent tree. But I won't show an example, for . Does anybody speak Russian or Czech? and they're both pretty natural, , and accepted. and in the second one it implies that if you convert that into a constituent structure. You have a constituent that's broken up by other words in between, which is pretty interesting."
    ],
    "Topic 5": [
        "unfortunately, you'll be spending the  few weeks with me rather than David, and we will talk a little bit about structure. we'll talk about syntax and semantics. And you can also post on the Ed of the course for that. But you should go with whatever is officially released. we're going to ask, what is syntax? ,  it's easy to forget this, especially  that we're in modern times. and that's how you might define a token. and it's a discrete approximation also of speech,  the speech modality of language. Let me give you some examples. because the parts interact with each other quite closely and quite tightly. But I can give you some examples of this happening in more obvious ways. one example, one obvious example from English might be. ,  air comes out of your nose. ,  if it's a I know it's that you. , someone give me a noun gato. They work over entire sentences, or when things are spoken. those are some examples of why it might make sense to work at a level. the goal of today's lecture, then, is for me to provide you with evidence that this structure exists. then, you can describe it in those terms in terms of hierarchical structures. And  then, what we're gonna talk about today is syntax. In the linguistic sense, we're going to talk about how structures in a sentence can be formed and assembled to create larger units, larger structures. their words, you can combine them with other elements to form bigger structures. And you can combine those structures with other bigger structures to form ever bigger structures until you get to the level of a sentence, and that's where we'll stop for  with syntax and in syntax there are some key concerns. And we'll get to that after the art and syntax. Then we'll get to semantics. this idea of syntax and that languages have structure is not a new idea. , this is an idea from multiple millennia. And  that it's it's going to be very similar to some grammars that you can read about today. they there are multiple categories of things, and they combine in certain ways. And that's also a description of the morphology and the syntax of a language. You can read more about it. But ,  grammar is not a new idea. there's another notion of grammar, which is that the it's this  a onerous thing that's really scary. in this course, we're only going to be concerned with descriptive grammars. and sure we need to have some standards  that people can communicate with each other efficiently, especially in a written form. , if it's a hierarchy that means there are these intermediate nodes and some intermediate level representations. A constituent is a group of words or other constituents really that behave as a unit. These are examples of types of constituents. And  the answer is, they share some similar distributional properties that we can test for. Sometimes there are many factors that go on. one test for constituency is that groups of words can appear in a similar syntactic environments. And this also gives you evidence that it's the same type of constituents. let me give you an example for examples. that's technically grammatical, even though we're not sure exactly what it means, and you might struggle to come up with a metaphor where that makes sense. However, here are some examples of things that are not noun phrase constituents. , ,  you have, you have to come up with some syntactic environments where the type of constituents you're interested in could fit there. And this is a slightly different test. Another slightly different test is you replace it with a pronoun. in fact, that's  what pro words pro forms. You can use them to refer to something else, and often you can use them to replace an entire constituent. then it becomes he beat me up. ,  these are all additional evidence that it's these are all noun phrases, the same syntactic type A 3rd test is, it can be used to answer a question on its own, especially in formal language, who beat you up. But here are some of the most common ones that are in most grammar formalisms, and there are likely others. it depends on your theoretical perspective, and where you come from,  for our purposes, we don't have to care  much about that we, for our purposes, we can define them operationally through their distributional behaviors. And then you have to answer with  going to the park? They're  pro forms for verbs as   to do . but when it comes to, when it comes to verbs. it's not based on the semantics of the sentence. it's about the where it's located in the sentence, not about what its function is. And  that's a systematic thing. And they also have to take a certain number of arguments. They have to appear in certain positions around the sentence. You  want this course's difference. the others, constituents,  there will be a constituent that fills each of these slots that fills each of that needs to fill each of these roles, and they have to appear in a certain syntactic environment which can be called subject or object, or something else, or a prepositional object, and  forth. You can also have words that function similarly, but they'd be. They appear after a noun phrase, and those are called post positions. There are some post positions. and other languages prefer to have post positions . Can you replace this with some  pro form? ,  that covers  the linguistic elements of what wanna talk about. what is the formal model of grammar that we're going to use to account for these and other syntactic concerns. And in that context a formal grammar is a set of rules and other associated things that help you generate a set of strings that make up a language. what's the set of strings? And you want to figure out which sets of strings are grammatical and which sets of strings are ungrammatical. however, it turns out for the syntax of natural languages, where  multiple constituents and some categorizations and other phenomena. , if you're talking about the syntax of a programming language  of python, or something  that, or if you're in a compilers class, or something  that. They're they're  a series of rewrite rules where you have something on the left, which represents some elements that you're working with, and it rewrites into something on the , which is  a sequence of smaller parts that form that bigger part, that bigger chunk. and then you can use that to generate or to recognize. ,  S. , generates Npvp, there's only one option in this grammar. then you get, as we writes to Npvp. it rules  this would be a string. As long as you have the  left hand side you can always use them to rewrite, and that's what makes them context free. There are other classes of grammars. or you can add conditions. R is a set of rules also called productions. the ends will be the constituents, ? In constituents they will be symbols in the set. N. Whereas things that are words will be the terminal symbols, and then they'll be set part of the set. ,  let us use this formal machinery and work with it, and use it to describe a very, very small fragment of the English language. our extended example would be, let's develop a Cfg that can account for verbs with different subcategorization frames. , , we have relax and steal and kick and give. But I'll just replace the part to do with verbs for the verbs you want to account for. the way to handle this. it takes care of that. And that's how you create a grammar that respects the subcategorization of these verbs. and you might also want to make your nominal structure more complex. And this gives this . That's all of the sentences currently admitted by this grammar. if you want to make it more interesting, you can also work on the Np part and make that more interesting. That's , that's not what the question asked for. all clear, obvious, or completely. You want to work with an actual sentence  a real sentence. Gives us a Vp and Npvp gives us an S. And then you can draw lines. You have one S. Node. That's how  it's not a sentence of the scrum. here are some problems with the grammar. You can have the opposite problem, which is over generation where it generates sentences that are not grammatical. This also makes sense, because adverbs, as their name suggests, are associated with verbs,  they somehow modify the meaning of the verbs or the verb phrases. It depends on who you ask. , I shouldn't look back. You can't have multiple rules. you can have multiple rules. But I'll let you do that on your own. I know that the dog barked. that I know that the dog barked. that I know that the dog barked and at infinitum. in general, you can describe that with some  grammar you can have Sp rights to Npvp. you can have one branch of it which gives you barked  the dog barked, and another branch of it that goes to  know that something. Because,  anytime, you have a sentence of a certain length, you can make it longer by adding, . And  in practice, the sentences are not that long. but in terms of what this formalism suggests theoretically. and you can contrast it with other systems that  are fixed . And that's the type of communication. , this is all controversial and potentially new, but one way that they may or may not differ from human language, at least many of them seem to differ in human language in that they cannot be arbitrarily long. before we do that, we will add both forms. the main problem we have is   we can generate the ball. And then you can have the boy kick because kick is currently v, 2, the ball. I'll try to make this more distinct. this will be part of Nps. I will use 3 s.  . , does this make sense? then people have come up with schemes to make this a little bit more tractable. , , here, we're really talking about features. it can be implemented and read by a computer to do parsing and whatever. And in a dependency grammar, you  draw arrows directed  directed edges to connect and describe this relation. And here's the here it is for that particular sentence, ,  study will be will not have, will not be the child of anything, because it's the head of the whole sentence. and then from there you can draw a directed edge to student and to 4, and you can label them. , in terms of singular and plural. But let's forget about that for . But the advantage of talking about dependencies as opposed to constituents is that it exposes the syntactic relations, the grammatical relations much more easily. whereas in the dependency grammar it's much more clearly exposed. And  if you're interested in understanding the semantic relations between words and noun phrases, and you need it for further processing. People often work with dependency grammars rather than constituent grammars. dependency trees can be converted into constituent trees. and constituent trees can be also converted into dependency. Trees, if  what the head of the constituent is ? , let me do this example. , for also is associated with that ? They rarely cross in other languages. enjoy your reading, break and happy Thanksgiving."
    ]
}