{
    "Topic 1": [
        "Jackie Cheung, Professor: It sounds  nothing. probably want to switch about this one second. . . , we'll get started in a few minutes we should leave time for people to get here, because the main entrance was blocked. and I'll try to play around that question. Bye. . before we get there. reminder that  week is Thanksgiving, plus the fall reading break. . Question. , my slides could be wrong. . no. . And I remember giving a lecture on that a long time ago in September or something. ? , for example, he should have taught you about n-gram models, ? ? And then, most recently, Lstms, ? . if,  many students in the course you were, you got interested in Nlp because of all the recent excitement with e-learning. ? ,  it's easy to forget this, especially  that we're in modern times. We even have spaces between the each word. then. The pronunciation of words that are  to each other will often affect each other some of the most of the time. if you're from the Uk. For example, chances are quite high. one way in which the pronunciations of words affect each other adjacently, and those dialects of English, for example, would be that in most situations they don't pronounce the r at the ends of words and syllables, but they would pronounce it if the following word starts with a vowel. ,  and sometimes there's even a phenomenon called intrusive R, where there originally wasn't an r in the word, but then you add the extra r in to smooth that out. a. ? not everyone from there has it. But it's really a nasalized valve. ,  air comes out of your nose. and the vowel is no longer nasalized. . Then you don't pronounce the n. but the vowels neither . , and that seems  it. If it's a question, for example, there are particular intonation patterns for questions in English, and it depends on your dialect. It depends on whether it's a yes, no question or whether it's a question with a question word, and  on and  forth. . . In the linguistic sense, we're going to talk about how structures in a sentence can be formed and assembled to create larger units, larger structures. and by convention, you mark, and grammaticality with an asterisk. And he wrote this whole grammar of it, and you can translate it. ? You can read more about it. ? ? And that's simply not true. ? there's no relation at all between the level of formality versus the complexity of a system. Perhaps. however, it's good to try to separate the 2. ,  for our purposes here, we want to build computational models of language as it is. . I said earlier this lecture, that the main goal I have for this lecture is to convince you that there exists hierarchical structures in language. And in the domain of syntax. Somehow they form some unit, and then there are properties that you associate with a unit rather than with each of the words that make up that unit? And we can try. . Suppose you have a context. , after this. The muscles from Brussels. ? , you can't. Question. . That's . . , here's the second test. Suppose you have Jean-claude Van Dam. The muscles from Brussels meet me up. You can also say it was Jean-claude Van Dam, the muscles from Brussels who beat me up  it's the same group of words, and you can put it in different environments. You can say I was beaten up by Jean-claude, madame, the muscles from Brussels. you can move these around. That's what they do. ? as you might expect, linguists don't agree, and there are different formalisms and different languages might  have different syntac categories that seem to make sense for them. ? That's a good question. I think it depends on your theoretical perspective, and where you come from,  for our purposes, we don't have to care  much about that we, for our purposes, we can define them operationally through their distributional behaviors. is it the . you might need to do something , ask questions , you did. What? What did you do? There are also replacements for verbs. remove things and you skip over them. But . . but when it comes to, when it comes to verbs. . subject is the most common one, the , most at least in English. The  most common one is direct object. The ball is the direct object. and sometimes you also get sentences where you have multiple objects, and one of them is the indirect object. Here is the indirect object. ,  that in that case it would be a transitive verb that takes on both the subject and a direct object. But and then something that takes  2 objects would be something , give usually verbs that involve the change or transfer of something. Usually you need,  the transfer, the recipient, and the thing being transferred . . . Do you mind to give an example of that? ,  there, there's both a subject and a direct object. . And then the other the prepositional phrase, you can add it on, but it doesn't count as an argument. What's the? . and you need to include a direct object, and with give, you need to include an indirect object as . . ? They have to appear in certain positions around the sentence. and you can call them subject and direct object positions and  forth. , there are many other subcategorization possibilities  with the verb wants. And then this is called an infinite table clause, because it's to learn. You can have something  a prize which takes a subject, an object, and also a prepositional object. The minister apprised him of the new developments. and  the minister is the subject. Him is, I guess, an object, and then prepositional phrase, object of, and it has to be with of you can't say the minister apprised him in something, and you can't simply say the minister apprised him. But but what I expected. this is another subcategorization. . a subcategorization is a slightly different in that. the others, constituents,  there will be a constituent that fills each of these slots that fills each of that needs to fill each of these roles, and they have to appear in a certain syntactic environment which can be called subject or object, or something else, or a prepositional object, and  forth. , that's what this is all about. Maybe we can do this together because we're running behind time. . on, on. . prepositions are these words that help indicate relations between things in a sentence, and they appear before a  phase. You can also have words that function similarly, but they'd be. They appear after a noun phrase, and those are called post positions. There are some post positions. But . and other languages prefer to have post positions . why is it? ? You can ask, when is the  assignment due? No, you cannot do that . What else? . Because remember, we're computational linguists. We're not just linguists, we're computational. ? That's a correct arithmetic expression. ? Again, there are good reasons to question this assumption, but we're gonna run with it. ,  there are good reasons to question that assumption. , a formal understanding helps us to develop appropriate algorithms for handling and dealing with hierarchical structures and syntax plus? Fsa's our finance data are particular ways to talk about and describe regular grammar. And they are  used in Nlp as . and Fsa. , in Nlp. . I'm going to start by giving you  the form of it, because I'm guessing that maybe when you're thinking about describing programming languages, you might have seen something similar before. ? Oh, we might have a sentence which we'll denote with some symbol  S. And a sentence should be rewritten into a noun phrase, plus a verb phrase. You start off with a starting symbol which for us will be the S. The sentence. It's a very simple grammar. And then Np. Rewrites to this, and then Vp rewrites to V, and then V rewrites to some of one of these sperm. ? for B rewrites to rocks. then, you have non-terminals, which by convention are written with capital letters. Again, this is just a convention, and the terminals or leaf nodes or words in our case, which are the , which correspond to words, because we're applying it to natural language. what? here's a,  this is . There's 1 called context Sensitive grammar. In the context of something else or Np. a Cfg is a four-tuple. technically, you can rewrite to an empty symbol. ? And also you need a designated start symbol, which is an element of the non-terminal symbols. And within the system. . here, for our purposes. That are larger than single words. . ? Rights to Npvp. The subject is there in  this 1st element of np. ? this means we need to change this rule from Vp rewrites to V, to Vp. Rewrites to V, or Vpv. Or Vp. Rewrites to Vnp and P. And not only that, here comes a subcategorization part we need to subcategorize. v, 1 would handle relaxes. And that's how you create a grammar that respects the subcategorization of these verbs. But that's . That's , that's not what the question asked for. we're done with this. . , . and we can do that. . And this the second. This is also an Np. and then B 2 Np. Each of these is a rule in the grammar, and since the top node is an S, which is a starting symbol, this is a sentence accepted by the grammar and everything that doesn't work . You have one S. Node. ? ? suppose you add, , relax here. ? ,   time someone asks you, what did you do in class? how will we do that? we can do that. question can you have  an empty element inside? It depends on who you ask. It depends who? You ask. . ? But . There can only be one. Is that your question? . It's the wrong word, but  Vp rights to 2 different things. ,  this is , vp, rights to v, 1 Vp rewrites to v, 2 nt, Vp rights to that. ? And I think the question also asks for prepositional phrases. ? and then the Np is  the subject,  it can be  any noun phrase. you see why? ? Because,  anytime, you have a sentence of a certain length, you can make it longer by adding, . And  this is an interesting property, and which is suggested by Cfgs, which is that in natural language there seems to be no fixed upper bound in terms of the length of a sentence. , we have a limited amount of  brainpower and memory, and  forth. . supposedly animal community again, super controversial, but animal communication,  with  bee dances, . They they make vocalizations, and you can teach them to sign. And they have properties and  whales, clicks. , they there. . Subject. . ? Do you agree? and then through the Vp branch you can go to Vp, v. 2 np. And suppose we also want to model , I, . . we have v. 1. I'll try to make this more distinct. S's, , that's . Because. , . this will be part of Nps. . then. . 3 s. What do you prefer? Something. . 3 s. no. This is . ? ? . ? ? it can be implemented and read by a computer to do parsing and whatever. if you're interested in that ask me, and I can point you to references. . and we have the student studied for the oh, I should go back to full screen mode. . if you have the student study for the exam you can have, you can talk about how each phrase has a head word, and you can connect the head words to each other. , for example, for the entire sentence, the head word is said to be studied within the constituent of the student. It's student within for the exam. Question. , how do you identify headwords? It's the student that lets  that the entire phrase of the student is singular. What's the what's the head word more generally, it's the fact that the student is a noun that causes the student as a whole to be a noun phrase. It's not V that causes the whole thing to be a determiner phrase, although there are theories that do that. But but ,  for our purposes, it's the student that causes the student to be a noun phrase. and it's a 4 that causes this whole thing to be a prepositional phrase. Again, there's a lot of complexities and different theories that do different things. But let's forget about that for . How do you tell that something is a direct object. To the left of some Vp or something, and that's quite complex. Also, the other thing is that you can convert between them. Trees, if  what the head of the constituent is ? ? you might need to do a little bit more work to if you want to ensure that   a binary branching trees with internal structure that we talked about before. They rarely cross in other languages. They cross much more frequently. Some. , a little bit. What other languages? , , we're out of luck. You have a constituent that's broken up by other words in between, which is pretty interesting."
    ],
    "Topic 2": [
        "The block people here. it's just some announcements and reminders. And you can also post on the Ed of the course for that. And if,  many of them, you're reading and following a lot of machine learning blogs and  forth, you might be under the assumption that language is a sequence of tokens. while language could be described as a sequence of tokens. , and Texas seems to naturally come as a sequence of discrete tokens ? we might think that language is a sequence of tokens. But it's important to remember that text is just an approximation of language. And that means that there  are a whole bunch of other phenomena where the most natural way to think about them and characterize them and describe them and model them, is not to just think about them as a sequence of tokens. 1st of all, language is not just a linear sequence of tokens. It's in these very small ways that it's really hard to notice, especially if you're used to speaking a language. You never notice how the pronunciations of words bleed into each other themselves. how many people here speak a dialect of English? to add a consonant between 2 words. But some people do. , more people. And if you think about it, there's  no consonant there. then you print. that's another example of pronunciations bleeding across words also intonation patterns. It cuts across multiple tokens in the sequence. It's not even an idea from last millennium. People are just as smart as today. Some people say, even smarter than today, because they don't have  technology or whatever. There's some option where you can add certain things, but other things are not allowed, and there's a prohibition, and they talk about affixes, say, and then they always receive some augment. for example, there's this famous book on grammar called the Elements of Style by Strunk and white. Has anybody heard of this book? And also this book is just bad because it contains lots of linguistically inaccurate descriptions. They tell you how you should write. It tells you that it's some normative standard about how they think you should write in order to have something be correct. All varieties of language have some grammar. what does this mean? Justin Trudeau is a noun phrase. then the  question is , How is it that what does it mean for these words to form a unit? And  the answer is, they share some similar distributional properties that we can test for. one test for constituency is that groups of words can appear in a similar syntactic environments. That's fine. That's fine. And  then they fail. Constituents can be placed in different positions or re replaced in the sentence as a unit. Another slightly different test is you replace it with a pronoun. , it's just  as an example of something that fails. That's not a  face constituent. the question is, there's not people online. it's about the where it's located in the sentence, not about what its function is. but, for example,  adjectives, can also require certain arguments, and you can talk about the relations between a determiner  the versus its noun,   the ball or something. For example, an intransitive verb is one that only requires a subject. You can say I relaxed my muscle. , for example, in English, there's this passive structure which changes a transitive verb into one where it only requires the subject, and it changes the mapping between the syntactic and the semantic roles of the arguments of that. Other languages have different role, different operations and things  that as . I didn't understand for steel, maybe. for steel it would be something , . You can add it if you want, but it's not obligatory. with the number here. I should have explained the number. Here is just the number of arguments that it takes. And they also have to take a certain number of arguments. there's an analogy to be drawn there. And in fact, we'll make this analogy quite explicit in a model later on in the course. You probably want this course's difference. Or Ed? the beginning. it would still retain its meaning. And you cannot answer just on , when is the  assignment due on. How about replacement? The  assignment is due, then, is where then? ,  that covers  the linguistic elements of what wanna talk about. , some people, ? And maybe some of you are taking that course or another course in formal grammars and formal language theory. and in this context language simply refers to the set of strings that you want to accept. what's the set of strings? ,  there are other formal grammars you might have heard of and encountered, or use finest state machines  finest state automata and regular grammars. I'll be  back. We write to something  a verb, and there are some options for that as . this is already a very simple Cfg,  this is, I'm starting off with an example. and then you can use that to generate or to recognize. To keep rewriting until you end up with a sequence of words. You just use a different rule there. Can be rewritten to something in the context of something else. or from  on I'm gonna start saying Cfg, to be shorter. We love those they have a N. And Sigma and R. And S. Where N is a set of non-terminal symbols. Sigma is a set of terminal symbols. In this particular form of something from the non-terminal set rewriting into some combination of things in the non-terminal and terminal sets. And also, if you're allowed empty things, then it causes parsing to be much harder. But that's a that's an aside. If you can start from the starting symbol, and we write to a bunch of terminal symbols. N. Whereas things that are words will be the terminal symbols, and then they'll be set part of the set. Sigma. our extended example would be, let's develop a Cfg that can account for verbs with different subcategorization frames. ,  I'm gonna start off by copying the initial grammar from before. ,  what was it we wanted? the way to handle this. 1st of all, everything requires a subject. because we  have 3 options. For example, there's a whole bunch of things to do with  nouns, and that and it does that. However, there's something annoying called subject-verb agreement. Or, , let's do adverbs first.st ? How do we add adverbs to our grammar? Any suggestions? to modify this grammar, to add adverbs. , first, st you need to add the adverbs. we can do softly or quickly or slowly. and  we can add it . From the good question, can you have an empty elements inside. if you're thinking about this formally in the from the Cfg perspective, yes, you're allowed to do it by my definition of Cfg, because here this is 0 or more , you can have a non-terminal rewriting into an empty SIM  into nothing from the perspective of linguistics. I think, in mainstream. There are some other people who say that there's there's something wrong with that approach, and that there may not be empty elements. The the empty elements are  to do, parsing properly with empty elements, becomes  intractable because you have to posit all possible combinations of up to infinite number of empty elements everywhere in your sentence. yes, in our example we would have deleted the other Vp mapping ? What we develop. It's just one mapping, that's all extended. What do you mean by mapping? , I shouldn't look back. But it's  just one big rule. In fact, this is just a shorthand. You can write it  that knows that rewrites to S  your initial starting symbol. It's because we have an instance of recursion where you have a non-terminal symbol rewriting to itself eventually. and assuming that you have no empty elements, and,  you, have actual words there. And some people think this is a big deal. and you can contrast it with other systems that maybe are fixed in some way. There might be a fixed set and fixed inventory of symbols and signs that can be produced, but they don't have the same properties that,  Of recursion, where you can have, . otherwise is just the base form of the verb. before we do that, we will add both forms. For with an S and v 1 without an s. and likewise. We'll have B 2 with an S. And v, 2 without an XS. And then we'll have. And not only that we also have to pass that information up to the level of the Vp. if you think about it, the agreement works between the subject and the and the verb , and the subject is  all the way over here on the left. and then the verb is inside the Vp, which means that we have to pass that information along somehow. Singular nouns would be  balls. , this is confusing. 3 Si guess. , does this make sense? and, as you can tell, this becomes very hairy very quickly. then people have come up with schemes to make this a little bit more tractable. we have a feature, which is that a noun can be singular or plural. then, although we don't talk about it much in this course, people have developed theories of grammar that incorporate that. And you can specify things much more plainly and elegantly than we're doing here. It's just a different view of the syntactic structures within a sentence  grammatical relations, they induce a dependency relation between the words that are involved. It's 4, perhaps, and within the exam. It's exam. And here's the here it is for that particular sentence, ,  study will be will not have, will not be the child of anything, because it's the head of the whole sentence. for example, in terms of singular and plural. for the exam. you can directly tell here that there's a subject relation with this arc. How do you tell that something is a subject? Deterministically. if the dependency edges don't cross each other. ,  the students study for the exam. You start from the leaf, nodes, and you see what is associated with it. in the student study for the exam here, the leaf is B, and there's a student there. Because  the exam is and is essentially a leaf, a new leaf note, because you've merged them  you can. then it becomes for the exam. that's potentially a problem. And , dependencies can cross in English. Does anybody speak Russian or Czech? You just have to trust me. you can have dependencies, graphs where the edges cross each other . He tried to reach me. It's  he has me tried to reach, or he has tried mean to reach. And  in the 1st version there's no crossing dependencies. And in the second version, there's a bunch of crossing dependencies,  that he has me tried to reach  has a crossing dependency because it's reached me. not tried me, but reach me. you will get discontinuous constituents. And then we're going to talk about parsing algorithms. enjoy your reading, break and happy Thanksgiving."
    ],
    "Topic 3": [
        ", we're talking about universities that you're you're pumped and that you're open. Hi, everybody! please be sure to get on top of that. please watch out for that. is it doing the 11.th , when I checked and my courses was the top. And crfs, maybe crfs and Lcms,  that's great. but it's a discrete approximation of language. and it's a discrete approximation also of speech,  the speech modality of language. For example, just the articles that  the. At least that's my approximation of how you say it. , someone give me a noun gato. Maybe their words, you can combine them with other elements to form bigger structures. and this is a notion called grammaticality. this idea of syntax and that languages have structure is not a new idea. , this is an idea from multiple millennia. From the 4th century BC. developed a grammar for Sanskrit, which is a classical language of South Asia. they there are multiple categories of things, and they combine in certain ways. But ,  grammar is not a new idea. there's another notion of grammar, which is that the it's this  a onerous thing that's really scary. And  we're more interested in descriptive graphics. These are called constituents. what are constituents? These are examples of types of constituents. ridiculously annoying and tame, is an adjective phrase. There are many different kinds of constituents. And and it can obscure things. But  you should try multiple tests. But I'm gonna give you some tests. this is your syntactic environment. This test. technically, this is a test for a noun phrase constituents. that , tests for different constituents. , ,  you have, you have to come up with some syntactic environments where the type of constituents you're interested in could fit there. You can passivize it. And this is a slightly different test. then it becomes he beat me up. ,  these are all additional evidence that it's these are all noun phrases, the same syntactic type A 3rd test is, it can be used to answer a question on its own, especially in formal language, who beat you up. If someone asks who beat you up, you cannot answer the muscles from that's not a constituent. And I already said this orally. There's not really a universal list of syntactic categories, because. Another question supposed to be applied. these tests seem to be qualitative and also are there other tests for other types of constituents? yes, that these tests are qualitative and also yes, these there you have to define different tests for other syntactic types, syntactic categories. And then you have to answer with  going to the park? There might be also tests in involving a phenomenon called elision, where you . far, we've covered syntactic categories and constituents. , we can talk about the relationships between the different constituents, because the it's not just that they there exist constituents in a sentence. but they also relates to each other and systematic and regular ways. or the wallet was stolen by a thief. and then the subject of it is the wallet. purely, structurally. it's not based on the semantics of the sentence. Oh, it was the thief who stole the wallet,  shouldn't the thief be the subject? ,  structurally, the subject is still the wallet. they tend to impose some  constraints on other syntactic elements that must appear to form a valid sentence. and that's called subcategorization,  subcategorizing verbs into different kinds of verbs, depending on what constraints they add to the other elements in the sentence. and here, from the examples, before steel is also transitive, and kink is also transitive. And there are some regular processes that exist in languages that can systematically change the categorization. Sure. the active voice one would be a thief stole the wallet. and technically by a thief. and then the list here is the relation to  what else it requires,  the list of argument type types. that's why they're they're called arguments in both cases. It takes a subject. It is a constituents. Can somebody identify what the preposition is in this sentence? Yes, thank you. Do you want to see? Go back to those tests and think about it . Yes, you can move the whole thing to the beginning of a sentence. and compare that against our alternatives. And you can answer. Any other tests? we have pretty strong evidence that this is a constituent of some kind. here we're going to take that machinery and that theory and apply it to natural language. You can be working with arbitrary symbols. You can be working with,  the language of arithmetic. where you have symbols involving plus and minus and multiply and divide and numbers, and you want to figure out what forms . In our case, we're going to take that form of machinery, and we're going to apply it to natural language. ,  why do we want to do this? Maybe there are implications for cognitive sciences and language learning. Generates a regular language. however, it turns out for the syntax of natural languages, where you might have multiple constituents and some categorizations and other phenomena. And before we continue, I'm gonna close the door. , if you're talking about the syntax of a programming language  of python, or something  that, or if you're in a compilers class, or something  that. They're they're essentially a series of rewrite rules where you have something on the left, which represents some elements that you're working with, and it rewrites into something on the , which is  a sequence of smaller parts that form that bigger part, that bigger chunk. and you apply the rules of a Csg. ,  S. For example, generates Npvp, there's only one option in this grammar. This rules which is accepted by this grammar. that's part of the language described by this grammar another one is  this, rocks. If you want the intuition, the reason that is called a context, free grammar is because all of these rules you can always apply them. R is a set of rules also called productions. and the star here means you can have 0 or more of them. we're just pretend it's  one or more, but I guess it's 0 more also works. You have a derivation to where it's a sentence accepted by this grammar. the ends will be the constituents, ? All of the constituent, the syntactic categories. In constituents they will be symbols in the set. it takes care of that. these become 3 separate symbols in our system. We also have to draw a tree. Do we also have to draw a tree? you have to draw a tree anytime you  work? You want to work with an actual sentence  a real sentence. this is a grammar ? It describes the possible sentences that can be accepted. this describes all of the possible sentences that could be accepted. But it doesn't work with any particular sentence. if you want to work with a particular sentence  this, this steals this, then you would have to draw the tree. , we can draw lines to indicate that they correspond to rules in the grammar  . That's how  it's not a sentence of the scrum. But clearly that grammar is not the grammar of English, because we say other things too. here are some problems with the grammar. that's called under generation. that's something in the actual language that should be accepted. But it's not accepted currently by the grammar. You can have the opposite problem, which is over generation where it generates sentences that are not grammatical. , then we can have overgeneration. it becomes an engineering project, ? it becomes a grammar engineering project. , , or wallet, I guess. here it's , softly. this grammar will accept a sentence  the girl kicks the balls softly or quickly, or whatever . the A constituents. They probably will teach you that there are empty elements, and they give evidence for that. suppose, given a grammar, given a sentence, give me the parse of that sentence from the perspective of parsing empty elements cause a big headache because you don't know where by definition, you don't see where the empty things are. You can't have multiple rules. you can have multiple rules. This is  6 different rules. these are 6 different rules just for the sake of saving space and using the vertical bar, that's all. But literally, it's just a safe space. It's these are, think of these as 6 different rules. Also,  chimps monkeys. If you want. That's currently accepted by this grammar. ,  we have to split them into different categories and rewrite the rules to make it all fit. we have versions with S and versions without S. Oh, oops! Versions with S versions without. and I  realize that I hate my naming terminology. And for I,  you can have debt n and also I. Ts, how about ts. is that we want to. then the mechanism we have for making sure that the other part of the sentence knows about this is to propagate that information up and down through the syntax tree and through the rules described by the syntax tree. and we want to be able to pass that feature information up and down the tree. Here's evidence for hierarchy and structure in sentences. there are  alternatives. And in a dependency grammar, you  draw arrows directed  directed edges to connect and describe this relation. and then from there you can draw a directed edge to student and to 4, and you can label them. here this is a subject relation, and here this is a prepositional phrase, argument, relation, and from student you can draw an arrow to V, and from 4 you draw to exam, and from exam to B.  dependency. But the advantage of talking about dependencies as opposed to constituents is that it exposes the syntactic relations, the grammatical relations much more easily. whereas if you draw things out in a tree. The only way to do that is to describe some here, at this portion of the tree you might find the subject as an Np. whereas in the dependency grammar it's much more clearly exposed. For example, you're solving some information extraction task. And then you turn that into a constituent. , for also is associated with that ? You can merge it with 4 . And finally, you have studied, which takes in 2 arguments with  students and for the exam, and  that you can merge those 3 together, and you get a ternary note there. But at least there's some mechanism to convert a dependency tree structurally into something that which is a constituent tree. And also note that there are no labels here. You have to define additional rules to convert between from the these, these grammatical role labels into the non-terminal labels in the constituent tree. and they're both pretty natural, I think, and accepted. and you can draw the arrows using how the syntax of German works. and in the second one it implies that if you convert that into a constituent structure."
    ],
    "Topic 4": [
        "outlook is phone. It's not helpful. , I'm gonna stick with this setting unless someone tells me they prefer something else. unfortunately, you'll be spending the  few weeks with me rather than David, and we will talk a little bit about structure. that means there are no classes or office hours or lectures. , no classes but that the reading assignment is due later this week. The reading assignments are not meant to take a long time there, but they are there, those to help complement the material that we discuss in the course. Also, we're gonna be releasing the final project description hopefully by the end of the week. Yes, good nods. And first, st we're going to look at syntax. we're going to ask, what is syntax? And then we're also gonna look at a formal system for describing structure called context-free grammars. And my goal in this lecture is to give you evidence to disabuse you of that notion. it's  a lot more than that. and that's how you might define a token. because the parts interact with each other quite closely and quite tightly. That is non rhodic. You speak that a version of English  that. Or another really obvious example is in French. how many people here speak French? French pronunciation is  famous for having these effects. And it's a really difficult thing for learners of French to grasp. if you have,  a in French, . A  a masculine, singular version of a How do you say it? And yes, that's . However, again, if the following syllable starts with a vowel. Suddenly the end appears. Whereas if it's something else that's masculine, singular. that doesn't start with a vowel. Yes, there you go. They work over entire sentences, or when things are spoken. But for today's lecture, the most salient phenomenon here is that we can think about and analyze the internal parts of sentences and find patterns that we can better explain through positing hierarchical structures. the goal of today's lecture, then, is for me to provide you with evidence that this structure exists. and also to come up with a formal computational model of this structure. And  then, what we're gonna talk about today is syntax. And you can do this recursively  you can start off with  the smallest possible units. And you can combine those structures with other bigger structures to form ever bigger structures until you get to the level of a sentence, and that's where we'll stop for  with syntax and in syntax there are some key concerns. one simplification and formalization here is, you can assume that for every single string of tokens. The second key concern that we might have, and why we might want to work with internal structures and syntax is, we might want to use the syntactic structure to help us infer the semantic structure to help us come up with a meaning representation of that sentence. And we'll get to that after the art and syntax. Ago some of the earliest people to have thought about this that we are still aware of are grammarians from, say, from South Asia. and it's very interesting, because even back then. And and they come up with, and they produce descriptions of the languages that they speak . And you can see that it's it's going to be very similar to some grammars that you can read about today. And that's also a description of the morphology and the syntax of a language. it's really interesting. And it's it can be even captured systematically by books. No, which is good because this is not the type of grammar we're going to talk about. I that here's another common misconception, which is that somehow things that are non standard or informal or casual somehow have no grammar. It's quite possible that the informal or casual version of a language has a different grammar compared to the formal version, the written version and the prestigious version. But they can all be described using rule, based systems. The informal version is not even necessarily simpler or worse, or lazier, or anything  that. , a noun phrase, something  computational linguistics is a noun phrase, the word the single word. It is a noun phrase. You can have adverb phrases. And   I'll go through a bunch of tests to check whether something a group of words is a constituent or not. Sometimes there are many factors that go on. And this also gives you evidence that it's the same type of constituents. I saw and then blank. you can put you can put anything that is a noun phrase,  and syntactically, it would be correct even if semantically, it might not be . You can say I saw Jean-claude Van Dam. this is also a noun phrase. That's also a noun phrase. You can say things that are syntactically correct, but semantically meaningless,  you say, I saw computational linguistics. You cannot say I saw a van from Jean. You can't just say I saw a van. You can't say I saw Ondi. Yes. But but the type of constituent stays the same. ,  Jean-claude, madame, the muscle from Brussels would be replaced by the pronoun he. because it's singular 3rd person masculine. But  the type of constituent that you have is going to be called its syntactic category. noun phrase. But here are some of the most common ones that are in most grammar formalisms, and there are likely others. Yes. Question when you define a constituent  noun phrases, is there a definition, or is it defined by its tests? When you define a constituent such as a noun phrase? Is there  a definition, or is it defined through the tests? yes. ,  Jean-claude Van Damme relaxed. Here the verb is relaxed, and the subject is this noun phrase constituent of Jean-claude Van Damme. Here the verb is, was, or was stolen. note here that subjects an object here. We're defining them purely, syntactically. it's not based on. The boy, keep the ball here. relax is an example of that. you have to relax. Your shoulders to do this exercise properly, or something  that. And  that's a systematic thing. When you pacifize it, it becomes the wallet was stolen. Yes. you might have a function that is , multiply XY, and it takes in 2 numbers x and y, and it multiplies them together. Some really simple function. I want to learn about computational linguistics. and,  before, it's not only verbs, but other things can do this  difference, depending on your dialect. Yes. But in English, it's overwhelmingly propositions. Yes. for whole purpose. Yes. exactly. For  for syntax. And , we're going to talk about the formal computational model that we're going to use to describe these hierarchical structures. what is the formal model of grammar that we're going to use to account for these and other syntactic concerns. ,  there's a whole field of research in formal language, theory, formal grammars. And in that context a formal grammar is a set of rules and other associated things that help you generate a set of strings that make up a language. where we assume that our goal is again to characterize exactly the set of strings that form valid sentences of English. there's an exact correspondence between them. and fsas correspond to a class of formal grammars called regular grammars. here, then, formal grammars, you can also talk about them as  possible sets of strings that you could describe with that formalism. it's more natural and useful to use a more powerful class of formal grammars. And these are in particular context, free grammars. Somebody is really upset that I'm talking about context-free grammars. what are context-free grammars. It's made up of a noun phrase plus a verb phrase. And a verb phrase? here a vertical bar is just a shorthand. to generate. then you get, as we writes to Npvp. Yes, what makes what about this makes them context free? What about this makes them context-free? you should just try to ignore context, free the meaning of context, free. there's a formal definition. formally speaking, this is what makes it a context, free grammar. Here's the formal definition. There are other classes of grammars. or you can add conditions. You can be , I'm allowed to rewrite an S. To an Mpp. But that's not super important for our purpose. The formal definition is what we should go by. , and this is the formal definition of a context-free grammar. , for example, we have relax and steal and kick and give. Relax. relax steel cake give. And , I'm going to put them in a 3rd person just to make things simpler. we're good, because,  this 1st rule of Sv. the thing we need to change is we need to change what can be a verb phrase. And  we can change the subcategory,  that not all of them are simply these. and then v. 2 handles, steals and kicks. and v. 3 handles gives. And this gives this . if you want to make it more interesting, you can also work on the Np part and make that more interesting. This deals this. this is a noun phrase, ? I should type that. this is an Np steals is v, 2. Gives us a Vp and Npvp gives us an S. And then you can draw lines. And you can check that. if you had this, this deals, you would not be able to find a derivation  this, where at the top. here the you can even argue  this steals. This is not a very interesting example. you're not allowed to say this relax in English. at least not in  the standardized formal version. This relax, even though it should not be accepted. I'm also going to make the Np structure more interesting. I'm going to say that a noun phrase can also rewrite into a determiner plus a noun and a determiner can be something  V, or maybe a and a noun can be something  a foul or boy or girl, or something else. to make things slightly more interesting. Yes. or yes, that's . and semantically. , in the  linguistics you're likely to encounter in linguistics courses here. , I'm just talking about  Cfg and what they do and how you use them to describe syntax. , we're going to start talking about how to systematically recover these trees. in practice you probably have to limit yourself to . At most, I will have,  this, many empty elements in a row, or something  that. And then parse with respect to that. But , that's a really interesting question. , here's an interesting phenomenon. He knows that. in general, you can describe that with some  grammar you can have Sp rights to Npvp. The Vp is the verb phrase, and then it gives you . But the really interesting thing here is that you can have it. why is this interesting? if it fits within this structure of  this, this described by this grammar. that's why it isn't. And that's the type of communication. Verb agreement issue. except for irregular verbs. the only thing we really have to fix is, we need to ensure that if you have a singular 3rd person. then the verb also has an affix of s or es, depending on the verb. Relax the boy or the boy relax the boy, kick the ball. We can have the ball the boy. And then you can have the boy kick because kick is currently v, 2, the ball. we have both 1st person and 3rd person. That's exactly . I made a mistake. ,  that means that we also have to indicate that this is a verb phrase, where there's an S on the verb. We need one rule, which is  Mps Vps, and one role, which is Mpvp. , this is going to be . And there are 2 different kinds of noun phrases, the ones that are 3rd person singular, and the ones that are not 3rd person singular. all the 3rd person singular nouns would be  a ball boy girl wallet. And then all of the not 3rd person. I'm gonna say, 3 s. Rather than 3rd person's. it's 3rd person. What  by S, here is 3rd person singular. , this is just a noun. the  nouns that are not 3rd person singular will be handled by this rule. nouns, and that our 3rd person singular would be handled by this rule. And then you have to ensure that the 3rd person, singular nouns agree with the 3rd person. Singular verb phrases. But but yes, hopefully, I didn't make any mistakes. Every time we create a distinction we have to think about , , is it? And then you have to think about where that information needs to go, because if it if it affects something that's very far away in the syntax tree. , for example, here, we're really talking about features. A noun can be 1st or second or 3rd person. And we want to have rules that check and ensure for correspondences between the features and different parts of your sentence. And that's really cool because it's a computational model. But we won't have time to go into that in depth. But , that's a rant for some other time and for us, we're just sticking with standard context-free grammars, where your only option is to create these very complex symbols. And then here is one formal computational model to describe said structures. There's an alternative which is also very popular in computational linguistics in Nlp called dependency grammar. Yes. People often work with dependency grammars rather than constituent grammars. dependency trees can be converted into constituent trees. and constituent trees can be also converted into dependency. The way to do the conversion is to check. ,  that you've merged those nodes, then you can check. you don't get exactly the same thing as  the  constituent grammars that we talked about before. But I won't show an example, for . Does anybody speak German? Does anybody speak, Farsi, and they're really into Persian poetry. time we're going to talk about  our natural languages, context-free grammars."
    ],
    "Topic 5": [
        "It's awesome. Oh. wasn't expecting  team one. Oh. And in the meantime, if you have lighting preferences, let me know. we'll talk about syntax and semantics. And also and if you need any help in finding final project members, if you haven't started thinking about that yet we will provide help for that as . But you should go with whatever is officially released. groups of 3, please. I'm gonna summarize where I think we're at in the course, and then you can  I'm if I'm off base. from my understanding of what we've done  far in the course we talked about text classification where we treated passages as samples, ? that was  the 1st content topic. And then with David, you should have talked about text as sequences and looked at sequence, labeling problems. And smoothing hidden Markov models. for the  part of the course, we're going to look at hierarchical structures. We're going to look at some particular characteristics or properties of English syntax themselves. That's great. Often we interact with each other through textual means. And in English. It's 1 view of language. Let me give you some examples. But I can give you some examples of this happening in more obvious ways. one example, one obvious example from English might be. , in other words, you don't pronounce r's at the end of syllables , instead of saying car, you might say car or  there, you would say that. A a common example of this is, if you have , if you want to say, I have an idea of what is happening in those dialects, you might say an idea of what's happening. you might notice this if you if you   time you, you chat with someone from the Uk, you can try to pay attention to the speech and see if they have . How do you say? But you think there is, because it's spelled UN. suddenly the end reappears. ,  if it's a I know it's that you. You tend to talk about utterances rather than sentences. those are some examples of why it might make sense to work at a level. That's not at the level of individual sequences of tokens. then, you can describe it in those terms in terms of hierarchical structures. And  you don't always have to go through linear sequences of tokens, as you might with the Lstms. One key concern is what makes up a valid sentence of a language. There are some strings that are accepted by the language, say English, and they're part of the set of valid English sentences. And then there are other strings which are not accepted, and  they don't form valid English sentences. this is a valid sentence is a valid sentence, a sentence, this valid is, is not a valid sentence of English, and  you can say that the 1st sentence is grammatical, and the second one is ungrammatical. This means this arrangement of words don't form a sentence of that language  one goal that we could have with syntax, and by looking at internal structure. is to specify an all and exactly those sentences of a language which are grammatical. and that comes to  very tricky and philosophical questions about what is meaning and what is a meaning structure have to do and what counts as a meaning structure. Then we'll get to semantics. Panini, which I'm not pronouncing  . Panini. I will say, though, that there's another usage of grammar which is, which is not the one that I'm gonna be concerned with in this class. And if you don't write in some particular way you get yelled at. these are style guides essentially, and they're prescriptive. , it doesn't tell you how a language is as and how it occurs. in this course, we're only going to be concerned with descriptive grammars. Sometimes it's even more complex. and sure we need to have some standards  that people can communicate with each other efficiently, especially in a written form. , if it's a hierarchy that means there are these intermediate nodes and some intermediate level representations. A constituent is a group of words or other constituents really that behave as a unit. you might have heard of some of these terms,  a noun phrase and an adjective phrase. 3 people on the bus is a noun phrase. These are all noun phrases. and you don't just have noun phrases. You can have adjective phrases  very good, is an adjective phrase. and the more tests that a group of word pass, the more likely it is that it's  a synaptic constituent. let me give you an example for examples. For example, you can say I saw it. That's that's a grammatical sentence of English. You can say I saw 3 people on the bus. that's technically grammatical, even though we're not sure exactly what it means, and you might struggle to come up with a metaphor where that makes sense. However, here are some examples of things that are not noun phrase constituents. that's not a constituent. that's the 1st test . is this a test for a noun phrase constituent. in fact, that's  what pro words pro forms. You can use them to refer to something else, and often you can use them to replace an entire constituent. That is translated by Namda solution process. all of these examples are with working with noun phrases. But you can have verb phrases,  phrases, prepositional phrases, clauses, sentences, and  on and  forth. for us, a particular syntatic category will be defined. I will say it's useful to have it if it forms some coherent group where all of these things that we call say that they belong to the same category, they all behave distributionally. Similarly. for verb phrases, for example. They're  pro forms for verbs as   to do . ,  it can get very complicated. And  we can talk about grammatical relations between constituents. here are some that are as very widely known, and probably you've heard of them, and they're related to verbs in particular. But in general there are many more grammatical relations than this. you can talk about what are called arguments of verbs,  verbs can have subjects, and they can have objects, and they can have direct objects and indirect objects. No, here the thief is part of a prepositional phrase. she gave him a good beating him. and,  before, there are many other grammatical relations. These are just some of the common ones related to verbs that you might have heard about before. there are many other relations. verbs and usually predicates in general. You might have heard of terms  intransitive verb or transitive verb or ditransitive verb. Maybe if you've heard of those terms, those are essentially different subcategorizations of verbs. , different subcategories of verbs. You can say I relaxed after a long day at work. relax also happens to be a transitive verb. There are 3 things there  often those require 3 elements, and then they're called ditransitive verbs. Here it's not obligatory. the in terms of what's required by the verb, it's just the subject. Oh, these things? here it means that you need to include a subject. you can think of a parallel to say programming languages if it helps where you can talk about functions. , in natural language, analogously, you can have, you can think of , there are things  this as  where you have predicates which are  the verbs or other things, but mostly verb, for , and they take on arguments, and then those arguments are. I don't think that sounds weird to me. And then one of these prepositions. I'm not sure which one I prefer. I think I prefer from or to. I'm not sure exactly, and maybe in your dialect it's a different preposition. It's talking about what certain predicates expect. And that's part of what helps you figure out if this the sentence is grammatical or not. ,  here's an exercise. Identify the prepositional phrase in the following sentence, and give arguments for why? 1st of all, what is a preposition? You raise your hand? Did you want to say something else? Yes, on Saturday, October 12? th Great? ,  why is it on Saturday, October 12.th   give me some arguments for why, this is a constituent as opposed to just on. Why, it's just on, not a constituent. Why is it not  on Saturday or on Saturday, October. think about you. The whole thing is a prepositional phrases. That's a great test. on Saturday, October 12.th The  assignment is due. you cannot say. On Saturday the  assignment is due October 12.th  you could, but it would be very strange, and it means something very different which doesn't make any sense, I guess, and you cannot say on the  assignment is due. Saturday, October 12.th Great more tests. On Saturday, October 12.th I suppose you can also just say Saturday, October 12.th When is the  time to do? Saturday, October 12.th You can do that, too. that's evidence that Saturday, October 12th is maybe also a constituent. But it's a it's a different constituent. Can you replace this with some  pro form? I don't know. You could say  is due then? Yes, great. Replaces on Saturday, October 12.th , great. And 1st of all, maybe I should say, what makes something formal. for example, who has heard of automata theory? , for example, you can. For  I don't know if I said that before. But we're gonna , go with it. For  to say that we're gonna pretend that we can model English or any natural language as sets of a set of string. And you want to figure out which sets of strings are grammatical and which sets of strings are ungrammatical. they're very practical because they tend to be much faster to , compile and run and process , but  just to use the same terminology again. They can be used for tasks such as stemming and lemmatization and morphological analysis stuff that we talked about at the beginning of the course. ,  that's what we're gonna talk about . But essentially, they talk about what? Possibly they talk about our rewrite rules. in natural language you can talk about. and then maybe a noun phrase. Can we write to  a word  this? It means, or ,  a verb can be  to is a verb. Can you write to kicks? However, can we write to jumps and over? Can we write to rocks? maybe it rules  this would be a string. As long as you have the  left hand side you can always use them to rewrite, and that's what makes them context free. But that's not very useful most of the time in natural language. ,  let us use this formal machinery and work with it, and use it to describe a very, very small fragment of the English language. But I'll just replace the part to do with verbs for the verbs you want to account for. Account for? We want to account for? We can have the intransitive option, the transitive option, or the ditransitive option. Writes to Vnp. We need to come with subcategories of the verbs, because there are going to be some verbs that are that take one that are intransitive,  they only take the subject some verbs that take 2 arguments and some verbs that take 3 arguments. and you might also want to make your nominal structure more complex. , all this grammar can do is say, this relaxes, this steals, this, this kicks this. That's all of the sentences currently admitted by this grammar. For  any questions  far about the example. all clear, obvious, or completely. Unclear fair? Or is this all great question? I'm going to attempt to use the drawing mechanism. Oh, no. Don't want it to be red. 1st of all, is under generation  it misses a whole bunch of valid English sentences. It doesn't know about ,  noun phrases  boys and thieves and balls and wallets, and  forth. I think I used a different, a more complex example when I was thinking about this. you might say that relax and relaxes they should be. They're they're both intransitive verbs, ? And  then this grammar would overgenerate because it generates it accepts the string. If you want to account for what's  in Ingram. we can work on this, we can gradually start to modify our grammar to account for more and more phenomena, and to fix issues with over and under generation. You can say I was a grammar engineer. ,  let's extend our grammar to account for say. prepositional phrases. ,  we want to account for, say, prepositional phrases. And , we need to integrate this into our grammar somehow. and it seems that the most obvious way is to put it in with the verb phrase. This also makes sense, because adverbs, as their name suggests, are associated with verbs,  they somehow modify the meaning of the verbs or the verb phrases. But they're they're useful to account for many phenomena from the perspective of parsing. Turns out to be  everything there are complicated answers. we need to keep the original rules. But I'll let you do that on your own. It's the same idea you have to say, what is a prepositional phrase, come up with the internal structure of a prepositional phrase, and then integrate that into your grammar. Consider the following sentences. The dog barked. I know that the dog barked. that I know that the dog barked. that I know that the dog barked and at infinitum. you can have one branch of it which gives you barked  the dog barked, and another branch of it that goes to  know that something. what this would say would be that it's possible to have sentences of that are arbitrarily long, arbitrarily long. I know that, or he knows that, or she knows that or something, somebody knows that ? It's arbitrary, and really the main constraints are due to our processing power. And  in practice, the sentences are not that long. but in terms of what this formalism suggests theoretically. And and this model suggests that you can have sentences that are arbitrarily long, infinitely long potentially. , for example. Have you heard,  bees, dance in a particular way to signal to their colony about the location of  food or shelter and stuff. , this is all controversial and potentially new, but one way that they may or may not differ from human language, at least many of them seem to differ in human language in that they cannot be arbitrarily long. you can in principle have sentences that are very, very long. here's another exercise. Let's fix the subject. fortunately, English has a very limited amount of subject-verb agreements in the present tense, . in the present tense. ,  let's fix that part of the grammar. ,  let's start with this grammar and then fix it. ,  what is our general strategy gonna be any ideas? the main problem we have is   we can generate the ball. Because, through this branch. , split them into different categories  that they absolutely great. and we'll have 3 v. 3 with an s. and then v. 3 without an s. And that means we have to duplicate all of these rules. Oh, no, these are all. And that means,  we need 2 versions of this rule. we're going to have 2 types of nps, those kinds of envy Nps where you they have to take an S on the verb, and the  Nps where they have to not take an S on the verb, ? But I, for example, would not. , for example. boys, girls, waltz. Do you see why I hate my terminology? Oh, no. Oh. I will use 3 s.  . the general strategy then. How do we incorporate that into the grammar? Usually it means that you have to split up a category into multiple categories. that's the general strategy. through a feature-based account of a language and properties of words and grammar rules, and  forth. And they should take she this they should teach this in linguistics, but I don't think they do. far, then, we've talked about. Grammars are just a different view of the hierarchical structure of language. But it shares  this basic property and assumption that it that it's hierarchical. That is tricky. you can think about . some of it is through which elements specify the properties for the whole phrase, the whole constituent. that's 1 aspect that helps, . And there's a preposition, prepositional phrase, argument, relation with this park. it's not  obvious, ? And  if you're interested in understanding the semantic relations between words and noun phrases, and you need it for further processing. , let me do this example. that's a constituent. Similarly, the exam. you need to. You need to figure out the label you. And it turns out you can do things the other way around, too. Possibly Farsi a little bit. And this is an example from German. You can say either. These are both sentences of German. I think we'll end there for ."
    ]
}