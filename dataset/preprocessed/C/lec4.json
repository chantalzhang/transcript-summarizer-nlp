{
    "Topic 1": [
        ". All . All ? we still examine different text classification techniques and then we examine at least three. All . Is everyone clear with the exercise, the one we did last time, or do you still have questions with the last exercise? . If one. Time is zero, . For the time where you have zero, , that's a really good question. you can add a very small term  10 raised to the power -10 for example. There is additive smotting, and there's even more. All . Maybe I wouldn't display it, but I will try to open it on my computer. All . if you have not installed psychic line, you should install it and start playing around with it to be useful for your assignment. . . Will we be building our own model? . I think probably I've seen this in the last class and I think there was a question on the normalization of constant Z. it's defined difficult. most of the time if you look at materials online, this is the  formulation you will see. If you replace it by Z you can get the exact formulation we have in class. Yes, it's a constant. it's a normalization constant. But that's constant. what is the constant value? . . I'm trying to tell you that this Z you can reformulate what we add in the format class to another notation, which is 1 / 1 plus EU raised to the power minus U. Do you get it? Z is not equal to,  Z is equal to east raise to power U + 1. But if you rewrite all this formulation, it will be equivalent to 1 / 1 plus E raise to power minus U. . But of course you can generalize this  that it produces vectors or even matrices. while neural network was not very successful in the early 90s was because the models were not trained on large amounts of data. And one of the tasks that  demonstrated this is language model. We probably know about Mila. And also it's  transformed the field. the idea of the feedforward neural networks. But for the feedforward neural network, the idea is that everything just moves forward. But of course, nowadays there are better ways to initialize the network  that you can get better performance. for the feedforward neural network, I've been talking about weight matrices or your weights. Every word can serve as your X. This is a vector. , . I think there are many functions and you can just give me random numbers, flows numbers from -1 to one, and then to generate and fill up your matrix with just random numbers. it's just weight is just  a random number random matrix. And but you don't do this every time. It's a vector. . the. it's  you have a vector X1X2X3X4X5 and then you have a weight matrix. , that's a good question. All . We don't know why neural network worked the first time. And especially for computer vision, this rectifier seems to work very, very . , . An example of an NLP task that you have multiple outputs is if you want to do part of speech tagging. If. You can do character level tokenization. But for some other task  where you have  word level tokenization, the number of types will be more. any. Will. depending on the task. In the case of path of speed tagging, maybe the number of types is just how many path of speed tags do we have. You have  18 types or path of speed types. , it's for every class. And then you then you have  something similar to what we did the other time. You have  something called a normalization constant, which is the sum of all this probability, ? . . What Yi? . . And then you have. This will be your Yi. You have something  this. if you want to select what would be the  value, then it's very clear for the proposal, ? That this should be the  . The idea is that when you have a neural network, you have a predicted outcome. And you sum it all over every instance of Yi that you need. , just  a constant. . how do you? instead of doing sum over all the samples in your training couples, the alternative would be just to take a small mini batch of the training corpus and update the weight. And for SGD, it's just the mini batch is just the size of one. You can  parallelize things over the network and that's why it's been done. And then you do this over and over again. . how is backpropagation done? this is the idea of the chain rule. I've been hinting about this a couple of times. Because here we are talking about derivatives. But in NLP we are operating on discrete values. But this can also be converted to A1 auth encoding. . A is a word in English to some, I don't know, zebra. for one not encoding, , you set it to one position 33.  that means you can have a matrix definition of your vocabulary. there's no way to know if 100000 and 0001 are similar. If you are able to find a good representation for every word in your vocabulary then you don't need to levertize and then you can learn everything using a neural network. Similarly for sentence representation, once you have a vector for a word, if you want to represent a sentence you can just Add all this together ? But this probably this is not a good way to fully represent a sentence. what? , for the word tokenize. , OK. Modern trends in neural networks for NLP Nowadays what we do the field has changed a lot. Large scale pre training is an example is this. And this is how techniques  birds model, Roberta model has been developed. And after you are training language model this way, you can fine tune this language model by removing the Max language model head because you don't need this again and replace it with another head for the classification. You can train a model for multiple tasks at the same time. GPT can do many things. It's not only able to do generated text, it can also prompt you to do classification. It can prompt you to do even regression tasks. that means you can train a neural network to solve multiple tasks at the same time. for the case of computer vision, the Imagenet data set that was trained on CNN can also be used for other tasks  object detection. You can benefit from large scale pre training and then just fine tune this for your downstream task. And this is very important. Another difficult thing is that neural networks tends to work very , but it's difficult to interpret. Why should we be investing  much time on linguistics? And of course, which will be, it will be cool to be able to develop new tasks and challenging tasks for NLP. The C plus evaluation metric you can use to evaluate your model if it's good is accuracy. Or 0.7 if it's one. this is always not good. weights of each class are important. , I have an exercise for you. if your classifier is doing a good job for Class 1, the  one will be on the diagonal. this is your exercise. . . you can do that at all. But I guess you already solved it. ."
    ],
    "Topic 2": [
        "OK.  again for starting late. , thank you. Naive base. And then we apart from naive base, yes. SVM, yes, there's another one. Thank you. we spent a lot of time on naive base and I want to assume by  you probably have gone through the lecture notes and redo the exercise to understand how to do the calculation. Yes, yes, you can have some bias in a data set. the data set is very similar to the test sets, but you still have to do the calculation. But that's a great point. All , hopefully, I think we'll also treat smolting in this class. yes, 1 / 3 rather than 2 / 3, yes. the reason is that we are trying to compute the probability of dozen review notes and then we are trying to compute and the probability of dozen review notes. Is that clear? I think programming assignment one will be released. you can try out different techniques we studied  naive base, SVM and  many techniques. I think they would have there will be a tutorial to guide you if you are not able to install the  packages and some probability basics and review. it could be naive base, it could be logistic regression, it could be neural network, it could be perceptron, SVM, anything. And we and it could be  any classification task from basic tasks  trying to identify if it's a spam or not spam or this is a positive review or a negative review and  many tasks. because the logistic regression formulation is very simple and also  about the naive basis, very simple assumption that some of you were able to give examples why this would not hold, why independence assumption would not hold every time. But nonlinear models are more robust to model different inputs. The best way you can turn this into a probability distribution is to use a function that  ranges from zero to 1. I hope that clarifies it. And then this Z will be used to normalize it  that you can get the probability distribution. , yes, , . Distribution. Figure out the Z. That just to ensure the distribution or calculating it. that means you don't need to calculate Z again, do you understand? Do you get the point? , is to turn it to a probability distribution. It's a  learning model which automatically learns nonlinear functions from input to output. And then you could even have this weight to be a tensor. And that's why if you have less data for your problem, you should probably just try these basic approaches  perceptual  logistic regression and naive base and  on. , but if you have infinite amount of data, I shouldn't say infinite large enough data, then I think neural network will be able to model it very . when you're trying to predict what is the probability of the  token given the previous or the context or the probability of the old distribution of what's in your corpus, the more data you have, the better you are going to be able to model this. And this has been responsible for many applications  language modeling. I told you and one of the success stories and it's good that our community is also part of the success story. If you want to do a classification task, then you still need to use a function to convert it to  a probability distribution  that you can say this is the  class. Yes. Or you can typically you can also set this to 1A constant, , or. And then you use a technique called backpropagation to improve the weights  that it gets better to be able to map the input to the output. All , I hope that answered OK. , it's  an intercept. You pass it through an activation function of G Yes. the dimension also will be the it's a dimension of your layer, the 1st. And then you pass it through another activation function and then the last one is  Y equals to H2 W 3 and the dimension of Y will determine if it's a multi class classification. Is that clear? And another one that was popular was tan H And the reason is because I think we showed you  very nice plots of sigmoid because it can model everything from zero to 1. And then people are trying to go slowly to understand the different process. Yes. if you cannot linearly separate your data in a high dimensional space, then it's nonlinear. think about the formulation. , OK.  at the last layer, I told you that at the last layer, depending on the number of outputs, we have treated a very simple case where you have binary output, you have y = 1 or y = 0. You are not familiar with part of speech is just very basic,  part of speech in English language or I don't know French language or any other language that you can distinguish between a verb and a noun and a proper noun and a punctuation and  on. , you don't have two outputs. And in that case you need a function that can turn this to a probability distribution. And in English language you can have 26 characters to be your size of your vocabulary. And then you can also add some punctuations, some other common symbols. And for this you can use a software function to convert the output into a probability distribution. Yes, what is what? Is that clear? For binary, you don't need to probably do this because it's very simple formulation. But for multi class, we typically use  the softmax and it's very important in the development of many. the problem is that when you have your Yi, you don't have a probability and then. you can have cross entropy for a binary case. You can also have cross entropy for what do you call it for a multi class case. You can have one times log of what was predicted, maybe 0.5 + 0 times the others, ? OK, this is the most technical part of the lecture, but this is also very simple if you understand the chain rule of derivatives in calculus. Is this clear? there's a function blocking U to assess what you want to compute the derivative for. Is that clear? the simplest way you can convert it is what is the position of this word in the vocabulary. Yes. If someone says a bank, it could mean a financial institution, it could mean Bank of a river. you need something that captures the context in which the word appears. And pass through a model, yes. If someone says this is a dash bank, you want to train a language model to be able to predict the blank space or the Max token. You pick an existing model and then you fine tune it on a downstream task. Advantages of neural network. And one key thing is that it reduces the need for future engineering. you don't need to do lemmatization and stemming and all this feature engineering again. And that's why you have a model  CHAR. I've only showed you one, which is the learning weights. Previously, you cannot even explain what happened in the network. You don't know what happened. There's sometimes when if you incorporate domain knowledge, you can  boost your performance than a lot of engineering with neural networks, how to do better, multitask and transfer learning to new domains, to new languages, to new setups, to new tasks. How many correct predictions over the entire test set? Some of these have been motivated from information retriever,  computing. This is the idea for micro average, the sample equally, that's the micro average. Yes, yes, yes. Are you sure? Half. OK.  micro average, micro average will take you some time to calculate. Thank you for joining the class. Thank you. in an architecture  this where?"
    ],
    "Topic 3": [
        "can you mention them? Perceptron. Maybe the answer will be very obvious  even. It's possible that you ruined the probability that what probability of Y equals spam and Y equals non spam. let's assume you do a count. Let's assume you do a count of what is the probability of a certain word in your token? And it's zero. And then this will make it make it to be non zero. I think the reason is, let me try to open that. You have no just appearing once out of the top three possibilities of grade equals a. And the good news is that it will be on test classification. All , yes, I think T office hours were posted on edge. if you need help  with programming or some concepts, you can reach out to them. And the idea is that given an input X, we want to learn a function that will give us Y. The way is this coming from. the intercept B, ? if you let's assume this is U, this is the original formulation of logistic regression in terms of sigmoid. The idea is that you want this probability for every variable for every Y you want to calculate to sum up to 1. Let's substitute everything we want to calculate as U and this will be 1 / 1 + e raised to the power minus U.  if you multiply the numerator and the denominator by E raised to power U,  you can transform the first formulation to this and then you can replace because everything EU raised to power U + 1. Do you have questions? it's a constant that needs to be calculated. , you cannot assume any constant, ? This constant can be calculated  that you can normalize it to form a distribution. And you calculate it. How exactly do you calculate Z? Train the model or run this calculation. Because it's equivalent to this. How do you need to calculate it? each neuron takes a scalar input and produces a scalar output. X1 is just  a scalar, and a neural of X would be a scalar. This is possible. simpler models  SVM, excel where you have less data. And it just gets better. And that's why the more people throw more data. And  the models are trained on trillion soft tokens. And the more data, the better is the performance in general. because the more data you have is just the better the performance. And there's a lot of research on how to do initialization. You have a vector of impute, an example of a vector of impute. Just assume you want to classify if a sentence is spam or not. Let's assume you have a five word 5A sentence with five words and then you have X1X2X3X4X5. Usually it could be the same activation function. You mentioned randomizing the initialization of the weights, that we do that for devices as . you're just keep on changing the weights and then adding a bias occasionally in order to  go. Remember, we're trying to learn a function that correctly maps the input X to Y. if you multiply X.  let's assume. One of the dimension of the weight matrix will be equivalent to that of your input and then the other dimension you have to. And then you can add them together. For example, if it's the first layer of your network, that's a dimension. If you have four classes, then it should be the dimension of your last layer by the dimension of the number of outputs that you need, ? All ,  the activation function, there are many choices of activation functions. We can guess. This is why we, this is what we wanted to achieve because we want it to be able to learn inputs that are nonlinear. in linear data, I think, I think it's very clear with the SVM formulation, something that you can just draw a line to separate the data and then you're gonna have  a hard margin and a soft margin that  separate them in a high dimensional space. by the way, SVM can also work for nonlinear data if you use kernel functions. I can show you an example on the board. In those scenarios you cannot really separate it  with a line. But this one, you try to concatenate different layers together. that means every single word in your vocabulary you want to classify the tag. in that case you don't have to refute the size of your output. The size of your output depends on the size of your entire vocabulary, the number of types that you have. in the case of parts of speech, the output will be the number of the types you have in your vocabulary, . Let's assume your vocabulary  is 35 when you Add all the symbols. your output, let's say you want to do categorized characters, your output will be the size of your vocabulary. You can have the number of types to be 50,000 which is your vocabulary size. in the case of language model, your input vocabulary size is equal to your output vocabulary size. It could be any word in your vocabulary. you need to have the same size of your input as also at the output. your input will still be different because it depends on how many words you have in your vocabulary. , K is the number of class in your output layer. In this case we use a random variable X. Let's assume we want to do output. Our output would be Y.  for every, let's say we have in part of speech, you have 18 types. It's not very complicated. 18 types at your last layer and let's say you do this neural network, you have another one to 22 and at the last layer you have 22 to 18.  you have a probability. you have the output because these are scalar values. Or let's say 2, you have 0.001, you have zero. to make it sum to 1, you have. Converge into your probability  that you can select processing and that's why you need something All   , if you want to do the optimization you need a loss function  one of the most popular loss function that we use is the cross entropy loss. But it's a very simple idea that for every Y you have, you try to compute what is the cross entropy between the Y, the original Y and what you have predicted. A very simple example is if you have an output of one or zero, you can do the calculation. And one of the algorithms that's used till today is back propagation and one of different artists talk they have tried to replace it but they have not found a good alternative to  since 1986. All your width matrices can be  categorized as Theta. It's as simple as that if I remember correctly. for the SGD overview, the idea is very simple. You have a function of the input and the weight matrix you want to learn, and you have the training samples SK YK. And you can repeat the same thing for W one once you have access to the last activation function which is W one with respect to data of W1. I think it's important to treat a word representation. Everything is a number, ? It's either a cat or dog. let's assume you have. You have 50,000 words in your vocabulary or types in your vocabulary. You can rank them alphabetically from A. Let's assume this is the last word in your vocabulary. You sort it and you number them. The first word is the first. There are other ways of what is called component wise vector multiplication and there are also more sophisticated options  concatenation. You can just concatenate the word representations. No, we are  different word representations. what Vec, I don't know if you've heard about that Glove embeddings. And  we have  sentence embeddings because people find there's very obvious limitation of word embeddings. And that's why nowadays we focus more on sentence representation. What we do  is we do what is called large scale pre training on a big corpus and then we do fine tuning. And this is called pre training fine tuning paradigm. apart from live pre training, we have also made a lot of advancement in hardware because  we have better hardware that can train on a large amount of corpus and we can also highly parallelize these operations on GPUs which have better functions than playing games. There's also more efficient use of input data via with sharing. It needs a lot, a lot of data and by  you should be aware of that. And then you have things  number of eating neurons. Nowadays things are better because you can ask to explain the prediction. It just gives you the answer. neural networks for NLP, there are many open questions on how to use linguistic structure. Some people say let's just forget about linguistics and there's still debates at top AI conference. Let's assume you want to do spam classification. And the problem for spam classification is that spam appears  rarely,  it's very rare. What is a precision you can say out of what has been predicted? if you have a spam, even though you have 5 out of 500, if your classifier misses all the five that are spam input, you're going to penalize the classifier heavily because you have what is called a macro average. This is the confusion matrix which I already drew on the board. But you can have it more complicated than this, where the  answer will always be on the diagonal. This is very simple and I already gave you insurance on the board. Just divided by two. On the bot, but it's in the bot."
    ],
    "Topic 4": [
        "OK, Can you hear me? All ,  today we'll move to Lecture 4, which will be on nonlinear classifiers. Very briefly, who can remind me what we do? After perception then , Neural networks. OK, let me give you an example. And there are a lot of smotting techniques. There's a nice smotting and there's souvenir smoothing techniques that we typically use in NLP. if you add the smotting, you will still be able to rank the probabilities to be able to decide which one should have the bigger class. OK, OK, let's move to today's lecture. My guess is that it will also you also require  tools  NLTK or psychic line. The best answer is with to see the assignment  that  what exactly you should do. OK,  , tutorials come in  few weeks. today we're going to continue again on that and then we'll talk a bit more about neural networks. The major difficulty for linear models is that they cannot learn very complex imputes or any example that is nonlinear. And that's the major issue with linear models. let's substitute everything we calculate and the Theta values we want to compute A1A to AN&B. , if you're able to compute,  the idea is that if you're able to compute all these parameters, you can get the Z, ? Each time you modify. And you have , , , once you have the values A1A2 to AN, you can calculate Z, , yes, it's the better of guessing those parameters, . E to the power of U because U is all these terms and U is dependent on the values of A1A2 to AN, ? Then you don't need to think too much about what is caused at Z. All ,  let's go back to artificial neural networks. artificial neural networks can be said we  biologically inspired. this is a very basic idea. I've been following the GPT story. And GPT 2 was trained on  40 gig of data. all the professors that have been working Asia Bengio has been working on for many years and when it became successful, they just applied the same principle for language modeling through the same architecture to speech recognition. And also the same thing for objects detection. The only thing that is going back is when you back propagate the loss and you do back propagation, then you need to propagate back propagate the loss just because you need to update your parameters and make them get better  that you can get better output. You get an input and then you compute. And then because you have randomly initialized weights, those weights are sometimes small values from maybe -1 to one. And initially they don't mean anything because you just randomly initialize. that's for the initial phase you randomize, you randomly just put some random weights. And your learning phase is  what will modify this width matches to get better in predicting the  output. OK, give me one minute. I can just repeat what I said. Is that correct? and then you do the same thing. And then you can have a maximum of 0 and X.  that means you can have a simple activation function called the rectifier, which also seems to work better than Sigma functional time H  there's a lot of research. , I don't think not more this time, but there were a lot of research then on what is the best activation function, What is the best way to initialize the network Because everyone was lost. Kernel functions  takes it to a high dimensional space that it's separable, but in a nonlinear way. Yes,  I didn't get your question. But there are some cases where you have multiple outputs. Is that correct? We tokenize all our characters in English language by characters and every element in our vocabulary. A would be  a token, B would be a token, C would be a token. Work just for the work it will work. For example, you want to predict what is the  word given the previous. If you use UDB, we have  universal dependency. But your output will be 18 because you have  18 path of speak tags and out of this 18 you need to determine what is the path of speak tag for this word ? It's the sum of all these values, ? OK. And this can work for binary. imagine you have a vector of output. you have maybe 0.1 here you have 8, you have 22, you have 19. You have different values. This doesn't sum to one, ? You want to. you can  you need the loss function to know how far away are you from the words from the correct answer. An example, this is one of the most popular loss functions used for classification. Classification is cross entropy loss which is coming from information theory. this would be your first loss from the output of the neural network and then you back propagate it and then you continue. All ,  for training neural network, one of the popular approach is the gradient descent. and the idea is that you find the gradients of loss functions with respect to the parameters of the network and gradients it's from derivatives, you have to do the derivatives and then  which direction, what's the direction of the gradient to modify the parameters of the network. If you do this over many iterations, you improve the values of Theta and then you can get a true Theta that can correctly map your input X to Y. OK, Do you have a question? Do that when you're back propagating when. You're back propagating I. , I think it's  for stochastic gradient descent. the difference between the original gradient descent or the stochastic gradient descent is that for the gradient descent, , . you randomly initialize it, and then you modify it to get better. for the gradient descent you do the sum over the entire couples. you compute the loss over the entire training data. But for stochastic gradient descents is that you can randomly sample one and then you compute the loss over this one. You have a loss function of L and the idea is that sample a training case XKYK and then you compute the loss over this training case and then you compute the gradient and then you update the weights of Theta. And of course, while doing this, the only important thing is that you also save the values of the inning weights. You save it somewhere  that you don't lose the values because you also need them to perform the derivatives. the idea is that you if you want to do back propagation from the output Y to go back to the input, you have to do you have to compute the derivative also from the output back to the input. if you compute the loss, the derivative of the loss over W3. Do you have a question? it's  you want to compute what is the derivative of sine 3X and then you have to 1st compute what is the derivative of 2X. you compute the derivative over this and then you compute the derivative over what is inside and then you multiply the derivative. And then you do the same thing for if you want to compute the loss for if you want to compute the derivative of the loss with respect to W2. Again, you start with the loss over G3 and then the loss of G3 over G2 and then the loss of G2 over W2, because this is what you want to get. You want to ask a question? Every other thing would be in between here. does not mean something useful because there's no notion of similarity. If you multiply them, you compute cosine similarities. We give you 0 and that means we need to come up with a better representation for every word in our vocabulary that have the notion of similarity. And that's why I think in the early 2000s we started working on word embeddings. How do you have a good representation of the word  that you can have notion of similarity, You can know what is the difference between,  you will know the similarity of different words or different types. , no, you don't need limitization because if you are able to find a good word representation, the similarity between use and using and used will be very similar. They will have high similarity. I think we, we need word embeddings just because we want to know, we want a better representation for words  that there is a notion of similarity between words, ? For example, you cannot, you cannot do word sense disambiguation very  with what embedded? Can we have  a better representation of the entire sentence rather than the word? We don't really spend a lot of effort in learning better word representation. Just give me one minute. if you train this, this is an example of what we call maxed language model because you are trying to fill in the gap. And if the language model is able to fill in the gap properly, that means it has learned a very good representation of many words in the large text composure provided. We call this pretraining. They're building better deep learning models. the first one is you are able to learn better relationship within input and output. even something a bit far away from what it has been trained for. Another example in NLP is that if you train a language model or a mass language model on English Wikipedia, you can also use it for other tasks  sentiment classification, topic classification, question answering and  on. And this is the idea of transferring. there are other challenges on neural networks. There's no notion of interpretability. the last part of the lecture is the evaluation metrics. if you have 10 examples and your classifier is able to predict 7 correctly out of 10.  what's the accuracy? if you use the notion of classification, the notion of accuracy, your classifier can  just ignore all the spam output and still have a good accuracy, ? there are better ways of thinking about this. What is correct? And you can talk about the notion of recall that out of how many are correct of the entire class. there's a way to easily distinguish it by building a confusion matrix where you can have true positive, true negative, false positive, false negative. your precision will just be true positive divided by true positive plus false positive and your recall will be true positive divided by true positive plus false negative. apart from that, you can also compute what is called the harmonic mean F1 score because this error is better than accuracy for many tasks just because there's a notion of different  errors that your classifier can make by computing this precision and recall. That means you take, you want the classifier, you want to treat all classes equally. If you have the micro average, that means you want to sum over what's correct, and then you have weights. You have more values there, and for Class 2 you have more values there and  on. in this example, what is your true positive? In this example, five. And what is your true -20  you can compute the precision. 1/3 is correct. , that's correct. That's correct. that will be what 0.85 zero .285 + 0.616 / 2 And OK, what's the accuracy? accuracy, I don't have the notation on the ball. accuracy would be true positive plus true negative divided by the entire thing."
    ],
    "Topic 5": [
        "OK. , everyone. , I think we can start. who can remind me of what we did on Monday, Lesson 3 or Lecture 3? Logistic regression, yes, . OK, I think there's another one . In the case of binary, when you do the calculation, we'd give you 0.  there's a way to address this in NLP, at least in statistical NLP, and we make use of what is called smoothing. And smoothing is that you had a very, very small term to the probability  that it's non 0.  you can add. , because they say it doesn't. The first announcement is that assignment one. I think. OK.  we're still continuing with this very simple notation. And that function can be anything. I'm going to do a quick recap of logistic regression because I think there was a question on the notation which I want to clarify. this is logistic regression. one thing you have to note about logistic regression just  any other linear model because we are trying to we covered linear models last time and today we are going to cover nonlinear models. just to clarify, in the case of logistic regression, you are probably more familiar with this notation by 1 / 1 + e raised to the power minus U. Are you more familiar with that for logistic regression? And this is called the sigmoid function. everything you calculate in the case of logistic regression, you want to look for a certain weight that allows you to model your input X to give you Y. And that weight in logistic regression can be referred to as A1A2 to an and your bias term. this is your weight or what is called the Theta parameter for logistic regression. An example of that function that is used for logistic regression is a sigmoid function. it's not  an hyperparameter. , these are parameters. You need to find these parameters to be able to calculate it. It can be generalized to that. theoretically, I think probably there's a paper that proves this, that any function you can model with a neural network and that's why the more data we have, neural network has not disappeared because it is able to model any function provided you have large enough data. GPT one came the train on, I don't remember a few gig of data. It seems to work through the same architecture to machine translation and for every task it just excels. You probably didn't learn about Imagenets, which is a very large data set that was trained on, I think convolutional neural network. there are different kinds of feedforward neural networks, , there are different kinds of neural networks. We have the feedforward neural network which is and then we have recurrent neural networks which we are going to study in the subsequent classes. Nothing is going back. you randomly initialize your weights and then you use that weight to compute what will be the output at the  layer, which is the hidden layer. But for a simple just to clarify things, imagine you randomly initialize all the weights and all you have to do is that you multiply the input by the weight matrix. You get the output and then the output will serve as input to the  layer. You multiply the weights and then gives you another output and then the output will serve as input to the final layer which is the output layer and then you get the output which is your Y. You can mathematically formulate it as this. Then you multiply it by your weight matrix. what is in the middle here is  a weight matrix and then plus the bias term and then you  pass it through an activation function G and then you get H1. Then H1 will serve as the input for the  activation function. H1 will  be the input for the  layer and then you multiply it by the weight of the  layer which is W2 and then you pass it through another activation function. Zeros. You just do this once for the initial stage. , it's  a bias term. All ,   you can do the math. It's  a hyperparameter you have to define ? here it's  multiplying 1 by 5 matrix or vector by another matrix that is 5 by 8.  your output will be 1 by 8 and this one by 8 also will be the dimension of your bias term. you can have for the first one you can have a dimension of 5 by 8 and the  one you have a dimension of maybe 8 by 22, and the last one we have a dimension depending on your final output can  be 22 by two because you need to project it back to your output. each one will  be the input and then you multiply it by width matrix 2 plus the bias term of two. for many, many, many years, people use very simple activation function  sigmoid. And many years after when we add what is called the revolution of neural network called deep learning, and then people just make this much, much simpler. OK, why do we need nonlinearity? Why do we need nonlinearity? yes, linear and non linear data, that's a good question. we should see that you need  a non linear function that can  separate your data. The thing is that if you pass it through this function, it makes it nonlinear. This is  you can simplify this to be something  a logistic regression. If you don't have this big weight matrix, it's just a linear function and it is at the last layer that you try to add the probability. as you are passing from one layer to the other, you need a function to make it nonlinear. How many people are familiar, are not familiar. at the output layer you are predicting values for all the 50,000 and this is a problem in NLP because  you have a huge output layer that you  need to and  you want to determine what would be the tag out of all these 50,000 types. No, there's no limit. if you have 50,000 at the input, you also have 50,000 at the output. , I and OK, I  OK. for every type in the part of speech, will be your XI, will be your Yi. But you need to compute this probability over every Y.  you're going to have a probability for every item in your output layer. It's very similar to what we did for the logistic regression. , it should be softmax over Yi if Yi is your output. This might be a silly question. What is the XI? What is the XI? It's just E raised to power XI. OK. OK, I need to use it. you take this over a soft Max function of Yi and then you convert everything to a probability. eventually you have soft  parameter of this and then you have this will be  minimized and then this will  be maybe 0.002 and  maybe this big value of 22 will  be  0.3 and you have other values  0.02. This is what you get when you do the feedforward neural network and your output is called this Y dash and then you have Y which is your gold standard correct answer. You can still use it  in 2024. The idea is that you have this is your parameter Theta. And then you perform what is the derivative over Theta, and then you have the learning rate. And then you subtract the Theta from the product of the learning rate, multiply by the derivative, and then you improve Theta. Think the derivative. The weight of every layer is what you randomly initialize. The only hyper parameter here is your learning rate. It's as simple as that. And if you do this over and over again, it can lead to faster convergence because you don't have to. if you want to compute what is the derivative of West three, you compute the derivative with respect to the weight because you want to modify the weights matrix, ? you compute the derivative with respect to the weight. this is  this means you are computing the derivative first over the activation function before you can compute what is the derivative of the activation function with respect to West. And if you substitute 2X to EU and then you compute the derivative of sine U and then you multiply the derivative. OK. . here when you are talking about words as the first question is how do you convert words to numbers? there's a need for you to convert every single word into words, a representation. And if  the position of this word in the vocabulary, you can say whenever you see word dog puts it as #33 in my vocabulary of 50,000 tokens, whenever you see word X-ray puts it as number, maybe 49,000. . The first index is A, the last index is 50,000. , that's a great question. It can be used, but there was word embeddings before attention is all you need paper. If you have a single representation for bank, that representation is probably biased to one of these two definitions, ? It's different to attention. for any NLP task you want to do nowadays based on neural networks, you're going to start with a pre trained model. You're not going to train everything from scratch again. And this concept is very related to transfer learning, which I will cover in 2 minutes. And one of the good thing with neural network is these two things I want to mention is multitask learning. you don't need to just train a model for a single task. It can do many things. Another nice thing is you can do what is called transfer learning. That means you train a neural network for a particular task and then you can initialize weights of your new model for the new task you want to solve and then do transfer learning. That means you don't have to train for every task from the scratch all over and all over again. Also, there are  many hyper parameters to tune. But there are   many hyper parameters because better algorithms have been developed that and  you don't only have one learning rate to do hyperparameter tuning for, you have momentum, you have other weights, you have decay parameter and  many parameters. What is the function to use? And when is linguistic feature engineering good? This is very simple. That's 70% ? you can have  maybe 5 spam out of 500 examples. Because it's just about the correct prediction. I think this is very important to distinguish between precision and recall. I don't think I OK,  let me try to do that. Then it's easy for you to compute. if you have a simple confusion matrix, that is very clear. And while doing this every you can do what is called macro average, which is a very important concept. That's the idea of macro average. You have weights for each sample equally,  everything that is correct, you combine them together before you do the average Y for macro average. For each class you compute the F1 score and then you have equal weights for both of them. you compute what is if you have two classes, spam or no spam, You compute what if the F1 score for spam, what is the F1 score for no spam? And then you say F1 score of spam plus F1 score of no spam divided by two. OK, confusion matrix. All , OK,  this is a task for you. what is the,  what is called? what's the precision for this? What's the precision for spam 0.2? , 0.25 is correct. What is the recall one third? And what is the F1 score for spam 217 2 / 7 and that's what's 2 / 7? OK. What is the F1? What is the precision for non spam to third? What is the record for non spam 0.57? OK, all . what's the F1 score of non spam 0.615? what is the macro average for this? What is the macro average for F1? that means you have to do the precision. You have to combine the F1 of spam and F1 of non spam macro average is equal distribution, ? This is very simple. How did you get half everything? , great. OK. The macro average. OK, I did some solution. I had a quick question about softmax being the output layer for RNN."
    ]
}