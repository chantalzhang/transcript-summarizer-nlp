{
    "Topic 1": [
        "but , I would go there. We'll continue our discussion about part of speech tagging. And  last time we are trying to compute,  Emily. We use, , , pr ovn  they have various different symbols. And also, , we made that  assumption which we call the Markov assumption. We also call it the Markov assumption. here we  relate this to Markov chains. 1, which is your 1st part of speech which you can compute, based on uniform probability, that how many times does this part of speech occur, or how many times does this part of speech starts my sentence across the entire compost? And then you multiply that with the State transition probability for it to move from one part of speech, from the previous part of speech to the  one, and then you multiply this with the emission probability to  generate a word given the tag because there are many possibilities. And we also have the emission probabilities. which is the probability of generating award given. Just because we'll leverage on this in this class and here to compute your pi high height. High equals the part of speech divided by all the possible sentences that are in your training couples. And we can also compute this with very similarly, assuming this is  a diagram. distribution where we can count how many times you have in our couples, how many times do we have a tag? I, followed by a tag. and the last one would be the emission probability which we can also compute as number of times you have one K. Contact. but I'm dividing it by how many times we have qt equals I in our compass. in the last time we have. On the observation which is  we are trying to compute the judge probability given the parameter of theta. why didn't we include the new one. That's already  4 possible bots that you can have. we're trying to create  a table of all possible state sequences. And then we go step by step. the values inside the States can be referred to as what is the probability of observing from one to T, because for every State take a random one there for every State we are saying that just think about every value you select from a colon. In the time step of one. ,  and here we want to copy what is the probability of the observation from one to the to that time where you are to that time step. And what's up to ? If what is by J. . Yes, I don't understand why we can multiply by zoom. look at what you're trying to compute probability of 4 1 wants to see. and then key of T, ? Do you remember we factorized stuff? Which is the pi eyes, or, , that would be your pi, j's. based on what we have done. And then if you do this over and over again. this is   the formula,  where you are. I don't know which one works better for you the algorithm or the training. And the marginalization is very similar to what you have in the because when you're when you get to the 1st  you're going from the back to the 1st state. that's the back order going there. if you are multiplying all the probabilities together,  you see here, we keep multiplying the alphas and the betas together. If you're implementing probabilities and then you multiply a lot at some point, you will get some value very close to 0. they're different from the one the same age in the? you're talking about the normalization. this is an assumption of the algorithm. and then you have the summing. But something very similar to this. and then 2 rest of our heads. You'll see that they  books. you see that they're  equivalent. , , if you're implementing it, they are not equivalent. which  you can read up more is very similar to the forward. But when you  do this calculation, you have to also keep track of where the maximum entry to each cell came from. We  would do this together. we have the 2 parameters? And the is the emission probability. you have the emission probability. and the better it is together. Think it's  easier to take a picture just all . what are we going to do based on all the information you have on the board? What is the formula for the 1st state you have to compute? what is the initial step of abilities which is your pile here. This will be 0 point 1 times your initial probability, which is B times 0 point 1. you multiply the 1st pi high with the force emission probability, because B is your emission probability. Oh, I'm good is 0 point 1.  this will be 0 time, 0 point 2 times 0 point 1.  for the second one, what should be the value? this would be the second state probability that was 0 point 5 times 0 point 5. You did 0 point 3 times 0 point 7. . Seat is 0 point 3, I agree. We are still at the what do you call this? For all this establishment, 0 point 1 0 point 5 0 point 7, and the initial set probabilities are also given as 0 point 2 0 point 5 0 point 3 is the 1st color play. Minus one would be 0 point 0 2 0 point 2 5 and 0 point 2 1.  you're going to take each of these and multiply by the Aij and Bj of ot. for the 1st one we take 1st X, we go for 0 point 0 2. , what is the Aij 0 point 5? And  we have 0 point 0 2 times 0 point 5, which is the 1st States there times 0 point 9, and then you add that with 0 point 2 5 again times 0 point 2, which is the second value times 0 point 9. ? you only multiply by 0 point 9, because here, this is Bj of Otp. we go to  this is B of one at timestamp of 2 ? And then if you go to Dj of 2, a time step of 2,  the value will change. you can compute the second Val the second cell, which would be 0 times 0 2 times 0 point 4, which is for the Y column times 0 point 5. . which is, you multiply 0 point 0 7 2 9 times 0 point 5 times 0 point 9, and then you had it. you are multiplying with the same state value. And that's why we are multiplying by the same value. But in the case of the 1st one, you'll find out that we are  multiplying by a different value. We don't have 0 point 1.  instead of 0 point 1 in the 1st column we have 0 point 9, because the symbols are different. And at the end of the day you can sum if you sum everything you have your period of all given to them. You still have one, because your contribution from the last column is one, then this would be one times 0 point 5 times 0 point 9, because 0 point 9 is what you are emitting there. plus one times 0 point 4.  if you observe in the last column. you find out that you multiply 0 point 5, and then the second,  you multiply 0 point 2, then you multiply 0 point 1, and for background it's very different. and also the better one. We may not be able to go through the calculation step by step in a class. They have a clear understanding. for the em algorithm is that you 1st initialize the parameters randomly. And  that you 1st assume a set of parameter values and based on that you try to estimate what will be the value of theta. or at that time step with that value. once you move to time, step one. And for the maximization, then you find what will be the a better, Theta k plus one at the  time. Step that  maximize the likelihood over your training data. Here we have to parameters. it's very similar to what we are estimating previously. This is  the transition probability from time step T to type, step t plus one. where you can estimate it's based on your training data. Remember, we don't know this parameter of T of Theta K. But we can estimate it with some random initial random values. after we have calculated this one and then we have the normalization constant, which you can obtain by marginalizing over all the possible values of I.  at the Easter we can compute something very similar to the transition probability from one State to the other. Given our observed value of value and the initial value of Theta K, and we do very similar thing. Given a randomly initialized Theta K, then the  stage is at the maximization step would be  to compute a new value of Theta K plus one. and the new value of Theta K plus one we be. New values for a better for are betters, and then you continue  at the maximization stage. and at the maximization stage you can  get a better value. I see the similarity with the in the past, but I don't see why it took  , you see the similarities? it's also very similar to your emission, your emission probabilities. this is what we're trying to do . And what we are trying to do is that if I can have a likelihood given. My new parameter of theta improves the previous one. you have 1st a random initialization of everything that is there. you have random initialization of the entire state. If you get a new parameter of theta. Given the new title you have. Then  that em is working, but at some point, if it doesn't improve again, of course. you have to have random restarts. And it's often nice to have small values of label data that you can  estimate better beta parameters for your alphas and betas before you start the emigrating . But of course most people are not using this against that. this is very important for information instruction. it's often better to use a better tag where you can  map."
    ],
    "Topic 2": [
        ", can you hear, me, , , I, guess, we can start   Lecture 8 where we'll be talking about. I have to remember Emily for any Markov model where we assumed just a background model and today we're we're going to try to give you more general framework that has been used because people have developed a couple of algorithms for Hmms, and this is what we will be describing today. that's a glory for every word in our text. yes, there was a pantry. And here we also talked about stats for when you're trying to build a model for this  task. And similarly, for the part of speech. It's a general term that is used. And in Markov chains we talk about how we can decompose the joint probability which by  has been removed from the board. They are the Union variables and the o's are the observations what you observe. Tax the different ones that occur in your sentence, and then the observed variables which is your all, would be the words. 2.  in the diagram face that we described. Observe observations, or observe variables. Are all the States which represent your part of speech. And to compute it,  it's very important to do this revision. and then you have the a high J, which is computed that given. What would be the likelihood of a sequence of observation which is the probability of all given Theta and this Theta. and it's also possible to compute this if you don't even have any level. and then you can  try to improve on this state values iteratively. Do you remember the auto marginalize? yes, and that's what we have here. how do you compute the likelihood? this is , it's possible paths that you can take. And then  we  try to create a lot a table of values  which can be referred can be compared to  having a trellis. you have the 1st observation, and you move to the second observation, and you move to the 3, rd to the 4, th to the 5, th and  on. and once you get to the hand of it. the queue which refers to the state values the part of speech, and then given the theater. Then you multiply that you  observe this? this is how you compute the Alpha G's. for every Alpha they can be computed as this. which is the 5G, and then you need to multiply this by  observing forward with this conditional independence as a machine. Have we already marginalized for queue? The probability of Dt beginning this sentence, . the probability of a meeting, the what you observe? You have probability of q 1. You have probability of q 1. But you do have a mission probability. , what probability of a meeting that world? , then, the complicated thing is that if you want to  compute for the States of, , Alpha, Dt. C.  everything, all the States before that  contributes to the value of the  state, which is Alpha, Dt. You have calculated to have the probability of O given Theta. first, st we create a chalice for high equals, one to hand. I, I equals one to hand. you create the States across every part of speech. And then, for the 1st one, which is the Alpha. it's very clear that this will be all, and to n squared T. Because you have n twice. , from the last cell. And then you go backwards. Because the law of probabilities  does allow us to do this. What's the probability of T plus one to T, because  you're starting from T, which is the last cell and then given Qt equals. you need for you to  pro progress backwards. And then you go backwards. There you marginalize our A from for every timestamp, from one to 3. ,  as  here, there's a lot of probability multiplication which  can cause problems. It's better to work in Logan, because, instead of multiplying all you have to do is to just sum. the formulation you have below is very used, especially if you're implementing. Is that different from the same A's in the forward propagation? if you compute this, whether you go from Ji to high. Yes, there will be distant. Is it clear from this illustration. this is a very, very common trick that is used when you have to multiply a lot of probabilities. we have talked about how to compute the probability of all we have competed. , we have describe the probability of all given theta, which is the likelihood. And , how do we  label the samples? Which is what is the  queue. , instead of as summation, you  just take the maximum instead of summing. here,  because of it's a lot of material. A and B,  what is a. yes, please, what is a. yes. , I believe this is all we need for the exercise and for the forward backward. and what we want to compute is  want to compute the exclamation mark at ? because we want to go for exclamation mark at. all the probabilities for exclamation mark is what you multiply with . What you what you observe. we have 3 things we are observing. 3 states and 3 things we're observing. at the 1st State we are observing just the exclamation back. We said, every state in the 1st column contribute to what would go to the what? which is the Alpha Gt. and after you have computed everything on that column. the every values here also contributes to the finer outputs. If you observe, you find out that what you are multiplying with are  the same for full of column 2 and column 3. what you're going to have is that we said in the last column you're going to have once you have once throughout. But the formula is slightly different. here, here you have the A high J.  what is your aid? And then you propagate everything to the very 1st column. , but I will encourage you when you get home you should try to work it out yourself. ,  for the vitabi algorithm,  the difference is  you take the maximum. And what is the maximum of the 1st column? and you continue the game. we need to guess them. And then we try to learn this iteratively. We have some epsilon, and then we have some. where we compete, what is the probability of Qt. Equals a tag given what we observe? What is our state, what's the probability of Q equals? I , what's the probability? and after that you recalculate what should be the exact value for Gamma, T. And epsilon of C.   the idea for the east step we have 2 parameters that we want to calculate. We have a gamma I of T, which you can decompose to be this probability of Qt. I given what you observe and the parameter you are looking for. and we can then use that to estimate what is our Gamma T. Of T. Gamma, high of T, and after that, and the probability of Qt equals. we know how to estimate what is the probability of Qt equals. and then you can divide by what you marginalize. if you have gotten the initial value of the Gammas and the Epsilon. the Alpha ij it's very. You can compare it to what we did previously. Where , how are these equations derived? Given what you observe ? that is the alpha values. , for the other one, which is the d of IK plus one. But  of the expectation maximization. we cannot just apply vitabi algorithm directly. And the old idea is that you find out you don't know the value of Theta  the value of teta  will be everything in your trellis, the States all the values of Alpha or the values of Beta. is it better than the previous one you got, and then you do this iteratively until you, until you find out that there's improvements in the estimation of the likelihood. And , this will improve your performance because you have an initial estimation of the parameters of alpha and betas. in practice, Emi guardian can be used for different tasks. does give you a very good performance. you're not only looking for a word and giving it a tag. But you can have something  noun phrase that you want to tap instead of just a word. You have this plot, Mcgill. And then you have organization. if you see my gear university, this makes it an organization,  you can have what is the beginning of the entity, and also what is the  word that follows the entity. and then you also have the old tax, signifying that there is no entity   East is not an entity located is not an entity."
    ],
    "Topic 3": [
        "last time, we defined path of speech as a synthetic. , we have things  nouns and examples  restaurants, dinner. And also we talked about different ways of being part of speech different schemes, ? who can remind me of the different schemes? We  examine 2 popular schemes? where we use  nlp for proper nouns and for universal dependency. what is the Gedian variable? With some initial probability you can then generate what will be the observed word which would be, o 1, and also, if you go to Q. if you want to say probability of OQ, where all and q are random variables, and all are all your observed. You can say, this will be the product of the q. This is the way we competed. and then we have the count of this. This is in a very unsupervised way. we are going to examine things  the em algorithm that is very popular in mushroom. Do you have questions from the last class. you have exponentially many paths and risk to priority. if you remember the calculation, I can open the slides, but it's very, very similar to what we did in the last calculation. you should be familiar with aig multiplication with Bjot, because this is what we have been doing early on. in the last column, once we have calculated everything for distrellis,  you can. This is the way we calculate it, which is the Pi, J. Bj. Given the current, you can also do probability of the current. That's the 1st token one. 1, 1, 1, 1  for the backwater guardian. If the values are very, very small, you can have what is called  on the flow. because the AI is the way we calculate them quits. , I see what you mean. I've tried to connect, and there's a reason why they connected both together. but you can use the forward backward algorithm and combine this with Em algorithm, which I will show you a couple of slides. which I can show with a very simple illustration on the board. And  what is the maximum? Of your computer at some point? But if you take the maximum all probabilities and you do the do it this way, you can  get a signal. If you take the Ag mass over all the probability of Qo, given theta   this is  vitabi algorithm, which is very, very popular. We are supposed to multiply. No, no, we're not doing the arts. you multiply this by dj, of old thing video of Ot, we are concerned about apps. we must buy with the apps. And once we have calculated that , we are interested in the 1st value. And then if you go to video voting ot. it's just the same computation. Why was prime by December is because we are supposed to produce art twice ? and  you are trying to do the same calculation. for the phone, I've already , we are going this week for the calculation for the back, or I don't think just you're doing this in terms of the values you pick. go to the folder garden. and then you do the calculation. just using the formula and then doing the calculation. That's 0 point 2, 5, and is the maximum that you use in the calculation. you always, instead of doing the submission you take the maximum . And then you update the current parameter. And this is  the idea of the em algorithm. And this is what we want to connect to the Em algorithm em is expectation, maximization. , but initially you have some random values for them. It's very, very related to having a sub version of the mle. and then you do this iteratively, and if you do this iteratively over many iterations. And the question is that when do you stop your Em algorithm you are likely going to stop. When your likelihood doesn't improve at some point it will stop improving, and then you can stop  this is the idea of the em algorithm or another way to stop is you can perform the prediction over your development set to see if things are improving. for the Em algorithm, we are trying to find the local minimum. You can, that you calculate is what we call the is our theater. and after you have a random initialization of everything, then you want, and I want to. Computer is better than the old one. That's the best em can give you. there, if there are a few things you need to consider when you are working on things  the emigrating you have what is called random restarts. First, st you have to train multiple models because this is an unsupervised setting, and if you have very bad initialization. And  you can pick your best parameter by always computing. What's what is the result on the development set? Another thing that might be helpful is to have a more better initialization by using an initialization based on some external source of knowledge. This will  lead you to better convergence than having a random initialization. And then we have some very popular baselines on the Wsj couples, which used to be a very popular benchmark. And also, you have a very popular task named Entity Recognition, where you want to identify popular categories  personal name, organization, and location. When you're searching for an important thing on the web, you want to get the entity   that you can get the  information. and if you can get the entity ,  this will already have an entry on Wikipedia, and then you can extract information about the entity. Here you're not only annotating a single word. when  we have different schemes for named entity, recognition. here we have my view university. We also have other skills , there's another one BIOE. You have the intermediate of the entity, and then you have the old tax,  everything that is not an entity will be an old tax  and but  this is  the most popular tagging technique which we call the iob 2 tagging scheme, where you clearly define the beginning of an entity and the ending of an entity."
    ],
    "Topic 4": [
        "Scheme, and then there's a universal one. Here we , this is an example of the pantry bank  scheme. And why, what was the motivation for having a more universal dependency scheme? try to generalize across other languages. you need to consider the current world if you want to compute the mle. you also need to consider the previous context. we established that, using a very simple diagram example. we have the probability, the judge probability of all the observations and all the States. 2, you can generate with initial probability, O. 2, and then with state transition probability, you can go from q. And here we also talk about the model parameters which we can estimate using an mle. And today we are going to try to have something  a  a generalized, how do we  compute parameter theta that is more general. this is what we want to. Consider today, I must warn you. Today's lecture is highly technical. we have the initial probabilities. And then we have the transition probabilities from Qt. Plus one which we can also compute using mle. And here you're just concerned about which part of speech can begin. If  the previous State, can you predict the previous? J, and then we normalize by the number of counts of act. this is where we ended the lecture the last time where we talk about , after we compute the likelihood. And  we want to consider how to  compute the maximum a posterior. we want to estimate . Also, we want to compute what will be the best States. And here what will happen is that you 1st initialize the state with random probabilities. Because this is  what we are looking for. Eventually, the part of switch. But you can marginalize to give you one. if you marginalize over all the queues, you are going to get payable? And this is a very simple law of liability  you have the joint probability, and if  all the joint probabilities. You can just marginalize to get the other one. the problem is that if you try to do this. and then you have 3 observations. , and the solution is that we can try to solve this using forward algorithm which is coming from dynamic programming to avoid unnecessary recalculations. we go from one state to the other. And here, this is  every possible combination that can happen. and then you have all your observations that happen via time. We are saying that it depends on the previous state at time. T.  if you want to compute for time, t plus one, it depends on everything you have on time. T.  if you want to compute for  Alpha, determiner of 2,  it depends on all the values you have from Alpha Vb. And then what is the state that you have? Because this is everything that has been observed from one to 2. ,  here we can compute what will be the initial state probabilities. what is the initial state probabilities? this is how you can compute the initial state probabilities which depends on. that's the initial state probabilities,  initial state probabilities. you need what is the initial state vulnerability. if you were , let's go to last. We have the initial probability. where you have an initial state, you don't have transition probabilities,  there is no way you multiply. and then you don't have state probability that there's nothing to multiply ? and that's why you got this. We are saying that this depends on everything from time one to time. and then you multiply, which is your aids, your aids will be your transition probabilities, and then you multiply by the emission probabilities. But what we are saying  is that in the middle you don't have the you don't have the initial state probabilities. you only have the values at the previous state that you need to multiply. we can  transform this into an algorithm. 1, and then you have an algorithm that goes from one state to the other. we can also have a backward algorithm which is the reverse of that. in the backward algorithm, instead of starting from the initial state probabilities that you have. , we assume we are starting from the last. And that's why it's called the backward algorithm. And here, and  based on because we are just because this is the probability of a joint probability you can do forward or backwards,  I showed you the last time in the language model. ,  here you want to call instead of completing the alphas! The alphas are the are the one we computed at the forward algorithm in the backward algorithm, you compute the betters. R, , here the backward algorithm is  very similar to the forward algorithm. One thing we assume here is that we don't have initial step probability. you  in the initial probabilities for the alphas, you have to multiply the step probability with the probability of  admitting a word, and here the last one you just give them once. the values are known, and after that, then you go backwards by  completing this. And after that you can  marginalize again. You have initial state probabilities which you need to multiply would be betters. I will show you the offer, the forward algorithm again, in forward algorithm, everything is already integrated. the last stage you just need to marginalize over what you have at the end. And in the backward algorithm you still have your initial state probabilities, and then you have your probability of emitting the 1st word. You need to assume, because you need to have something to start your going  to. You need to have values to start. You need to have some initial values to do your calculations  for the afford algorithm,  this is very clear because you can assume the initial state probabilities. But for the backward algorithm we don't know. we can just assume the maximum value you can get for probability is one. And then you just assume that value. for the forward backward algorithm here you just out to multiply what it offers and the betters together. this is what is called forward backward algorithm. And before, after you multiply the alphas and the betters together. if you have, if you have to multiply a lot of probabilities. And you need to take the Agmas most of the time. and to prevent that, it's better to work in logarithm. And, , if you take the log of all these products of pis,  you will have a summitation over the log pis, and if you assume that log pi is plus AI. You need to apply Bayes theorem to inverse the yes, , because you are going backwards. Oh, , because, ,  to be honest,  they are the same. if you are going to the backwards, yes, yes, it will change a bit. , you have forward algorithm backward algorithm. And then there's 1 to combine both. You don't have the labels. let's let's give  very, very, very simple example. Let's assume everything here is  base 2.  where we have log of truth. And what they say is that this is stuffed off. it's just the maximum of all the pis. that's a p 1, 2.  hey? let's take a very, very, very simple example. Because if you multiply a lot of probabilities together at some point, you're just gonna get 0 because of the floating point limitation of your what do you call it? And also we can use it here at implementation stage. you start with something very similar to this with a forward algorithm. let's assume we have 3 States XYZ. Which also corresponds to your path of speech tax. And then we have 2 possible emissions which are your what do you call it? And at and we have the probabilities. , it's a state transition probability. You have the You have your State transition probability for the forward case. even without looking at a solution. I may have to write the formulas on the board, because  it's more difficult. for the time we are guarding. I just need to have the values. if we want to compute that for every stage for the forward algorithm. What is the initial state probabilities multiply by the admission probabilities. you are going to multiply each scale probability by the observation. which is the this and the probabilities for each of the States. what are you going to do after that? , it's just you just have to pay attention to the It's really simple, if otherwise,  it's straightforward. And the second reason is because you are at the same States. And also you are trying to emit the same word. ,  for the backwater gardening. But while summing the difference for the backward algorithm is that while summing all the values, you also multiply by the initial step probabilities. And for this, what we can do is that we can initialize the States with some values. and what we are going to do is  doing this, you predict the current state sequence using the current model. And then you repeat the 1st time you predict the current state sequences, using the current model. and then you update the parameter of your model. you, you learn a better parameter to be able to predict a better state sequence. in the forward backward algorithm, where? and you go through your data to estimate a better parameter of the model. you don't have what's what you should start your calculation with. But you randomly initialize them, and then you just estimate at that state. What you need to do is that you get a better estimation of the parameters, and then you redo your calculation. at the expectation, you get a split account for the eating structures and using the current Teta K and the eating structures will be your part of speech or your State's values. the parameters you want to estimate , we call them probabilities responsibilities. Our gammas we want to estimate. And after that we can  compute what we? once we estimate these values,  these values will be corrected at the maximization stage. and then you estimate the parameter of the model that you are looking for. I, because in this algorithm, we are using the forward backward algorithm. I comma O, because we can estimate it's using the alpha I of T from the forward algorithm and the beta I of T from the backwater guardian, which we multiply together. Which is also coming from the from the forward backward algorithm, the forward algorithm you have, you are computing your Alpha J of T, and then we multiply by all the values in the backward algorithm. We want to compute new values for, alphas! where we have the joint counts divided by the count over the initial states before we move to the  State. Better values for all the all the teta values you are looking for, and then you will go back to the expectation, to the E step. you will have better values for your teachers which you don't know. You are looking for the States. And here you are trying to compute what is the transition probabilities. You will not go very far. we also have some other sequence modeling task  chunking where you  find syntatic chunks in the sentence. And this is very, very important for information extraction you want to. But of course, here you need to detect the spans of multiple words. Again, you need to identify the span. You can view them and concordion allocated in Montreal. What is the problem with this scheme? But this is a very busy scheme where you just label it as You just label each occurrence of the entity with the Tag organization. Where is this the beginning of the span of the entity versus? Where is the entity ending rather than just having a single span, because an entity can be a single word. It can be multiple words. in the I/O or B tagging here, we try to know what is the beginning of an entity. What is the ending of an entity. It's not a full organization, you have to say, my girl, because  my gear is also a name of a person. S. Where you can have the start of the entity. You have the end of the entity. ,  we'll stop the lecture here today  week. October second, we have the end of workshop at Mila. I encourage you to attend."
    ],
    "Topic 5": [
        "And today we are going to be going through some algorithms that are popularly used for this task. We have verbs, we have adjectives, we have preposition adverbs and determinar. The  part of speech depends on the previous part of speech. where the  part of speech we always depend on the  on the previous part of speech. Markov assumption, where you have to reduce the context instead of saying, you have all the context, saying, when you make this conditional independence assumption. the States would be the part of speech. And here we make an assumption here that, given the State one. The sentence is as simple as that. This Emily, is often calculated over your corpus. which is  the map, we want to  estimate what will be the theta? But this is what we want to calculate. and this data can be calculated by examine examining all the possible states that can happen when you're trying to go from one part of speech to the other. the one no one doesn't consider the but of course, , it's not that it doesn't consider it. And the simple answer is that you just have to marginalize over all the state sequences. Imagine you have 2 states, and then you have 2 observations towards to be generated. imagine you have 2 states. And then we calculate the probability. we  say, the  state depends on all the values you have computed in the previous States. having a trellis of possible state sequences. here you have all the States, and here your States are the part of speed tax. once you have calculated all the state values, you can marginalize the entire values to calculate what is the probability of all given theta. What's the probability of the current tax. We I gave you an inch of that last week. we have not marginalized for you. This is what we did the last lecture, ? we have this big equation here. These are the previous states. all the values you sum over them. and then you sum up all the values across all the different States. that means all possible ways  that means you consider all possible ways in your part of speech. just  marginalize marginalized means you sum over all the probabilities to get your Po given data because everything previously contributes. And then you continue to  sum up all the probabilities until you get to the last one. And at the last stage you can just  sum up or everything. These are your different states, which are the part of speech tax, and T from one to T. These are your observations. And then every word in your sentence which are your observed values. And if you want to calculate, what is the time complexity of this? 1st you sum over N, and then you have another for loop over N, and then you have T. And once you have completed all the Alpha Tj's you can marginalize to have what is the probability of all given Theta. You can say, what's the probability of the previous? probability you can do both probability of the current given the previous, and also you can do the reverse, which is probability of the previous. But all our betters at T, the last stage are given once. and then you multiply that with the benches. , that's the  we have an assumption that you just give them one. there's a simple trick that we use. and then we can have what is called this log some trick. I can give you an annotation with  a very simple example. you just sum the properties together. , if you want to log some trick, is what is the log of sum of probabilities? , what is the log of sum of exponential of probabilities? I didn't understand the forward and backward multiplication this one. you have 3 different algorithms. Because you can use this, both for the supervised setting and unsupervised setting for the all supervised setting. the log sum trick is  the log of the summation of exponential of probabilities. They're serving across all the exponentia. Here, we're saying, this should be involved of the maximum here in this very simple example. And then if you solve across this,  you're saying, pi. , 2 cool and even trying to do very, very simple example. This is just be very close to 0. you have the States here, and then you sum instead of summing, you just take the maximum. And then you take the maximum over all the datas also you have the same runtime. You have to keep track because you need it for the  to calculate the  you need you to calculate the  column ? You need to keep track of it. we have a short exercise which I hope will help to clarify things if you do it yourself. You have your what's this symbol? How would you calculate it. the only difference is that we're going to take the maximum instead of the submission. ,  this is the exercise. But then to get the app symbol, it looks  it should be 0 point 3 wide scale. And then we also have a 3 part of speech. that means we, if you want to compute. And after that you sum up everything together. we may not able to. that is in the supervised setting, where you have all the observed values. And also you have the labels which is your sequences your States. But you could also have this in a in a unsupervised setting where we don't have the state values. it's ,  that everything is unsupervised. Because, , we ,  what we're trying to Co calculate. here you're trying to say, what is the probability of the States? it's very similar to what we did in the in the earlier stage, where we want to go from one State to the other. It's  the same idea with what you're trying to achieve. But we are trying to just learn everything unsupervisedly from the data. There are no labels to train on. And  we want to do everything  our supervisor. Then that means I'm making progress. the difference between the supervised and the unsupervised is the supervised. You can  estimate your offers and your betters, which are your offers. That you calculate are your betters. But in the unsupervised setting, you cannot estimate this. I want to see if you are making progress. as long as you are making progress, that the likelihood over your observe or your training data, which is all your observation. Your offers are your betters as long as it keeps improving. That means you have to stop. There's no guarantee that you're going to get very good results. If you're interested in more proofs, there are proofs of correctness above bound Welsh correctness which you can  check with additional materials. the only supervised setting, which is about vash a guardian with no labor data often gives very poor results, because there are many issues , if you don't have a very good initialization. You're going to likely have very poor performance. You can  estimate how good is the performance. If you just labeled everything as organization? And you can also solve these tasks with Hmms, and as  in the future lectures using crf."
    ]
}