{
    "Topic 1": [
        "but , I would go there. . . We'll continue our discussion about part of speech tagging. and vitabi agony. ? all . last time, we defined path of speech as a synthetic. For example, we have things  nouns and examples  restaurants, dinner. And similarly, for the part of speech. . And in this case. . . in this case the States would be the part of speech. Tax the different ones that occur in your sentence, and then the observed variables which is your all, would be the words. which is q. 1. 1 to Q. if you want to say probability of OQ, where all and q are random variables, and all are all your observed. and then, Q. Are all the States which represent your part of speech. You can say, this will be the product of the q. q. 1. But . we have the initial probabilities. And then we have the transition probabilities from Qt. It's not. Q. , the  States? I, followed by a tag. . we want to estimate in this case. We don't know. On. . And here what will happen is that you 1st initialize the state with random probabilities. Do you have questions from the last class. . yes, the queue. These are the States. . . . . And this is a very simple law of liability  you have the joint probability, and if  all the joint probabilities. . you have exponentially many paths and risk to priority. we  say, the  state depends on all the values you have computed in the previous States. we go from one state to the other. . . you have the 1st observation, and you move to the second observation, and you move to the 3, rd to the 4, th to the 5, th and  on. . And then what is the state that you have? this is  computing. And what's up to ? Because this is everything that has been observed from one to 2. ,  here we can compute what will be the initial state probabilities. what is the initial state probabilities? We I gave you an inch of that last week. this is how you can compute the initial state probabilities which depends on. If what is by J. In this case. that's the initial state probabilities,  initial state probabilities. ? Yes, I don't understand why we can multiply by zoom. you need what is the initial state vulnerability. No. That's . . . We have the initial probability. ? where you have an initial state, you don't have transition probabilities,  there is no way you multiply. Do you remember we factorized stuff? Because you might. You are. and then you don't have state probability that there's nothing to multiply ? and that's why you got this. , ? ? And then if you do this over and over again. if you do this. all the values you sum over them. But what we are saying  is that in the middle you don't have the you don't have the initial state probabilities. ? and then you sum up all the values across all the different States. . And then you continue to  sum up all the probabilities until you get to the last one. And at the last stage you can just  sum up or everything. we can  transform this into an algorithm. here. you create the States across every part of speech. j. 1. Of o. in the backward algorithm, instead of starting from the initial state probabilities that you have. , we assume we are starting from the last. of the . you can do both. Because the law of probabilities  does allow us to do this. Here you have. What's the probability of T plus one to T, because  you're starting from T, which is the last cell and then given Qt equals. One thing we assume here is that we don't have initial step probability. you  in the initial probabilities for the alphas, you have to multiply the step probability with the probability of  admitting a word, and here the last one you just give them once. You have initial state probabilities which you need to multiply would be betters. I will show you the offer, the forward algorithm again, in forward algorithm, everything is already integrated. And in the backward algorithm you still have your initial state probabilities, and then you have your probability of emitting the 1st word. with enough. at one. we can just assume the maximum value you can get for probability is one. And then you just assume that value. . ? if you have, if you have to multiply a lot of probabilities. It's better to work in Logan, because, instead of multiplying all you have to do is to just sum. and then we can have what is called this log some trick. if you are multiplying all the probabilities together,  you see here, we keep multiplying the alphas and the betas together. you just sum the properties together. And, for example, if you take the log of all these products of pis,  you will have a summitation over the log pis, and if you assume that log pi is plus AI. , if you want to log some trick, is what is the log of sum of probabilities? ? , what is the log of sum of exponential of probabilities? If you're implementing probabilities and then you multiply a lot at some point, you will get some value very close to 0. The state. I understand. J, . Is that different from the same A's in the forward propagation? You need to apply Bayes theorem to inverse the yes, in this case, because you are going backwards. . Here. Is this one? . the other side. And then. . A's. . . I've tried to connect, and there's a reason why they connected both together. . all . the log sum trick is  the log of the summation of exponential of probabilities. Some of this. Let's assume everything here is  base 2.  where we have log of truth. 2. 4, 2, 6. That's . . video here. we here. . this would be. You multiply this by. this is a very, very common trick that is used when you have to multiply a lot of probabilities. . Which is what is the  queue. you have the States here, and then you sum instead of summing, you just take the maximum. that's the vitabi algorithm. ? . But when you  do this calculation, you have to also keep track of where the maximum entry to each cell came from. You need to keep track of it. . here, I think because of it's a lot of material. let's assume we have 3 States XYZ. And at and we have the probabilities. we have the 2 parameters? ? You have the You have your State transition probability for the forward case. . I think the only difference is that we're going to take the maximum instead of the submission. and the better it is together. What is the initial state probabilities multiply by the admission probabilities. ? Bj, of o, 1. the last one. No, no, we're not doing the arts. We are still at the what do you call this? ? we have 3 things we are observing. In this case. 3 states and 3 things we're observing. at the 1st State we are observing just the exclamation back. which is the this and the probabilities for each of the States. ? . You? ? Multiply by the Aij. In this case, what is the Aij 0 point 5? you multiply this by dj, of old thing video of Ot, we are concerned about apps. we are. , it's just you just have to pay attention to the It's really simple, if otherwise, I think it's straightforward. you only multiply by 0 point 9, because here, this is Bj of Otp. ? Video voting. And then if you go to video voting ot. it's just the same computation. And why is that? . you are multiplying with the same state value. And that's why we are multiplying by the same value. But in the case of the 1st one, you'll find out that we are  multiplying by a different value. ? and in this case you are trying to do the same calculation. Here? you find out that you multiply 0 point 5, and then the second,  you multiply 0 point 2, then you multiply 0 point 1, and for background it's very different. And after that you sum up everything together. But while summing the difference for the backward algorithm is that while summing all the values, you also multiply by the initial step probabilities. and also the better one. ,  for the vitabi algorithm, I think the difference is  you take the maximum. and you continue the game. you always, instead of doing the submission you take the maximum . for the em algorithm is that you 1st initialize the parameters randomly. But you randomly initialize them, and then you just estimate at that state. What you need to do is that you get a better estimation of the parameters, and then you redo your calculation. the parameters you want to estimate in this case, we call them probabilities responsibilities. Here we have to parameters. What is our state, what's the probability of Q equals? , but initially you have some random values for them. Equals. Remember, we don't know this parameter of T of Theta K. But we can estimate it with some random initial random values. we know how to estimate what is the probability of Qt equals. I comma O, because we can estimate it's using the alpha I of T from the forward algorithm and the beta I of T from the backwater guardian, which we multiply together. the Alpha ij it's very. where we have the joint counts divided by the count over the initial states before we move to the  State. Better values for all the all the teta values you are looking for, and then you will go back to the expectation, to the E step. and then you do this iteratively, and if you do this iteratively over many iterations. you will have better values for your teachers which you don't know. . here you're trying to say, what is the probability of the States? It's very. , for the other one, which is the d of IK plus one. We don't know them. ? There. And  we want to do everything  our supervisor. all . for the Em algorithm, we are trying to find the local minimum. And the old idea is that you find out you don't know the value of Theta  the value of teta in this case will be everything in your trellis, the States all the values of Alpha or the values of Beta. You don't know them. . . ? . is it better than the previous one you got, and then you do this iteratively until you, until you find out that there's improvements in the estimation of the likelihood. Computer is better than the old one. If you're interested in more proofs, there are proofs of correctness above bound Welsh correctness which you can  check with additional materials. You will not go. you have to have random restarts. And , this will improve your performance because you have an initial estimation of the parameters of alpha and betas. This will  lead you to better convergence than having a random initialization. ? this! . But you can have something  noun phrase that you want to tap instead of just a word. And also, you have a very popular task named Entity Recognition, where you want to identify popular categories  personal name, organization, and location. When you're searching for an important thing on the web, you want to get the entity   that you can get the  information. You have this plot, Mcgill. And then you have organization. If you just labeled everything as organization? when  we have different schemes for named entity, recognition. Organization. my guild, together. ? . I encourage you to attend."
    ],
    "Topic 2": [
        "And  last time we are trying to compute,  Emily. We have verbs, we have adjectives, we have preposition adverbs and determinar. what is the Gedian variable? This is the way we competed. this is what we want to. probably the most one. Key one. To Qt. Plus one which we can also compute using mle. Just because we'll leverage on this in this class and here to compute your pi high height. J, and then we normalize by the number of counts of act. but I'm dividing it by how many times we have qt equals I in our compass. in the last time we have. the mle. But this is what we want to calculate. and this data can be calculated by examine examining all the possible states that can happen when you're trying to go from one part of speech to the other. This is in a very unsupervised way. But you can marginalize to give you one. Do you remember the auto marginalize? if you marginalize over all the queues, you are going to get payable? And the simple answer is that you just have to marginalize over all the state sequences. You can just marginalize to get the other one. That's already  4 possible bots that you can have. we're trying to create  a table of all possible state sequences. And then we calculate the probability. And then we go step by step. And then  we  try to create a lot a table of values  which can be referred can be compared to  having a trellis. having a trellis of possible state sequences. and then you have all your observations that happen via time. We are saying that it depends on the previous state at time. T.  if you want to compute for time, t plus one, it depends on everything you have on time. In the time step of one. ,  and here we want to copy what is the probability of the observation from one to the to that time where you are to that time step. look at what you're trying to compute probability of 4 1 wants to see. and then key of T, ? We are saying that this depends on everything from time one to time. this is   the formula,  where you are. just  marginalize marginalized means you sum over all the probabilities to get your Po given data because everything previously contributes. And maybe this is easier. first, st we create a chalice for high equals, one to hand. This is the way we calculate it, which is the Pi, J. Bj. And if you want to calculate, what is the time complexity of this? And after that you can  marginalize again. and then you multiply that with the benches. And you need to take the Agmas most of the time. I can give you an annotation with  a very simple example. and to prevent that, it's better to work in logarithm. because the AI is the way we calculate them quits. you're talking about the normalization. they are distant days. Yes, there will be distant. You don't have the labels. which I can show with a very simple illustration on the board. I think, 2 cool and even trying to do very, very simple example. You'll see that they  books. This is just be very close to 0. You have to keep track because you need it for the  to calculate the  you need you to calculate the  column ? We probably would do this together. Exclamation Mark. How would you calculate it. Think it's probably easier to take a picture just all . and what we want to compute is  want to compute the exclamation mark at ? what are we going to do based on all the information you have on the board? what is the initial step of abilities which is your pile here. because we want to go for exclamation mark at. all the probabilities for exclamation mark is what you multiply with . you are going to multiply each scale probability by the observation. what are you going to do after that? We said, every state in the 1st column contribute to what would go to the what? that means we, if you want to compute. we go to  this is B of one at timestamp of 2 ? And then if you go to Dj of 2, a time step of 2,  the value will change. And also you are trying to emit the same word. what you're going to have is that we said in the last column you're going to have once you have once throughout. We may not be able to go through the calculation step by step in a class. , but I will encourage you when you get home you should try to work it out yourself. They have a clear understanding. And also you have the labels which is your sequences your States. or at that time step with that value. once you move to time, step one. We have some epsilon, and then we have some. Our gammas we want to estimate. it's very similar to what we are estimating previously. This is  the transition probability from time step T to type, step t plus one. and after that you recalculate what should be the exact value for Gamma, T. And epsilon of C.   the idea for the east step we have 2 parameters that we want to calculate. We have a gamma I of T, which you can decompose to be this probability of Qt. and we can then use that to estimate what is our Gamma T. Of T. Gamma, high of T, and after that, and the probability of Qt equals. if you have gotten the initial value of the Gammas and the Epsilon. You can compare it to what we did previously. Because, , we , let's look at what we're trying to Co calculate. And here you are trying to compute what is the transition probabilities. But we are trying to just learn everything unsupervisedly from the data. this is what we're trying to do . Yep. You can, that you calculate is what we call the is our theater. That means you have to stop. And  you can pick your best parameter by always computing. But of course most people are not using this against that. And you can see the Hmms. And this is very, very important for information extraction you want to. and if you can get the entity , maybe this will already have an entry on Wikipedia, and then you can extract information about the entity. this is very important for information instruction. But of course, here you need to detect the spans of multiple words. Where is the entity ending rather than just having a single span, because an entity can be a single word. It can be multiple words. What is the ending of an entity. And you can also solve these tasks with Hmms, and as we'll see in the future lectures using crf."
    ],
    "Topic 3": [
        "And here we also talked about stats for when you're trying to build a model for this  task. you need to consider the current world if you want to compute the mle. And also, , we made that  assumption which we call the Markov assumption. It's a general term that is used. We also call it the Markov assumption. here we  relate this to Markov chains. And in Markov chains we talk about how we can decompose the joint probability which by  has been removed from the board. and then we have the count of this. I. Oh. this is where we ended the lecture the last time where we talk about , after we compute the likelihood. And  we want to consider how to  compute the maximum a posterior. Eventually, the part of switch. how do you compute the likelihood? , Chinese, ? Alpha, Nn. And Alpha. CD. this is how you compute the Alpha G's. for every Alpha they can be computed as this. Have we already marginalized for queue? we have not marginalized for you. You're good. This is what we did the last lecture, ? we have this big equation here. , what probability of a meeting that world? alpha t, minus one. you should be familiar with aig multiplication with Bjot, because this is what we have been doing early on. in the last column, once we have calculated everything for distrellis,  you can. And then, for the 1st one, which is the Alpha. 1st you sum over N, and then you have another for loop over N, and then you have T. And once you have completed all the Alpha Tj's you can marginalize to have what is the probability of all given Theta. , from the last cell. And the marginalization is very similar to what you have in the because when you're when you get to the 1st  you're going from the back to the 1st state. that's the back order going there. That's the 1st token one. , that's the  we have an assumption that you just give them one. There you marginalize our A from for every timestamp, from one to 3. ,  as you can see here, there's a lot of probability multiplication which  can cause problems. , I see what you mean. this is an assumption of the algorithm. you have 3 different algorithms. which I'm still good. Oh, , cool. this is our assumption. and then you have the summing. But something very similar to this. They're serving across all the exponentia. it's just the maximum of all the pis. And  what is the maximum? and then 2 rest of our heads. Because if you multiply a lot of probabilities together at some point, you're just gonna get 0 because of the floating point limitation of your what do you call it? Of your computer at some point? But if you take the maximum all probabilities and you do the do it this way, you can  get a signal. And , how do we  label the samples? , instead of as summation, you  just take the maximum instead of summing. it's very, very similar. Whoa. We are supposed to multiply. The alpha, j's. What is the formula for the 1st state you have to compute? This will be 0 point 1 times your initial probability, which is B times 0 point 1. Oh, I'm good is 0 point 1.  this will be 0 time, 0 point 2 times 0 point 1.  for the second one, what should be the value? this would be the second state probability that was 0 point 5 times 0 point 5. You did 0 point 3 times 0 point 7. . Seat is 0 point 3, I agree. For all this establishment, 0 point 1 0 point 5 0 point 7, and the initial set probabilities are also given as 0 point 2 0 point 5 0 point 3 is the 1st color play. the second column. To the second column. Oh, alpha. which is the Alpha Gt. Minus one. your Alpha Gt. Minus one would be 0 point 0 2 0 point 2 5 and 0 point 2 1.  you're going to take each of these and multiply by the Aij and Bj of ot. for the 1st one we take 1st X, we go for 0 point 0 2. That's the 1st aid. we must buy with the apps. And  we have 0 point 0 2 times 0 point 5, which is the 1st States there times 0 point 9, and then you add that with 0 point 2 5 again times 0 point 2, which is the second value times 0 point 9. ? And once we have calculated that , we are interested in the 1st value. you can compute the second Val the second cell, which would be 0 times 0 2 times 0 point 4, which is for the Y column times 0 point 5. . and after you have computed everything on that column. which is, you multiply 0 point 0 7 2 9 times 0 point 5 times 0 point 9, and then you had it. If you observe, you find out that what you are multiplying with are  the same for full of column 2 and column 3. That's whatever reason. And the second reason is because you are at the same States. We don't have 0 point 1.  instead of 0 point 1 in the 1st column we have 0 point 9, because the symbols are different. And at the end of the day you can sum if you sum everything you have your period of all given to them. You still have one, because your contribution from the last column is one, then this would be one times 0 point 5 times 0 point 9, because 0 point 9 is what you are emitting there. plus one times 0 point 4.  if you observe in the last column. go to the folder garden. And then you propagate everything to the very 1st column. we may not able to. And what is the maximum of the 1st column? we need to guess them. And after that we can  compute what we? after we have calculated this one and then we have the normalization constant, which you can obtain by marginalizing over all the possible values of I.  at the Easter we can compute something very similar to the transition probability from one State to the other. It's very, very related to having a sub version of the mle. When your likelihood doesn't improve at some point it will stop improving, and then you can stop  this is the idea of the em algorithm or another way to stop is you can perform the prediction over your development set to see if things are improving. it's very similar to what we did in the in the earlier stage, where we want to go from one State to the other. that is the alpha values. It's  the same idea with what you're trying to achieve. And what we are trying to do is that if I can have a likelihood given. you have 1st a random initialization of everything that is there. you have random initialization of the entire state. and after you have a random initialization of everything, then you want, and I want to. You compute the likelihood. There's no guarantee that you're going to get very good results. there, if there are a few things you need to consider when you are working on things  the emigrating you have what is called random restarts. First, st you have to train multiple models because this is an unsupervised setting, and if you have very bad initialization. What's what is the result on the development set? the only supervised setting, which is about vash a guardian with no labor data often gives very poor results, because there are many issues , if you don't have a very good initialization. You can  estimate how good is the performance. does give you a very good performance. Here you're not only annotating a single word. here we have my view university. this makes it clearer. But apart from this."
    ],
    "Topic 4": [
        "David Ifeoluwa Adelani: . , can you hear, me, , , I, guess, we can start   Lecture 8 where we'll be talking about. And today we are going to be going through some algorithms that are popularly used for this task. who can remind me of the different schemes? Yes. yes, there was a pantry. Bangkok! Yes, thank you. Here we , this is an example of the pantry bank  scheme. Yes. Yes, thank you. we established that, using a very simple diagram example. Markov assumption, where you have to reduce the context instead of saying, you have all the context, saying, when you make this conditional independence assumption. we have the probability, the judge probability of all the observations and all the States. In this diagram? Yes. They are the Union variables and the o's are the observations what you observe. And here we make an assumption here that, given the State one. With some initial probability you can then generate what will be the observed word which would be, o 1, and also, if you go to Q. 2, you can generate with initial probability, O. 2, and then with state transition probability, you can go from q. 2.  in the diagram face that we described. Observe observations, or observe variables. Consider today, I must warn you. And in this lecture. And we also have the emission probabilities. And to compute it, I think it's very important to do this revision. The sentence is as simple as that. This Emily, is often calculated over your corpus. and then you have the a high J, which is computed that given. And we can also compute this with very similarly, assuming this is  a diagram. What would be the likelihood of a sequence of observation which is the probability of all given Theta and this Theta. Also, we want to compute what will be the best States. On the observation which is  we are trying to compute the judge probability given the parameter of theta. Because this is  what we are looking for. yes, and that's what we have here. Imagine you have 2 states, and then you have 2 observations towards to be generated. imagine you have 2 states. and then you have 3 observations. , and the solution is that we can try to solve this using forward algorithm which is coming from dynamic programming to avoid unnecessary recalculations. here you have all the States, and here your States are the part of speed tax. and once you get to the hand of it. once you have calculated all the state values, you can marginalize the entire values to calculate what is the probability of all given theta. the values inside the States can be referred to as what is the probability of observing from one to T, because for every State take a random one there for every State we are saying that just think about every value you select from a colon. T.  if you want to compute for let's say Alpha, determiner of 2,  it depends on all the values you have from Alpha Vb. Alpha, Dt. Alpha, Jj. What's the probability of the current tax. Then you multiply that you  observe this? do you have question. Yes. The probability of Dt beginning this sentence, . the probability of a meeting, the what you observe? if you were , let's go to last. You have probability of q 1. You have probability of q 1. , then, the complicated thing is that if you want to  compute for the States of, let's say, Alpha, Dt. C.  everything, all the States before that  contributes to the value of the  state, which is Alpha, Dt. and then you multiply, which is your aids, your aids will be your transition probabilities, and then you multiply by the emission probabilities. contributes to the  one. You have calculated to have the probability of O given Theta. I, I equals one to hand. These are your different states, which are the part of speech tax, and T from one to T. These are your observations. And then every word in your sentence which are your observed values. do you have question. You can say, what's the probability of the previous? Given the current, you can also do probability of the current. Given the current ? Yes. 1, 1, 1, 1  for the backwater guardian. there's a simple trick that we use. And here, for example. the formulation you have below is very used, especially if you're implementing. Yes. yes, . A Hi, Jane. they're different from the one the same age in the? What's the probability? if you are going to the backwards, yes, yes, it will change a bit. Yes. for the notebook. let's let's give  very, very, very simple example. And what they say is that this is stuffed off. Oops. that's a p 1, 2.  hey? let's take a very, very, very simple example. Here, we're saying, this should be involved of the maximum here in this very simple example. And then if you solve across this,  you're saying, pi. , , if you're implementing it, they are not equivalent. that's the locksome trick. we have talked about how to compute the probability of all we have competed. , we have describe the probability of all given theta, which is the likelihood. If you take the Ag mass over all the probability of Qo, given theta   this is  vitabi algorithm, which is very, very popular. which I think you can read up more is very similar to the forward. A guardian. And then we have 2 possible emissions which are your what do you call it? A and B,  what is a. yes, please, what is a. yes. , it's a state transition probability. And the is the emission probability. you have the emission probability. even without looking at a solution. Yes. Yes. you multiply the 1st pi high with the force emission probability, because B is your emission probability. Yes. What you what you observe. Hi, Ben. do you have question? ,  for the backwater gardening. But the formula is slightly different. here, here you have the A high J.  what is your aid? hey? And for this, what we can do is that we can initialize the States with some values. Equals a tag given what we observe? I , what's the probability? I given what you observe and the parameter you are looking for. Given our observed value of value and the initial value of Theta K, and we do very similar thing. and then you can divide by what you marginalize. Given a randomly initialized Theta K, then the  stage is at the maximization step would be  to compute a new value of Theta K plus one. and the new value of Theta K plus one we be. And the question is that when do you stop your Em algorithm you are likely going to stop. yes. yes. I see the similarity with the in the past, but I don't see why it took  , you see the similarities? You are looking for the States. Given what you observe ? it's also very similar to your emission, your emission probabilities. Then that means I'm making progress. I want to see if you are making progress. as long as you are making progress, that the likelihood over your observe or your training data, which is all your observation. Then  that em is working, but at some point, if it doesn't improve again, of course. That's the best em can give you. yes. You're going to likely have very poor performance. in practice, Emi guardian can be used for different tasks. we also have some other sequence modeling task  chunking where you  find syntatic chunks in the sentence. you're not only looking for a word and giving it a tag. Again, you need to identify the span. this is an example. You can view them and concordion allocated in Montreal. Where is this the beginning of the span of the entity versus? You have the intermediate of the entity, and then you have the old tax,  everything that is not an entity will be an old tax  and but I think this is probably the most popular tagging technique which we call the iob 2 tagging scheme, where you clearly define the beginning of an entity and the ending of an entity. There's no lecture. October second, we have the end of workshop at Mila. Thank you."
    ],
    "Topic 5": [
        "Forward, backward, forward, backward. I have to remember Emily for any Markov model where we assumed just a background model and today we're we're going to try to give you more general framework that has been used because people have developed a couple of algorithms for Hmms, and this is what we will be describing today. that's a glory for every word in our text. And also we talked about different ways of being part of speech different schemes, ? We  examine 2 popular schemes? Scheme, and then there's a universal one. where we use  nlp for proper nouns and for universal dependency. We use, , I think, pr ovn  they have various different symbols. And why, what was the motivation for having a more universal dependency scheme? try to generalize across other languages. you also need to consider the previous context. The  part of speech depends on the previous part of speech. where the  part of speech we always depend on the  on the previous part of speech. 1, which is your 1st part of speech which you can compute, based on uniform probability, that how many times does this part of speech occur, or how many times does this part of speech starts my sentence across the entire compost? And then you multiply that with the State transition probability for it to move from one part of speech, from the previous part of speech to the  one, and then you multiply this with the emission probability to  generate a word given the tag because there are many possibilities. And here we also talk about the model parameters which we can estimate using an mle. And today we are going to try to have something  a  a generalized, how do we  compute parameter theta that is more general. Today's lecture is highly technical. which is the probability of generating award given. And here you're just concerned about which part of speech can begin. High equals the part of speech divided by all the possible sentences that are in your training couples. If  the previous State, can you predict the previous? distribution where we can count how many times you have in our couples, how many times do we have a tag? and the last one would be the emission probability which we can also compute as number of times you have one K. Contact. I think we just completed it. which is  the map, we want to  estimate what will be the theta? and it's also possible to compute this if you don't even have any level. and then you can  try to improve on this state values iteratively. we are going to examine things  the em algorithm that is very popular in mushroom. why didn't we include the new one. the one no one doesn't consider the but of course, , it's not that it doesn't consider it. the problem is that if you try to do this. just think about it. this is , it's possible paths that you can take. And here, this is  every possible combination that can happen. the queue which refers to the state values the part of speech, and then given the theater. which is the 5G, and then you need to multiply this by  observing forward with this conditional independence as a machine. if you remember the calculation, I can open the slides, but it's very, very similar to what we did in the last calculation. Which is the pi eyes, or, , that would be your pi, j's. But you do have a mission probability. based on what we have done. These are the previous states. you only have the values at the previous state that you need to multiply. that means all possible ways  that means you consider all possible ways in your part of speech. I don't know which one works better for you the algorithm or the training. 1, and then you have an algorithm that goes from one state to the other. I think it's very clear that this will be all, and to n squared T. Because you have n twice. think, ? we can also have a backward algorithm which is the reverse of that. And then you go backwards. And that's why it's called the backward algorithm. And here, and  based on because we are just because this is the probability of a joint probability you can do forward or backwards,  I showed you the last time in the language model. probability you can do both probability of the current given the previous, and also you can do the reverse, which is probability of the previous. ,  here you want to call instead of completing the alphas! The alphas are the are the one we computed at the forward algorithm in the backward algorithm, you compute the betters. R, , here the backward algorithm is  very similar to the forward algorithm. But all our betters at T, the last stage are given once. the values are known, and after that, then you go backwards by  completing this. the last stage you just need to marginalize over what you have at the end. I think you need for you to  pro progress backwards. You need to assume, because you need to have something to start your going  to. You need to have values to start. You need to have some initial values to do your calculations  for the afford algorithm, I think this is very clear because you can assume the initial state probabilities. But for the backward algorithm we don't know. And then you go backwards. for the forward backward algorithm here you just out to multiply what it offers and the betters together. this is what is called forward backward algorithm. And before, after you multiply the alphas and the betters together. If the values are very, very small, you can have what is called  on the flow. Oh, , because, , I think to be honest, I think they are the same. if you compute this, whether you go from Ji to high. I didn't understand the forward and backward multiplication this one. , you have forward algorithm backward algorithm. And then there's 1 to combine both. Because you can use this, both for the supervised setting and unsupervised setting for the all supervised setting. but you can use the forward backward algorithm and combine this with Em algorithm, which I will show you a couple of slides. Is it clear from this illustration. you see that they're  equivalent. And also we can use it here at implementation stage. you start with something very similar to this with a forward algorithm. And then you take the maximum over all the datas also you have the same runtime. we have a short exercise which I hope will help to clarify things if you do it yourself. Which also corresponds to your path of speech tax. You have your what's this symbol? I think I may have to write the formulas on the board, because  it's more difficult. I think for the time we are guarding. , I believe this is all we need for the exercise and for the forward backward. ,  this is the exercise. I just need to have the values. if we want to compute that for every stage for the forward algorithm. Is that clear? Is that clear? But then to get the app symbol, it looks  it should be 0 point 3 wide scale. And then we also have a 3 part of speech. And then you continue. And then you continue. the every values here also contributes to the finer outputs. Why was prime by December is because we are supposed to produce art twice ? I think for the phone, I've already , we are going this week for the calculation for the back, or I don't think just you're doing this in terms of the values you pick. and then you do the calculation. just using the formula and then doing the calculation. That's 0 point 2, 5, and is the maximum that you use in the calculation. that is in the supervised setting, where you have all the observed values. But you could also have this in a in a unsupervised setting where we don't have the state values. And then we try to learn this iteratively. and what we are going to do is  doing this, you predict the current state sequence using the current model. And then you update the current parameter. And this is  the idea of the em algorithm. And then you repeat the 1st time you predict the current state sequences, using the current model. and then you update the parameter of your model. you, you learn a better parameter to be able to predict a better state sequence. in the forward backward algorithm, where? And this is what we want to connect to the Em algorithm em is expectation, maximization. And the idea is that you 1st assume a set of parameter values and based on that you try to estimate what will be the value of theta. and you go through your data to estimate a better parameter of the model. it's , the idea is that everything is unsupervised. you don't have what's what you should start your calculation with. at the expectation, you get a split account for the eating structures and using the current Teta K and the eating structures will be your part of speech or your State's values. And for the maximization, then you find what will be the a better, Theta k plus one at the  time. Step that  maximize the likelihood over your training data. We want to estimate. where we compete, what is the probability of Qt. once we estimate these values,  these values will be corrected at the maximization stage. where you can estimate it's based on your training data. But you start? and then you estimate the parameter of the model that you are looking for. I, because in this algorithm, we are using the forward backward algorithm. Which is also coming from the from the forward backward algorithm, the forward algorithm you have, you are computing your Alpha J of T, and then we multiply by all the values in the backward algorithm. We want to compute new values for, alphas! New values for a better for are betters, and then you continue  at the maximization stage. and at the maximization stage you can  get a better value. Do you have questions? Where , how are these equations derived? But in this case of the expectation maximization. There are no labels to train on. we cannot just apply vitabi algorithm directly. My new parameter of theta improves the previous one. the difference between the supervised and the unsupervised is the supervised. You can  estimate your offers and your betters, which are your offers. That you calculate are your betters. Is our theater. But in the unsupervised setting, you cannot estimate this. If you get a new parameter of theta. Given the new title you have. Your offers are your betters as long as it keeps improving. You will not go very far. Another thing that might be helpful is to have a more better initialization by using an initialization based on some external source of knowledge. From an external ledge. And it's often nice to have small values of label data that you can  estimate better beta parameters for your alphas and betas before you start the emigrating . And then we have some very popular baselines on the Wsj couples, which used to be a very popular benchmark. What is the problem with this scheme? But this is a very busy scheme where you just label it as You just label each occurrence of the entity with the Tag organization. it's often better to use a better tag where you can  map. in the I/O or B tagging here, we try to know what is the beginning of an entity. It's not a full organization, you have to say, my girl, because I think my gear is also a name of a person. if you see my gear university, this makes it an organization,  you can have what is the beginning of the entity, and also what is the  word that follows the entity. and then you also have the old tax, signifying that there is no entity   East is not an entity located is not an entity. We also have other skills , there's another one BIOE. S. Where you can have the start of the entity. You have the end of the entity. , I think we'll stop the lecture here today  week."
    ]
}