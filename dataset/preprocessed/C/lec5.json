{
    "Topic 1": [
        "David Ifeoluwa Adelani: North campus. . I will skip the review of last time. just because of time. I'm on Slide 2 . the view of language,  far,  in the in the past lectures we have examined. How? But you would also notice that we do not fully care about the context of the of the words,  we only treat them as bag of words or bag of engrams. . And then are we also recording the lecture? Since it's not recording? , I'm recording. ? . . how, how to do , on downstream tasks. , could you please not join on Zoom? ,  common feature instruction strategies destroy much of the information in the text message. in the in the case of knife is very easy to explain. If a word is in the sentence. What are distributed are following what is called Zip's law. Is this a? Is this 1 word or 2? Is it foods plus ball? and then that's that's the. What's the word from? The other. This is more character separation. on slide 6. if you will have a word , and then most of the time. then you are going to have,  6 words or 6 word tokens. You. ? There are some cases that are not very clear. In Slide 7, we have the Fuzi cases. I have the word  run and runs. They're going to have different ind index in your vocabulary. The same thing is to do what's called  lemmatization and stemming. And by , who can remind me about. What are you going to do? Yes. Nope. . . . . We have different kinds of stem up. we treat this as a stem. Some will not. Which is more. Yes. . if you take  the sets of all rewards you have we, which is, which is a calculator. W. And smaller W are going to be separated. The question is that what  corpus can you use? Yes, the Oxfordination. . this is . and it's very easy to see if you look at in slide Number 11, you have the rank which is one, and then you have. This should be clear. . And then, if a word is infrequent also, what will happen? That's what is going to have a lower rank. . Also. we can generalize this to see if mandible it's law which here they are  adding a constant. you have to introduce a con, a constant that  relates F to R, and that is B, I think that should be clear in slides 12, and after that for us to have this Mandel Broth's law, which is a generalization of this law. This would. this will be different. because it's  you are trying to model how every language, what is the a law that gets every language. And I'm going to show you in the  slide that it. because different languages have different structures. and if you add a log,  F equals to P over r plus 5 raised to power B, and you are going to have this notation? Where? . Or was there? , that's a great question. B, you may be able to model any language. ? Practical implication of this most war types are very rare. Depending on the language. . a small number of waters make up the majority of waterkins that you see in any compost. this is what I was talking about. you have. which is very different for that of English. That's on the Sgm end. And why is because the world is morphologically rich. . here we . For the 1st one, I think it's clear that we have 6 warnings. you have 7. You have  7 phone names, but here you only have what? You  have 8 footnotes. What is called Lovewood model? And then you use this to build probability distribution. When we are trying to classify whether something spam or not in information retriever, and in  many tasks  remind you the task of language. Model the task. The task of language model is , you're trying to predict what is the probability? that's his language model. you could also see it. and then. the smart of you represents that the value it can take. What is ? For? you can multiply by a little confusing. if you. ? . yes, , it's  equivalent. This can also be. hold on your phone 2, 1. if you have . in bidirectional oil, you can do it both in the forward direction and the backward direction. I equals 2. . , you're going to see it should be W.  , it's with Wk, and then you start from one QWK. All . I had the cake. what would language model capture. 3. Also you can use it to capture what is the correct syntax, and also to it can also be used. For discourse analysis. When you use your mobile phone. The idea is that the way I'm speaking  is supposed to transcribe the text, and if it's a good ASR model one of the models, there is a language model after you have really acoustic model. That is . , you may not be able to generalize across many tasks. if we are looking at what we are talking about the parameters. how do you build your statistical language model the 1st thing you are supposed to do is to gather a large representative training purpose. Something  the English will give you there and then you learn the parameters from the couples to build a model. For example, all the unigram probabilities, diagrams, triangle probabilities, and then use this to build a language model. . we make use of this assumption, which is called conditional independence. ? Because it's a unit of distribution. What's the probability of every single word? That's your name . , there is no quantum system. that is  a trigram distribution. an example here is, what is the probability of cuts that's in Slide 29. that's a unit gamble of religion. How many times they can cut or come  that. ? all these are what we call the embali estimates. Yes. , that's the unit distribution. Yes. Yes. The type are in your list of words. Yes. . , , . . Yes. if we count the probability of each word based on all the words in the purpose, aren't you always going to put those V on every word, because we use no context and use the most frequent words, which is the , but usually for a standard language model. You combine this difference. , . the matrix into the model. ? you said, if you are using a tool, a library, . Yes, you have to tokenize it , and the choice of tokenization can be. You can split it word by word, or you can do some single one. . Here, we are focusing on a more general language task. . . yes. Not what? you have completed your Gram language model. That is. and then you have. That's that, that is that is. is not, not, is, is not, not, is that? That's it. It's it's it is . That? Yes. 5. yes. . . . And that will be again. Also 2 or 5. , what of is this? Yes. . what happens whenever the denominator  that's in regards nothing. ? . 6 out of 5 not out of 6. , yes. . For this reason. . . . 6. . . One. . what about you? ? . Yes. . you. , . yes. ? Yes. . From what we can see ? . what do you call it? . let's talk about likely or unlikely outcomes. If you are observing a likely outcome, less information is gained. observing the word gay d. But if you are observing a more rare information, more information is gained, and we can  use this. information of X in slide 36 equals the log of 2 of one over B of X. ? and if you do, the math one over pi will give you negative. . you're gonna have logarithm of 2 raised to power 0, minus log of 2 pi, and then this will give you negative. ? I have an example in Slide 38. Yes. . you are you talking about slide? 35, 35 or 36. . no, it's the 6 30.  that's the formal definition of what is information in terms of this is the definition from information to Europe. Why is it one over the 1,002 in the law? This is more. Of of how information is defined. And then we're trying  trying to borrow the concept into Stats square of you, . The idea is that entropy is the, of course, entropy is the minimum number of bits in order to communicate some message. P. But we can. We already train a model cube which is our model distribution. at test time, what you are going to use is you're not going to use, say. The model distribution should be close to the true distribution. ? . for it. Every caffeine is beyond my control today. . . I'm . Go for me. Yes. ."
    ],
    "Topic 2": [
        "How can we model language  far in context of test classification? where's the zoom link? I don't know. there is a there's a zoom table. And then we have examined. We don't really care about the order. We just care if the world East in the what do you call it? But we don't really care about what is the context we don't really care about does it depend on another world? And here we are going to try to move to something more realistic in how do we do language modeling  that we can also factor in the context or the previous words before the word we're trying to predict. does that word depend on previous words or not. I want to remind you of what is the word which you probably know? , but the problem is that it's not very easy to determine sometimes. Especially if you move from one language to the other. It's always difficult to have a clear definition on how to separate. You can clearly see what we're trying to say in the 1st world we have  3 words. And but the problem is that they are not really separated by space. And this has caused a big problem in Chinese. One of the best way to separate your words in the sentence is just to use what's the space. But when I started working on Nlp, this seems to be very trivial, ? Just separate by space. But there are some times where you have things  a poster fee, and then you have to determine how do you separate them? For example, the cat sat on the mat. and then you can separate. and the question is, how do you separate? They're very similar, all , but they are. And one of the ways you can match them  that you don't have very similar worlds that are talking about. If you want to combine, run, and runs. For for the 1st example. , this gets more tricky. and then we also have the way where you could combine and normalize words  realize. I'm doing the same, for happily we just remove either way. The last few words at Carras. and also you have cases , apart from,  what is British, English, or American English. Similarly, you have things  fragment and fragment on our cases. how many times does, for example, in this example, how many times does the word cat occur in the card sat on the mat. how many times does the word d or call in the sentence, if you are still following. I haven't lost you 2 times, and for the relative frequency you have to just normalize this by the size of your sentence. which I think is correct from the world. You always need a corpus, and of course you could also have several copperam that you will combine to form one gigantic text. Let's assume you want to analyze words in general in the English language. What is the representative coupons for English? What is the representative corpus for English? If you want to analyze all words in English. That is one answer. A very good example that can be representative will be  the English Wikipedia. and I think it has maybe the gigs of text that is big enough to  capture different cases in English. probably it's not the best, but it's still one of the one that is representative enough or English. we have other examples that are previous previously used  a brown corpus. The word that is infrequent might be around 30,000. if you have this inverse relationship, that means you have to introduce a constant. which is a file and B, but typically what is being used is that B is equal to one. it's not very complicated. And some people just say, this file is depending on the language. initially, when they did this, since there is work perfectly for English. This is probably not  true. There's no way to recover this if you missed that. For example, in English. adding more words to the , and then it becomes longer, and there are many, many languages  that in the world, for example, subun languages in Africa, , you just keep adding. here I want to talk about the phone to work ratio, which is very simple to how many phonemes are in this sentence? and then you can also divide it by one. and for the second one Cantone east. And similarly for the French word. Here you have 8 phonemes. but it seems  it's the same work. Of the  world? As what's the probability or trying to find what's the probability of the entire couples. if you are asked to put it in  world, given this context, may we add a little. let's try to view this problem realistically. What is the probability of the world equals Lamb. You can say, language model is the probability distribution over a sequence of words. Just imagine you have a sequence of words. Combine all the text in English booking period into a gigantic text and then complete. using the book and maneuver of the country with all those things abandoning CPU of w. 1, even W. 2 W. 3 WN. probably. probability of the blue one. and then you can also do it this way. Probably the way our group expressed, this is probably not the best. You hang on as . they are  looking back. for the 1st formulation, I did, that is correct. but sometimes it can also produce incorrect facts. It's an example of a simple language model which is sentence completion. , typically we find a solution that maximizes a combination of tax-specific quality. and if you just focus on having a very good probability distribution, it may not work  on different tasks. there's a trade off for this. building models giving lots of data from real world. ,  the model is, what is the probability of the world? and once the model is fixed, you can then evaluate it on the text data. what's the probability of the world? or M. Graham. It's very similar to what I've explained on the Board for unigram probability. For diagram. And if you say the cat is sitting on the mat, if you just say what's the probability given just one previous word. That means you'll say, what's probability of Mat? That will be the count of all the times. Hats appears divided by the count of all the words in your compos. D,  that's the count of how many times the cards or call. Don't change the order, please. That means you have D, followed by what the cat divided by the Count of B. That's the background distribution and the diagram probability of the cat given P to D.  that is, if you want to model feed of the cats, that will be the probability of cards. Give 1 50, and then that will be the count of the feeding cat. How many times you have feed the cats  that occurring in a couples divided by the counts of feeding. devices was . How many times the occurs multiple times. I thought this was    in part of space. world of the world, just our own. , you're just saying, how many times? You combine the diagram, the diagram, and the unigram together? No, you don't know. I know that when you tokenize we can tokenize into that. We don't have to. But you don't really have to do anything ? You don't need to do it because it works. , because you're focusing on just classification. 5 over 15. . 6 over 15. . 2 about 15. . 2 or 15. . for the diagram. how would you compensate? That's called my Internet? And  by 5. ,  it's the probability the count, of the number of times you have that which is 2 times divided by the what the count of that . the count of that. And how many times do you have that? You have that 5 times ? and then for that is, that will be the count of the number of times you have words that's East. divided by probability, or the count of the times you have that ? what's the answer? No, it doesn't affect the calculation. It just affects how you group damage to tools. typical. it's that we are having is always something. Is this, is that, and say, I think included. if there's nothing you can compete again, you can back off to from diagram to Unigram. But this basic knowledge is probably enough for this lecture before we make this more complicated. San Francisco. I think all of you are more focusing on what's the problem, what we have go to the  one. every background and every diagram that you can see in this in the couples. ,  normally, if you if you're trying to test very similar to test classification. You need to divide your data set into training data and the tested data. But you could also have the validation data. race to power the cross entropy. And then, if it's biased, then you can easily determine if the probability will be 0 1.  there's no need to. Or we don't need to know. I'm just taking this from definition. but if we already know the probability distribution. But the basic idea is that if you want to know what's the cross entropy between the true distribution P and the model distribution in Nlp, what happened is that we don't know the true distribution. And we want to use this model distribution to determine what is the probability of the test. The starter does it , and because we don't know the true definite, we don't know the true distribution of fee. the true distribution at all, because you don't even know it, and you all focus your attention on estimating having a good estimate on the model distribution. But since you don't know the true distribution, you use the model distribution instead of the true distribution. We are just saying that this will be 2 ways to power. true."
    ],
    "Topic 3": [
        "That is what we have been doing. you have a question. If you are already in class at least difficult here. Then we're going to examine more statistical language modeling based on n grams, and then we'll do or maximum likelihood by relative frequency, and then we'll touch on how to evaluate our language model. and I don't speak Chinese. But if you're a native speaker. And I'll be because it's a 1 of the 1st acts they have to tackle is how to do word segmentation in Chinese language. and if I speak of the language will be more obvious, because things are  stuck together by characters. But do, is there a notion of war segmentation that you can  agree on for Chinese? The apostrophe and tea are often separated from the can. Because also the apostrophe and tea appears very frequently in our corpus on how do you even ask a question? and then we have the following word, tokens, which, if you count every instance. You can ignore every other occurrence of the. The question is which we also treated. Probably the second class is, how can you merge them? Yes, Barry. Semi! Is that is that scaling or lemmatization? What it says of democratization, because happy and happily are different. if you want to go to the root word. , what if we have  happiness? Happiness. Happy if you have happiness if you want to cover happiness to happy. To be honest, I'm not even sure. Is it the American way of writing it versus the British way of writing it. Yes, you just  to chunk it. We can also have a way of capitalization. if you before you do before you compute your. Probably another person will say it can be dictionary. Oh, British national couples. It differs for different languages. what was the motivation between adding the additional parameters  this? Was it experimental? And then we have something that can analyzes that if you can provide the  file and . Do you have an idea of morphology? I can explain. And then for this  thing, it's really difficult to. And you see that for the initu, too, which is very, very different. you can compute this probability, but sometimes, when we are working on this, we often ignore this random, variable which the statisticians may not be very happy with us about. But anyways, we would do. We do this , equivalently. Using this chain rule of conditional probabilities which I can explain. we need to. I need to review then, this topic that I realized $4 ready to, . Given the rest. Express the hospitality on document I know you are. , this is probably the this is the better formulation going down here for what you are describing. Yes, per student, the product term be from I of one to N, and then WI, plus one that I minus one. But for the second formulation we have to modify. then it should be able. It should be easier to predict. 2 or Gpt. There are many applications of language model. You also need a lot of model to be able to go one after the other to generate what is the  thing after the purpose. or they'll add the language model probability. But also that task may not realize. The idea is that we want to find what is a set of parameter that describes the data. It just means you have to estimate every single probability for that text. And if it's a neural network that means you have to find a set of parameter that  can model everything, the entire text. That means you have to estimate all the probabilities. That means you have to learn a parameter theta that  is able to predict the  word given the focus work. Rdb, learning model. I have an exercise for you to processing very soon. The maximum likelihood estimate. Because you wouldn't need this for the exercise. verb will be . , this is your exercise. We just need those. Oh, for the, for the assignment. I think you're supposed to generate from . And then once you have those grams, you have the matrix, and then you feed that into the model. Yes, I agree. you want to estimate the probability of that's that ? it should have been easier if I'm projecting. How do you compute over 5? And and that did. happy end of the sentence. We always have,  another end of sentence token that will always support that. I think the thing is that if you have the end of the war token. I don't get a question. if you add end of end of sentence token, you already added for the entire purpose, and then you compute it. , our goal is that we want to just estimate every probabilities we can estimate from that text. the idea is that, can you estimate the probability of every word. ,  the last sec section is that once the model is fixed. for us to test , if you remember, what we are trying to do is to estimate what is the likelihood of generating the testcos. from information theory. And the more information that you need to know. This is what we want to measure if the information is very trivia. That is the idea from information to. Go back to information theory to  develop a metric that we can use to estimate how much information in bits. This is by definition, I'm not adding anything. If there's maximum fairness that means you cannot easily predict what will happen. the information, why is it locked to a 1 over? Pdf. oh. you want me to talk more about this. You have questions that's the end of luncheon."
    ],
    "Topic 4": [
        ", let's let's start. be discussing Engram language model. Some people are able to join. far, we have only cared about how to. or you at least mute yourself. Because we use  bags of engrams. we can use and grams instead of considering one word, we can consider 2 words that follow each other. and then it's a very simple law, which I'm going to show you is just trying to say, what is the relationship between the frequency of the word and the ranking. One of the most popular technique is what is called a publicity. Award is the smallest unit that can appear in the context. I forgot to say, we are at Slide 4.  what is the smallest unit that can appear in an isolation. If you have a world  football? Or is just football? Also we have peanut butter? it's even gets more complicated if you move to some languages when slide 5, and when we consider the Chinese example. We go to autography, word types and tokens. But we could also differentiate between token and types where types are unique occurrence of the world. And here, in the 1st example, cat marks on Saturday you have 2D's, and then and then, when you're trying to say, what are the, what types that will  go to your vocabulary. And then you have  5 war types. Let's assume you want to build a vocabulary. Or let's say, if you want to convert Rons to run. to be honest, it could be both in that case. Then maybe you can. but before you build your vocabulary, if you lowercase everything, it's better than if you don't lower case. In slides number 8, we can compute what is called word frequency, or we, the popular word that we use is what is called time frequency, where you want to compute the number of words in your compost. We have to say. This is very simple, it appears just once. Relative frequency. for you to do any calculation. Someone says the Oxford Dictionary. English Wikipedia has been contributed by  many people. , this one is very simple. You just want to connect the frequency of what type to the rank of the world type. we are saying, the frequency of war type is inversely proportional to the rank of what type. The frequency is inversely proportional to the rank. I believe there's no question. This is basic math. We had 2 more constants. We should reclaim from the law of login. and then when it moved to other languages, you find that? Thanks for the other question, where we are trying to see what  equation works for different languages. and in lips you have 80% of words appear all at once. , you have a root word, and then you can append to the left or to the   how that some language is, just keep, append it to this suffix, just skip. also we have several, what types! But when you pronounce it. you find that both alarm and accidents are  good consternation of this one. , probability of reward. Given the context merely at a little. What's a plan for? If you apply the chain rule continuously, , you just use the law of conditional probability. did I miss something it's starting from. Minus one to your hospital. , example, a good language model should assign a higher probability to a grammatical string of to a grammatical string of English. because I heard they might, I would appear more frequently because it could have more combinations than I had in cake. They do capture some linguistic knowledge. and then you cannot refer to this as a good fact. It's able to predict a good fact and a correct spot. just because it seems to it, as if it has a good probability. automatic speaker automatic speech recognition. in terms of status quo nlp. and you can mathematically,  calculate this for either a unigram model or a diagram model. If you just want to build a statistical model. Minus 2.  if you say the cats is sitting on the mat. And then you want to predict the math. that's a simple example. the byground probability of cats are given. just  that! Do you have questions on this? the web  would be  a type. do you have questions? let's start with the unigram. I just have a question about unicrons. I guess that is the unigram. That is probability of is that  you are going to take them 2 by twos. And then, if you want to compute the probability for every unique types there and graph types, what are you going to get. 5 years. I'm not very good. you can skip the last discarded, because there's nothing. A probability of it's eat one of our 2, and the probability of it is. there's something where the cover which is back off. and then you can back off from diagram to buy ground bylaw. Chance of a child. we can  use the model to evaluate it and test the data. after you have feed the data using parameter theta on everything on your training purpose, how do you compare? and how do you evaluate it? there are 2 ways you can evaluate. and then you can also use publicity. Because publicity is just too. The plot of entropy versus the coin tos fairness. Then you can have a very high expected value of the information which is the entropy eye entropy. sweller of this and code information. The message is drawn from then we can compute what is the cross entropy. And if you do this over if you are able to, , you're trying to say the if you have a good model. and from this we, this leads us to publicity and in publicity."
    ],
    "Topic 5": [
        "the 1st thing we want to look at is how words are distributed. Until today publicity is still being used even for the big models. Is this one word or 2 words, and if you translate it, for example, to German, you have something  Foosball or endless person. I think by  this is very obvious. How many words are there? Would it bematization or stemming? What are you going to do without a dramatization of studying? Because then we are just  truncating the suffix for the second one. for the second one. that would be limitations. Again, if Apple will cover, because we even have different  rules. S. And for this to happen. but you could also have different occurrences. If I throw you that out to you? But you could also just have a very big compass. Of course this will not capture all the social media slants and words that have been used. And the Wall Street Journal couples are  many of us. That's run for one also is connected to the world with the largest number largest occurrence of the largest frequency in our campus. and I think by  you understand the relationship inside 10 that will stay. you have the word deed that is around one. the long tail. And the problem is that some words we only appear once, why? we appear 10,000 times ? we have to know that, and that's why we have a long tail and sometimes this long tail are very important that if you miss them you will not be able to. these issues will often cause problems for us in terms of designing models and evaluating their performance, as we will see. 40% of the words appear once in a corpus. and then you have a handful that come very, very many times  you have a lot of stop words, a lot of preposition that appear  many times, and you have other words that appear fewer times in Bulgaria which I don't speak. You have same number of water types, or call for fewer tokens. you will just have many words that are purchased very few countries. Very, very long word. Why do we count words when you come to what's very important for  building? Because statistical language model is just by counting how many words appear in your couples. ,  this, what frequency are important for test justification, as you have seen in previous class. Given some context. , where we define a random variable W equals a small W. Given a context C,  the random variable here is W, which can take any word in our dictionary. And then we have a context. What's the probability over all the words that have been that's not worth it. And you can decompose this probability. This probability. And then we can also decompose this from W to. Given all this. and you can also reverse it because it could be probability of WN. because using the law of probability again. using a lot of probability. These 2 are incubarents. For example, you're wearing a fancy hat and a lower probability to ungrammatical strengths. for example, fancy you our hearts be wearing sometimes you can have something that is. has a good probability in terms of language model, but it's not very grammatical,  the example here in Slide 21.  also the length, the length of the sentence, and the reality of the words in the sentence affect the probability. you all agree with me that the probability of I had the would be greater than the probability of. Interestingly, they also capture many facts about the world, because if it's a correct fact. , if it's a correct answer because it will occur, it will co-occur with the context, multiple times in different sentences. And that is why, a longer model  Gpt. and I probably believe this is the topic of your assignment to  determine if the fat is true or not, and then we across fire for it. The longer model can also predict an incorrect fat. you prefer that it suggests what's for you when you're typing. ASR is a very a big application of language model and also machine translation. If you are just shooting for tax specific quality. Given the context. For neural networks. Given the context? What is the probability of the  word given the context for Unigram. There is no context. What is the probability of the  one given one single. we are smoking. that's probability of WN. Given WN. Minus one and trigram. What is the probability of Wn. Given Wn. Minus one comma WN. Given. The last 2 words that proceed are . The last 2 words of the format. Given the. word data in no context, no context. probability of each word. Oh,  you're counting one types. It depends on what you're counting. It's unique. unique words that appear in your in your couples. the unique occurrence. that's what they can occur 10,000 times. D, of course. Can you compute a unigram and background language model, using the following sentence. what is the probability of that's what is the probability? No, no, for any level, , , once you have the data. what is the probability of that? What's the probability of is. What's the probability of? what's the probability of what else it? But  you have probability of that. What's the probability of that? And that is how you have 2 over 5.  what's the what's the background probability of that is 2 or 5. . 1st word, or the second word. in the case of that is, are we dividing it by the occurrence is, or the occurrence of that? , it's not divided by the occurrence of the 1st one before the second. The 1st word these words, it depends on the second, the second one. The second word always depend on the 1st one. at the end of the sentence, if nothing is happening,  this is more  a unigram. what's the probability of is one over 6. , probability of is that 2 of us should be part. Probability of, if not 2 of us. Probability of not his. I guess we almost covered everything. But I felt  a factor identified   as the vibrant is, and then and 7 isn't  an additional option with  one or 6 or 12.  you mean for the background? your background. that is the goal of language here. What is the probability of the test compost. 1st we can use cross entropy. And there's a nice connection between cross entropy and publicity. consider some random variable X distributed according to some probability distribution. Of course, we can define the information in terms of how much certainty we gain from knowing the value of X. there's no need to waste a lot of that beats to encode that information ? and entropy is just what is the expectation? What is the expectation of Ajax the expected amount of information we can get while observing a random variable. an expectation of Ix would also give you this formulation, which is, you sum over all the pis log of lot of Api. If you had a log within. Do you want to quickly explain how information and the logarithm are related in pivots. here in the cross entropy, . and the cross entropy is also defined this way. we come up with an approximation of what is the how to estimate the cross entropy which will be defined as minus one over N log of 2 of the probability of the model you have trained. And then you compute the cross entropy based on this. The cross entropy. Some people formulate it differently, which would be if you assume log of E. This can also be the exponential of the cross entropy."
    ]
}