{
    "Topic 1": [
        "today, we'll consider sequence modeling with features with a focus on linear chain conditional random fields conditional rental fields. And then you can  boost the performance by one or 2 points. And then we also have observed variables . For the case of Patos Beach. it will be the States will be the pos tax. Thank you bye  for the case of   again. you can also do that . And if you have been able to compute all the statistics needed. And in that case a discriminative model is more realistic. We're back to the . we consider a couple of our guardians. And if you're able to compute that, you could also integrate that into the computation of What do you call it? and then you compute your needed parameters. and then you maximize in the maximization stage you try to get a better initialization. just to revise what we did is that it's possible to compute this iteratively, even if there is no labor data. again, for the emigrating  that we believe that we randomly initialize Theta K and your Theta K is all your  all your A's and B's in the, in the last formulation, you randomly initialize them, and once you have randomly initialize them. And after the at the maximization stage, then  that, you see, can I get a better Theta K plus one. I computed the expert accounts based on initial values or randomly initialized data K, and at the maximization stage, can I get a better one. that I can have a better initialization. for the expectation maximization, it boils down to 1st  computing the probability of every State given the observation. And then, if you apply the E step. that normally you initialize what is your Sita K. Because Theta K. You don't know, because there's no data to learn it from in the unsupervised case. You need these parameters A and B and the parameters A and B are computed from this grid that I showed you the last time. And once you are able to compute this, you'll be able to run your Em algorithm even for the 1st state, because you randomly initialize your A's and B's because they are known, unknown. If I go to the example. Random values for all this. because there is a formula to use. You do with some random initialized values, and then you are able to compute your Alpha highs and your better eyes. And then you can  compute your gamma apps at the E step, and then for the second parameter, you can do the same thing . And once you have computed all this. And based on this, you can  run the maximization stage. And the maximization stage is that you  want to get a better values or better initialization for A's and B's. the hard em, this is more. why the soft em, it's more. A small soft em soft where we belong into soft game. and then you can go back to your E step and perform the algorithm again. And of course, there are some additional proofs that you can check here. And we said, , there's some issues with this, because it's very sensitive with to your initial in your initial random initialization. and they are not very realistic. And, , in this sentence, Maggie University is located in Montreal, Canada, and my gear University is an example of an organization. , let's assume you want to analyze. I can make a note that has been written by a medical practitioner. and even for general domain you can expand these named entities, , you can distinguish between different kinds of location and GPS, which is geographical political entities,  you can distinguish between entities  city names, country names. continent names, and then you can distinguish it from entities  mountains. and reverse and give them different kinds of entities. But  we have many schemes. Let me try to give you  a 3 letter word, and I can show you the different schemes. on the board you have,  3 kinds of scheme. the 3rd example, here you have what is called  the start of the entity. The 1st question is, how do we have more features to? how can we include this feature into the standard agent? these are some of the shortcoming of standard Hmms, because  it's difficult to add new features. And the answer, today we want to talk about crf, which gives us an opportunity to have new features  to our model. the question is ,  the idea of crf, because , these are  research driven work. how do we modify the original formulation for our dhmm. for the linear chain crfs, conditional random field. one example of the feature is . Then we have a feature  that. Once ending with Ed, we can add another feature  that, and  that, we can add more features to our modem. That is the idea of adding new features to Cf. this requires a lot of feature engineering. Which I believe you are quite familiar with after seeing this for the last 2 lectures. In the last one we have. We have the initial pro activities. ,  because F of K would be one, ? Is that a 1 or 0? each of the 3 can do. There's no analytical emery solution. why aren't we normalizing here? Given the sense you minimize but of course there are other metals  the conjugate gradient, or what is called the Lbfgs. You minimize the negative log, likely. And then you do this for a while. And here we can  compute what is the gradient of the log likelihood based on the Crm formulation. Can you see it's too tiny? we want to take the log or a full drop of all this. And then the availability of why. and if you substitute what you have, then   computes. Here you have the love and the exponential which we can cancel out. And then we can also apply the derivative for this one. and if we take the revenue team of this one no. And here we have to sum over all the States wide information of solution of team 5, 1, oh. I'm  if you want to speak with the regulative one. we really have the submission of vaccine here. one t, 1 t management. You will find out that everything here is very similar to the formulation we have here above. the only difference is this solution can  go out. This does not depend on why,  it can ring. When you have the solution. The final expression on this , salvation solution over, apply. There's a 3rd sum over. No, no, because in the formulation here. this is all about, why? in terms of the interpretation of gradient. We have the expression here. We have this expression on the left side. minus the expression on the  hand side. And what is interpretation of that? which is on the left hand side and the  one. because these are uncrafted features based on what you have in your compass. and the last one which will be the expected distribution of Fk. If you're able to achieve this. I believe this is correct. The  step, if you get out of that. it's it's in the sound for all the eyes, and then in the  line you take it out, and then we take the derivative over just one log set of exposition. But I'm working on that. , where each class is. And we are calling the which will be one the hard em, ?"
    ],
    "Topic 2": [
        "especially for a token classification. , people don't use it that much as before with the Llms. That can do almost everything  but for token classification task. We  find Llms to still struggle a bit with talking classification tasks. If you want to improve performance? ,  for the Ed Markov model, this is a summary of what we have been talking about in the last 2 lectures. we have a Markov model, where we have different states  q. and the observed variables will be the words,  this is a generative model. And I want to emphasize in generative model. Given an input X for generative model. to    to compute the joint probability. Why, as you will see in a couple of slides, crf is an example of a discriminative model. in the generative model to reemphasize. If you are supposed to find an output y. I'll try to learn the joint probability between X and Y, and the assumption is that if you can learn successfully the joint probability between X and Y, then you learn everything about the model. you can  compute Y.  that is the idea of generating model was, as , in practice, this is  not possible, because for you to compute that you have to make some assumption which is  the Markov chain, assumption or independence assumption to  be able to compute these probabilities. They do not make this assumption because some of these assumptions may not be realistic, ? And a very good example is the artificial neural networks. Which is a generative model. Can we learn this general probability? It's difficult to learn this joint probability over all the States, and therefore we make the Markov. we make a Markov assumption where we  explore conditional independence assumption. And also we add, that is what we call the state probabilities, and for the emission probabilities, we say, if you are able to successfully learn the generative model, we will be able to also have the initial probability, which is the probability of absolute given the State. And this is a very simple example of a generative model for a diagram distribution where we see Q. And you can also emit all one based on every single State. You can also emit every, all eyes based on every Q state. Because our focus is to learn this joint probability, which is the probability of Q comma O given the parameter theta. first, st for you to compute this probability of Oq. you need to know what is the likelihood. and the likelihood is what is the probability of all given Theta. And we said last time that you can easily compute using dynamic programming, you can compute the forward. You can compute this likelihood. Into the computation of the entire State probability, which is probability of Q. Comma. When the 4 are going in the sum over all the previous States. And the last time we also said, . It's also possible that you can do this when you don't have any labor, they do no label data, no problem. Then you can use em algorithm to  estimate the labels. Even if there's no label data. You repeat for a while, and then you predict the current state sequences, using the current model. that again, you repeat for a while very similar to what in the last slide you predict the current state sequences, using the current model with the vitabi algorithm. on to go more into the details. what we discovered is that . and after that you can  compute the transition probability from going from State I. this one where you have you can compute. A question is probability of O. Given Theta K, you computed using the entire forward algorithm at that state. better probability distribution you can use. then the  thing is that if you have all these values, you can run your forward identity ? I just want to be sure that I was showing the same thing on zoom on the class. that is Tita K, . A's and B's here will  form your new Tita. This is distribution of loan books. and  for the em, this is what we are doing. , if the likelihood doesn't improve . that if you can estimate the likelihood given Theta K plus one, and the probability is bigger than probability of theta. ,  this is where we stopped the last time. And today we can move to the  topic off. I forgot the last the last thing we discussed is that apart from part of speed tagging, there are also other sequence bodily tasks  chunking where you are not only classifying every token. And also we describe the name density, recognition task, which  doesn't require you to annotate every single word but import. Montreal is an example of location, name, density, recognition can be different, depending on the domain. I have to emphasize that. Our task also in the biomedical domain where you will not be identifying locations because that data, there's no location. , technically, you can have location. You can have something  age. You can have something  disease type. , depending on the domain, you can also define your set of named entities. if there's no scheme here, it's very difficult to know the entity. Come and you them and you need a scheme , that you can annotate this properly. the 1st one that was proposed is, I will be one. I will be 2, because this is the one that is mostly used in standardized data sets. the difference is In the first, st  we're trying to address the problem. And then we just say, , every entity start with iod. and then you have the inside of the entity, which is the eye. if you can add this feature even just by writing rules, without all this fancy, according, you may be able to detect a lot of proper notes. But the difference between this approach is that  we are going to consider something more of a discriminative model , I said in a generative model. You want to lend the judge probability  if you want to classify y given x. This is what you want to classify, but for generation model. You 1st have to build or try to learn the joint probability, and once you learn the general probability, everything becomes obvious, of course, with some assumption  independence assumption to make it easier. , the only way a reason why we have independence assumption is  that it's tractable  to compute this joint probability. those are the generating model. we can learn the probability of Y given X using this function. here you find out that we can learn a function of Fk. And I'm going to define what will be F of K. This is the new function you want to learn. remember, what we want to learn is the probability of y given X. and also the emission probability for a meeting award given attack. we  replace what we have in the , we have a function from one part of speech which is in the YT. and  we replace it by this indicator function of yt minus 1, 2 dt. This is the very simple definition. it's just going to give you either one or 0 indication function. We want to know if a word is capitalized. , what is the probability? and the interesting thing is that when you're working on Crs. part of speed tagging, using Crm, you have to come up with a lot of features. , you can say yt equals a part of speech  we showed in the last slide. Is more of a generative model. And Rss Crf is more of a discriminative model. for the forward algorithm and , we need to compute the likelihood. And also we need to find the States that maximize the joint probability that's for the  and for the crf, what we are doing is to  compute the Z of X, which I'm going to show you how to compute based on forward algorithm again and for the Vitambi authority. B, and then we have, the transition for a brief space. For the Crf, we are just interested letting a function. every other thing is the same for the forward agriculum. I believe that's that's the way the author has proposed it. But I don't think it's it's going to ruin anything. But the way we learn it is that, unlike you can. But we have to learn. But of course there are also other good Edison methods that can be used in terms of convexity, which many of you may be familiar with , when you have a convex function. It's going to become a convex function which I believe that. We normalize in a the definition of. you  don't need to normalize. , aren't we using the forward algorithm to compute Z of X. because that's the probability, . that's what we said, . you walk in the direction of the gradient to maximize l. Of theta. which approximates using the second derivative . And that's how you can convert a concave function back to a convex function. first, st you compute every other thing you need to compute using the forward algorithm  a forward algorithm. This is what we're going to find after taking the derivatives. Bring up the submission all . This will be equal to one over 0 of x. the entire thing, one over Z of x. And if we take derivatives for this one. If in 2 of x,  this is  indirecting which I spare to evening. you have the same expression information over here, and then we are 14  slide. we need to compute the derivatives of what I'm inside, and then we have. commission in my old device. And then you have probability of 1, 5, 3, 1 x, ? Why don't see 14 number one x, 2. here you have , instead of having yt. if you want to know if something is capitalized, you can estimate this from your journey compass. and if you have the ultra regularizer over this you, if you take derivatives of Theta K squared, divided by 2 to I forgot this one Sigma Square. If you take the derivatives with a specific Theta K, you're going to have this expression, which is Theta K divided by sigma squared. Based on the law of derivatives where the 2 above is going to cancel the 2 below, and then you have minus Theta K divided by Sigma squared. But this is, there are webs, ? Think that's also a demo. It's just  a number, ?"
    ],
    "Topic 3": [
        "and name them to recognition, and they were  some of the most important models that dominated the field for  more than 5 years. And today we're going to see how we can go from Hmms to crf. , they are not they're still very useful to today. And  some of these methods can still be useful in practice. the last time we talked about,  we finished off with also provides learning of Hmms. If you want to predict what would be the output of y. Another example of the generator model. you are not able to compute the jump probability every time. Instead of saying that the  State depends on all previous states, we only say the  state depends on the last one. last time we discussed the following. Why, in the Viter we are guarding, you  just take the maximum. And then you do this attractively. 1st you randomly initialize the States. and then  if you're improving. and the idea would be to predict. And that current model initially, just initializes randomly. And then you update the parameter of your current model  that you can have a  based on the current predictions. And after that you update the current parameters using the current predictions as in the supervised learning case. Which is going from the previous state qt. I still have it in just a minute. And then it's updated at every iteration. let's assume you are a zoom. To be honest, you can generate this for me. and if you generate this from any probability distribution just to fill all these values randomly, initially. And you can run your background as you're ready. your B's your, , are your A's, because these are randomly initialized values. you already have the values of Eij gathered eyes. This is over a single tag. And and this is the stock version of the em, and we can  relate this to what we have done previously. The same way we competed. how do  you are making some progress? You  you're making progress when the training set likelihood , you're not making progress. And this is the way  you're making progress. And this likelihood keeps improving. If the values you have generated in the metrics are very bad. And of course,  this is just a revision of what we did last time. but you cannot try to  categorize things  the  phrase, rather than only is a group. You can have something  patient name. And one of the most popular schemes is something  I will be tagging. But if there is an entity that's an organization that  starts after you received in Montreal, then you will use the B, the B tag. This is the 1st one. these are   the different popular schemes that are available for any annotation. to the algorithm that makes it very easy to predict, , in English language. How can we add this feature? But you cannot use this to generate new samples of water pos sequences. if you learn a giant probability and it's a good one, you can  generate new examples. or you can use it for data augmentation to generate new examples. , there's an  have been used for a few years. that depends on the previous one. But what we are saying is that even while learning this function of xt going to yt. We are also saying it depends on the previous value of y and Z, and we want to learn that function. and for just to go back to the idea of the . in , we are concerned with 2 things are these transition probabilities from one participant to another one. Multiplied by another indicator function where yt equals. are you familiar with education function? For the that's a modification. Oh, that's the 1st modification. that with this we can  add new features. That we cannot add in agent hubs. What is the indicator function that yt equals stash? When xt is capitalized, we can add a new feature  that. Which we are unable to do directly with each of us. And there are many, many features you can add. you can say, , if the lens is less than 5, you want to add a new feature. If the 1st word is a determiner, you want to add a new feature,  there are new features that you can add. To be honest, you have to find features yourself  you can  just a minute. , which makes  very easy to extend. And then you'll say at the same time when xt is capitalized. that we still use things  your forwarding. the modification we do here is  hardened this function. based on the features we want to add. And after this modification, apart from this modification. Why is the exponential function? you have the exponential of this and these values here you find out that these are  just these are just indicator functions which I don't want or 0. It's coming directly from the formalization which they're very sure of. you'll be either multiplying by one or by E depending on the result of the multiplication. It's using gradient descent or one example of gradient deter method is Newton methods to find where the gradient is 0. We're just taking the maths. You'll find that you're gonna have this set of things. we keep moving from one state to the other. And for a gradient asset. But of course you can also do gradient descents where you have a negative of that. And then you can estimate new values of your data after taking the derivatives. I have the proof also here. then you are good to have summation. K. ,  this is very easy to estimate, but this will. This could take more time. and then we have the exponential of this. This call is the explanation. here we have y  music in the slides. But let me double check my notes, but  from the very 1st step the sum over, I we just forgot to write it  to the . it's just the last time. but I will confirm my notes, and  I'll post on it here. if you want to interpret it, , we have 2 things. As predicted by the current model. And  that if you do this stochastic gradient descent over many iteration you can  approximate the gradient decent category and  that  you initialize what is your values of details randomly, and then you randomize or order over your samples in your training compost. And then you find new values of Theta, and you do this over and over again over many iterations to find your ideal setup and ,  that's it. And it's just a perceptual or no. It's just  a weight, ? Because is it because it,  you get this exact state specifically that maximize is there? And we talked about both for unsupervised  we have no labels. Which cases would you prefer?"
    ],
    "Topic 4": [
        "Tasks  battles, please tagging, chunking. , when deep learning came around it was still very important to have conditional random fields to  improve performance. You can combine it with Crm by just appending the Crf layer to the  the last layer of your network. You can use this to  have the expected count based on the it is structures. And you do this attractively over a couple of  you. You do this over a couple of times. you randomly share rise your A's and your B's, your A's are coming from the forward. , you don't know them. Goshen is  not the best. But the initial value you don't know. And if you do this a couple of times. It's been very difficult for you to improve for the year majority. Better ways of initialization would be  using external knowledge, , from an external purpose where you already have more realistic counts that you can use. Accept the important entities in the sentence. all you need to identify will be things  disease of patients. the entities you will be recognizing there will be more than just personal name, and even personal name needs to be distinguished. You can have different kinds of symptoms. And here the idea for any error is that you  need to detect spams of multiple words that are relevant to the entity. You have the entities  organization. But there's a question if comma is  part of the organization, or if everything is a single organization here rather than multiple organizations. And here it's clearly that all of them are multiple organizations  Magu is different from you. Here you have what is called the beginning of the entity. if you have,  a multi word expression, you  don't use B, you just use the start of the entity, the inside of the entity and the end of the entity. and for every other one where they are not much a word you use. And this might be useful for Pos tagging, , we can have important features of our words that , we can integrate to this. because, if not, if you don't make this assumption, it's not possible to  copy this ? But for this community talk, it just learns everything in your daily. I know that which is  a modification of what you already know. the present one and the input  and we also have a normalization constant, which is all of our order in states. And here we replace the product of numbers by just a linear combination of weight and feature values. which we completely simplify the what we are doing. indicator it will give you either one or 0.  if the value, the relationship inside is true. you just have inequality inside, and if it's true, it gives you one. If it's false, it gives you 0.  that means this function of Dt to n, and we either give you one or 0. ? the more features you get. You can improve your performance just by coming up with new features. based on what you define? We are only interested in the value of Y give an X.  this is the forward I got in for Hmms. which can be user defined. But even if you have the log of exponential, the big number you can use this log sum of exponential trick that is not gonna ruin your estimation. it's just to make a privacy distribution. And at the last states, we just solve over everything. instead of maximizing the log likelihood. what if you take a derivative of this? we'll walk this on the board. and then we have a logo. But the 1st part 5.  this summation will go away because we're just concerned with one sister cake. this population will stay because of derivative. because this is the submission over every PPYI give them a time. Please go to the thing on Tikka here. David Ifeoluwa Adelani: Do you have questions? the last that one is  enterprise. If you go back to the last slide. the F of K is  a when you talk about the overall ingredient is the difference between this one. because this one is taken estimation over every, over the over, the entire Z of x, which is the orthogonal   that when the compost likelihood is maximized the gradient is 0.  the difference is 0.  intuitively. That's how crf is able to work based on the user defined features. You can have L. 2 regularizer. This is an example of L 2 regularizer. and for each Mini batch for Sgd. board line, minus log Z of X. It's the derivative of the sum of all of them. I will go by this again, and  send you an updated version on it. Or features we had in the slides. They didn't take exit into account. And it  it accounts for whatever features we include. And we're we're trying to  iteratively find,  the best labels, is there?"
    ],
    "Topic 5": [
        "today's class lecture 9, where we'll continue on hmms, and we'll  move to conditional random fields. And the interesting thing here is that you can  combine conditional random fields with some other deep learning framework  Cnn Lstm. That is based on transformers architecture. We today we'll talk about some of the shortcomings of Ed Markov models, and then we're going to move from generative tasks to more discriminative tasks and of course, we examine the linear chain conditional fields. and if we also talk about the ner models. The States will be the different ner tax,  we examined last time. Can you help shortly, Doc? if you want to solve the transportation task , you want to solve for y given X, there are 2 ways of going about it. It's you can   go through it with generating approach or discriminative approach. A very simple generative approach would be of naive base. This competitive approach would be  your normal Asvm. Given X, your approach is  to. Just give me more native model is just trying to optimize , can I get the a local minimum or the global minimum? If I optimize this loss function. P of all, given theta using either forward algorithm and backward algorithm. And for that you can use the vertebia algorithm and vertebia algorithm is very similar to the forward algorithm. if you remember, in the last slides. Are coming from forward algorithm and your betas are coming from the backward algorithm while trying to compute the forward algorithm and the backward algorithm. we go to the last presentation on Hms. and for the unsupervised setting you are not able to compute these alphas and the betas  but 1st you assume some initial values for the B's which is your emission probabilities, and then you assume some initial values for the ace with which are your transition probabilities. You'll be able to compute your alphas for the forward algorithm. And then you also be able to compute your betas for the backward algorithm. Yes, because you can  use forward algorithm. it means you initialize the entire table as random values. your A's are coming from the transition probabilities and your B's are your initial probabilities. at the E steps you already assume values for Theta K, ? This is the bow vash a guardian. once you re-estimate your A's and your B's, you have a new value of Theta K plus one. And then you go back to your end. Then  that you are learning a better estimation for your T. 10 k plus one. I can give you an example of 2 applications in the biomedical domain you can be. You can have something  doctor's name. there are different classification for any hour that we can  use depending on the application. last time we also talked about the streams. There's iob one there's iob 2, that is, I/O BA scheme, and there are many schemes. But  we just call it Iob or BIO, and if you see bio, usually we are referring to. you would have if you have You  have Mcgill after Montreal as another organization without space you would all use the organization for this. And everyone asking is the same. ,  what are some of the shortcomings of the standard? if a word is capitalized, this is most likely a proper no . And also what prefixes suffixes? But for discriminative model, you can  learn the probability of y given x directly without learning to join partners. And this is what we typically do nowadays. ,  this is more  a having a tax specific model for secrets labeling. And in 2021, someone proposed. FF key, and then we normalize it. in terms of the inference. But there's a slight difference. Times, which is the initial state probabilities. There's no closed formulation that you can derive that this is the mle for the crf,  you can. If you take the negative, or  when you have a concave function, if you take the negative of it. Yes, if you do it over everything. the forward algorithm isn't computing probability of y given X, it's computing Z of X,  it's computing Z of X, yes. You work to minimize the loss function. But  we can go through very quickly. and if you apply the log we are going to have some extra this time. Can we start over live? Why did you take the Derek and see of a loss function with respect to a particular? If you take the derivatives of the loss function with respect to a particular etc. what would the left is summation of I. Submation of T, and Theta K is done, and then what you'll have will be the meanings F of K. One C y, 2 minus one. we still have this one because it doesn't depend. You're going to use the chain rule. You can edit this here with respect to 2 x.  if you apply the channel. We have the solution over T, because we are only interested in one particular case. Yes, I didn't understand the last step where the 2 sums become 3 sums, and it says, can rearrange in terms of local State transition. we just changed the expression. the one on the left hand side can be seen as the empirical distribution of the future K. In the training purpose which you can  estimate. This means that finding parameter estimate by gradient descent is equivalent to telling our model to predict the features in such a way that they are found in the same distribution as in the gold standard. Another thing is that you can have regularize a regularizer to this. I know some of you ask about regularizer in your homework assignments depending on the model you used to. , but apart from using graded designs,  we said in the previous slides or previous classes that you can also use stochastic gradient descent. and the idea of stochastic gradient descent is when the mini winning batch size is one that's where you have stochastically resent. Then you compute your derivative over this Mini batch. That should be sum also for all the guys that should be some of our work. what is Z of X doesn't is not over the I's . And what is Theta K. Is it still initialization, transition and emission? But here there'll be many more  . , which algorithm, , , , when would you use the software  if you have,  an unsupervised problem?"
    ]
}