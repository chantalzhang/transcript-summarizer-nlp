{
    "Topic 1": [
        "Jackie Cheung, Professor:  the captions button disappeared. please use that because  this link no longer works. And we're going to look at how we got to the current state of generative AI from this line of work. And then we're going to look at some simple, basic methods from the past for document summarization. focusing on both single document, summarization and multi document summarization. But focusing on the extractive case. in traditional newspapers, when people still printed out newspapers, usually articles they have  this short one or 2 sentence  it's called a byline in between the title and the body of the article, and that sometimes appears, and it's  a very, very short summary of the rest of the article. and it exists as   that everything is online, that it also exists in that you have a short paraphrase of the rest of the whole article, and then you can decide, based on that, whether you want to click on to that article and read the rest of it. And even more than that, there are many other applications of summarization that are  in deployments. Because of all of the insurance stuff they have to handle there. here in this demo, there's a video and all that's very flashy, and it says episodes of hypertension. Technically, I should say that. And because, , my claim is that summarization is just. It's not just one thing. And that's because summarization that very general notion of figuring out what's important and distilling it down to a short passage, a short piece of text that's very under specified. And  we can discuss this and analyze this. there are  multiple purposes you can have for a summarization system. One is to be informative. or just , , you, you want to read things. A short version of things to understand . That's arguably more of an indicative summary, because that short piece of text is there to help you decide whether you want to click and read the full article. or, if you have , search engine results. And then the search engine results comes with both the link, and, , , a very short extract, that of the some relevant part of the article. , the reading assignments that you're doing for this class. And it  contains a mix of these purposes. I would say,  mostly informative and  critical. ,  a summary doesn't have to serve only just one purpose. In fact, arguably, you could say that  these different purposes,  some of them, you might not want to automate. In providing some critical summary of some material. there's there are all the obvious things  what is  the domain of the text you're working with, what is the genre of text you're working with and  forth. And I don't know other kinds of content. But  another basic way in which systems can differ is whether you're doing single document or multi document summarization. The reason for that is that in multi-document summarization there are additional issues you have to handle. Sometimes it's just  you have a news event, and then more things happen, and then you get more information. But have you seen,  , on Amazon? They  have,  short AI generated. it's AI generated reviews of  the overall user opinions. On the other hand, when you're in the process of  generating the outputs. If you have redundancy on the input side, then you don't want to say redundant things on the output side. you have to watch out for that and be careful about that. This is categorized is whether you have extractive summaries or abstractive summaries in extractive summaries. And it's  still an important task to work on for its own sake. Can you think of examples of this , why might you want extractive summaries, even if you have the capabilities to do a good abstractive summary? In that case, in those situations you don't want to accidentally put words into other people's mouths,  to speak. and  you might want to directly copy and have it exact wording as it is in the extractive summaries outputs, and even then you can get into trouble, because then, if you lose the context,  there are all these things  about. and that you can make an assumption that  for the general public audience. or even  an information retrieval system , if you type in a search query to on your favorite search engine, and it gives you some summary outputs of some, some of the of, say the 1st link that it returns. Then that seems  a summarization output. And it's it would not be that useful, for  a layperson. that's a type of another type of user tailored summarization system. That's why I claim that summarization is not a single task. You might need a very specific  model or technique. All of us  finds different things interesting. And then you can use that to say, . we cannot directly measure it. You  say many things about that topic, and you might say many related things about it. You can use word embeddings you can use. Convert every span of whatever granularity you're interested in into a representation. , you can define it as what is the average distance between this point and all of the other points. If this has low average distance to all of the other points in the source, then that's  located. You can  make arguments either way that you can, if it's some notion some ways of modeling content would be. What would be something that would be . then  you can say a little bit about this. It might be a bit farther away from , what's more centrally located, which is about  salmon and fish and ecological preservation. it works for this article. 1st we talked about centrality. just ,  it could be an article about sound, and it could be article about anything. ,  they're in the back. and they're all they have. It turns out that the beginning is the picking that the opening sentences is the best predictor of important information. Why, that would be the case. people are working on very, very tight deadlines. just chop things down to size  that the printing deadline can be met and the newspapers can be printed. Then news article writers were explicitly instructed to write in such a way that the article could be cut off at any point, and it would still be a coherent article just to fit the length budgets. And  then they will always start with the most important, and go less and less and less and less important in order to support that. You keep reading, or you abandon and say, , that's it. , last sentence, that's a good question. sometimes I put a tldr  at the end, which stands for too long. Usually it makes a lot more sense to read the title and then decide. Do you keep reading and then read the abstract and then decide. and it's only usually when you're in, when you're doing research. And  you just don't have time to read every single paper in the same level of detail from the beginning to the end. a summarization system can also do that. , , , let me just ask you. What do you expect to see in an article about an election? Hopefully, people are making their decisions in a rational way, based on this information. , ,  basic information about the process surrounding the vote. , ,  then, a summarization system could try to learn this. Either you just directly specify it , here is a domain. Here is the import important types of information, and try to capture as much of it as you can. those are the important kinds of information. There might be more than that, though, because then, after you find the important information,  to do some things to do with  aggregating common or contradictory points in order to draw some new conclusions or inferences from text. What I would say, is  one of the simpler settings which is single document, extractive summarization. Again, remember what this means is, you take existing spans , say, existing sentences. there are some incoherencies created by the extraction process. And then here are some discourse, features , is this sentence the 1st sentence, or the second sentence, or  you also throw in features related to  discourse cues , if are there words ? Who votes for lexical features? here that would be the words with the output decision of  is this sentence important or not? what is considered what would be an indicator of an important sentence in one type of article might not be an indicator in another type of article. that's why I would expect and work has found, that using lexical features with is less successful. And  that's why the discourse features are the ones that are better suited to this type of supervised learning approach. , then it would be. Then you try to directly implement heuristics that measure centrality. In which case, then, you can use these lexical features. because  the arguments I made about how words indicate the topic. but then that just means you need to do some adaptation or do different training steps for the different languages. They did have statistical models. it was based on what we can  consider a supervised learning method, using statistical methods. And then in the nineties Lynn and Hobie did something similar. And then after that, it's it's it's dependent on the specific type of newspaper corpus. It turns out that you want the 1st sentence of the second paragraph, and the 1st sentence of the 3rd paragraph, because of the particular way that those products were introduced and described. whereas on Wall Street Journal, which is  economic text, it's  the 1st sentence of the 1st paragraph, and the second sentence of the 1st paragraph, and  on, and  forth. We already checked BBC,  we don't have to go back. Another approach that people have taken is to try to implement this idea of centrality. And this notion is super important, because this also formed the basis of all information retrieval systems. Even  I'm sure it's used in current search engines. which is the idea that,  not all words are equally important. I don't know something to do with penguins. which stands for term frequency times, inverse document frequency. The heuristic here is that a term is important or indicative of a document, if it appears many times within that document, but is relatively rare overall. An Idf is a little bit more complicated? Here's another method again by Lynnon Hovey, which is very successful. The idea here is that they use tools from statistics in order to determine the important words and phrases within an article. What they did is they set up a statistical test. And then how many times words which are not the word needle appears in,  the related in the health domain versus, not in the health domain. Once you've set this up, then you can do a hypothesis test. and you can do a binomial test. and once you have that, then you can set up a statistical hypothesis and run a test. where you have a single parameter. then the difference here is in your second hypothesis. You compute this likelihood, using p. 1 and p. 2. There's a statistical test you can perform, using a log likelihood, the log likelihood ratio. when they applied it to their data, it works very . They show some examples in their paper which you can look at. And  this topic signature method works  extremely . And again, this is based on just a very simple notion. then it would score highly. And then that gives you some notion of  what this article is about in the multi document case. But in the multi document case, then there's additional things to consider. But , redundancy is both good and bad. You don't accidentally select,  the same sentence many, many times. and this is extremely simple, but it works quite . it's a good baseline for multi document summarization. what this means is you select the sentence that contains them, the words that are on average the most frequent. And then, after this, to account for non redundancy, what they do is they do this thing, which is not very  theoretically motivated, but it works, which is that they just update the probability of all of the words in the selected sentence by squaring it, and because these are probabilities that lowers it. , , what is it? that there's potentially another second very important idea about something that it's on the 1st one. Because you , that's a good question. that's why the squaring works. then you would decrease the probability of a relatively rare word and similarly, for function words. , some of the later work that people have worked on in this literature include,  updating this, the inference procedure. Similarly, with things , therefore, or whatever things that these words don't make sense outside of context. if you're doing an extractive system, then you should avoid selecting those sentences. by doing this they got a very high score. one of them directly had. and then some basic and all the other methods are , lower a bit lower down there. You can also do this via reinforcement learning. you think of summarization as a reinforcement learning problem, where your action, if you've seen reinforcement learning, the action, is to select a summary sentence or not selected. and then the reward signal is , how good your estimation of how good it would be to select. and if you have not seen reinforcement. Don't worry about it, but it's not the contents of this class, but it's  an alternative to supervised learning. , this is one of usually the 1st questions I get when,  someone asks me, I say, Oh, I work on summarization, they're . But how do you tell if a summary is good, isn't it all subjective? But and also if it's subjective, is not necessarily a bad thing. But , to evaluate a summary. one thing you could do is you could just ask people to rate a summary  from a scale of one to 5. This has advantages and disadvantages. what counts as a 5 out of 5 versus a 4 out of 5, or  a 1 out of 5.  just think about , , you use your phone to check the ratings of a restaurant. There, it just doesn't work. what is  a good reference? You ask someone to write that. You compute some notion of overlap between the reference summary versus the system generated. and between the system summary and the reference summaries. and here you assume that you have multiple reference summaries. you might ask multiple people to writes this reference summary, because  you believe there's a diversity in the range of possible answers or good summaries that people can write. and , because people are too clever for summarization. I'm gonna skip this because I don't think it's that interesting to compute where it overlaps. You ask people to highlight and find chunks of information in the in, in the reference summaries, and then you also ask people to find chunks of information in the system generated summaries. and then you ask yet more people to link the 2 of them. Here do you find it in both the reference summary and in the system generated summary. , and from there you can derive a score, and you can , you can check whether the system generated summary contained the chunks of information which are considered to be important because they appear in the human written summaries. you can, if summaries are , meant to be useful in the education setting for learning. , then you can directly check. You can ask students and to use a summary  you either. or you might be interested in testing whether summaries improve the speed of doing something else. and then they either they had one condition where the people had to just look at the documents themselves alone, or they can have access to both the document and a summary of the document, and that work showed that, having summaries can improve the speed of this process. You can do things  that. And how do you  do a generation test?"
    ],
    "Topic 2": [
        "today, we're going to talk about automatic summarization. There's a different link that I posted on Ed. for today, we're going to focus on summarization. we're going to look at what is the definition of automatic summarization tasks? I'll define what it means, what extractive summarization means. , automatic summarization is the task of turning some source text into a summary. This,  appears all over the place. This is one that I find quite interesting from the recent literature. it turns out that doctors  spend most of the time not necessarily directly interfacing with patients or doing anything clinical or medical. And  this is by nuance. This is a system by nuance called dax copilot. And then they've released a product in that space. I consult for Microsoft and Microsoft owns nuance. But I'm not at all involved with nuance or with this product. I'm going to call them . summarization can be a particular summarization task might involve specifying the purpose of the summarization system. and also assumptions about the users who will be interacting with the system. , if we think about the purpose. , hopefully, it's allowed you can read it  as long as you don't  copy from it and plagiarize it,  it's allowed. Then that's that's considered informative. It's an indicative summary provides a link to the source text to help users decide whether or not to read it. With respect to your query, that's also arguably indicative. And then, thirdly, a 3rd purpose might be a critical summary which provides an opinion of the source text. And I'm going to restrict myself to texts for the purposes of today's discussion. You could also summarize other kinds of material. It might be that if you're doing, say product review summarization, then different users might just have had different experiences and different judgments of a product. also, there may be redundancies between different documents. And this is  both useful and a problem to handle. If you have multiple people saying the same thing that may increase your confidence and trust that the thing that multiple people said is  true or is important. some domains require the exact wording to be preserved. Oh, you're quoting me out of context,  the context is not what I meant, that  thing, but at least the problem is slightly better in the extractive case, whereas in the abstractive case, then, people could really just say that claim that you're miss misattributing what they said. where there's no particular point of view taken, and that the what, the goal here is to take the source texts. Because what counts as generic and what counts as  neutral,  neutral, without adding your own point of view. they might have similar views on what, in terms of , and also similar states of background knowledge that  that you can generate a -called generic summary for them, and then it. that this term exists generic, but then  it's worth taking it with a bit of a grain of salt, even though lots of people use it in summarization and  elsewhere. you can have user tailored or query focused summaries where the summary reflects a particular specific specified goal or priority of a user. But it is query focus. Or you might just decide to generate different summaries based on the background of the user in the in the medical space there is a corpus called up to date, not a corpus, a resource called up to date. , except it's not a wiki. They assume a lot of basic background knowledge in medicine. and that up to date also contains another version of those same articles that are written for lay people  a lay audience,  patients,  to read and try to understand what's going on. And  you can imagine that, based on the assumed background information. And and there's a lot going on and depending on the specific settings. And let's also think about  which of these are easy to quantify and measure with the techniques that we've discussed in class, and  which ones are slightly more difficult. And and we might have different judgments about  what kinds of information, we trust, and what we find important, based on what we trust, and  forth. But even if that's the case, it's not practical. using  other heuristics or other cues. And and again,  it's a lot of the reasons that people choose. And  some of these, this is not super costly to compute. And  that means that you can pick up on the cues of this distributionally, because it means that in the simple case you can just look at word identities, and  that,  the word appears again and again and again in  in a passage. A more sophisticated version of this might be that, , you have a lot of a bunch of text, and then you can use your favorite embedder. some  transformer based model or Lstm, whatever. And then, once you have that just visualize in your mind that this all exists in some high dimensional space. we want to remove , stop words. Do we want to remove stop words? Quite , , assignment one. that's all I'm going to say. It would be such that it matters, but  depending on your stop list and depending on the specific nature of the data set. you should not remove it. I'll say, BBC, , , is our assumption  true. After a hundred years salmon have returned to the Klamath River, following a historic dam removal project in California. that salmon appears a lot  in this article for obvious reasons. clearly, this article is about salmon, and  salmon appears a lot, but also related words will also appear   fish appears, and  trout appears, and  forth. we just saw this example of the salmon article on BBC,  there. it's just to grab attention. But what about for  event based news articles. , these are all cues good clues for news. , it turns out that  all the clues, you said are . All have an effect for news articles for event based news articles. again  the days when things were printed on actual physical newspapers. and that means editors sometimes have to. for news that turns out to be the case. ,  within the thread,  all the posts, are already ordered . just in case some people didn't hear a lot of the times in a long post sometimes, people would ask a question, and then the last sentence, it might summarize, give some context, , , that's ? you usually to get into an area  to read  hundreds of papers to , understand the lay of the landscape of the work in that in that space. , in fact, in new summarization. it took,  10 or 15 years for the field to beat this baseline, which is just to take the 1st 3 sentences of an article. It turns out that is a really hard baseline to beat, and it took us many, many years in order to beat that. and  as to think about how this piece of text is related to background knowledge and queries. And then you can  see the point. I'm also gonna put costs. Where to donate for relief. Yes, , depending on what it is yep. Done properly for you guys. And again, these days,  one, strategy, is to just throw everything into a pre trained, generative model. and  do some prompting or whatever, and try to get outputs. the specific words you're going to use and how you're going to put the sentences together and all that. this is simpler because  by extraction. this is the simplest, because most of the work has to do with content selection. ,  , all of the work is in content selection. you need to work on  the ui , how do you present the summary to indicate that this is an extractive summary? But then you try to indicate that to the user. ,  one approach is just to frame this as a supervised machine learning problem. we throw in a bunch of words , here is  a bunch of content, words and function, words or whatever. between these 2 classes of features, which do you think would be more successful if we take a supervised learning approach? , think about this for 5 seconds, and then I'll ask for a vote. if you had to pick one and just one. which class of features would be more successful. ,  more people voted for discourse features, and  I would agree with you. But for  simpler kinds of machine learning and simpler kinds of supervised machine learning systems. And the problem here is that in summarization a summarization system might have to handle articles that are about very different topics, even if they're all news articles. Another could be about  global warming. A 3rd one might be about some election, and a 4.th One might be about the economy, and they're all different from each other, and they all use different words. Then there'll be different trends. ,  , people tried this. the very early work is not really machine learning. even back then they had some statistics. People back then were very smart as . that's that was learned here. And I already discussed this. , one basic version is to look at how many documents does there exist within some reference corpus. and then you divide that by the number of documents with some term T, you add one to avoid dividing by 0, and you take the log for it, or something  that. suppose the appears in almost all the articles. Then the computation will be  this. when you take the log of it, it's close to 0.  you get a very small number. whereas penguin, if it appears very rarely overall, and your whole corpus. Or you can use this to, you can use these tf, idf, weightings within each sentence or each span and come up with some vector, representation. where 1st they determined 2 sets of articles related articles and unrelated articles. It's the occurrences of that term across the related and unrelated articles. and then you would record all of these. P. That you use to model each of the 2 rows, the row one and row 2 in your table of term distributions. they did this for both unigrams and bigrams. and then the second one is clearly about  a cigarette, cigarettes and tobacco, and  smoking and  forth. because it's saying that if this word appears more than you would expect according to some background distribution, which is what the binomial test captures. and there might be redundancy in the documents, and how to combine information between them. And  one very basic system is appropriately named, some basic by Neiko von Vander Wendy. first, st what they do is they compute the relative frequencies for all words,  they just take the number of times the word appears, and divide it by the length of the article, and then of all the articles. this means you're less likely to select a very similar sentence in the future, because the new average probability of the words in similar sentences has been decreased. There is a word that's pretty important in all the documents they  solving. , that's  why the score increase. people have current summarization into  a knapsack problem or something from which  encountered. If you've taken a theoretical Cs class where you're trying to select sentences with high informative informativeness score, informativeness score, while with while respecting your overall length budget, and  you also add some constraints. It turns out there are other things that people have tried. , if you're doing extractive summarization, you should avoid sentences with words whose interpretation depends on  the overall context. and other work on modeling the coherence or flow of  the summary sentences. Oh, AI systems are better than people and taking over the world. there's something called Gerund clauses that this approach removes also restricted relative clause positives. There are certain  syntactic constructions that they remove. She said whatever without consulting us,  just remove that, and also lead adverbs. very close to human level scores even back then. I forget which one it was. This was  a query focus summarization task. then you can do extractive summarization by training a neural method on some large scale data sets. then it's just a supervised learning problem. It's it could even be  a sequence labeling problem where you feed in a bunch of sentences one after the other, and then you have to label them , is this important, or is this not important? And if you have a budget, then you can  rank the sentences by  the probability of being in class one of being important. We've done this in my group as . ,  for the last 10 min, I'll just quickly talk a bit about evaluation. And then for linguistic quality. and you can have different sub questions on  different aspects of the summary also. You would have to collect new judgments from people also. People are , I don't know if you've met people before, but they're really annoying and difficult to work with for science. , , different people have different interpretations of the scale. I've noticed people in Montreal tend to be very nice. But if you go to other places where the scale might be different, a 4 out of 5 could be a very good restaurant. if one research group runs this human judgment. It's not a good comparison. an alternative to this is something I mentioned a bit earlier called rouge scores. And then, when a system generates a summary of the same source. rouge, N means you're computing it over. and then in the denominator, you would count the total number of N graphs in the reference summaries. The difference between blue and rouge is that blue is more precision oriented in that you, the denominator, is mostly defined in terms of the number of words or n-grams in the system generated translation. But you see whether the summary enables users to do something else better or faster."
    ],
    "Topic 3": [
        "And then the link to the readings by Nikova and Mccune. What are some important sources of signals to determine whether a piece of text is important and should be included in an automatic summary. And then, finally, we're going to end by talking a bit about summarization evaluation. , because they have to document every single patient interaction they have to write down  the summary of  what happened and  what they, what course of action they prescribe, and then they fill out billing codes and stuff. And that's a big cost to doctors. And it turns out that  there are technologies out there for a summarization that claim to do a good job in summarizing these clinical notes. And it's possible that these different contexts will lead to different possibilities in terms of the modeling techniques that you should pursue. And  we can break it down. an informative summary tries to be a substitute for the original source material. and it tries to express as much of the important points as possible that were in the original source. what are the important points in the original? That's arguably a  summary of the original article. We're we're asking you to read. to me, it's a little bit It might raise concerns either ethically or otherwise. potentially , depending on the specific context , do you trust an automated system to do a fair and unbiased job? You can summarize  structured data  weather data or something you could structure, you could summarize  figures. If the multiple documents are by different authors, then chances are that there may be conflicting information or contradictory information between them. And finally, you just might have to do extra work to combine information from multiple sources, from multiple documents. whereas in abstractive summaries  that you need to synthesize the contents and then produce some potentially novel text for the outputs which requires more advanced semantic analysis and natural language generation. Was the 1st thing that the field worked on  determining which sentences should be important. you can make other assumptions as  that are clearly not generic. And then it's starting to feel a bit  an information retrieval type system as . And , if you're a doctor, you would know all about this, or if you're in med school, or whatever. And those articles are written specifically for doctors. And  they contain a lot of jargon. , I'd  to talk a bit about how do we  determine what content is important? And   the direct way to estimate importance in  a person , if it does exist in  a person as opposed to, if it existing in text, would be , you have some   scanner that  you directly aim at someone, and you can read their brain signals and see  how much it activates or something I don't know. this piece of Texas would be important to this person, and this piece of text would not be important. However, of course, that technology doesn't currently exist, as far as I know. the 1st type of signal is the distribution patterns of X within the source text material. And then also the relation of the text with the background domain inquiry. ,  the 1st is distribution patterns and texts. It's it's it's the idea that the most important theme or the most important information. And then  all of these fans are these points in the space. and then you pick the points that are centrally located, and those would be the spans that if this assumption holds would express the important information. , oh, we're  looking , what does centrally located mean  that you can come up with computational definitions of it? oh, I  have to reshare the screen for the people online. Is there any happier news? Less important is, is there any sentence? , you can read it. Are there any sentences here that  you consider to be less important. Here it's arguably less important. Here's another piece of information that's a bit less important about  negotiations and failed negotiations. For the purposes of this article the process of how this came to be about the dam removal with negotiations, and  forth, that might be to them it might not be as important. And you can look at this because  if you embed this in some information space,  because this is about negotiations and more  political issues. And there are expectations that we have. About how a piece of writing is written. , how a piece of writing is written, because we're we've seen many instances of that type of writing. and   we can use those patterns themselves as  to help us inform, find important information. Doesn't also depend on what exactly the office are trying to get our attention on   to put emphasis on something that no might trigger the interest of the audience. , it could just be a specific word. They're trying to hook you into reading it. At the start of paragraphs. 1st and the last paragraph, the 1st and the last paragraph. Just because we all have very limited attention spans. you might only have,  the patience, or at the time, or whatever to read,  one or 2 paragraphs. But , what about inside of a post itself. Post , a lot of times. The last sentence, because that's where the post original poster asks the question after giving a lot of context. What about in a scientific article or in academic writing. And it might contain a lot of the summary of the important information , in the back. In fact, if you take courses on  academic reading and writing your advice that for most papers. You don't read it from beginning to end  in scientific articles. You read them in detail, and then you go through everything. those are all examples of  using discourse structure, and you use the general way in which a piece of text is expected to be written in order to help you find it and locate the important information. don't discount the score structure. A 3rd major signal for determining importance is a little bit different. This one is more about looking at how this piece of text is related to other ways in which we structure knowledge in our minds. Or the doctor kilometer range? what to do if you're affected  suggestions and advice for those affected. But, , you see that we have very strong intuitions about what  information should be expressed in an article about a natural disaster. you can expect that the summary should also contain this information. Here are some of the basic slots and basic thing types of information that are expected. And you can use even use those to check for  comprehensiveness, ,  it's missing something  advice for people who are affected. that's really important critical to include. , again, we have very strong intuitions here. Or  if you have multiple articles of the same type,  you can try to induce this and come up with  a representation of  the kinds of important information that exists. one is to do some  analysis or content selection which is about determining what to say, and  using our signals just  to determine what is important and novel and interesting and relevance. And finally, a 3rd step, which can sometimes be non-trivial, is to do synthesis of this information, also called surface realization, which is to determine the specific final form of the summary. and then you concatenate these sentences together to form your summary. just determining which sentences to select, whereas you don't really need to do very much transformation or synthesis or extra work for surface realization. this is an automatic way to generate a label. they didn't have to ask summarizers to  or human experts to select the correct. They had a heuristic to automatically generate that as . ,  unsurprisingly, the title is , usually considered the most important according to this notion. some of you propose,  the 1st sentences of all the paragraphs. Whereas for economic news, it was , just what's in the 1st paragraph. the baseline method in this style of in this genre of text should be just to select the 1st sentences of the article up to the word length limit. Ex,  explicitly, or even if not, then implicitly. already experimented with this a bit in  the 1st programming assignment, or  in the second one as . By important ,  important to determining how central it is. And  tf, , is usually just the count of the words in the document, , how many times does this word appear? There's usually some  rescaling by passing it through a log function. for it, here's a worked out example. but it appears just twice in the current article. The fact that,  the you have this  much larger number inside the log is usually enough to  increase the weight of the penguin sufficiently of these rare words sufficiently that it scores highly in terms of tfidf  the idea here is that you can take this approach or other approaches. , the is that called centroid,  medoid. that's R, and not R. , , if you were summarizing a document about vaccinations, then in your related set, you would have articles in the health domain overall. and then in your unrelated set you would have articles in the finance and Education domains, or something else. , , in the summarizing articles about vaccination tier. One might be  needle or something, and then you would see how many times needle appears in health articles versus non-health articles. You can treat it the same way as you treat all of the other terms. And then, finally, once you have 2 likelihoods. And then this is the statistic associated with the test. And then you can rank terms and sentences by oh,  it should be ranked terms by this statistic, and then select sentences with words that score highly on this. the 1st the 1st set of articles, is clearly about  something to do with jails, and  forth, and the justice system, , and jail overcrowding. because then you have to consider that there might be conflicting or contradictory information. , I said earlier, ? dependency is good because it helps. It's a it's a, it's a computable signal that . If everyone's talking about it, it should be important. But it's bad in the sense that you have to be careful when you're selecting sentences to include in your summary. and then you repeat until the summary length is length limit is reached. 1st you rank sentences by their average word probabilities. And  you just run this iteratively until you reach your length limit. And  we can reduce the weight of that a very important for it, Solomon. of course, this was a very basic system. then this breaks the knapsack assumption, but then you add additional constraints about,  the relations between sentences,  that you don't want them to be redundant with each other. or alternatively, you can select those sentences, but then you might want to cut out these discourse cues , therefore, and  forth. in 2,006, Conroy et Al. She said that they would never do that, he said. And then these days, of course. the reward signal would just be  how good it was to select the sentence. Just  the reading assignments, rubrics, and  forth. But ,   we have the summary content which is ? Does it accurately reflect the original content,  the source content? Does it contain the most important content? Does it include non-redundant contents? It's  the grammaticality of the individual sentences and the coherence of the output overall. You cannot interpret the scale the same. You cannot  take the same numbers and scores and used and compare with them in  a different setting. the overall process for this approach is that you ask human experts ideally, or you somehow automatically construct a reference summary. This came after and they called it rouge. Here the denominator is defined in terms of the length of the , that's defined in terms of the length of the source article. there's 1 called the pyramid method, which is a  structured content evaluation. it's   a structured analysis of for each important piece of information which is called a summary content unit. You either just give them the original material they're supposed to learn, or you give them the original material plus the summaries."
    ],
    "Topic 4": [
        "in news articles, , it can appear because, you can think of both the title of a news article, and this is  this is called the Byline. They spend a tremendous amount of time doing paperwork. I gather it's even worse in the Us. And  that's a big overhead. and then you can check out whether their demo and  forth. And  it highlights specific phrases, and then it also generates the overall paragraph,  the patient reports something or other. It's a deployed contacts for a summarization system. What are is it just one possible task? Or is it  a range of possible tasks? , summarization is  a family of tasks with many different flavors. There are many different possible contexts in which you might want to do that. what would be an example of that? Indicative, that's the second possible function. in the example I showed earlier with the news article with the byline. none of these is really they're not disjoint  a summary can have some mixture of all of these different purposes. But it can be some mix of them. And this, this factor has a large impact on the types of techniques that you should consider and the kinds of things you want to do. , a 3rd way in which summarization systems can differ can be in terms of the format of the expected output. this is more of an artifact that there's some   homogeneity in the population of researchers and the tasks they work on. here is where summarization starts to get  the boundaries between something  a summarization system or something  a question answering system starts to get blurry. You might want to just generate an update summary that provides what has happened since then. ,  then, those are all of the different ways in which summarization systems can differ from each other. And  how that informs the choices of how the field has developed. there's some way to approximate it, using,  some  fmri scan or something  that I don't know. instead, we have to try to model importance in a different way in text. These is that they're  easy to compute or easy to , and not. And then there's some  something to do with discourse structure. And that gives you a clue that , that's an important topic. But , for visualization purposes , imagine  a 3D. , this is a happy news article. what is the main topic here? I thought it would highlight all the cases. It's art is talking, explaining about why dams are not good for fish, because it blocks fish migration, and how dams were demolished, and  forth. But here, this is a subtopic, too. , if you have,  a slightly longer, a bigger budget for your summary. But here there's a subtopic to do with dams. ,  hopefully, the  screen is being shared. then the  queue would be discourse structure. the  queue that I claim is often used in summarization systems is discourse structure. And what  by discourse structure. Is that the way that we write passages? Where do you think the important information would be contained in a news article? , it's not syntactical or semantically relevant to the article. that's also part of the score structure. And that's typically called a hook. , but  that  that fits here as  within the score structure. , at the start of paragraphs, because that announces the main idea. The end of paragraph at the end of paragraph yep, could be could be. And , the more important stuff tends to be at the beginning. It turns out the most important clue. Besides,  the headline, is the beginning. And  that's the way that the news was written. either by time or and also by  the number of Upvotes. The conclusion, , that's . And then  then it's  abstract intro conclusion. and in that case you would really  skip around ? You would read the abstracts of everything, and then read the intros of everything and conclusions, and only for the small subset of papers of ,  10 or 20, or whatever papers that are directly relevant to your current project. And a 3rd major source. because then this is about the relationship between our general expectations and understandings of the world and what is written in texts. Number that's covered in casual. All summarization systems need to perform these steps in some form or another. or you highlight them, or something. this might not work for all of the different signals of importance that we discussed. we want to design a supervised system for summarization. Because or therefore that might indicate Kate some conclusion. and other features of discourse structure. the reason for this is that this is really tricky, but if you think about it. it depends on the nature of the supervised learning system. you have a very direct relationship between the input features. One could be about salmon revitalization. On the other hand, these discourse features about  how an article is written. then the question is, would I have the same opinion for unsupervised system? But the topic can change. but  the where, where there would be interactions would be  different languages might involve different genres might be correlated with different genres of text that speakers of those language tend to use, and different expectations. And  then the discourse features might be different across languages. Where the input was the source text plus some human written abstracts. And then for each sentence in the human abstract they find the position in the source article that has the highest similarity to it. And then what they found was unsurprisingly that,  in different kinds of corpora, usually the title is the most important sentence. with the highest overlap with the human abstract. They did have  they did have human written reference abstracts. both of these cues that  you guys had proposed both of these ? But it turns out that in news text the opening of the article  acts  a summary in and of itself. It's used and it's learned through the training of current models. and you can capture this distributionally at least. , if you see the word be in an article that tells you almost nothing about the topic of that article or the contents of it. whereas, if you see the word penguin, then that gives you a pretty relatively strong idea that  the article is about Antarctica, or it's about  ecology and conservation. And again, this was a really important and basic tool in information retrieval for the longest time. It would be  a 35 times in the current article Times, the log of whatever. And this might, you would expect this to turn out to be a relatively small number, because what's in the log when you divide it out. And then you can find,  the centroid vector. , you can find the most central vector within the collection of vectors that you have and select that to be the most important sentence. , it will either appear or it won't appear in each article, and then you have to use combinatorics to account for all of the ways in which the appearance or non-appearance might occur across these sets of articles. then, the likelihood of your data set given, this hypothesis is given by this expression. And then for each of these you can just fit what's the best parameter that maximizes that likelihood. This is really an instantiation of centrality. you have to avoid redundancy in that way. What they do is that they use unigram frequencies with a simple update term to account for non-redundancy. and then you select the best scoring sentence to add to the summary. by default, it's just  the best scoring one sentence. the question is , what if this down weights  the really important words , if it was a salmon article, then you've down weighted the word salmon. But salmon was really important. This is the genius of the squaring. which is that if it was a very common word in the set of articles  about salmon. when you square it, you're decreasing it by less. And  then, by squaring it, you're decreasing it by less. And they're much more sophisticated methods. But even back in 2,006, we were able to achieve this as a community using these simpler methods on some tasks in summarization. Used the topic signature idea introduced earlier with a sophisticated non redundancy, module, and  some rules just to  eliminate some parts of sentences deterministically. And , and  they got . They had 2 versions of the systems, one with the O system, one without. ,  this was this was an fully automated system. this was the system where they combined their method with just looking at word overlap with the topic of the summarization that is given, and by doing that their scores were   within the range of,  the humans which are all of these letters. You can do this in any number of ways you can do supervised learning. You can think of it that way or unsupervised learning, it's  a different learning paradigm. the advantage is that you can You can change the question and tailor your question towards what you care about. Even between cities, or  even between neighborhoods. if you see a 4 out of 5 restaurant in Montreal, it's  it's a sign you should avoid the restaurants. Study on one set of summary summarizers. , if you have a different research group that tries to replicate that study and  reruns this human judgment study, you cannot take  a 4 here and compare it to  a 3.5. And that's expressed in this rouge. rouge one would be over unigrams. And then here in the numerator, you would count the number of matches and N. Grams. whereas for summarization, because , you want to make sure you're comprehensive and capture everything in the source text. here in the second study, what they did was they had a  artificial task of here's  a hundred documents. Find all of the documents that are relevant to this particular topic."
    ],
    "Topic 5": [
        "I'm trying to look for it. this week we're gonna look a little bit at automatic summarization and text generation. And we're gonna look at some of the ideas that have been important these fields. , but this goes to show summarization is in deployment, and it'd be nice to talk about , what is summarization. assumptions about the source text assumptions about the format of the outputs. ,  do people use  close notes or anything  that when  reading novels or trying to redo things for English class, is it allowed? And  different techniques that you want to employ, depending on what you're trying to do. about automated systems that provide some  critical summary. , another way in which summarization systems can differ from each other is simply the nature of the source. But let's restrict ourselves to text summarization. And  you have to handle that somehow. that you copy and extract parts of the source text. , , for quoting research. , what about other very high stakes. Yet another way in which summarization systems can differ is in terms of the their assumptions about the users. in the field there's this assumption that there exists something called generic summarization. author's views and preserve them, and just , restate them in the output summary, and try to preserve that as much as possible. more and more I would question, and  others would question whether such a notion of a generic summarization system  exists. could be very tricky to define. It makes sense to them that you're not injecting too much of your own opinion. And again, within the past few months I've noticed that this has been integrated to in  the popular search engines. the idea behind up to date is that it's a resource where they have reference. it's   a encyclopedia for doctors, and that they have,  a documentation of  all of the common things that can happen  syndromes and diseases, and  tools and diagnosis and things  that, and then they can look it up. but without as much technical terminology and driving. You might want to give very different presentations, and  to define terms in one case and not in the other, and  forth. And  that is interesting as . or , if you have a situation where you have there , I said, there's an evolving current event, and then  one set of assumed knowledge in your readers, and then you can. even before we look at any particular systems. ,  1st of all, importance is a very tricky thing, ? That we think would correlates with this. here are the major classes of signals that have been explored in the literature. The basic idea is that we can approximate importance by looking at a notion that I'm going to call centrality. If it's very far, then it's not . let's take a look at  a news website. , I'm gonna pick a news site. we assume that it has something to do with salmon and river. I'm sure it's important to the people who are involved. But again, if we assume this notion of a generic summary exists. And and sometimes we're explicitly taught to write in a certain way. sometimes authors might try to peak interest in a certain way and write something that's not. The reasons, , if you think about it,  it's pretty intuitive. There's a newspaper,  every day, ? And even , even today, that still makes sense as a strategy, even if things can be updated online and  forth. And then you either think it's this is interesting. the previous 2 types of signals are just, you can directly get it by analyzing the article itself. What do you expect to see in an article about a natural disaster. and in fact, people have developed templates for this of  here. the baseline quote, yep, demographic breakdowns  just let  time and how to vote. then  how they're  implemented in specific systems. But   older systems, because then they're easier to , think about and analyze and see what's going on. let's start with the these 3 steps, looking at these 3 steps for the simplest. ,  , ,  , let's think about this. is more stable within a particular genre of text, and a within a particular summarization setup. in unsupervised systems, then things are different. Then then we can look at difference. to spoil it, because  I'll talk about this a bit later, but unsupervised systems. Would it change based on language? , very, very early in the fifties and sixties. It's more , here are some heuristics for  trying to. They trained us to a supervised method as . And  this is  starting to relate to the question about unsupervised methods, and that this is an approach for trying to reweight words in order to better understand their centrality, their centrality scores, , or how central they are computational. And this is the idea of term waiting. the way that this has been implemented is through this idea of tf, idf. And  this separate reference corpus, you need some other source of  data for this. and reweight and score terms, and then, after that, you can just directly use those scores, and  score each of your sentences by say, it's average, or its total tf, idf weighting. or I  idf score ? and then for each term, Ti. And  each term is either the term of interest. Ti, or it's not the term of interest some other term. what you can do is from the 1st row. You can ask, what is the probability that occurrences of Ti are distributed between R and not R in this way? this is a binomial distribution, because it's  a you can think about the some theta which is  you model. the 1st test is that the term Ti is not characteristic of the domain. that means the distribution of occurrences of Ti between the related and unrelated articles is the same as for all of the other terms. this is  the null hypothesis, which is that this particular term Ti is nothing special. Is that the term Ti is important to the domain, and the distribution of occurrences of Ti between the related and unrelated sets is different from the distribution for all of the other terms. ,   far, we've looked at single documents. If you talk to them about solving. But then this week's gonna go down. that salmon should have a relatively high probability. Then, even if you don't remove them, then the function words they won't  selecting them shouldn't be  that bad  selecting function words again. rather than always selecting a greedy  greedily, what you think is the most important  sentence,  you can turn that into an optimization problem and there. you can do all sorts of stuff  that and turn it into some interesting optimization problem and solve for that. , with pronouns, you should avoid them, because usually the pronouns refer to something in  the previous context, , if you have an it and you extract that sentence, and you don't extract the context, then you have a dangling pronoun , because you don't know what it originally points to. And in fact, even back in 2,006, I know  these days, there's all this talk about. I don't expect you to know what these are, but . there's there's a metric called rouge, which I'll introduce in a bit, and then their particular system. ,  their particular system is  this system. , 1 of the systems. , if you're able to derive a label for each sentence as  a label 0 or label one. and then you have to define it a certain way. You have to think a little bit about  evaluating for the contents of the summary, and also for,  the overall quality,  the linguistic quality of the summary. How would you rate the quality of the summary and this,   all the evaluation approaches. This means that you do not require gold standard summaries which could, which could be tricky to produce. But the disadvantage is that this is expensive, because every time you change your system. and every time you want to do a new evaluation and compare systems. And it also means that results do not generalize across different evaluation runs. Not necessarily over individual words. But say, rouge, 2 would be over bygrams, and  forth. But ,  there are yet other evaluation methods. that you get this is really expensive. It's a human annotation that's really expensive. Yet another very different approach is an extrinsic evaluation. the idea behind these extrinsic evaluations is you don't directly look at the summary in and of itself. and then afterwards you can quiz them and see whether they do better on the quiz if they've if they have access to the summary. far in this lecture, in terms of the methods I focused on the extractive approaches. And then  class, we can talk a little bit about approaches to neural abstractive summarization, which is interesting, because then we need to bring in this idea of natural language generation."
    ]
}