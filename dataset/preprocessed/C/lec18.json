{
    "Topic 1": [
        ", . , ,  today we'll be talking about neural machine translations. . . When you want to predict the  world they are. And the question is, how do you decide on what would be your candidates? Words that you will produce? And the interesting thing is, some of these techniques have been developed . in this community of. we have some work that have been developed also at University of Montreal and And  I think one of the authors of the attention mechanism is  still around. You want to align every word to 0 or one target word. And for the simple case of Ibm model one. You don't need to care about the world or that. And we said, if you don't have any training data to start with but you have larger labor data, what you can use, you can use expectation maximization. Again. once you're restricted to your training with that, then , this  reduces the number of parameters you need to compete. ? for all the variants of the Ibm model. you have. Why, Vashawan only allows water to order alignment. Do you allow just 3 words or 4 words? And after that we have the newer-based statistical engine. if you have a pair of language that you don't have a lot of data, then typically people advise phrase based statistical, empty ? I'm going to tell you how to address pairs of languages where you don't have a lot of data even in Europe, based empty. You don't have a lot of data used, statistical, phrase-based, empty. You have a lot of data used in your basement because you have a lot of training data to have a good model. rather than having a world based statistic, statistical, empty. whether you are using the statistical Nt or you are using the neuro based empty. Because when you're trying to decode you're trying to produce the translation of a sentence of a sentence, 1 word at a time in the case of an Lstm and a group of words at the same time. In the case of a transformer architecture, you will still need a decoding algorithm to  search for the best possible translation that you will give back to the user. And how do you do this? But this  start by creating a 1 complete candidate translation. More grammatical? , you're trying to formulate a more grammatical sentence. and the same thing. Why? Yes,  . Yes, but I don't understand. , you can have something  that. you try. ? , you. You try this, and then if this is not a proper match, then you replace it with another one. There's a better match. . . You have the memory for every input that you can have. you can have input, a BC. at the last layer. is this clear? . It's the same. Yes, of course you modify as you go. You need to send every outspot. And what can get wrong? And we'll talk about a modification of this, we where  an attention is introduced. A day will keep you away from a doctor or something  this. ? . That will keep you away from the doctor. And and you can sometimes you also have a sentence where you can use a pronoun. and this is why you have this interesting diagram. The actual issues. You? . I didn't know I was blocking. that  shows you. that means you need to gather because the X is your input. ? And then, if you multiply them together. It's , of course, you can read the paper for the more details. you can see attention as something as a salt retriever. in a retriever system, you have, you want to query. you have the query which is a representation of what we are looking for. ? here it's very similar. You have a representation of what you're looking for as a query, and then you have the key, which is a representation of an entry in memory. and then you have the value, which is the information that is being stored on general, what we use to formulate. All . And this is from the newspaper, and here you have, what is, what is the model supposed to focus on? ? And by this people started thinking about, oh. You cannot. Does it give you a good interpretation of what is happening in the neural networks? Yes, . . ? But there's sometimes, if you analyze some other parts of the layer, you don't see anything going on. Sometimes it helps to give you some additional information. . But in the case of Martian translation, . I'm sure if you visualize for some other sentence, you may not be able to see a very plain trick. ? And  what we have. and I remember there are many papers that says, this is all you need. But . . Biology seems to work. I think it's really good. Although we have been trying to replace it. Maybe you can try to replace it in that . The context length is just too long, and it has forgotten a lot of the information. all this can be easily paralyzed, parallelizable compared to Rnns. and then you have an embedding. and the goal is to compute the  layer of water representation at Layer L. And what we are trying to produce is that given X and the embedding of the words  given the word and the embedding of the word . and by  you should know how to compute the embedding of the word, even if you don't learn it yourself. A lot  using what's back and all that. you have a sentence, and every you have a sentence, and every word in the sentence also has its own embedding. using the structure of attention and  learn what is Z.  we want to learn a distribution over words to decide how important each word is in order to compute the representation of the  layer. Head, and then you want to know which of these is more important to w. 1, and then you do the multiplication across the embedding. And then you want to learn this. And the way we learn this is using the query, key and value. in the query you want to use of this word as a query. because we want to compute the representation at the  layer. we always need a query for the key. We want to use this vector to decide how important the world is to another world as part of the attention computation. And for the value. And this is how you compute a value. Yes. ? ? . The queries and the keys are those words from the same sentence, , the query. ? You have  a vector representation of query, vector representation of keys, vector representation of values for the same. And then you multiply them together. in practice, query, key and values are just randomly initialized vectors. ? until you train the model, they don't mean they don't capture any information. what the embedding do capture information because the embedding is connected to the word. But these other values queries, keys, and values. They don't. At the beginning they're just randomly initialized. But in practice is that you have a good representation for the, for the, for every word. and then you have a representation for what it means for the query keys and values, and then you just multiply them together. we want to know that if you have thinking machines. what is the most important thing? Is it thinking or machines? This is what we want to compute. ? You want to know which part of this sentence. And here, when you compute, you multiply the query with the key and then you have this value. and then you do that for machines. You do that for thinking you do that for machines. and then you apply softmax. And in this case you. You have the value. You multiply the softmax with the value. I know you have. Yes, how important is the quality of the embedding of the word to the results of it. . everything, including embedding the queries. keys and values are just lined. Yes, wouldn't it be faster? And  I told you, machine translation is trained on very large amounts of parallel sentences. In your training data to  learn a good representation. Try to improve another thing. . ? And this is the idea of multi-year edition. Yes. you're learning 8 different key value queries. Yes, what about the embeddings? The embeddings are shared. . yes. . good. And then and they've moved to transform our architecture. And they achieve very good performance. Only you always have the embedding. It wants to have a good encoding and also do a good prediction,  they use words. we need a good encoder. But then why they do this? ? You just need to predict one world at a time. . But  I think we are making a lot of improvements in terms of developing more powerful gpus. We're just feeding the entire context in order  far to predict the  token. . And Chatgpt came, and then everybody forgot about that. And then. you have. Does this sentence follow this sentence, and the way it was trained is very simple. It's very easy to predict if this sentence will follow each other. if someone is talking about a sentence in economics, and there's another sentence in out. that means if you train on more data you don't need. Yes, I'm . Because, sometimes reviewing is a very noisy process. And then. They  train on more data, 500 1 billion Watts this was published in Europe's, and it has  175 billion parameters. And  they put it behind. , and also Gt. , you can just predict. try to. the model has been trained on just simple  sentence prediction. But if you prompt the model. this movie is just a waste of time. the interesting thing is that almost all Nlp tasks, I would say, oh, I don't see any tasks that cannot be converted. You corrupt some part of the sentence, and then the model  reconstruct it. , however, for the T. 5, they are  not reconstructing the entire sentence. What they are doing is  just providing what is missing in the input  you have this example in the original paper. Thank you. X. Me. ? you want to know if sentence one and sentence 2  similar. ? we also have a multilingual version of this that was trained on, I think, one on one language and a lot of data, 6.3 trillion tokens. Some, the pre trained model can already do that, but it's just that. It's not very good, but if you fine tune it explicitly to do this, it will give you  better result. we have other text to text models that will be interesting to mention, but model, which is probably the topic of your reading assignment, which is similar, objective as the birds model, but with an encoder, decoder, transformer architecture. Out? . ? This can also be trained ? And in this case you are not training on just not you are not training on monolingual text. But you're training on parallel text. I forgot the citation, but  they were created by I think Meta, and discover,  over 100 languages which they have trained on parallel examples in all these 100 languages. Unlike the Mt. 5, which is just based on which is not based on parallel data, . lastly, I wanted to share one interesting finding, which is even if this pre trained model has not been trained on a language. But if you take,  an original pre-trained model,  empty 100 for Swahili. But if you fine tune it on high quality compost you can  just have. Suppose you have,  5,000 parallel sentences, and then you can fine tune this pre-trained model. whether the one that has been massively portraying on unlabeled data or massively portraying on parallel data, and then you can see some huge improvement in performance, whether you use Mt. 5 or you use Mbat. The last thing I wanted to talk about is current trends  for Nlp, including for machine translation, is that you can just prompt a language. . And  sometimes there are interesting things you can also do is that you can provide instruction, you can provide a demonstration which   an example. and then you can  provide a query, and then it will give you the results, and I believe all of you are already aware of this. You can also prompt it where you provide an example, and then you provide an explanation on how it arrived at the answer, and the explanation doesn't have to be much. the few. good. Thank you."
    ],
    "Topic 2": [
        "lecture 18. if you still remember the large language models, language models that we talked about. That's you have different words that are equally probable. There's nothing technical about prompting, but there's there's some ways you can do prompting that will give you better results  just to continue from what we did last time we talked about Statistical machine translation. and then you need an alignment model to be able to do this. and once you are able to get the alignment model, and you can have more than one alignment model. And then you need to pick the best alignment model to use. And then also there's no need to care about the likelihood, the likelihood of fertility, for example. because maybe you don't have a direct translation of that word in a language we talk about some languages that do not have concept  grandfather and broader. and then at the end step, you can  compute the likelihood given a parameter theta and  in practice you don't initialize this likelihood you're trying to compute uniformly if you have a reasonable size of lexicon. what happens is that in practice you try to restrict this only to your training data to available training data. when the sentence length are high, there's a need to come up with an algorithm to be able to  compute these probabilities more efficiently. Then you decide and then model 5  makes use of dependency structure of alignments,  that they don't depend on each other. all these ones are  the series of Ibm models that have been developed. But in practice nowadays what we usually do before the neural models is what is called phrase-based, statistical, empty. , previously it used to be. The idea is that what if you can model immersion translation even with context? And here the idea is that instead of splitting the sentence into words. you need this  distortion parameter to be able to address this. And you need to decide which is the best . and then this sentence is being translated to English. But  the question is, how do you make it? and these languages can have different preferences. May I ask a question about hill planning? The greedy step is to take the change that produced the maximum probability. How changes are applied. or is it random until you stop improving. for the neural machine translation this often leads to the current state of the art model. But also this requires a large amount of data to be able to train immersion translation previously for any single pair of language. You need at least a million parallel sentences between. one very successful approach is to use recurring neural networks. you use this state and memory information to compute what would be the outputs that will be sent to another state, and then you can compute. every Rnn state can  output something. in the earlier lecture we gave different forms of Rna, that is available because it's very, very flexible. you can use. You can use it for a for a topic classification where you just need a single output,  it does not need to output values at every single state. And then you can have another task  language model that you need to output of every State, because you need to produce what will be the  word. What you need to do is that you need to understand the entire sentence ? for you to understand the entire sentence. you do need to output at every state of the Rna. Vector this context vector produce  summarize all the information in the of the sentence as a. Vector  once you have this information, you can pass it to the decoder side. Same vector passed to every cell in the decoder? Yes, it passed to every cell in the decoder. Vector no, it's passed to the  cell in a decoder. and then once you produce the  the outputs add the  cell, which is W. They need to also pass it to the  one. , you encode every single sentence. And then you have this context, vector, Z, and then you pass it to the decoder side, where you  decode it. Or you can use a bi-directional r and n. to have a better encoding of this single vector. and then you  pass it to the decoder side. because you can have all these outputs and then use one of those algorithms that we talked about to  determine what will be the translation. But what is the problem with this? The problem is, if you go from one Rnn state to the other. As you move across the memory stage, you tend to forget one or 2 things . we had this thing a project that we're doing recently. And then the translation was an apple. if your mother doesn't know what to pay attention to  we tried it for a language West Africa, and then it gave banana  the translation was correct. But the only thing that was missing is I changed the word apple to banana. I'm  just this gives you a very useless sentence, because it's not a banana idea that would. the question is, how can the model learn to  focus on the most important one, which is apple ? the question is that is there a way, we can  model this? And the answer is, Yes, we can model this. And the short answer is that from this encoder decoder instead of just  throwing away the outputs at every encoder memory cell. You need you. You need to keep the output of each memory cell. And then you use this to compute what is called the attention. at every memory cell. And here they are using a bi-directional. And then the outputs you use this output at every  every Rnn memory cell to  compute the attention  you can think about attention as trying to compute what is the dot product between 2 vectors. this attention will  be used to multiply the final output you got. I hope you get the idea. attention can be seen as a soft version of a retriever system from a memory which has a key. If you have the sentence. and then you have European seems to be very important, and then you see that even though it was swapped in the French sentence. You see that the model was able to pay attention to this, even though there's some reordering happening. the model is able to pay attention to it. It's not attention. You might repeat what you said about whether or not attention tells us what's happening in a network. if you just analyze the attention on what. But it doesn't give you a complete picture of what's going on in the neural network. does it depend on the model or it's random within a model. every model has some structure going on. Let's do away with recurrence. Let's do away with what is everything. But let's do away with recurring network, and let's just use attention. , we're still using attention since 2,000. And we're still using transformer architecture since 2017, and the title of the paper, which  attention is all you need. This is all you need. But at least we see that, for  it still seems that attention is all we need. And the interesting thing with the attention, architecture, or the transform architecture is that it has been applied to different modality, to vision. far no one has been very successful. But there's a way you can address that which we already discussed in previous class. But there's a problem. Rnas, which they struggle because, , you're passing information from one cell to the other, another cell, and at some point it's the length. another problem with recurrence. and it's difficult to paralyze what attention you can paralyze everything in training. for the attention in transformers, you have the sentence. Ps. Vector but  in practice. What we do is just we just randomly initialize. , this is same attention. You because you save attention is that you're multiplying it by itself. the key values are for the sub centers. that means giving a sentence. ,  imagine that you represent an entire sentence with a vector. That is your vector then, you  need to initialize different information. For the same sentence. and it's safe attention, because you are doing attention to the same sentence. Do you understand? and then you take the sum you sum. But what we do normally is just to learn everything. But people find if you just learn everything, it's it's better because, the 1st application of this is machine translation. there's no need to, . The way you have done it for a single sentence. Maybe  is a parameter. it's , you are having multiple ways of viewing the data. you, you have one way, if you do a single self attention, and if you do it 8 amount of times, then you have 8 different ways of viewing the data and then you can  concatenate all these results together and pass it to the  leg. Are the weights shared between each head? Are they the shared? Only use the Gp. because you see that every model, whether you're using encoder only, or the decoder. You have to learn the embedding,  you can just ignore the encoder path if it's not necessary. for bird's model. He wants to understand what is in a sentence, to be able to make a prediction, to focus on the encoder path. But they still retain the Rnn. it's really expensive at training, you can paralyze things and just wait on different gpus. But at the prediction time you still need to compute all these attentions, ? it's really expensive. But Rnn is very cheap. This was really a big problem, because it's very expensive at decoding. and the way this model has been trained is also a language model. This is a world Maxed, in this sentence, ? and it will also train on what is called  sentence prediction. You have a sentence by sentence, and then, if 2 sentence follow each other, you'll see a  continuation. and it was strained on a large amount of data. Then it used to be large, which is  800 million watts. which we also have a multilingual birth model. But people have made different arguments that you don't  need this  sentence prediction. the Roberta model, which never got accepted,  just did A did not use this nest sentence prediction, and they were still able to have similar performance. You don't need this second task of  sentence prediction. And what they did was to just scale the amount of data I told you about Roberta scaling the amount of data, and they had better results. And they can even do away with one of the pre-training tasks, which is the  sentence, prediction for Jupiter 3. It's really a big model. And at that point it was too big to be released. But nowadays we also have,  more than a 75 billion models that have been released. Then language model just with that single example which we will call one shot. To just prompting the language model to give you what will be the performance to give you the result. with a few examples, and the model will be able to answer him. other interesting architectures that I think it would be nice to mention would be  architecture  T. 5, model. you  apply a noise function to your text to mark something out. and then the model is supposed to reconstruct the entire sentence. strain on the C 4 corpus. Then you can give it another sentence. an analyze sentence. And it will give you it will generate what will be the outputs. entertainment, news, or political news ? Government related news. But this is very easy to address. because it's a multilingual model. it can do what is called crosslingual transfer. , this was very interesting to people working on multilingual nlp, because  you can do this for different languages by starting with the multilingual model. And we also have the multilingual variant of those models. We have the bat model with 25 languages, and we have another bat model with 50 languages. There's no difference in the pre-training task they are doing  a Max language model. Interesting things with the noise function to see what part of the sentence should they, Max? in this model. But you cannot use this model to generate for a language called Luo, because it's not covered valuable training. you can use it as a translation model, ? , which is very cheap. And oftentimes, if you use a machine translation model you often can get slightly better performance than if you use or models that have not seen parallel sentences. Model this guy, you can come to language model. Another thing that can boost the performance is what is called chain of thought prompting. And this is the answer. But there's a way. and by this you can have a better performance of the model ? , , and on Monday we're gonna have another class on multilingual nlp or crosslingual."
    ],
    "Topic 3": [
        "David Ifeoluwa Adelani: , I also have people on zoom. That is probably more relevant to our models have been trained  than Ibm models , nobody uses that. today we'll try to finish off with Ibm models. And also we'll touch about we'll touch on some important concepts , how do you  get your Machine translation output, because when you're trying to decode from the decoder side or any other model you use, you need some authorities to decide  what will be your finer translation. there are different probabilities, and there are sometimes that's the same. also today we'll talk about attention mechanism, , which  will bridge the discussion from Rnn and Lstms to the transformer. It was one of the 1st models from a set of 5 models, and if you remember, from the last class, the most important thing we discussed is that you want to align every word from English to another language, say French, which is our example. You don't need to model different distortions of the world. caring about if a word  translates as phrase in some other language. where you start with counting. we don't have this street assumption that every word has to go to 0, or one word. and, for example. in this in this example. and then you can have a group of words that  mean the same word. you have a single word in English, and  means 3 different words in the target language. On how many words can you allow to model a single word. for the phrase based status. Look at empty. That is the idea of the free space smt. you  just split it into phrases. depending on the language you're trying to translate to the word order might be different. you can have a group of phrases that will. For example, if you use the beam search. it's very common that people will specify a beam size of either 5 or 10, and usually with the with higher being beam size, you'll be able to get  a better translation result. the idea is that you  want a single translation from different candidates. you have this word in German. And then you can  combine them. and when you combine the translation of 2 words into a phrase, you can split up the translation of a phrase into 2 subphrases, and rearrange the part of your translation. , what we discussed in the Ibm model is still true. Yes,  I understood. and which you agree with me that this is not available for many languages of the world. which by  I think you're familiar with it. But you can design it in such a way that it doesn't output anything at the except at the last stage. You can use Rnn for different tasks. A little bit cool? Will it modify it? Also, another one is that you can train an assembly of translation models and decode by averaging their output probabilities. There are some sentences that if you don't get the say the  word or the anchor word correct correctly. It's maybe apple and a dog. and then you use it to refer to another word. The model already completely forgot about the word you are trying to refer to. Because  we keep just one output. Oh. You need to gather the output at every. Rnn. and then it's a measure of similarity. what is the most important  where you do  a dot products. They used to unlock the value. in a Ir system. you're looking for a document. and then you have a query, and then you want to check the similarity between the query and a document. These are just  random mattresses that we give different rules and one random matches we serve as the query matrix and another mattress will serve as a key mattress. Another one will serve as a value mattress, and then by construction, because of the way the attention has been computed, you assign different rules to the different mattresses to  give you to model what is called the attention. , and if you look at the matches, the visualization of the of attention. And you see that you see that it's very bright. And you have another example here. But this  gives you an idea of what the model is trying to do. this visualization of attention gives an idea that the motor is paying attention to the most important part of the sentence. But at a certain step it shows you what the model's paying. And then sometimes that's just completely random. , for example, if you take a birth model and you analyze the lower layers, it has been shown that, just analyzing the lower layers, you can see that it's capturing some synthetic information taste  part of speech. the idea is to  replace Rnas because Rnas has a lot of problems one of the bigger issues of Rnas are things  vanishing gradients, ? and then you can allow flow of information from one world to the other, because everything is  you're computing just mattress multiplication. they are very cool ways of paralyzing matches multiplication. And then it's  a computing those multiplication trying to understand how with vector, is similar to the order. And then the  thing is, how do we compute Z.  the idea is that we can  just do mattress multiplication. I'm  imagine that you have a representation of w. 1 w. 2 CW. different vectors. And then we just do the matches. Multiplication based on the attention computation. Initially, they don't mean anything. They don't mean anything at the minute. , when it 1st came on, nobody understood it. The and the guy  had a very good visualization of what they're trying to do. for this example. , for example, if you have a pronoun  it, it might mean that. , although you are trying to attend to me. But I have another word that was referring to me. And you are able to get this just doing this dot product computation. , it's 1 encoding. Instead of starting with one hard encoding, we start with the embedding of a different model. , initially, it might be faster. for example,  a million sentences. and then you just have everything is just  master's multiplication. And and then the model figure out what is the most important. Or are they different? They're different. different models. They use different parts. The gpt model. Based decoder, who has an idea why they retained the Rnn. ,  they probably just use transformer everything. this computation is N squared. Because you're doing multiplication. When Transformer 1st came. And there's a lot of research  focusing on how to improve the efficiency of at the decoding time. How to minimize this N squared to something. Because Rnn will feed one token and predict the  token untransformable. it's  we had this transformer era we have. and then we have the bet era that's  rain for  4 years. But instead of predicting the  token, what you  predict is a Max token. And then you have to train a model to  predict what is the Max token. Imagine a Wikipedia text. Maybe it's not very likely that you follow each other. And then with this task we are able to know. to predict if the sentence follow each other, and combining this Max language model with  sentence prediction, you can  create a very powerful encoder that can be used for many natural language, understanding tasks from sentiment, classification to topic, classification to any classification task based color. and apart from this, they also train a multilingual model on, I think, over 104 languages. But they also increase the amount of data they train. Why didn't Roberta get accepted? this is stochastic. It helps ? we also have,  a Gpt. Apis, . part of speech, tagging anyhow, sentiment, classification, and  on. the idea would be if you want to know what would be the sentiment of a text. You can tell the language model that the sentiment of this will be positive, and then you give it another example. The language model will be able to solve the task of sentiment classification. You give it an example, then it has an idea that oh, the movie is great means positive. if you give it another example. They knows that this is a negative sentiment. It's   a denoising task. And then it also retained the sentinel tokens. but  predicting what was missing. and also retaining the another sentiment token  the Y, and predicting what was missing. and they scale it up from a very small model to even 11 billion parameter model. And the idea is you can cast any task as a text generation task. you have a translation task translates English to German, and then you send it to the model it can translate. You have 2 sentences, and it can give you what would be it can give you the rating. , this one is I think similarity tasks. But one issue is that it's sometimes it's tricky for a classification task. You can model it to just give you for example, 0 1, 2, 3, 4. if you have a topic classification , categorize this into business news sports, news. Sometimes it can generate different things. And, for example, you can use this for the machine translation task. , you have a language in English, and then it will be able to translate it to German. and they also try to do different things. Should they, Max out just one word or a phrase, and then the model reconstruct everything. models  embat or T 5 has been trained on only on label text. This is just on label text. But you can also train on massively amounts of data. you can train on if you have a lot of data on  many languages. And then you have another B 200, which , they scale it from 100 languages. and for  that's from English to Swahili and Swahili to English achieve 25.2. And it's  you ask the model to  explain the thought process and trying to predict the output. You give it  an example. Short example is  8 example. Because the model also will try to replicate this chain of thought idea to be able to produce better results. Nlp, and ."
    ],
    "Topic 4": [
        "maybe you can talk to him at some point and then we have transformers and landing models, and we'll talk a little bit about prompting. The idea is that you can leverage the knowledge of your status to put Lm and  use it to do machine translation directly. And there are some times that this is effective, especially if you don't have a lot of data. But by the end of this class. You have a word that means another word, and then you can have a phrase that can also be translated to another phrase. you will need to do what is called machine translation decoding. we have a couple of algorithms, such algorithms  sh greedy hill climbing beam search  for the Greedy Hill climbing. And that's where you have the E climbing. and for the hill climbing, the idea would be is that, can you change the translation of a word to a phrase? Do you have questions before we move to the neural machine translation. for the neuro-based machine translation. because machine translation is a very, very difficult problem. It's  a search agreement. and  it's you're keeping track of everything you've already tried. Where you have the encoder, you try. But for machine translation. But I'm going to show you another approach where you don't. And this is where we'll talk about the attention mechanism. and that single vector is  partially decoder for decoding. But  you can reason about some issues with this approach. One word at a time. And once you forget some of this information at the decoder side. What is the most important word there? and when you will be referring to it when you refer, when you use a pronoun for a noun? For encoder. that the encoding is stronger. and then what will happen is that because you run the attention over a soft Max is going to give higher weights to some important information and give lower weights to information that not very relevant. The decoder knows what is the most important information to focus on. a knowledge base. And then you need a key  to access the information, and then you add the value. you can visualize the attention. This is one of the most famous example that you see in attention. the agreement on the European economic area was signed in August, 1992, what information are important and what information is not very important. you will see that The agreement seems to be very important. But that's a lie. That's a lie. you can have an idea of what's going on. , there are some times that if you analyze the weights. you  have an idea of what's going on. that's why I'm saying that you cannot use it as a complete interpretability tool. And with his attention getting popular. we  have what's called a transformer architecture. And the transformer architecture is very simple. We still fix the encoder decoder part. because you are passing information one time, one step at a time is also very difficult to train. You, the motor, can also learn it by itself. It might be that when you compute the  the  word representation is just going to tell you the same word is important, but sometimes it's going to tell you that another word is more important than the word. and sometimes it tells you, for w. 1 w. One is the most important. Then sometimes it tells you, W. 8 is the most important ? This is the value that is being stored. The vector stores. This value associated with the key. each view is associated with its own. But once you have the structure of computing the attention which maybe I can pull up the formula. I want to recommend reading the Illustrated Transformer. or which word in this sentence is  the most important. This model is telling you. This calculation is telling you that thinking is attending to itself. It believes that thinking is more important, ? this word, when attending to itself. It believes that the same word is more important. But there are some cases where  he believes that this, that itself, is not that important. That is more important. It's very important. , because you need to be able to encode the word very , and it's either from a separate model,  word to back, or the model  starts with one hot encoding and learns its own. Do you have other questions? you have enough information. do this n amount of times ? you can do this 8 times instead of one time. we have transformers in Mmt,  initially, Google translate was based on Rnn end. , they didn't change everything. The I try to just change the transformer encoder. And then  this is transformer. This is what you'll see. and then you have the decoder part. They don't use the decoder part. The decoder part. They never use the encoder badge. They focus on a decoder part T. 5. Model tries to use both. encoder, decode architecture. for machine translation, you need a good encoder to encode the entire sentence,  that you can pass this information to the decoder side to translate. Google translate, they did away to just do away with the they added an encoder, and then they have a better encoder, which is a transformer encoder. Piece decoder. Do you have an institution? But it comes from the line that it's less expensive. why is it very expensive? what they did initially, just to ,  decode Rnn is cheap for decoding,  they  just use the transformer encoder opposed to the decoder to save cost. And this brings us to another architecture that's really famous. And the idea is we can use the same architecture of the transformer. We're  focusing on the encoder only side which is on the left. and then you have an entire paragraph. It's just very difficult to run them. And T. 5 is quite interesting because they use both the encoder, part of the transformer and the decoder side, and it's very similar to the bets model, because they also use what is called span corruption. Why weak? and it has, , over 34 billion tokens. and then you have to be able to Co. To convert what has been predicted back to the class you are interested in. And  you can check the 2 architecture and see what is the difference? , there's no difference in these 2 architecture. The only difference is . , the difference is that bass is in an encoder, decoder architecture. I think it already achieved 20.1 blue score. I think it was almost  wrap up there. for the standard prompting you. You can provide just one example. the already give the model an idea on how to solve the problem. or you can provide more than one example. For example, this Mgsm was I think. ,  , you have questions."
    ],
    "Topic 5": [
        "I believe you can all see my screen. There are many possibilities. If you have translate if you have a sentence in English, and you want to translate to another language, let's say French or Russian. and then we say, we can formulate this using the base rule. Using, what is the probability of E multiply by the likelihood probability of F given E, and then we can take the Agmax based on the alignment model. for the Ibm series one Ibm model, one. And then you need to find an  an explanation in a target language for but for the Ibm version one or the model one doesn't  treat these cases. computing mle over every token in your target language and in your source language, for example, French and English. you might have too many parameters. in Ibm version 2, we don't assume that all possible alignment structures are equally possible. you can have a word that is attached to another word. Ibm version 2, Ibm model version 2  allows you to model this. version 3 and then this  models it more explicitly. here, for the phrase base, you can have different possibilities. and then you will be able to  compute the probability of the target phrase given the previous source first, st and then you could add some distortion. This distortion is important, because. That's that is supposed to be normally at the beginning, and then this is  shifted to the end. there are some parameters. And the idea is that, for example, the deal search the idea is that you have,  maybe 5 candidate translation. When you translate word for what the question is, can you get what is the best candidate translation for this word out of the different, many possibilities. What the English translation it's not very grammatical, as you can see, because you're translating word by word you have this week is the grain which at home. and instead of having a very strange grouping of sentence because, after translating by word, you can then have a better translation. , you evaluated by computing the probability of E, a probability of FA. Given E, where your translation depends on how you are able to align from the source to the target ports. neural network, one of the most appreciated successes of neural networks is application to machine translation. Because you need to translate from one language to the other. Are they applied systematically in a certain order. , it's  a heuristics, is a heuristic way of  getting the best translation. The initial language you want to translate from to the target one. On we use the recurring neural network in an encoder, decoder manner. And then you have the Rnn States. which is C, you can  produce. you cannot produce what is called  a context. And when you cannot  generate a translation of the sentence. the encoder, when once the context vector is encoded by the encoder. , that's the way it works. But for , all you have to assume is that you are only interested in a single vector, that summarizes everything in the source text. , there are some tricks to improve where you can reverse the order of input sentence. the problem is that the decoder doesn't know what is the most important part of the sentence that needs to translate. it doesn't make any sense again, . And this is the way it works. and by this once it gets to the decoder side. And the model sees that August, 1992 is also very important. that means we can interpret our neural network models by computer attention. I'm I'm saying it's not an interpretability tool. But it's a black box, ? Sometimes it doesn't help. And it seems to work. It seems to work to. And exploding gradients. Because if you have a really long context. it still has some issues in modeling long context dependencies. is this self attention. But this tutorial was very, very helpful. and it's still relevant to today of , more than 6 years ago this was prepared. and then you  normalize it. everything is lined is the initial input, one hot encoded, then everything. there are 3 parameters that you have today. hey? and then you have what is called multi-head attention. you have self attention, you have multi-head attention. the idea of multi-head attention is that you? you have the encoder parts. For example, the batch model only use the encoder parts. Gbt wants to make a prediction and generate text. Do you have an idea? Let's say. I know you can say, but why is it very expensive? And it's gonna trade. And  I guess. Maybe n, times log n, for example, to  reduce are this expensive inference. there isn't very simple. which is called books, covers and Wikipedia and it's up to 3 340 million parameters. Sometimes you have a bad review, and then the fact that's rejected it. but with the multilingual version got accepted. 3, which, you probably know by , is from open AI. in terms of the successes the idea is that if you train on what we call a self supervised pre-training. self-supervised training means that just using an unlabeled text, you construct a supervised tax to train your model on unlabeled text, and you have what is called self-supervised training, and the task for the for the birds model is just predict the missing token, which is a Max token. And if you're able to do this, you can create a very powerful encoder that can be fine-tuned on many downstream tasks. 3 came up with another idea which is really cool, which is, you can do what is called 0 shot learning. you can try to get a you can try to solve in natural language, understanding task by just prompting a generation model. you can say, this movie is great a name you. You can do this both for 0 shot and few shots. To your party. And then they have a final token. this was how T. 5 was created. And then you can have a grammatical check task which is a caller data set. And it's gonna tell you if this is acceptable or not, and you can have another generation task  summarization. And it's gonna give you  every task. Those intoxication tasks. If you have 4 classes. , , you force the model to always generate what would be the prediction? It can generate political or politics. And  we also have another branch an another idea. which is no supervised learning. M. 2, m. 100 was created. to 200 languages, and this seems to still be the state of the arts for machine translation since the last 2 years. and they also release an evaluation data set. That also is very popular, which is called a Flores 200, that covers  200 languages. And this picture shows you an example of the different language groups or regions that are covered. there's a way you can  just quickly adapt it to that new language. if you have a few 1,000 high quality translation compost for a new language, even though it's not trained during self-supervised pre-training or large machine translation per training, you can still quickly adapt it to a new language. this is an example of a language rule. In, I think East Africa, where  hilly has been covered in the pre-training blue has not been covered. this is  teaching the language model to reason before it produces."
    ]
}