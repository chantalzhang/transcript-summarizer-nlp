{
    "Topic 1": [
        "All . 7 cents lecture. How was the midterm exam was good. . it was not enough time some people finish before time, I guess. oh, . . From. When do we expect that we'll get the grades back? It's a conference you have an OP. . . . . . I guess. But of course. It's called Europe. this should not be very surprising to you, because Ibm is also a very, very old organization prior to Microsoft, Google and Facebook and open AI, for example. If you are supposed to take a makeup examination. . . Then we'll start thinking about the problem by discussing the noisy training model for empty on the Ibm model. Ibm model is quite interesting because it's based on statistical model,  just thinking about the probabilities that the word can be translated to the order by having something  a bilingual dictionary. What can you do with this? semantics. maybe that entity is not even translated. Maybe we have different colors, and you can even have different combinations of colors. and you can have things  kingship terms and slacks. and  on. it's difficult to translate an entity or not an entity , but a proper. . We have a translation of Elder by Yonga. all these  things can have confusion if you are working with a very a statistical model. Also, there are more advanced concepts ,  formality. we also have issues with pragmatic duration. For example, if you have phone Mac phone again, I didn't know a phone. In the 1st place. Maybe in Zulu you can also have suffix. it's  just different categorization of the nouns. some words,  we have the X view. Are you aware of those things  as your subject verb objects. Our special relationships are grammatically distinguishable. . see? . . in there are different languages require or allow different morphological, synthetic, somatic discourse properties. Oh. maybe you get a gist of it. . . , you can. The strong version is that language determines and constrains all human interaction and thoughts which is really strong, which I will personally not agree with this. The language determines and constrains all my actions and thoughts. ? But what we can do is to  just minimize the complexity and say, can we just achieve translating about events and participants or just focusing on eye level concepts rather than worrying about having a public transportation every time. And the question is, how do we measure progress? and you want to check Oh, . great. And based on this culture, you can have a blue scorn. That is one. ? . But , I think what we , you can do unidram of unigram diagram trigram. This call that the metric gives you and that of the human. if it's found. Are you aware? Something , edit. this is the tier. This is what you're trying to measure. f. 1. character enforcement. ? You extract the different embedding. By the way, maybe this is useful for some of the projects that you'll be working on. you typically expect you to evaluate a statistically based metrics  blue score or Chrf and an embedding based metric  Novid scope. . . This is based on things  using bilingual dictionary. The other one is. what if you have languages that have very they're very similar or very similar synthetic structure, if it can modify the synthetic structure a bit. then you'll be able to do a translation to retarget cycles. The early efforts early Mt. and there are some. Just create this bilingual dictionaries that you can see if you go to linguists language, libraries, or something  this, you are going to find a lot of these bilingual dictionaries. the disadvantage is, what should an interval look  they? . The Ibm model is a statistical model. When I look at an article in Russian. probably just think about the time of war, or something that you want to decode something using a noisy track channel. . I'd  to. , you are . and then you can  count the number of times the world in the. Canadian answered, which is  a parameter debate in English. And then you can do count? it's  you're creating  a generative model. All . . because the autograph is  different. metric and metric is just written differently. and of course, you can also use some advanced dynamic programmability. another language is in the middle was on the hand. How many of you are aware of the linguistic Olympiad? . You can do it. and then you based on your understanding. can you translate from English to maybe a Canadian indigenous language, that you have no information about, and they give you. she's a , that's it. I understand. . . Cook. What? . what if I you? . Are you? ? ,  or , maybe it's just you. And then you have we? It will only have one example, but it would maybe be included. you pass this Olympian exam. . Understand? Even if you have understand, understood. And then it's not very clear what changes if you move from understand to understood. can you. Decode is what we change from, understand to understood this. But . . for the Ibm model one. Ibm developed a series of 5 differential models that make increasing powerful assumption. because everything is just attaching it. d is probably in the French. ? And then, after you have known the alignment,  you just produce the indices. . And and then you can also oh. and based on this. But the question is, how do we compute probability of a given E,  of course we are make we are. And one way you can do this computation is just computing what is the uniform probability of the translation lines. or you can also compute the uniform probability for each possible alignment. I cannot give more details, because this is just the I'd be a model that I'm trying to split at the moment, which is just the formulation. probability of a given E is mostly concerned about the translation lines at the moment. the  one is probability of F given e comma a. You can just compute it if you have a bilingual corpus using mle. using the number I made. Oh. with each other ! The counts, what did we do. ? Given the current parameters to compute the expected value of the Count of F of E over the training data. here, if you look at our formulation. PF. And after that she can marginalize over a.  if you imagine a light over here, you're gonna get the one I'm talking about. , maybe I should just write it. If you want to compute this, this will. This will probably  ef and they will pass to each other. and then you can. there, maybe there are different ways to do the alignments, . You have different ways of doing this. The length of the sentence is also one of the things we should consider based on what we talked about. but here we are. This is probably just using em a guardian. What is the probability that mansion or mason is the translation of red? Or is the translation of red? But if you run this multiple times using the em that you might be able to achieve to go to the to the more plausible counts that you're expecting. for example, here, what , if you just want to compute mle for this. This is the count of how many times you have Mason and Red together. and you'll see that they are called together in the 1st example. How many times do you call? At some point you are going to see a different count for what you are expecting and based on the count. and then you cannot apply the Ibm model, which is just based on the statistical model. Rather, you tend to restrict it to what Ps based on how it is aligned in the set experience. And this would be Ibm model 2. and also because the Ibm model often gives very poor performance compared to genero base model. ."
    ],
    "Topic 2": [
        "hopefully, before you go on break? , which is very similar idea, because in the EU they have some official languages  English, German, French. and it should be due in 10 days time. hopefully it  bridge the whole methods with the new methods. And you populate new things from the renewal cycle. This will be on Wednesday. the time? machine translation, we'll talk about why, it is a hard problem inferability and superior wharf hypothesis. Then we talk about the vocros triangle. This is a German text, automatic text verified. From German to English. pragmatics, discourse, and   why is empty difficult? are pragmatics and discourse. For example, you could have a color  bonds, orange. or maybe the translation of Orange does not even exist. in this language. It does not distinguish between mother and father. moda or father, or in  rather mother or father in Chinese. there's no meaning of broader. And also in my  I'm looking at my native language, and I was thinking, oh, do we even have translation of Broader? and I think,  Chinese. You have to say the mother of the mother, the mother of my mother, or the father of my father. if you speak Chinese, maybe you can relate to this and then also some languages does not  They don't have plural and some languages they don't have. You cannot even say  for plural, for example, in our language you have to really specify the number to indicate Laura popularity. For example, in German formality is a very strict concept ? And if your translation model has only seen data that are very, very impolite, it could be a problem  that is an example. you suppose, or contain an assumption about the world. But in some languages this concept does not exist. But in another language, this may  have a problem. ,  we also have other examples, for example, in morphological examples where you have different levels of requirements of for inflection. And then this  would change the meaning of the world, and then the prefix might  change present tense to past tense and future tense. every items  nouns can be categorized into different classes. Vso verb subjects objects. Does Arabic use vso or sov vso , correct. you might be interested in something  politeness. It will produce different kinds of text. and if you're interested in this, you can check this website called Walls world at philosophy. We have different other linguistic aspects and non-linguistic aspects that  refer the overall culture of the speakers of the language. the pro, the question is, is it even possible to produce a perfect translation. for example, from English to Chinese or English to Hindi. Every task you can perform in English. Or there's some cultural differences that will make this impossible. And, for example, between French and Arabic, it's, is it possible? Is the problem they'll come perfect is the problem. It's that's the complication. do you have a high quality translation between English and German. and if you read it, maybe a native German speaker. The language. Legal documents are different in different countries. And it might be a problem. in terms of spatial organization, in a language called Quotayore use uses an absolute system. For example, they don't have things  left  ahead, and everything just is not eastward sound. and the idea is very simple. You want to do  a matching of n-gram matching of the words . And one way to do this is using the bluescope   the metric was introduced in 2,002, and the idea is that if you can have a metric  that just doing the counts. yes. Why is it important to do endrams , why can't you expect a number of words that are differentiated. Do they correlate with each other for every sentence you have tested. if they correlate, that means the metric is good. if they don't correlate, that means the metric is bad. Yes. We also have other metrics another one is called material score which is  a 1 to one match very similar to what you're talking about. Distance? Distance where? What's the distance to that of the reference? And more recently, we moved on to an embedding, an embedding based evaluation you have probably seen a bird's model in this lecture. I don't know if you have seen this  very similar to the birds. Yes. Yes. Yes. For example. You have the soft surface to the target surface. if you have words in French, you have the words in English, can you just do an alignment to the words and then create a machine translation. if you can get, if you can take the source language to an Interlingua. Some of these dictionaries have been created by many linguists as , for example, even English to including very low resource languages  people. the Interlingua is  a conceptual space common to all languages, that if you can take the source text to this intelliga, you'll be able to do the translation and the advantage is that you can use to develop a general empty system. You need this bilingual dictionary, because without this information you cannot do this alignment with Interlingua, adding a new language only requires translating into the Interlingua. That means all these different languages will be sent to the same space which by which you cannot be decoded. , at that point. and the idea is less as though we want to translate from English to Russian. suppose we are trying. Suppose we are translating from Russian to English. Which of the following is correct. Yes. Why I was speaking about  the model of the previous page, that. the pov is your prior, which is also your land model. You need to do match one word to another. suppose you have a lot of text in English and in German or French. We call this the word alignment model. Yes. you have to do all the counting. something  you remember the way we did part of speech. Yes,   when you do the counting, that's the way you will do the training of the model. for the sentence alignment. and then you have something  metric and metric. World. and then you have metric in French as  has something similar in English. these are examples of cognit words. and then you can define a similarity function between the sentence and some of the words. the dynamic time working, which is for any distance. even after the sentence alignment we do not have words that are aligned. You have to think about things  possibility of translation. you can have a word that is beginning in the sentence in one language. the Maths Olympiad. we also have linguistic Olympia. the idea of linguistic Olympia, this is very simple. They can give you a language or 2 languages. And then they give you rules of the language, and they ask you to perform a task in a language  given some rules. this is a very simple one. that means you have prefix beginning the world. Yes. Yes, all ? Yes. Yes. Yes. , it's possible. You get the idea. does everybody agree with that? each source word is aligned to 0 or one target world. And if you do this  an alignment,  you have a no node allows. What's an F to align to nothing in E. That means it's possible that you don't have a word to word matching, or a word in that sense. I cannot pronounce French words very  at this point, and then you have different word to word mapping, and sometimes you find out that the order is different. you can do the world toward mapping. But , the  alignment in French goes from 3 to 4, and then you have. And then you have another word standardized. the length of your alignment model is the same as your target model. ,  as your target text. We make a very simple assumption. this is a very strong assumption. because the assumption here in the other slide is that you want to align every word to 0 or one target. for the expectation maximization, you initialize the parameters. Step and the M. Step for the E step. we have probability of FE, which would have the composed probability of F given e comma a given E. We have computed the probability of a given E. We have computed the probability of FE. Yes. when we have a sentence. And we're aligning English word or away from the line already have . what does it mean to marginalize over all the alignments? you can follow different rules in doing the alignment. And you can marginalize over different rules by which are doing the alignment. in order to do this, we need to predify all possibilities of ways of doing. Yes. Yes. it possible to give an example of how? There's 1 example. And here you have an English sentence and a French sentence. This is what you want to complete . the idea is that you just have a sentence French. You have another sentence in English. How possible? and then you can create different alignment rules to  match to compare the sentence in English and sentence in French. Too many parameters. But as we'll see in the  class, there's a way we can adapt this  that you can have something."
    ],
    "Topic 3": [
        "David Ifeoluwa Adelani: Hi, everyone airports. the I don't know. I don't know the exact timeline. ,  today we'll be talking about the machine translation machine translation which is a very interesting direction. initially. It was an interesting military project. But since it's a very, very old project one of the earlier versions is the Ibm model. I wanted to inform you that we'll have another reading assignment that will be released today. and this will be on machine translation. if you have a bilingual dictionary that you can map everywhere to another. and then can you create  a statistical model for machine translation. for machine translation. this is an example of a machine translation? because you could have languages with different syntatic structure, with different morphology, with different semantics, and  on. For example, you have commonly cited examples will be colors us in English. but we don't have a term for brother. which is also important, is this is former term some languages are very strict. some words such as again stop, or more. because in English you can use this  words which already assume the presupposed information in the common ground between the speaker and the air. In this language our call starts me sets, which is, I think, a Canadian language such usage usages do not elicit a challenge from the air. It'd be different for the preface we use for a car, we different for the preface you will use for a house. but you also have languages that use. different languages use different structures. And then Hebrew Arabic could do  to left, and then you have this. Things can affect the new it can affect  your machine translation model. We have a semantic which  is talking about the meaning. formality. And then the translation will be different. , I think. And they interact in different ways. I must comment that not everybody believes in this. should we just improve on machine translation task, and then we can solve all the languages, all the tasks in different languages. it's something you can think about. , is it possible to have perfect translations within subgroups of languages? , for example, between all 9 languages to have exact translations. but saying something is perfect. You might not agree with the translation ? But the formality is not there. I think there's I forgot, there's common law. There's all this  law that different countries adopt. But nowadays, if you ask many people. if you can have 98% of the cases, you must agree with it. But , cases that . The language you speak affects your thoughts. how do you do a translation  that? and maybe this coming from culture or historical, that everything they want to do they have to use the navigation, go left,  to the river, or get the fruit or something. Your you will use east to west. ,  for machine translation. Theory is, can you even have your offer translation? and then your machine translation model produce another one which we can call the hypothesis or the output. and to see if they  correspond to what you have in your reference. You can fix the engram, Count, to  2 or 3 or 4, and then you can check how many of the engrams  do much. And then the way we are going to use to measure. you can check unigram. Your hand is one. if you ask human to rate the quality of a transition from 0 to 100, and then you have a metric giving you a value from 0 to 100. But this doesn't scale to if you transfer this to other languages. it's  the 1st size was created was created for mostly Latin based languages. and this is why it's called a precision based  and in practice, blue incorporates an additional brevity, penalty and a geometric mean over several values of N.  in practice, what we do is that we don't just focus on one end , which is a question you mentioned? And  we do a geometric map over it. and between the output and the reference. and also languages that use their critics, languages of different scripts , if you use languages that focuses more on character that works  Chinese. you send in your hypothesis, you send in your source text, and you send in your reference text. If you send in your hypothesis, you extract the embedding you send in your source text, you extract your embedding, you set in the reference text to extract the embedding, and then you concatenate. That's why it's changes in Miss Square Arrow. If you're interested in machine translation. workplace, ? it could be word by word. ,  our developers are triangle. this is  a triangle to describe different ways. You can do translation at the bottom. This is  a lower level way of the machine translation. Would you be able to do a better translation ? or it's a statistical based machine. Researchers developed a set of bilingual dictionary rules to map from one language to the other. And the question is that can you use this to translation? And if you have a multi-way machine translation model. for the statistical machine translation, which is what we're interested in. This was developed by assuming a noisy chatter. cool. and I don't know where this is coming from. When I took. you have the original text, and then you encrypt it, and then you send it through this channel. , you need to decrypt it to get information. This is  a question to you. you need to compute. , because we don't know. You can use this to  create an alignment model to do the translation. the sentence lines, the cognate words. for example, if language use a similar autography,  an example of a cognate word is , for example, reference and reference. you will know this is reference, ? Can you can  do one to one translation based on this similarity. and then factors to consider for the word alignment. Another thing that is very important, especially if you move to different languages is the word order. let's play this game. , they can give you 2 languages. And  you have a single word. What are the Swahili morphines for play? And they told me what display? And cook, I don't speak soilly. Why, pica. why not papika or Apica. I don't know. I think I provided some for a solution, but not for everything. Think  the past tense uses we, and the current tension is . I don't speak soil. I can ask my soil, speaker friends. I think we are going to do more than one or 2, and then we'll move to more neuro-based, empty. , each word. You just say, , every word maps another word. some we can align. And since each word is, you're aligning 0 to one,  the length is the same. And this is one way you can use to compute it. I think I can. They are provided . and in this case. the length has to be the same. And here we can  use compute this using Emily. and the idea of using Md. and after that you can multiply it. But the question is that let's assume you don't. You don't have. you cannot use Mle. Also part of speech, when we are not able to compute it? We use em already? this one can be expressed by marginalizing over a do you have question? it would be a language based rules and not dependent on the length of the sentence. I think there might be an . and then you want to compute this translation model from F given E uniformly  here. What information is the translation? But this is a very , this example is probably too simplistic, because  we have exactly the same value. You don't know the translation. You don't know which word is the translation of the other, and then you just can't. and then you'll be able to do the translation based on this. but in practice you don't initialize the translation model of F, given E. Uniformly given reasonable sizes of lexicon. There's more  there's slightly better."
    ],
    "Topic 4": [
        "Hey! Hey? We depend on the availability of the Tas. but  I'm not sure it's very difficult. For example, there was some pioneering work that happened also in Canada, , Canada was bilingual is bilingual and some of the earlier couples that was used for bashing translation is  based on Canadian parliamentary proceedings, which by law it should be in French and in English. you have this couples that could be used to train machine translation models. , I'm not sure if English is still part of it, or it's just a language that they cannot do away with but  they have this official language, and then they need to translate between all this set of languages. And it can be quite interesting. oh, 4, 30, , that means you are taking it. I  national processing. not to talk about mountain range, or do you just describe it somewhere, somehow. We also don't have a term for grandmother or grandfather. This is very clear in English, ? For example, Ibansu language  Swahili. we also have languages with noun classes which a good example, would be the Bantu languages in Africa, where or something you can call grammatical gender. and also we have Syntac differences  what other differences? And sometimes,  you can write from left to . there is a marketing translation tax that is called formality, where you could give a text in English and then give it a formality, level, formal or informal, and then it will produce the translation based on this information. language structures which you should be aware of has a lot of information about many, many languages of the world in terms of their linguistic structures. the 1st hypothesis we want to consider is a pure wolf hypothesis. some people have argued  that for you to work on any language, if you can create a perfect translation system, everything you're able to do in English, you can do it on any single language. is this not true? if you have a perfect passion translation system. Perfect? you can have high accuracy, no highly accurate translation. The the way the Lego language supposed to be written is different. And , Lego law is  Lego proceedings. , I'm not sure. it was just something wrong is going on. very few linguists  believe in a strong version of this. if something says, there's a sentence, says the coffee southwest of the dinner plate, this is  that's strange. because you would typically say, check the left side or . and also in some languages in English, you do left to  in Hebrew you do  to left, and in and this other language good. Because this is a very , there's a theory. And there's practicality. You have a text in English. You have a text in other language, say French. You have the text in English. You have the text in French, which is the reference. And then you want to compare this hypothesis with a reference. And how do you compare? the number of words unigamp. the way you evaluate how good the metric is to  give it to humans to judge how good the metric is. and the way you do that is to produce a correlation between. You cannot check. it has a high correlation with human judgment. it's worked very  in English. if you move to a language that use a different script  Hindi even siri exclude  Russian. and a more popular one these days is what is called character. Because for some languages you need to measure things at a character level. And then you need to use metrics to  consider the character, level information. Architecture you use  a multilingual birds model, and then you can train an estimator model. The embedding together are connected to the linear layer using the feedborn neural network, and then you can train based on the miss. this is a regression task. And based on this, you can  create an estimator that you can use for estimating the performance of a machine translation task. Do you have questions? the number of edits one is it where it is when you're making the edits, the edits that you're making? Or do you need to include semantic information? And this is the higher level. What would be the interling in our case if you want to use  a neural based machine translation. a system that is trained on and works for a set, a set of specific pairs of languages for direct translation to happen. , for example, if you have it costs a space that you can map it to one example in deep learning is. This is just  a theory that has not been implemented. I say this is really written in English. but it has been coded, is some strange symbols. and then you need to. using your knowledge of base? and then the P of F given E will not be your translation model. if you combine this, you can  use  a very basic statistical model also for translation. and then you want to count the number of times that this ward is associated with another world because they always call together. you want to train a model probability of F given he with probability of the source given the target. you have a text in English And another text in French, which consume the same text in 2 languages. And then you have a text in English. You have the text of French, and then you want to see ,  Canada is equal to Canada here. But it's  accounts based model  that you use to train the alignment model. It's just that it's it's  in French. but they have the same roots. Because most of this language, for example, the Latin days English is not Latin based, but it borrows a lot of words from Latin. And before you  do any fancy account of mines. for the water alignments. Many, too many mapping is impossible to do the mapping directly. It's really fun. They provide the rules based on your on your linguistic knowledge and the rules that have provided. he is language in East Africa that is morphologically rich. Which can I try to translate a phrase in English? let's decode this. If you have all these words in Swahili, and this is the translation in English. This is this should be straightforward. you have to decode the rule here, ? Pika. I'm trying to confuse you. what is high, based on what you can decode what? but the roots what will still remain constant . if you can decode this. which will also be the topic of your reading assignment. And model one is mostly basic. Very basic assumption. you don't try to model different distortion of water. and then you don't try to model likelihood of fertility  some phrases , Take a walk you don't try it, but maybe this is more complicated for a morphologically rich lover  Swahili. And then you have these 2 words that  mean the same thing, and calling and the mandate  I didn't. you see that standardized in French is coming before the containers in French. which is common the 5th world is  going to the 9th world in English, and  on. if you want to formulate it. Probability of, let's say, a French. What given English  you can materialize over the alignment which is a and this one to one based on probability theory. use the Bayes rule to say. this is a joint probability given E. And this will give you the probability of F. Given Ea. Multiplied by probability of a given E. Joseph is in the past room. the probability of F given E. And the alignment model. Is just counts the probability that this word occurs both in English and French, and then you normalize by the count in English. with which you can start with the initial counts in, run at random, and then you try to modify the accounts until you are able to find a true distribution. Probability of A. F given E. Randomly, and then you do the SE. And for the M step, you try to compute a new value of probability of F given. E, ,  what's the probability of the alignment? Given a. What is the probability of a given ef  probability of a given ef possible can be decomposed this way, using the Bayes rule which is the pro the pro the product of probability of a EF. Given probability of EPE. Given E. I can try to write it on the board. a and all its possibilities consider both the language and the length of the site. What's the probability of this? This is very straightforward. both in English and French. think about having a lot of sentences. let's say 2 million sentences with different translation. this word is the transition of the other is going to have a higher probability. too much memory and computation. And after that we're going to examine the last model which would be neural based. because you are likely to use that  than the Admin model. thank you."
    ],
    "Topic 5": [
        "I think we have very few number of people today. , this week. we will not be working on it this week. maybe  week. you have the results. Fingers crossed. It used to be a very, very difficult problem. even the current models doesn't solve the task. It's just very good at the general machine translation domain but there are some specific domains that even if you try it the current models may still fail. For example, it will still fail for low resource languages, it will still fail for some domains, for example,  medical domain finance domain, and very, very specific domains that maybe they care for. In the industry. but machine translation has been there for  long. In the era of the World Wars and also it became an interesting projects. Similarly, we have some European coppers  European Parliament also have. the outline of today's talk will be Oh, there's another announcement. I think, by 4 pm. by 14. . And why is this a difficult problem? in this lecture we have been taught about concepts  morphology syntax. one good example is, considering the lexic lexical gap that could occur between different languages,  lexical gap,  the meaning of the word may not have a translation in a language. which may not have a good translation in another language. you have to say 2 items or 3 items. and this might make machine translation challenging. if you have the word  mark called again. this already presupposes that my account. This this word again,   mean slightly different things depending on the context. You have a lot of prefixes, and I don't know. the way the prefix you will use for a person will be different from the prefix you will use for an animal. I don't remember it. And then you have pragmatics, where, depending on the context, for example. Is this correct? Can you also do the same thing in English? Because, depending on the context. The answer is, yes,  . But if I do translation of Lego documents. But it's possible to have a highly accurate translation in a particular domain for saying, parfait translation. They will tell you, , maybe for some languages, machine translation is solved because maybe on 100 cases. for sapir hypothesis. Which version language may influence human actions and thoughts slightly in highly specific ways. some language even just reject everything. And then you said South West. Even , unfortunately, we are not at a point of worrying about Sapia worth identity. If you're aware of text generation a bit, you might be aware of this metric called blue score. this is one of the metrics that you can use to measure the quality of machine translation. How good is the metric is if it aligns with human judgment. but typically, I think people use  public 2, 3, 4. And the reason is that. or your evaluation sets. when blue was introduced, they also do the discretion surprisingly. Then maybe something doesn't work again. blue is  focused on what is called precision. It's precision oriented for each engram in the proposed translation you have to check if it if it is found in the reference translation. We we do for N equals 1, 2, 3. but me blue is not the only metric that is available. Then we have another one called translation Error Rates which is the number of edits required. Are you familiar with edits? Which signifies, how many edits do you need to make to this text  that it's going to look  the other one. if I edit the hypothesis. especially for morphologically rich languages. If you use blue score, it doesn't correlate at all with human judgment. They even have their own specific character and tokenizations. an estimator is you share the model, giving a pre-trained model  Bert's model. Or what's the approach? How can you do a direct translation from a source text to a target text and for direct translation. this is the theoretical concept, and we try to see if we can have what? For direct translation. implies a system. if you want to do translation, you have to convert every the entire sentence into a single vector and then send it to the decoder which is going to decoded to your, to the language of your interest. This is prior to the planning ? it might be difficult to work with such an expressive. but maybe  it's a little bit possible. I will  proceed to the code. you can look at a machine translation  a form of encryption and decryption. the 1st or the second one. , the second one. , he is the prior. Pof, yes, . , that's correct. the key thing is that we need to do an alignment. and then maybe my French is not good, but you have some. what's that attribute? it's  supervised because you need a lot of text and a translation. There are a lot of tricks you can use. You can make use of various tricks to get sentence alignment. You can try to use tabs to send sentence lines longest common subsequence of characters. this  may make word alignment a bit difficult. , that's good. And you saw the task. That's seems to be correct. we can have fun today. Maybe you're correct. , I think Peter might be correct. These are more confusing. I also don't know the precise answer for this one. And and I, , that's looks correct should be correct. Maybe this is more complicated. , it may be truly . And then, if you look at the because morphologically, return is, they just keep attaching,  you can have a very long world, and they just keep attaching, prefix, depending on what has been added. , your answer seems very possible, because that's the only thing we can say here. And you have to remember. you say one is aligned to 1, 2 is aligned to 2. or Lee the L with apostrophe is null, because there's nothing is attaching to it. which by  you should be familiar with this  formulation. What are you going to do? for my lecture? But I'm just gonna write a second  of. depending on the language. I think the neuro base is a little bit more interesting than this Ibm world."
    ]
}