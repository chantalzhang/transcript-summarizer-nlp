{
    "Topic 1": [
        "today, we're going to talk about compositional semantics. But I'll I'll have extra office hours in my office during class time. last class, remember, we were still talking about lexical semantics. If , this is, these are the ones that are about  our 2 words, do they fit some lexical semantic relation? this is the idea of distribution of semantics. Or is there something better you do because it turns out that there is something better. And I should also say . Why do we care about this? And  that rather than use raw counts. it's no longer a natural number. if it turns out that w. 1 and W. 2 are totally independent of each other. this is one tweak to just doing the raw counts. You can also talk about principal component analysis. And as it's applied to information, retrieval sometimes called latent, semantic indexing, or something or latent semantic analysis. And  the way they do that is, you can apply this Svd method. And here, this is  a dimensionality reduction technique. K, you can prove that the matrix you get by applying this as truncated. this is just to say, this second point here is just to say that it corresponds to something else called principal component analysis. it'll be helpful to look at this graphical intuition of what truncating, truncated Svd does, or principal component analysis. Is a famous model that does this from 2013. It's it's still the same. all this to say, this is  been. This has been how distributional semantics has evolved. you start off with  some  count-based approach. And then there'll also be neural approaches, and, as I also mentioned last class, it turns out that you can prove that the Skipgram model and the Svd based model. You can prove that they're equivalent to each other . With Sv, do we need to ? Is it good enough just to know,  the high level idea that we're  suppressing it, and only keeping the relevant information or the ones that don't. for, say, for the midterm, , it's enough to know that it's about that. distribution of semantics and lexical semantics done. And it's about compositional semantics. the thing with lexical semantics is, it's only about the meaning of words, or  small phrases. in compositional semantics, what we  to do is we  to talk about, say the sentence level where sentences have meanings that you can derive by looking at the parts of the sentences. ,  we have to talk, then, about the principle of compositionality. if I say to you, you may disagree, but comp. We still need some procedure to build up the meaning of the entire sentence through composition. In the last lecture or 2, when we were talking about lexical semantics. And just look at how they do things and then apply it to natural language. it evaluates to false ? that's 1 potential view of . that's another way to connect the language to the world. language is compositional, but it's also not perfectly compositional. in particular idioms or expressions whose meanings cannot be predicted from their parts. ,  , , you might, you may or may not know that kick. There's no function that you can apply here to derive the overall meaning from the meanings of each of the parts. , , the type of events is usually preserved  kick. usually there are still aspects of meaning that are shared between the idioms and the overall meaning. which is that things are often relative to each other and not strictly about. Might be your prototypical red. That color I don't know what to call it. And this was a tradition that started in 1970 with Montague. and it's the idea of using a logical formalism to represent the meaning of a sentence with a tight connection to syntax. which is that you can then talk about natural language inference as applying logical rules of inference. if you pick a particular logic as we will, then, whatever inference procedures are defined by that logic, you can also apply them to natural language after you've converted natural language to that logical form. You're still going to have to translate between that and natural language. ,  then, what is inference? and  these are cases of semantic inference. , it's , in terms of natural language sentences. this is  natural language inference. But we're gonna still turn this problem into  a problem of logical inference by defining a procedure to convert the natural language sentences to some logical form. 1st order, logic has these components, and  we need to define them,  that we can talk about translating natural language to 1st order logic. this is the side that is more  a semantic. If you have something that takes in multiple arguments  this, then it's usually about some relation. , , I can say, , I can create a function called Montreal, which takes no arguments, and it just returns an element of your domain of discourse which corresponds to the city of Montreal. The the idea of city, the city of Montreal. And  here, wug and blog are these are predicates. But if it's not a bug, then you don't care then. and then it can be a block or not. it takes in 2 arguments, 2, 2. And you  to review logical connectives. And then we'll also define a procedure to combine those pieces of logic together in order to form the overall meaning of the sentence. Is there any order of the old? Yes, there, there is an order to which you apply things,  that this whole thing is within the scope of that universal quantifier. , are we  with this? Oh, you mean this dot? Those are all credit between the grade up as a function. the way that I'm doing it here is to say that grade of is a function that gives you the grade of the student, the X entity. and that's equivalent to a. ,  the second sentence to translate is students who only do. ,  I'm gonna delete it. But if you define it with a truth table. This is  the minimal domain of discourse that you can have to get this to work and satisfy all of those requirements. You can have , . you define it  that it turns out that you got an A and then, friend, one gets a B, and then, friend, 2 gets a seat is something  this. But both of those points to the same. in order to not confuse things. , we change the grade of any student. , we change any of the grades. You can look at logical rules of inference and derive conclusions by using logical rules of inference. , we need to have some algorithm for constructing these logical formulas at the sentence level from the its parts. in order to do that, we need to use another tool from computer science and whatever. we need to use lambda calculus. and that will help us to define a precise algorithm to do . allows you to describe computation, using mathematical functions and the computations that we're going to be doing is we're going to be building up a 1st  logic sentence as the as the meaning representation of the sentence by using fragments of logic in the subparts of the sentence. and lambda calculus can also be formally defined, and it can be defined recursively. Then there's lambda X of T, where t is a lambda term. and then there's Ts, where T. And S are both lambda terms. But we need this because we need to be able to store partial computations as we're building up the meeting representation. you have some lambda, X some lambda expression lambda, X of T, and then you apply it to some expression. , what it does is it replaces all instances of x within t with the expression s. , , lambda x of x plus y apply to 2 simplifies to 2 plus y, because you're replacing everything that's X here with X with the with the 2, you can even take lambda expressions and plug them in as  as  arguments within the body. if you have lambda, XX, lambda XX, you can simplify that. Here there are 2 copies of X.  the argument here is this identity function. this means that you have 2 copies of the identity function. And then you can further reduce this by taking one identity function and passing it to the other identity function. something that takes in a Y and returns 2 copies of it. and what you're returning 2 copies of here is the is the identity function. if you have,  4 lambda terms A, BC, and D, you 1st simplify a B, and then you take the result, and then you  do Beta reduction with Z, and then you take that result. And then you do beta reduction with D,  we only really, we're not really using,  the a full power of lambda calculus, that much we really only needed to store partial computations. ,  why do we care about partial computations? Why do we need lambda calculus? When we have the entire logical expression. we'll have some fragment of the meaning representation with things that need to be filled in. And the predicate takes in 2 arguments, Y and x. because you need something that is disdained. And you need, , you need to have something that's disdaining something else. And then it's still missing the semantic agents is the thing doing the disdaining. They have pieces of logical representations with things to be filled in, and then you, you express those things to be done using lambda expressions. And then you use lambda calculus, and you do composition up the syntax tree to get everything to go into the  place to get the logical representation of the whole sentence any questions about the high level idea? And this lambda stuff is  that we say, , what object is applying to something else. And then and the lambda terms, let you express that  it's missing 2 things, and then you're going to find it somewhere else in the sentence. And then the procedure of  finding it is by doing composition. what we're doing is we're doing syntax driven semantic composition. what that means is, we have to go back to Cfgs. and we have to augment them with lambda expressions with the idea that every time you're doing a syntactic composition. You're also correspondingly doing some function application in lambda calculus, in order to simplify in order to in order to take these pieces of meaning representations, and  merge them together to make a larger, a meaning representation of a larger bit. then, syntactic composition goes hand in hand with this,  semantic composition. and you can write it down formally, as  your semantic attachment. this is your Cfg rule. Here, I'm using dot sem to refer to each of those the partial the pieces of logic representation there, and you're combining them  in order to . gets the overall meaning representation of the entire constituent, the larger phrase. ,  if we do this bottom up. and for the small pieces it's  for the lexical rules is pretty easy. Comp, 5, 50 is a constant and , for reasons we're gonna type, raise them  that they're gonna be lambda expressions apply to constants. here's gonna be what we store. Comp, 5, 50 will be lambda XX applied to the constant Comp. And then the associated rule, for,  Np. our predicates inside a lambda expression of type E to T. What does this mean? , the rule of N rewrites to the word student is associated with this semantic attachment of lambda X. Student X.  it's a predicate applied to  where X here is  a variable in the lambda expression. suppose you have the verb rules. then its semantic attachment looks  this. Is associated with this semantic attachment of Np, dot sem apply to Vpsem. And this is what it would look . It's you go bottom up. for each of these rules you look up. The semantic attachment associated with those rules which you can find from the previous slides will look  this. and then for each rule you then apply this  you look at the semantic attachment associated with those rules. and then  for these unary rules, you just pass up the semantics. the rule was, Np, dot sem applied to Vp, dot, sem. that means np, dot, some would be lambda XX, comp. 5, 50, because you get that from this, from its from the subject. You get from the Vp semantics. And here is where you do, beta reduction to simplify this expression. ,  you take this entire. the way you do this is, you take this entire expression, you plug it into where X is. and then this is what you get. And then you take Comp. 5, 50, and you plug it in where X is within the smaller piece, and then you get this expression of there exists a. E. Events which is a rules event, and the ruler of E is whatever is denoted by the constant comp. then,  at a high level, the idea here is every terminal rule. you take those semantic attachments, and then afterwards is a mechanical process of looking at your augmented Cfg with those semantic attachments and then running those procedures. And then we needed lambda calculus to store these partial computations. ,  the questions are  the grammar does not have to be in Cnf. And then, in terms of the order in which you apply things. You have to look at the semantic attachment rule. here it says, Np, dot, some is the Punctor and Vp, dot, M is the arguments. ,  I'll stop there and we'll continue  class."
    ],
    "Topic 2": [
        "And even before that, just some reminders. please do that as soon as you can. Also, I assume that you've already started working on reading assignment 2, and then programming assignment 2 is out as . And then we after that we talked about hearse patterns and also bootstrapping with these patterns. one thing that I didn't really spend time on which we should talk about is , do you just record the counts? But also, even  I would say, a lot of search engines are still based on word of co-occurrences. it's important to learn that this for that reason, too. ,  here's the thing I wanted to also talk about. this looks a lot  some formulas you've  seen for looking at independence  of random variables. That's the definition of 2 random variables that are independent of each other. then you have a negative value. , that's , because it's the probability of both of those things happening. we just ensure that somewhat normally. It's because here it's  the probability of both events happening. ,  that  it's not exactly the  space of events that you're considering. Another thing that people often do is that they often discard negative values in the Pmi. The reason for that is that the fact that 2 words occur Co. here in this nlp, course, we have limited time,  I won't really talk much about all the math behind it. But I assume that you have, or will see this in another course. which has lower rank in the linear algebra sense. , you're getting a matrix with lower rank than the original. Is a method to help you get rid of some of those. And you can show that this is the best possible approximation among any matrix of this rank. graphically, this is what's happening when you're doing this approximation and this one dimensional approximation of the original 2 dimensional make. I don't have time to do this topic justice. But there's a lot of math and linear algebra and algorithms and people have come up ways to do this very quickly. And truncation and principal analysis, you find that all over the place. And finally, the other thing, of course, is that people also train neural models of word embeddings for lexical semantic for lexical semantics. The the embeddings associated with those words which are parameters of the model. Then you sum them up. and then you use that to predict the middle word that's missing. And this latent representation of what you predict. the more,  more popular one is the skip ground model, which is that you take the middle again. You take the middle word, and it's embedding. and you use that to predict one of the one or more of the words in the surrounding context. But but , sometimes they still use word 2 vec,  word 2 vec, although ,  they're not popular anymore for English. If you have,  a highly specialized corpus where you don't have a lot of data to train a transformer model. Or if you're working with a language where you don't have enough data to train  Lstm or transformer model. Sometimes people still use word embeddings of this time. Longer does the script skip ground have a lot fewer parameters? I don't think , because, , it's  because the parameters are shared and the sibo. we don't have time to , get into any more depth. The  topic is the, , the final major topic for the midterm. Compositionality is exactly this idea that the meanings of sentences is not just some. whereas lexical semantics might give you the meanings and behaviors of each of the individual words. And  this is really important. This idea of compositionality is really important, because you could argue that this is what lets language be really, really flexible and useful, and you can use language to talk about new situations and  forth. ,  people make a big deal of compositionality for good reason. , , to relates the linguistic expression to the world, one thing you can do is that you can assert that a proposition. But but the point here is that you're able to do that evaluation? But suppose I say it will snow tomorrow. we talked about compositionality  just very quickly. There here is also a very high profile violation of compositionality. or if you see something, it's the last straw 99% of the time you're not  talking about the last straw. these are clear violations of compositionality, because there's no regular  function. Interestingly, though it's not  arbitrary , there are still some commonalities in the meanings of  each of these parts, and the overall meaning. But it's still a composition is a violation of compositionality of root. And here's another more subtle violation of compositionality or arguable violation of compositionality. The meetings internal to each of the parts. ,   this is the idea of co-compositionality, which is that this is still  compositionality in the sense that the meaning of the whole is derived from the meaning of the parts. Really the same  red wine is very dark, ? or  red cheeks,  red cheeks. but you can still talk about somebody's cheeks being red, ? And hence the this approach is called Montegovian cement. , after reading this quote. but this is the assumption and the approach behind Montegovian semantics a dash we can use that natural language can be made as precise as  logic, logical languages of our logic. at least in at least the meetings. And I would also say that,  the other reason to pay attention to this stuff is that we can never get away from something symbolic and logical, at least not completely. If you say all wugs are blorks and all blorks are cute, then you can conclude that all wugs are cute. unlike in your previous logic classes. if you have,  a student X, it's usually in natural language, it's  checking whether X is a student or not. here, in course, is about a relationship between the student and the course. And the student being in, enrolled in the course. please tell me you've seen these . ,  then there's an existential quantifier and the universal quantifier. We can convert it to this form, for all X log. You're asking, is this a Wug? And that's what Wugex is doing. And and the implies arrow here. It's a it's a logical connective, ? Truth values, the one thing before and the thing after. ,  think about it this way,  the just, the logic and the sentences themselves, although that's what you spend most of your time  working out. ,  I'm we should do this exercise together. what  quantifier do you think we need? for all x. if X is a student. No wedge is closer to intersection. the reason is because the types don't work out. ,   this is a logical, this is a Boolean connective ? You need to have that relates to the X to the student . Do we have to bracket the entire thing. and the reason you don't is because , the quantifier takes precedence. No  this implication takes precedence over  the quantifier, or something  that. ,  technically, you can have student a student. C, yes, you can have student A, you can have student, and you can pass in A and B and C, it's just that they'll evaluate too false. I'm gonna put that you and friend one and friend 2 will be the ones where, if you pass it to students, you evaluate student of that re values to true and otherwise it values to false. And  And you're a very good student. And again, you're a very good student. And but  your friends don't. this would be an example of a possible situation or world where all of those logical formulas evaluates to true. You don't have to do that because you can. function application also known as Beta reduction takes this form. If this was confusing one thing you can do is you can rename the variables. , , suppose you want to measure you want to produce the representation of whiskers, disdain, catnip. each of these lexical items. And when you compose distained with catnip. and then you take whatever representation of catnip that you have as the argument, and then you can do beta reduction. then you can make sure that catnip goes into the  place  that it's the thing being disdained. And then later, when you see whiskers, then whiskers becomes the argument, and then it gets plugged in into the Y, and you get  something that's but here it's  some  semantic representation of Whisker's disdained cabinet. they all have this form where they need. I'm going to post the answer to these exercises. Rewrites to proper noun would just be the take the semantic representation of the proper noun, and that will  be the semantic representation of the Np. What you do is you create an event variable? E, and you assert that there exists a certain event associated with this verb with arguments. There is a it looks  there's an event, E such that e is an event of tight rules. and there's a ruler which is the X to be filled in. And then finally, , for  the composition rule of S. Rewrites to Npvp. ,  that was a lot. Ignore all of the logic. this you can remember from before , as rewrites. Rewrites to a proper noun, and then proper noun rewrites to the word comp. Rewrites to V, and then V rewrites to the word rules. E. Rules E. Ruler EX."
    ],
    "Topic 3": [
        "If you have a midterm conflict, please send me email,  by the end of today. then, there's no class during that time. ,  here are the algorithms from last class. And we discussed Lusk's algorithm and Yourowski's algorithm. do you remember what tasks those algorithms were for? ,  it's for a word that might have multiple senses and figuring out which one it is. But there were a few topics that we didn't cover in enough depth. I just wanna quickly go through that a little bit more. some notion of how much more likely or less likely than chance. Those 2 words co-occurring compares to observing them separately and individually. and then it co-occurs with the word linguistics 300 times. and the word linguistics occurs 2,500 times in total. , then, you can compute all of these numbers. ,  you can compute V and linguistics. because it co-occurs with linguistics 300 times you get this joint, . And  you can plug these numbers in. why are we dividing 300 by a million. it's   , this is the approximation there. , if you get a negative Pmi, often they discard it, and they just record a 0 instead. , , people often just discard them. it's a really famous technique. And that's how you can get an approximation. you're preserving as much as the most information you can. This is the approximation that preserves the most information. that's how the that model is trained. these are by modern standards. These are relatively simple architectures and ideas. Or is the just the embedding of the word. I would have to think about that. And then people have used linear algebra to try to  reformulate that and do a little bit better. And it does this by applying this matrix factorization algorithm. remember, what does Lexical refer to? Written language happens in sentences and in even bigger chunks, which  if we have time to talk about later. If you think about it. how do we think about that. the meaning of the sentence is, the is the thing that lets you perform that evaluation to check. You turn this into some logical form, and then you can evaluate it against  a database that's connected to some meteorological service, and then you can return. , but that has that is not really derived. You can't derive that from kick the bucket. This is the final thing that really set someone over the edge, and they get really mad, or something  that. The bucket is  a thing that happens. Imagine in your mind what the meaning of red is  on its own , , just think about Red! What does Red mean to you. For most people it might be  some very vivid   red. the RGB value of  2 55 0 or something  that, ? But  think about red in the context of all of these words,  a red rose. but is that it's not magenta violet? Purp again, some  purpley red. If you  analyze the RGB value of someone who's blushing. , it depends on their skin tone, but it's almost certainly not red. then the idea here is that the meaning of red  is influenced by what you're composing it with. Indeed, I consider it possible to comprehend the syntax and semantics of both kinds of languages, with a single natural and mathematically precise theory. , these days, with  neural networks and large language models, you may or may not have thoughts about this. and you can model the former with the latter. because we still have lots of observations that are non linguistic again, , say, weather data, , or  other, in sources of information from natural processes that people gather and put in some database. and you have to access it through some database. you have to write some SQL. If you want to have a natural language interface to all of that information. Inference is to make something explicit that was implicit before in language. ,  if you say something , I want to visit the capital of Italy. and the capital of Italy is in Rome. Then you can make an inference, which is, I want to visit Rome. because chances are, it'll be confusing for most of the rest of the class, too. or the classrooms and courses that might be one Mini scenario we want to model. , should we think of a pedicut as   an identity function in a way that as predicate and identity function, what do you mean by identity function  if it takes  or an example in the case, , in course, Xy  it just checks the identity of X and returns  yes or no  predicates. , it gives you some information about the properties of that out element. And, ,  you have an instructor of function that takes X and returns and other elements which corresponds to the instructor of the course. And also, , how about quantifiers? we might want to define some procedure that converts a sentence  the capital of Italy is Rome into some logical formulation. here I have capital of which is a function. and then Italy is also a function. It's a is a predicate. and then you can have a function to get the capital, from Italy, from the from Italy to some other entity, and the other entity is the entity denoted by Rome. And then there's also this abstract entity which is  the entity of Rome and the abstract entity of Italy. and then the capital of converts that Italy entity into the Rome entity. Or here's another logical sentence that all wugs are blorks. Yes, , for the second sentence are we  saying for every X, then applying the predicate of , the one thing is also a floor. And there was another question, . , that's a great question. the question is , where does this function of capital out come from? It's just, I'm just defining that this could take the form of a function to represents capital of. But if it's ever unclear, you can add parentheses to make clear the scope. this part is really confusing. because it's not intuitive, and the 1st time you see it is confusing. But  the reason, remember that the reason we're using a logic ultimately is to relate language to the world. Logic consists of the predicates and function names and arity. Because  it's this is, this can be difficult. One of them will get A, B and students who do 90 will get a C.  we should list the predicates and functions that are necessary, and make constants for the grades. ,  for all x for all students, how we do that is we write. And they'll get an A, how about we do sometimes it just. And to be pretty, to be more clear, we can add a parentheses. They're great as 8. . Do you think of wedge as  union? this, the wedge is just  Powerpoint. I can add another one, too, to make it clear. , that's a great question. It takes 2 truth values and returns another truth value. And then this is  a predicate in disguise. But if you want to be just super clear, you can just add the parentheses. But since we haven't applied it to a particular group of students and their results. , that would be really interesting. you have to be careful how you write the or  the or has to be over those, both the conjuncts, the con, the conjunct of both. ,   the wrong thing to do. this would be wrong even if you add a parentheses here. And the students who do neither get a seat any questions about this. but the abstract notion of  having grades that are better or worse. we have to , specify the meanings of everything. It's  ,  you have a name, and  you have a nickname. , that was my question. but just for the sake of not confusing us. ,  does this idea make sense. You almost never go to this level of thinking about particular scenarios and defining the particular scenarios. , that would still be useful without having to go through this layer. This is a different notion of variable compared to the notion of variable in 1st Square logic. And you end up with  one identity function. one thing to note is that function, application is left associative. then what happens is that you take that expression which you think of as a lambda term. , it's  a placeholder until you have . According to this theory, this approach. After that one step you're you end up with."
    ],
    "Topic 4": [
        ", the midterm is, ,  Wednesday, if I remember . It's not about the meaning of 2 words in a sentence. one important concept from the second half of last class is this idea of a term context matrix where you can build up a representation of the words in a corpus by looking at their co-occurrences. You're using the distribution of words in order to infer something about the meanings of those words. And  here, in the term context, matrix, you have the target words which are the words whose meanings you're trying to model. And you have context words which are the words that appear in the context of those target words in the Corpus. And you can define context in any way you'd . And  it's still based on these fundamental ideas. this has been a very important, influential technique, which is called point wise mutual information waiting. You instead record your term context, matrix. , it's just some   a score or some  a log ratio of ratio of likelihoods. , here  , here's the formula for point, wise and mutual information. You estimates the probability of both of those words occurring. and you divide that by estimates of the probabilities of each of the words occurring separately of each other  independently. which that means that this ratio is one which means that the log of it is 0.  that's   the chance probability. , that's saying you have 2 words. rather than occur recording the raw count in the term context matrix, it records a notion of association between the target word and the context word. Suppose the word be occurs a hundred 1,000 times in a corpus of a million tokens. it's 300 divided by a million for the though it's 0 point 1, because the is a very common word. it's a hundred 1,000 divided by a million. and for linguistics is 2,500, divided by a million. it's  300 divided by a million. Because you, it's you're looking at the entire corpus, and there are a million tokens. not really because if you choose a different base, that's  just multiplying all the numbers in your entire term context matrix by a constant. this is called positive point wise mutual information. But you can use Pmi, and you can get pretty good levels of performance if you implement some  ir system  some  basic ir system. in truncated Svd,  that you want to creates a version of your term context matrix. you have your original term context matrix, which has dimensions of the size of your vocabulary by the number of context words. I'm indicating the size of your vocabulary. number of target words here as the size of B, and I'm indicating the context word size, the number of context words as C, and then you can factorize it into the product of these 3 matrices. And here this Sigma K here is a special, because there it's a diagonal matrix that contains the singular values of this original matrix which you can think of as some  characteristic way of looking at the properties of that matrix. And it turns out that you can throw out some of the values of the in the singular , in this, in, in this, in this matrix, and also throughout the corresponding rows and columns in the other matrices to come up with an approximation of the original term context matrix. You're  taking the dimensions of the original term context matrix which explain the least amount of variance. the idea here is that  suppose M is the original rank of the term context matrix. You can also prove that this view of truncated Svd is optimal in a technical sense, in this very particular sense, which  suppose X was your original term context, matrix and xk, is your approximate term context, matrix of rank. This is an L 2 norm. you're looking at, how much is the difference between your original matrix and the approximate matrix. suppose you have just 2 context words. your original term context, matrix has 2 dimensions. And then you plot all of your word vectors in this two-dimensional plane. What truncated Svd or Pca does is that it finds all of the vectors. It finds,  some axes that best explains the variance of the points with respect to each other,  the points here are the closest, as close as you can get to the axis, to the axes as you have. and then, when you are truncating, when you're removing some of the singular values and removing some of the dimensions, you're  squashing everything into just a lower dimensional space. A space is this is the this is the squashing. you should look into that if you're interested in this and this idea of Svd. For in many applications  this block. And I mentioned last class that word 2 vec. I thought I would just briefly show the neural network figures for word 2 vec. the original model  proposed 2 versions of word 2 vec. In the 1st version called the continuous bag of words model. and then you look at the words, say 2 words before and 2 words after, and you take the rep. That's  the representation of the word. That's how you get a representation of the word  that's the continuous bag of words model. it's not  you have a parameter. You have a set of parameters for a word, 2 positions before, or something  that. you just have one set of. suppose your representations are size 100 or whatever. Then you have 100 parameters for each word in your vocabulary in both cases, . , only keeping the information that does affect variance. ,  that's the high level intuition. That's the high level intuition. It's not arbitrary, and you can derive the meanings of a sentence by looking at the meanings of the subparts of the sentence. 550 is a fantastically awesome class. then that sentence has a meaning. And it's how all of these words work together that you get the meaning of the whole sentence. and within this module what counts as a good meaning representation. We talked about the relationships of words to each other, ? The relationship of words to each other  with all these  somatic relations,  synony and autonomy, and some autonomy, and  forth. And we also talked about relating the meanings of each word to the things in the world,   to the references, and also  to the sense of the word. what about at the sentence level? But what about the aspect of the meanings of sentences with respect to the world? one way to do this is to really use some  ideas from logic. this is a term from logic is either true or false relative to the world. to figure that out, you have to take the sense of that, whatever that means and evaluate it against a world. , unless you , die a poor panda or something. of sentence meaning another is to convey information about the world. Then what that's saying is that it's conveying something about the world. and how the world is or will be . or it could be a query, as  , What is the weather  in Montreal  week? it's this  a sense of meaning that we're going to be talking about. The bucket means to die. The sack means to sleep, and  forth. and then it's kinda over  , which is, , the same as  to die. it's and it would be very unusual if there was an idiom where the event type is something  I don't know relaxing at home, and then it means to die, because then one of them is  a it's  a state of being , whereas  die is  an event that just it's  a point in time. And and when you put words together, their meanings  affect each other as . the tradition that we're going to discuss today and  class is to use logic to model sentence meaning. there is an advantage of doing things this way. and  that means at some point. ,   the logic we're going to pick is 1st order, logic or 1st order predicate calculus. this means you should ask me questions. first, st order logic can be defined as having a domain of discourse, which is a set of entities that we care about. there's a domain of discourse of entities. It has predicates which map elements of the domain of discourse to truth values. , , I can define the predicate in course XY, which takes in 2 elements of the domain and returns. if it's a predicate that has a single argument, often it does give you  the category, it checks the category. functions, map elements to other elements of the discourse. And  the difference between predicates and functions is what they return. In 1st order, logic functions give you,  a elements  other elements of the domain of discourse and a valence, 0 function would be a constant. And then you have logical connectives. ,  you have seen 1st order. the  , these are the basic elements of  1st order predicate calculus. it might be easier and more concrete  more examples. this sentence and this formulation is asserting that there's a something called Rome. Just think of them as  names. If it's a log, then it should be a port. This X ranges over all of the elements of discourse. you're for all X means that you're . then it should also be the case, that if you ask, is it also a blurb? In order for the whole thing to evaluate to true. or Boolean connectives, if you prefer to remember what they mean. but in general there will be. There are conventions and procedures,  that every word in English you can associate it with some piece of logic which might involve defining functions or might involve defining predicates and  forth. Is there an order to it. And  that means that to interpret a 1st order logic. you have to interpret it and evaluate it against some world in the end. as  as a set of sentences. In 1st order logic using those predicates and functions. and then an interpretation of it, or a model of it, is to apply it to an actual scenario to check the truth and false values of all of those logical sentences, or anything else that you might care about. ,  , and your interpretation involves specifying the domain of this course. and it also means specifying the functions of predicates. how do the functions  map entities to other entities? And how do the predicates  map entities to true or false? That's just  general statements or constraints about  things that may or may not be true. Whereas an interpretation is where you  interpret it in a particular scenario. ,  we're going to come up with a 1st order. and then we'll come up with an interpretation of this 1st order. where you and 2 of your friends are the elements in the domain of discourse, such that the above 1st order, logic formulas are true. ,  let's do this together. because we're talking about students. , because we're talking about some general property that should apply. think there's a way to have equations. I just want to make it clear is that the conjunction of these 3, which is in the conditions for the implication. then I'm wondering, can you just do  rolex students? We have  3 predicates, and we have grade of X is equal to a why can't we just have 8 here. But just a is a constant,  it gives you an element in your domain of discourse. And it wasn't worried about it was  82. . you're asking about the interpretation. Yet we can't tell if it's true or false yet. wouldn't that contradict the 1st statement that students who do both get an a . it's not defined by the set of logical symbols that I put earlier,  logical connectives that had earlier. You just have to define what it means. how do we interpret this logic within a particular context? ,  then,  ,  then come up with an interpretation where you and your 2 of your friends are elements in the domain of discourse, such that these 1st order logic, formulas evaluate to true. But once again, , it's important to , distinguish between  these 3 and the names of A, B and C here. ,  year Mcgill decides to change the names of all the grades. you get  Unicorn and , I don't know goose, and I don't know duckling instead of A, B and C, in which case,  you can change the names of the constants to  Unicorn and goose, and duckling, or whatever. Is this the domain of all variable factors. This is the domain of all variables. And , and then creative. And you can check ? You can check,  for all X student and study and do. Homework means that the grade of X is a you can check. you it applies for you ? and you can do the same thing to check for  this sentence and this sentence, and this sentence, this sentence. then, you have this abstract idea of  you as a entity, and then you have different names for it. I, B returns B, and see returns, see and equals is  a special predicate, and if you pass in  the same element of the domain of discourse. Is there a way to define functions  that , if that are defined beyond this input gives this output. Are there ways to define functions that are not just  this input, gives this outputs. For  just think of it that way that you can define functions. You define functions as this input gives this output. in some other cases there'll be some regularity in the data or in the in the patterns, such that you can check with in through some other means. For  the function is some arbitrary function that takes in elements of the domain of discourse and gives you elements of the domain of discourse. ,  what would be an interpretation such that the these sentences don't evaluate to true. But it's important to understand that fundamentally, this is how truth works in 1st order, logic and interpretations work in 1st order. You apply to some scenario with this interpretation, and then you check what's going on in. And then there's functional application. You can rename one of them, say, rename the 1st one to be lambda. The general idea that we're going to pursue is that each of these words. this is the high level idea, which is that you have these words. Is this  to assign things different words . it's it's to have some regularity and structure,  that,  ideally, you want all transitive verbs to look similar. They're looking for 2 things within some predicate. , that's a good way to think about. you're putting 2 things, 2 or more things together, using a production in a context, free grammar. ,  let's take a look at some examples. they will be constants in 1st order, logic. just pass it up common nouns. this means that it takes an entity   an element in your domain of discourse, and it tells you whether the entity is a member of that class. let's derive the representation of the sentence. ,  ,  1st of all. And what we're interested in is we're interested in the logical representation you build at the S level. And this is the logical representation of the sentence. it's , that's why associated population, it always applies another qualified into the inside. that's part of designing this augmented grammar. If you have a different rule that works in a different way, you have to follow that."
    ],
    "Topic 5": [
        "But first, st it'd be good, , to just quickly summarize and recap what we did last class. we need to figure out if we need to book a room for midterm for the makeup midterm and  forth. , this is yes, that's . It is intended in that context. And that's more for figuring out. ,  we did a lot last class. it's about appearing within 5 words of each other or something, and then you can report the counts. I mentioned that , this is  the beginning. This is  the 1st  iteration of this distributional approach to modeling meaning, and which eventually led to large language models. and in particular, with search engines, , you can take this approach where you model every document and every , every documents, by looking all the words in that document, and then you can compare the similarity of documents with the query terms, as you might type into a search engine, and you can use something  cosine similarity, which, remember, is the, it's  the cosine of the angle between 2 vectors. then the numerator and denominator here should be the same. If you see them in a corpus as much as you would expect by chance. then they're not really especially related to each other, or unrelated to each other. And , then, that you should record a score of 0, a Pmi value of 0 for that pair of words. If 2 words happen to occur with each other more commonly than you would expect by chance. that means the numerator here is greater than the denominator. then this score would be this ratio would be greater than one, and your log, the log likelihood ratio would be greater than 0. On the other hand, if 2 words co-occur with each other, less likely than you would expect by chance. Oh, oh, 3, divided by the product of point one and point 0 2 5.  this gives you some Pmi score there, which is slightly greater than 0.  that means that these 2 words co-occur slightly, more often than you would expect by chance. But  it's it's close enough that . you can just use that ratio as the probability. , does the base of the logarithm matter? Does the base of the logarithm matter? which doesn't matter if you're using cosine similarity. And  it's something you can try if you'd  to see if it helps. Occur less likely than you would expect by chance, is not very strong information of the anything of the meanings of those words. And if you use Pmi, and there's  , slightly, there's slightly different versions of this. , another thing you can do is you can do singular value decomposition. , how many people have seen singular value decomposition in some course. Yes, a lot of you, ? It has,  3 or 4 different names, because there's slightly different versions of versions of it that have been discovered in  different contexts. But yes,  you can talk about singular value decomposition. there's a standard method to do that lets you factorize the original term context matrix into the product of 3 matrices. And these are the dimensions of the matrices. And you're just getting rid of those dimensions and projecting your matrix to the remaining dimensions. What you do is you pick a K which is much smaller than it. you're getting in A, you're getting a smaller. And in practice doing this truncated Svd often improves performance, because you're removing some noise and preventing some overfitting of the model, because you can think about overfitting in this context, as  you observe, some counts co-occurrence counts which are just, random, and not predictive of the future, and  truncated. Svd procedure has the lowest amount of error. You look at representations of. 1st you sample a particular position within your training corpus. You sample a position in your training corpus. , does the skipgram have a lot fewer parameters? any questions about my school sematics? It's something to do with your lexicon, which is this idea that you have some abstract  list of  almost  a dictionary in your mind that lists out all of the words and phrases that you should know for knowing a language. But language doesn't just happen as individual, flexible items. Language happens in  spoken language happens in utterances. in particular, the meanings of all the phrases within it. ,  that and or just or, more broadly speaking, the meaning of a phrase depends on the meanings of its parts. Again, this meaning could be true or false. but we understand that by looking at and breaking down  the meanings of  . And  if you if we encounter a new situation where we discover,  previously extinct yellow dinosaurs that have been revived, or whatever I can talk about that, and  that has never happened before, but because  the meanings of each of the words, you can combine them in new ways, and we can talk about new ideas and new events that happen. we should also talk a little bit about some other properties of the meaning representations of a sentence. We  talked about 2 different views of it. that's 2, or  3 different views of meaning. you can still talk about the meanings of the sentences with respect to each other. Here's another potentially new sentence. Pandas are purple and yellow. what is the meaning of pandas are purple and yellow. in our current world, in this particular universe, with this particular earth, with this particular  pandas. If it's true or false. With respect to the current world. I don't think this is  true thankfully. You can retrieve the weather for Montreal  week and then give you an answer. It's about relating linguistic expressions to the world with these sample tasks of how we might want to do that. Likewise, piece of cake is not always literally about the piece of cake it could just mean something is very easy hit. It's just that the function that you use to compose and get the overall meaning is complex and is dependent on the parts themselves. It's  may  that very visa red, or  it's something slightly darker and more purpley ? And red wine is red wine. If you just look at it in isolation. People who are said to have red hair are also, , it's not exactly red, either it might be there's variation, but it might be more orangey, or whatever, or brownish. it's not just that there is  a fixed, static red redness that gets added to the noun that you're composing it with. Montague says, there's no, in my opinion, no important theoretical difference between natural languages and artificial languages of magicians. Query to access it, and all of that involves  manipulating logical forms and logical queries. You you're you still need. how many people have seen? I was expecting most people to have seen it. It's about , say, the students in the class or the topics that we study. Logic also has variables which are by convention denoted with lowercase letters, and they stand for potential elements of the domain of discourse. and you can have different valences which means different numbers of arguments. True, if X is a student that is in the course Y and false. Predicates give you a truth, value  true or false. you've seen  not, and or and implies and bi-directional entailment. Have you have people seen quantifiers? It's a constant and then equals. And this X here is a variable. The way to think about this is usually you're modeling some situation in some situation   10 critters or whatever. And then, when you're doing for all x, you're  checking for all 10 critters and for all 10 critters. And then you can draw a truth table for that  hopefully. if the left hand side is true. then the  hand side has to be true. If the left hand side is false, the whole thing evaluates to true. these all have precise meanings. Or we create this function just because we want to analyze the simplest of the capital of Italian. here is just an example. ,  a particular instance of a 1st order. to see if that if those sentences are true or false. Logic, characterization of the following students who study and do homework will get an a students who only do. before we get started, I'm going to close the door. Oh, , sure, that'd be great. students who study and do homework will get an a.  what do you think? , why did it create a new text box here? , students who study and do homework. ,  they also have to study. students who study and do homework. Do we need the.do we need? technically, yes, but it's not a big deal. This is just the and symbol. the students study and do homework. And the homework goes to just say, , you need the greater touch. on the left hand side here, this is  a logical expression, and it gives you a truth value in the end. it gives you just  that just the constant. A, by itself gives you the abstract idea of a. It doesn't give you a truth value. you need to , you need to  Ref. And it gives you a truth value  great of X equals 8 gives you a truth value. I'm thinking of for every x, then we apply the if condition to the X, and then if that, if condition is satisfied, then we get the grade. I don't understand the question. Oh, , do we  need the brackets at  in front of students and ending at a Oh,  technically you don't. But just for clarity, I'm adding. This is just saying that this is just an assertion that all students who study and do homework get an a. One of them will get a B  you can do it as  2 separate sentences if you want. And  students who don't study and do homework at A B students who study and don't do homework at A B. You can also combine these 2 and have  a more complex expression here. If we had an or  students who study or do homework. Because or is in logic is always inclusive or  it's 1 or the other, or both. Then, , you could use Xor. But , Xr would be correct. once again, to illustrate the difference between just having these logical expressions and truth , we don't know if these evaluate to true or false these sentences. because we haven't applied it to any particular situation or scenario. That's when you need to talk about interpretations. ,  this is, you need a domain of discourse which involves you and friend one and friend 2. And we also need the abstract concepts of  the grades. we also need the abstract concept of the grade of A, the abstract concept of the grade of B and C, ,  this is  going to be your domain of this. whereas,  A, B and C, here are the abstract notions of those grades. you study, and   does friend one. , Grena, this is a function. Because you study and do homework, and you get an a oh, . This should be lowercase and then your friend, the condition doesn't apply, because, your friend doesn't do, friend, one doesn't do homework. the condition this condition doesn't apply. And same with friend 2. And for all the abstract notions of  grades that also doesn't apply, because,  they're not students. A is  true if it's the grade of a rather than  a lowercase a.  the uppercase a is just a name. And and  it's a constant that gives you a name. C are just names of the abstract notions of  those grades. , if your friend somehow gets an A does, despite not doing homework, , then this will be, then the one of those formulas would evaluate to false. The other main piece that we need is we need to have some. ,  we need to build these meaning representations compositionally as because that's   the goal of today and  class. lambda calculus again,  seen this in another course. But there's a variable there X, which represents some piece of lambda calculus. S, and what it does express. That's  what this 1st function says. I've only defined this these terms a little bit loosely and intuitively, but you can go and be more precise about it, and can read up on work in that. It's  that we can look at each of the subparts of the syntax tree. and we can look at the syntax tree and build up the meaning representations bit by bit, until we get to the sentence level. , , for the verb disdained. , , cabinet being disdained by whiskers. using the syntax tree as your guide   a placeholder until we reassign. Found it and resigned, reassigned it. then what we'd  to do is we're assuming that the syntax tree will be useful for helping us derive these meaning representations. where you have,  some left hand side rewrites to N  hand sides becomes some function where you're taking the semantic representations. how are intransitive verbs handled in this framework? And then hopefully, it'll be a bit clearer. We just have a syntax tree. And then the semantics of it all is that you need to associate. every rule that results in a leaf node, , involving a terminal. And we need the logic, because that's what we're aiming for. which is, which is what we're aiming for in this whole exercise."
    ]
}