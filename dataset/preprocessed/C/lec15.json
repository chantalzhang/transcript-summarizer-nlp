{
    "Topic 1": [
        "? And even before that, just some reminders. the midterm is  week. . be . thank you. And that's more for figuring out. I just wanna quickly go through that a little bit more. ? And I should also say . ,  here's the thing I wanted to also talk about. some notion of how much more likely or less likely than chance. It's no longer  accounts. Don't play integration. I'm . . you have. Excuse me. . ? If you see them in a corpus as much as you would expect by chance. And , then, that you should record a score of 0, a Pmi value of 0 for that pair of words. If 2 words happen to occur with each other more commonly than you would expect by chance. On the other hand, if 2 words co-occur with each other, less likely than you would expect by chance. rather than occur recording the raw count in the term context matrix, it records a notion of association between the target word and the context word. , then, you can compute all of these numbers. . Oh, oh, 3, divided by the product of point one and point 0 2 5.  this gives you some Pmi score there, which is slightly greater than 0.  that means that these 2 words co-occur slightly, more often than you would expect by chance. . And . . ? What else? Occur less likely than you would expect by chance, is not very strong information of the anything of the meanings of those words. . . , another thing you can do is you can do singular value decomposition. , how many people have seen singular value decomposition in some course. But yes,  you can talk about singular value decomposition. And these are the dimensions of the matrices. And here, this is  a dimensionality reduction technique. And you're just getting rid of those dimensions and projecting your matrix to the remaining dimensions. , . What is? M, . you're getting in A, you're getting a smaller. Svd. Is a method to help you get rid of some of those. Svd procedure has the lowest amount of error. Here we have. This. This is  a norm. This is an L 2 norm. And you can show that this is the best possible approximation among any matrix of this rank. . this is just to say, this second point here is just to say that it corresponds to something else called principal component analysis. What truncated Svd or Pca does is that it finds all of the vectors. . again, I have. ? 1st you sample a particular position within your training corpus. And this, . You sample a position in your training corpus. These are relatively simple architectures and ideas. It's it's still the same. . But . all this to say, this is  been. . any questions about my school sematics? With Sv, do we need to ? , I think that's the high level intuition. . to. . for, say, for the midterm, for example, it's enough to know that it's about that. That's the high level intuition. ? All . . ? ? . Compositionality is exactly this idea that the meanings of sentences is not just some. What does pop? . . We  talked about 2 different views of it. that's 2, or maybe 3 different views of meaning. ? , for example, to relates the linguistic expression to the world, one thing you can do is that you can assert that a proposition. Here's another potentially new sentence. Pandas are purple and yellow. ? what is the meaning of pandas are purple and yellow. in our current world, in this particular universe, with this particular earth, with this particular  pandas. ? the meaning of the sentence is, the is the thing that lets you perform that evaluation to check. But suppose I say it will snow tomorrow. Then what that's saying is that it's conveying something about the world. or it could be a query, as  , What is the weather  in Montreal  week? You can retrieve the weather for Montreal  week and then give you an answer. we talked about compositionality  just very quickly. There here is also a very high profile violation of compositionality. ,  , for example, you might, you may or may not know that kick. ? ? There's no function that you can apply here to derive the overall meaning from the meanings of each of the parts. The meetings internal to each of the parts. It's just that the function that you use to compose and get the overall meaning is complex and is dependent on the parts themselves. ? I'd  you to . For most people it might be  some very vivid   red. Maybe the RGB value of  2 55 0 or something  that, ? But it's  that. It's not? Purp again, some  purpley red. If you  analyze the RGB value of someone who's blushing. Probably the RGB value. If you just look at it in isolation. . no. And hence the this approach is called Montegovian cement. , these days, with  neural networks and large language models, you may or may not have thoughts about this. because we still have lots of observations that are non linguistic again, , say, weather data, , or  other, in sources of information from natural processes that people gather and put in some database. and the capital of Italy is in Rome. Then you can make an inference, which is, I want to visit Rome. If you say all wugs are blorks and all blorks are cute, then you can conclude that all wugs are cute. how many people have seen? , that's surprisingly few. I was expecting most people to have seen it. If things are confusing. . Otherwise. And the student being in, enrolled in the course. by contrast. ? And also, , how about quantifiers? Have you have people seen quantifiers? People have seen quantifiers? ,  then there's an existential quantifier and the universal quantifier. here's an example. we might want to define some procedure that converts a sentence  the capital of Italy is Rome into some logical formulation. and then Italy is also a function. I'm taking. Here's another constant Rome. There's something called Italy. and then you can have a function to get the capital, from Italy, from the from Italy to some other entity, and the other entity is the entity denoted by Rome. . here Rome and Italy. Just think of them as  names. they're just names. And then there's also this abstract entity which is  the entity of Rome and the abstract entity of Italy. and then the capital of converts that Italy entity into the Rome entity. Or here's another logical sentence that all wugs are blorks. X implies block X. ? any questions  far? Yes, I guess, for the second sentence are we essentially saying for every X, then applying the predicate of , the one thing is also a floor. and if it is. And and the implies arrow here. And then you can draw a truth table for that  hopefully. these all have precise meanings. . it's existing function. here is just an example. . Yes, there, there is an order to which you apply things,  that this whole thing is within the scope of that universal quantifier. But if it's ever unclear, you can add parentheses to make clear the scope. . because it's not intuitive, and the 1st time you see it is confusing. ? That's just  general statements or constraints about  things that may or may not be true. ,  I'm we should do this exercise together. One of them will get A, B and students who do 90 will get a C.  we should list the predicates and functions that are necessary, and make constants for the grades. . . Oh, , sure, that'd be great. Thank you. . What would be? ? . . , why did it create a new text box here? There we go. . then. And to be pretty, to be more clear, we can add a parentheses. . here. , are we  with this? Which? . . is this? ? . I can. I can add another one, too, to make it clear. I just want to make it clear is that the conjunction of these 3, which is in the conditions for the implication. . Those are all credit between the grade up as a function. then I'm wondering, can you just do  rolex students? ? here we just have. it gives you just  that just the constant. It is a function. It's  a constant. You need to have that relates to the X to the student in some way. the way that I'm doing it here is to say that grade of is a function that gives you the grade of the student, the X entity. and that's equivalent to a. . Do we have to bracket the entire thing. Oh, , do we essentially need the brackets at  in front of students and ending at a Oh, I think technically you don't. but it's the same. No  this implication takes precedence over  the quantifier, or something  that. But if you want to be just super clear, you can just add the parentheses. you're asking about the interpretation. we're not there yet. for  there's no interpretation. But since we haven't applied it to a particular group of students and their results. there's no interpretation yet. that's . . wouldn't that contradict the 1st statement that students who do both get an a . no. . you cannot. . ? this would be wrong even if you add a parentheses here. ? Oh, . But , Xr would be correct. ? ? ? ? those are just names. can still be there. That's . , that's . You can have , . ,  we're not done. ? ? for student, for example. but not for 2. this would be an example of a possible situation or world where all of those logical formulas evaluates to true. A is  true if it's the grade of a rather than  a lowercase a.  the uppercase a is just a name. It's a function. You ? then, you have this abstract idea of  you as a entity, and then you have different names for it. , , exactly. Oh, . . You're . . but just for the sake of not confusing us. . It's  you have to. ? we're almost there. this is 1st thing. If this was confusing one thing you can do is you can rename the variables. You can rename one of them, say, rename the 1st one to be lambda. and then. one thing to note is that function, application is left associative. I've only defined this these terms a little bit loosely and intuitively, but you can go and be more precise about it, and can read up on work in that. each of these lexical items. And when you compose distained with catnip. . . , it's  a placeholder until you have . Found it and resigned, reassigned it. . ? then what we'd  to do is we're assuming that the syntax tree will be useful for helping us derive these meaning representations. what that means is, we have to go back to Cfgs. ? ,  if we do this bottom up. and then intransitive verbs. And then hopefully, it'll be a bit clearer. Mumbo jumble. We just have a syntax tree. ? The Npvp Np. far. that's just the syntax. It's you go bottom up. . and then this is what you get. 5. Any questions? ? then, to summarize at a high level, the idea here is every terminal rule. every rule that results in a leaf node, , involving a terminal. . it's , that's why associated population, it always applies another qualified into the inside. that's ."
    ],
    "Topic 2": [
        "welcome back. Also, I assume that you've already started working on reading assignment 2, and then programming assignment 2 is out as . And we discussed Lusk's algorithm and Yourowski's algorithm. It's not about the meaning of 2 words in a sentence. If , this is, these are the ones that are about  our 2 words, do they fit some lexical semantic relation? And  it's still based on these fundamental ideas. and in particular, with search engines, for example, you can take this approach where you model every document and every , every documents, by looking all the words in that document, and then you can compare the similarity of documents with the query terms, as you might type into a search engine, and you can use something  cosine similarity, which, remember, is the, it's  the cosine of the angle between 2 vectors. You estimates the probability of both of those words occurring. and you divide that by estimates of the probabilities of each of the words occurring separately of each other  independently. then the numerator and denominator here should be the same. , that's saying you have 2 words. then they're not really especially related to each other, or unrelated to each other. that means the numerator here is greater than the denominator. let's look at an example. And that's great. we just ensure that somewhat normally. , I think that maybe it's not exactly the  space of events that you're considering. which doesn't matter if you're using cosine similarity. this is called positive point wise mutual information. And  the way they do that is, you can apply this Svd method. And here this Sigma K here is a special, because there it's a diagonal matrix that contains the singular values of this original matrix which you can think of as some  characteristic way of looking at the properties of that matrix. you should look into that if you're interested in this and this idea of Svd. And truncation and principal analysis, you find that all over the place. In the 1st version called the continuous bag of words model. You look at representations of. and then you look at the words, say 2 words before and 2 words after, and you take the rep. The the embeddings associated with those words which are parameters of the model. and then you use that to predict the middle word that's missing. that's how the that model is trained. And this latent representation of what you predict. That's how you get a representation of the word  that's the continuous bag of words model. the more, maybe more popular one is the skip ground model, which is that you take the middle again. You take the middle word, and it's embedding. and you use that to predict one of the one or more of the words in the surrounding context. these are by modern standards. Sometimes they're still used. Or is the just the embedding of the word. you start off with  some  count-based approach. And then people have used linear algebra to try to  reformulate that and do a little bit better. And then there'll also be neural approaches, and, as I also mentioned last class, it turns out that you can prove that the Skipgram model and the Svd based model. the thing with lexical semantics is, it's only about the meaning of words, or maybe small phrases. It's not arbitrary, and you can derive the meanings of a sentence by looking at the meanings of the subparts of the sentence. then that sentence has a meaning. And it's how all of these words work together that you get the meaning of the whole sentence. We still need some procedure to build up the meaning of the entire sentence through composition. And  this is really important. we should also talk a little bit about some other properties of the meaning representations of a sentence. We talked about the relationships of words to each other, ? what about at the sentence level? at the sentence level. one way to do this is to really use some  ideas from logic. it evaluates to false ? But but the point here is that you're able to do that evaluation? of sentence meaning another is to convey information about the world. this is closely related. , but that has that is not really derived. This is the final thing that really set someone over the edge, and they get really mad, or something  that. Likewise, piece of cake is not always literally about the piece of cake it could just mean something is very easy hit. , for example, the type of events is usually preserved  kick. But it's still a composition is a violation of compositionality of root. it's not just that there is  a fixed, static red redness that gets added to the noun that you're composing it with. and you can model the former with the latter. ,  if you say something , I want to visit the capital of Italy. because chances are, it'll be confusing for most of the rest of the class, too. this is the side that is more  a semantic. or the classrooms and courses that might be one Mini scenario we want to model. there's a domain of discourse of entities. Logic also has variables which are by convention denoted with lowercase letters, and they stand for potential elements of the domain of discourse. It has predicates which map elements of the domain of discourse to truth values. , for example, I can define the predicate in course XY, which takes in 2 elements of the domain and returns. , it gives you some information about the properties of that out element. you also have functions. functions, map elements to other elements of the discourse. And, for example, maybe you have an instructor of function that takes X and returns and other elements which corresponds to the instructor of the course. Predicates give you a truth, value  true or false. In 1st order, logic functions give you,  a elements  other elements of the domain of discourse and a valence, 0 function would be a constant. , for example, I can say, , I can create a function called Montreal, which takes no arguments, and it just returns an element of your domain of discourse which corresponds to the city of Montreal. the  , these are the basic elements of  1st order predicate calculus. here I have capital of which is a function. It's a is a predicate. they take an element. This X ranges over all of the elements of discourse. You're asking, is this a Wug? Truth values, the one thing before and the thing after. if the left hand side is true. then the  hand side has to be true. If the left hand side is false, the whole thing evaluates to true. And there was another question, . Or we create this function just because we want to analyze the simplest of the capital of Italian. , that's a great question. the question is , where does this function of capital out come from? It's just, I'm just defining that this could take the form of a function to represents capital of. There are conventions and procedures,  that every word in English you can associate it with some piece of logic which might involve defining functions or might involve defining predicates and  forth. this part is really confusing. ,  , and your interpretation involves specifying the domain of this course. and it also means specifying the functions of predicates. how do the functions  map entities to other entities? And how do the predicates  map entities to true or false? where you and 2 of your friends are the elements in the domain of discourse, such that the above 1st order, logic formulas are true. ,  let's do this together. let's let's do this. , that's not helpful. Wait, wedge, . They're great as 8. . In a way, wedge? this, the wedge is just  Powerpoint. , that's a great question. the reason is because the types don't work out. It takes 2 truth values and returns another truth value. on the left hand side here, this is  a logical expression, and it gives you a truth value in the end. But just a is a constant,  it gives you an element in your domain of discourse. It doesn't give you a truth value. And it gives you a truth value  great of X equals 8 gives you a truth value. I'm thinking of for every x, then we apply the if condition to the X, and then if that, if condition is satisfied, then we get the grade. But just for clarity, I'm adding. This is just saying that this is just an assertion that all students who study and do homework get an a. ,  the second sentence to translate is students who only do. One of them will get a B  you can do it as  2 separate sentences if you want. You can also combine these 2 and have  a more complex expression here. , that would be really interesting. you have to be careful how you write the or  the or has to be over those, both the conjuncts, the con, the conjunct of both. ,  let me show you the wrong thing to do. And the students who do neither get a seat any questions about this. ,  then,  ,  then come up with an interpretation where you and your 2 of your friends are elements in the domain of discourse, such that these 1st order logic, formulas evaluate to true. ,  this is, you need a domain of discourse which involves you and friend one and friend 2. This is  the minimal domain of discourse that you can have to get this to work and satisfy all of those requirements. I'm gonna put that you and friend one and friend 2 will be the ones where, if you pass it to students, you evaluate student of that re values to true and otherwise it values to false. you study, and maybe  does friend one. maybe you define it  that it turns out that you got an A and then, friend, one gets a B, and then, friend, 2 gets a seat is something  this. This should be lowercase and then your friend, the condition doesn't apply, because, your friend doesn't do, friend, one doesn't do homework. the condition this condition doesn't apply. And same with friend 2. and you can do the same thing to check for  this sentence and this sentence, and this sentence, this sentence. here, capital. A capital B, capital. a returns. I, B returns B, and see returns, see and equals is  a special predicate, and if you pass in  the same element of the domain of discourse. It returns true. , that was my question. For  the function is some arbitrary function that takes in elements of the domain of discourse and gives you elements of the domain of discourse. , if your friend somehow gets an A does, despite not doing homework, for example, then this will be, then the one of those formulas would evaluate to false. You almost never go to this level of thinking about particular scenarios and defining the particular scenarios. we need to use lambda calculus. lambda calculus again, you might have seen this in another course. allows you to describe computation, using mathematical functions and the computations that we're going to be doing is we're going to be building up a 1st  logic sentence as the as the meaning representation of the sentence by using fragments of logic in the subparts of the sentence. This is a different notion of variable compared to the notion of variable in 1st Square logic. But there's a variable there X, which represents some piece of lambda calculus. Then there's lambda X of T, where t is a lambda term. And then there's functional application. function application also known as Beta reduction takes this form. you have some lambda, X some lambda expression lambda, X of T, and then you apply it to some expression. , what it does is it replaces all instances of x within t with the expression s. , for example, lambda x of x plus y apply to 2 simplifies to 2 plus y, because you're replacing everything that's X here with X with the with the 2, you can even take lambda expressions and plug them in as  as  arguments within the body. if you have lambda, XX, lambda XX, you can simplify that. And then you can further reduce this by taking one identity function and passing it to the other identity function. rather than lambda XX. And then Lambda Xx. And then you do beta reduction with D,  we only really, we're not really using,  the a full power of lambda calculus, that much we really only needed to store partial computations. Why do we need lambda calculus? It's  that we can look at each of the subparts of the syntax tree. and we can look at the syntax tree and build up the meaning representations bit by bit, until we get to the sentence level. The general idea that we're going to pursue is that each of these words. this is the high level idea, which is that you have these words. And then you use lambda calculus, and you do composition up the syntax tree to get everything to go into the  place to get the logical representation of the whole sentence any questions about the high level idea? And this lambda stuff is  that we say, , what object is applying to something else. it's it's to have some regularity and structure,  that,  ideally, you want all transitive verbs to look similar. And then and the lambda terms, let you express that  it's missing 2 things, and then you're going to find it somewhere else in the sentence. And then the procedure of  finding it is by doing composition. using the syntax tree as your guide   a placeholder until we reassign. That element  thing. what we're doing is we're doing syntax driven semantic composition. then, syntactic composition goes hand in hand with this,  semantic composition. and you can write it down formally, as  your semantic attachment. where you have,  some left hand side rewrites to N  hand sides becomes some function where you're taking the semantic representations. ,  let's take a look at some examples. And then the associated rule, for,  Np. just pass it up common nouns. our predicates inside a lambda expression of type E to T. What does this mean? this means that it takes an entity   an element in your domain of discourse, and it tells you whether the entity is a member of that class. For example, the rule of N rewrites to the word student is associated with this semantic attachment of lambda X. Student X.  it's a predicate applied to  where X here is  a variable in the lambda expression. then its semantic attachment looks  this. And then finally, , for  the composition rule of S. Rewrites to Npvp. Is associated with this semantic attachment of Np, dot sem apply to Vpsem. But let's look at an example. let's derive the representation of the sentence. And this is what it would look . Rewrites to V, and then V rewrites to the word rules. for each of these rules you look up. The semantic attachment associated with those rules which you can find from the previous slides will look  this. and then for each rule you then apply this  you look at the semantic attachment associated with those rules. and then  for these unary rules, you just pass up the semantics. at that S level. that means np, dot, some would be lambda XX, comp. After that one step you're you end up with. Lambda. 5, 50, and you plug it in where X is within the smaller piece, and then you get this expression of there exists a. E. Events which is a rules event, and the ruler of E is whatever is denoted by the constant comp. You have to look at the semantic attachment rule."
    ],
    "Topic 3": [
        "today, we're going to talk about compositional semantics. we need to figure out if we need to book a room for midterm for the makeup midterm and  forth. please do that as soon as you can. Not quite. ,  it's for a word that might have multiple senses and figuring out which one it is. And then we after that we talked about hearse patterns and also bootstrapping with these patterns. one important concept from the second half of last class is this idea of a term context matrix where you can build up a representation of the words in a corpus by looking at their co-occurrences. this is the idea of distribution of semantics. You're using the distribution of words in order to infer something about the meanings of those words. And  here, in the term context, matrix, you have the target words which are the words whose meanings you're trying to model. And you have context words which are the words that appear in the context of those target words in the Corpus. Or is there something better you do because it turns out that there is something better. But also, even  I would say, a lot of search engines are still based on word of co-occurrences. Those 2 words co-occurring compares to observing them separately and individually. it's no longer a natural number. for example, here  , here's the formula for point, wise and mutual information. this looks a lot  some formulas you've probably seen for looking at independence  of random variables. the random variables. then this score would be this ratio would be greater than one, and your log, the log likelihood ratio would be greater than 0. Suppose the word be occurs a hundred 1,000 times in a corpus of a million tokens. and then it co-occurs with the word linguistics 300 times. and the word linguistics occurs 2,500 times in total. ,  you can compute V and linguistics. because it co-occurs with linguistics 300 times you get this joint, probably. that's 300 divided by? , by a million. it's 300 divided by a million for the though it's 0 point 1, because the is a very common word. it's a hundred 1,000 divided by a million. and for linguistics is 2,500, divided by a million. why are we dividing 300 by a million. it's  300 divided by a million. Because you, it's you're looking at the entire corpus, and there are a million tokens. But I think it's it's close enough that . , does the base of the logarithm matter? Does the base of the logarithm matter? not really because if you choose a different base, that's  just multiplying all the numbers in your entire term context matrix by a constant. But I assume that you have, or will see this in another course. it's a really famous technique. in truncated Svd, the idea is that you want to creates a version of your term context matrix. there's a standard method to do that lets you factorize the original term context matrix into the product of 3 matrices. you have your original term context matrix, which has dimensions of the size of your vocabulary by the number of context words. I'm indicating the size of your vocabulary. number of target words here as the size of B, and I'm indicating the context word size, the number of context words as C, and then you can factorize it into the product of these 3 matrices. And it turns out that you can throw out some of the values of the in the singular , in this, in, in this, in this matrix, and also throughout the corresponding rows and columns in the other matrices to come up with an approximation of the original term context matrix. You're  taking the dimensions of the original term context matrix which explain the least amount of variance. the idea here is that  suppose M is the original rank of the term context matrix. , you're getting a matrix with lower rank than the original. And in practice doing this truncated Svd often improves performance, because you're removing some noise and preventing some overfitting of the model, because you can think about overfitting in this context, as  you observe, some counts co-occurrence counts which are just, random, and not predictive of the future, and  truncated. You can also prove that this view of truncated Svd is optimal in a technical sense, in this very particular sense, which  suppose X was your original term context, matrix and xk, is your approximate term context, matrix of rank. you're preserving as much as the most information you can. you're looking at, how much is the difference between your original matrix and the approximate matrix. suppose you have just 2 context words. your original term context, matrix has 2 dimensions. and then, when you are truncating, when you're removing some of the singular values and removing some of the dimensions, you're  squashing everything into just a lower dimensional space. A space is this is the this is the squashing. Is a famous model that does this from 2013. I would have to think about that. I don't think , because, , it's  because the parameters are shared and the sibo. it's not  you have a parameter. It's  shared parameters. suppose your representations are size 100 or whatever. Then you have 100 parameters for each word in your vocabulary in both cases, I think. , only keeping the information that does affect variance. distribution of semantics and lexical semantics done. And it's about compositional semantics. It's something to do with your lexicon, which is this idea that you have some abstract  list of  almost  a dictionary in your mind that lists out all of the words and phrases that you should know for knowing a language. But language doesn't just happen as individual, flexible items. in compositional semantics, what we  to do is we  to talk about, say the sentence level where sentences have meanings that you can derive by looking at the parts of the sentences. ,  we have to talk, then, about the principle of compositionality. in particular, the meanings of all the phrases within it. but we understand that by looking at and breaking down  the meanings of  . And  forth? whereas lexical semantics might give you the meanings and behaviors of each of the individual words. This idea of compositionality is really important, because you could argue that this is what lets language be really, really flexible and useful, and you can use language to talk about new situations and  forth. And  if you if we encounter a new situation where we discover,  previously extinct yellow dinosaurs that have been revived, or whatever I can talk about that, and maybe that has never happened before, but because  the meanings of each of the words, you can combine them in new ways, and we can talk about new ideas and new events that happen. If you think about it. The relationship of words to each other  with all these  somatic relations,  synony and autonomy, and some autonomy, and  forth. And we also talked about relating the meanings of each word to the things in the world,   to the references, and also  to the sense of the word. you can still talk about the meanings of the sentences with respect to each other. But what about the aspect of the meanings of sentences with respect to the world? how do we think about that. this is a term from logic is either true or false relative to the world. With respect to the current world. that's 1 potential view of . I don't think this is  true thankfully. and how the world is or will be . that's another way to connect the language to the world. Then it's a query. It's about relating linguistic expressions to the world with these sample tasks of how we might want to do that. in particular idioms or expressions whose meanings cannot be predicted from their parts. You can't derive that from kick the bucket. The sack means to sleep, and  forth. these are clear violations of compositionality, because there's no regular  function. And here's another more subtle violation of compositionality or arguable violation of compositionality. which is that things are often relative to each other and not strictly about. I'm guessing. but you can still talk about somebody's cheeks being red, ? And and when you put words together, their meanings  affect each other as . at least in at least the meetings. You you're you still need. If you want to have a natural language interface to all of that information. Inference is to make something explicit that was implicit before in language. ,   the logic we're going to pick is 1st order, logic or 1st order predicate calculus. 1st order logic before. this means you should ask me questions. in 1st order, logic. 1st order, logic has these components, and  we need to define them,  that we can talk about translating natural language to 1st order logic. It's about , say, the students in the class or the topics that we study. then 1st order. and you can have different valences which means different numbers of arguments. , should we think of a pedicut as   an identity function in a way that as predicate and identity function, what do you mean by identity function  if it takes  or an example in the case, , in course, Xy  it just checks the identity of X and returns  yes or no  predicates. if it's a predicate that has a single argument, often it does give you  the category, it checks the category. If you have something that takes in multiple arguments  this, then it's usually about some relation. here, in course, is about a relationship between the student and the course. And  the difference between predicates and functions is what they return. And  forth. ,  you have seen 1st order. It's a constant and then equals. X, and it checks. If it's a log, then it should be a port. And this X here is a variable. The way to think about this is usually you're modeling some situation in some situation you might have  10 critters or whatever. then it should also be the case, that if you ask, is it also a blurb? it takes in 2 arguments, 2, 2. Is there any order of the old? Is there an order to it. formally speaking. But  the reason, remember that the reason we're using a logic ultimately is to relate language to the world. And  that means that to interpret a 1st order logic. you have to interpret it and evaluate it against some world in the end. ,  a particular instance of a 1st order. Logic consists of the predicates and function names and arity. In 1st order logic using those predicates and functions. Whereas an interpretation is where you  interpret it in a particular scenario. the number of arguments. Because I think it's this is, this can be difficult. ,  we're going to come up with a 1st order. Logic, characterization of the following students who study and do homework will get an a students who only do. and then we'll come up with an interpretation of this 1st order. Logic. before we get started, I'm going to close the door. Thanks. students who study and do homework will get an a.  what do you think? we need some quantifier. what  quantifier do you think we need? , students who study and do homework. ,  they also have to study. students who study and do homework. Do we need the.do we need? This is just the and symbol. the students study and do homework. And the homework goes to just say, , you need the greater touch. We have  3 predicates, and we have grade of X is equal to a why can't we just have 8 here. you need to , you need to  Ref. I don't understand the question. and the reason you don't is because I think, the quantifier takes precedence. And it wasn't worried about it was  82. . And  students who don't study and do homework at A B students who study and don't do homework at A B. That's fine, too. If we had an or  students who study or do homework. Because or is in logic is always inclusive or  it's 1 or the other, or both. it's not defined by the set of logical symbols that I put earlier,  logical connectives that had earlier. because we haven't applied it to any particular situation or scenario. That's when you need to talk about interpretations. how do we interpret this logic within a particular context? we also need the abstract concept of the grade of A, the abstract concept of the grade of B and C, ,  this is probably going to be your domain of this. Is this the domain of all variable factors. we have to , specify the meanings of everything. you do your homework. , Grena, this is a function. And you can check ? You can check,  for all X student and study and do. Homework means that the grade of X is a you can check. Because you study and do homework, and you get an a oh, . Otherwise it turns false. in order to not confuse things. in practice. But it's important to understand that fundamentally, this is how truth works in 1st order, logic and interpretations work in 1st order. Logic. You apply to some scenario with this interpretation, and then you check what's going on in. In practice. You can talk about. The other main piece that we need is we need to have some. in order to do that, we need to use another tool from computer science and whatever. and lambda calculus can also be formally defined, and it can be defined recursively. there is some variable. Here there are 2 copies of X.  the argument here is this identity function. this means that you have 2 copies of the identity function. And you end up with  one identity function. YY, . something that takes in a Y and returns 2 copies of it. That's  what this 1st function says. and what you're returning 2 copies of here is the is the identity function. , , suppose you want to measure you want to produce the representation of whiskers, disdain, catnip. , for example, for the verb disdained. And the predicate takes in 2 arguments, Y and x. because you need something that is disdained. And you need, , you need to have something that's disdaining something else. then you can make sure that catnip goes into the  place  that it's the thing being disdained. Is this  to assign things different words . They're looking for 2 things within some predicate. you can check online. and we have to augment them with lambda expressions with the idea that every time you're doing a syntactic composition. you're putting 2 things, 2 or more things together, using a production in a context, free grammar. According to this theory, this approach. and for the small pieces it's  for the lexical rules is pretty easy. 1st of all. they will be constants in 1st order, logic. What you do is you create an event variable? E, and you assert that there exists a certain event associated with this verb with arguments. suppose you have the verb rules. There is a it looks  there's an event, E such that e is an event of tight rules. Com 5, 50 rules. ,  ,  1st of all. 550 vp. And then the semantics of it all is that you need to associate. and then vp.com. You get from the Vp semantics. X exists. E. Rules E. Ruler EX. And we need the logic, because that's what we're aiming for. which is, which is what we're aiming for in this whole exercise. ,  the questions are  the grammar does not have to be in Cnf. And then, in terms of the order in which you apply things. If you have a different rule that works in a different way, you have to follow that."
    ],
    "Topic 4": [
        "Jackie Cheung, Professor: Hello. But first, st it'd be good, I think, to just quickly summarize and recap what we did last class. then, there's no class during that time. But I'll I'll have extra office hours in my office during class time. ,  here are the algorithms from last class. do you remember what tasks those algorithms were for? , this is yes, that's . It's word sense, disambiguation. ,  we did a lot last class. But there were a few topics that we didn't cover in enough depth. Maybe it's about appearing within 5 words of each other or something, and then you can report the counts. one thing that I didn't really spend time on which we should talk about is , do you just record the counts? Why do we care about this? I mentioned that , this is  the beginning. This is  the 1st  iteration of this distributional approach to modeling meaning, and which eventually led to large language models. this has been a very important, influential technique, which is called point wise mutual information waiting. And the idea is that rather than use raw counts. that you observe. That's the definition of 2 random variables that are independent of each other. this is the idea. And  you can plug these numbers in. we have the point. Yes. it's   , this is the approximation there. Yes. And  it's something you can try if you'd  to see if it helps. The reason for that is that the fact that 2 words occur Co. And if you use Pmi, and there's  , slightly, there's slightly different versions of this. here in this nlp, course, we have limited time,  I won't really talk much about all the math behind it. Yes, a lot of you, ? It has,  3 or 4 different names, because there's slightly different versions of versions of it that have been discovered in  different contexts. You can also talk about principal component analysis. And that's how you can get an approximation. It finds,  some axes that best explains the variance of the points with respect to each other,  the points here are the closest, as close as you can get to the axis, to the axes as you have. This is the approximation that preserves the most information. I don't have time to do this topic justice. But there's a lot of math and linear algebra and algorithms and people have come up ways to do this very quickly. For in many applications  this block. And I mentioned last class that word 2 vec. I thought I would just briefly show the neural network figures for word 2 vec. the original model  proposed 2 versions of word 2 vec. That's  the representation of the word. But but , sometimes they still use word 2 vec,  word 2 vec, although , maybe they're not popular anymore for English. If you have,  a highly specialized corpus where you don't have a lot of data to train a transformer model. Or if you're working with a language where you don't have enough data to train  Lstm or transformer model. Sometimes people still use word embeddings of this time. , does the skipgram have a lot fewer parameters? You have a set of parameters for a word, 2 positions before, or something  that. Yes. we don't have time to , get into any more depth. The  topic is the, I guess, the final major topic for the midterm. Language happens in  spoken language happens in utterances. Written language happens in sentences and in even bigger chunks, which we'll see if we have time to talk about later. 550 is a fantastically awesome class. Again, this meaning could be true or false. What does fantastically mean? What does class mean? What does a mean? ,  people make a big deal of compositionality for good reason. and within this module what counts as a good meaning representation. to figure that out, you have to take the sense of that, whatever that means and evaluate it against a world. I guess, unless you , die a poor panda or something. If it's true or false. Maybe it's implemented as you. language is compositional, but it's also not perfectly compositional. or if you see something, it's the last straw 99% of the time you're not  talking about the last straw. and then it's kinda over  , which is, I guess, the same as  to die. What does Red mean to you. It's probably may maybe that very visa red, or maybe it's something slightly darker and more purpley ? but is that it's not magenta violet? I don't know. and it's the idea of using a logical formalism to represent the meaning of a sentence with a tight connection to syntax. Montague says, there's no, in my opinion, no important theoretical difference between natural languages and artificial languages of magicians. this is the idea. and you have to access it through some database. Query to access it, and all of that involves  manipulating logical forms and logical queries. and  that means at some point. maybe unlike in your previous logic classes. , it's , in terms of natural language sentences. first, st order logic can be defined as having a domain of discourse, which is a set of entities that we care about. True, if X is a student that is in the course Y and false. if you have,  a student X, it's usually in natural language, it's  checking whether X is a student or not. Yes. , good. or maybe most of you. I guess I yes, . It should be true. But if it's not a bug, then you don't care then. and then it can be a block or not. Remember that . In order for the whole thing to evaluate to true. or Boolean connectives, if you prefer to remember what they mean. the function of cat. Yes. here it's yes. as  as a set of sentences. and then an interpretation of it, or a model of it, is to apply it to an actual scenario to check the truth and false values of all of those logical sentences, or anything else that you might care about. ,  think about it this way,  the just, the logic and the sentences themselves, although that's what you spend most of your time  working out. to see if that if those sentences are true or false. yes. Evaluate to true. Yes, that's . An idea. for all x. if X is a student. Oh, you mean this dot? I think technically, yes, but it's not a big deal. Yes. Do you think of wedge as  union? No wedge is closer to intersection. Speak. Yes. Yes. And , yes. And then this is  a predicate in disguise. it's  a comparison. Yes. Yet we can't tell if it's true or false yet. ,  I'm gonna delete it. Yes, Xor. Then, , you could use Xor. yes. Oops. once again, to illustrate the difference between just having these logical expressions and truth , we don't know if these evaluate to true or false these sentences. to  figure it out. you can imagine that. you get  Unicorn and , I don't know goose, and I don't know duckling instead of A, B and C, in which case,  you can change the names of the constants to  Unicorn and goose, and duckling, or whatever. but the abstract notion of  having grades that are better or worse. Yes, is the domain. This is the domain of all variables. ,  technically, you can have student a student. B student. C, yes, you can have student A, you can have student, and you can pass in A and B and C, it's just that they'll evaluate too false. And maybe And you're a very good student. And again, you're a very good student. And but maybe your friends don't. And , and then creative. you it applies for you ? yes,  in this case the uppercase. It's  , maybe you have a name, and maybe you have a nickname. But both of those points to the same. hey? ,  what would be an interpretation such that the these sentences don't evaluate to true. , we change the grade of any student. , we change any of the grades. ,  does this idea make sense. You don't have to do that because you can. , that would still be useful without having to go through this layer. ,  we need to build these meaning representations compositionally as because that's   the goal of today and  class. and then there's Ts, where T. And S are both lambda terms. there's a lot. if you have,  4 lambda terms A, BC, and D, you 1st simplify a B, and then you take the result, and then you  do Beta reduction with Z, and then you take that result. ,  why do we care about partial computations? maybe it's a predicate. then what happens is that you take that expression which you think of as a lambda term. and then you take whatever representation of catnip that you have as the argument, and then you can do beta reduction. And then later, when you see whiskers, then whiskers becomes the argument, and then it gets plugged in into the Y, and you get  something that's but here it's  some  semantic representation of Whisker's disdained cabinet. Yes. , for example, cabinet being disdained by whiskers. , that's a good way to think about. Remember those. this is your Cfg rule. ,  proper nouns. here's gonna be what we store. how are intransitive verbs handled in this framework? ,  that was a lot. Ignore all of the logic. this you can remember from before , as rewrites. And what we're interested in is we're interested in the logical representation you build at the S level. the rule was, Np, dot sem applied to Vp, dot, sem. Np. And here is where you do, beta reduction to simplify this expression. And then we needed lambda calculus to store these partial computations. here it says, Np, dot, some is the Punctor and Vp, dot, M is the arguments."
    ],
    "Topic 5": [
        "If you have a midterm conflict, please send me email,  by the end of today. , the midterm is, I think,  Wednesday, if I remember . last class, remember, we were still talking about lexical semantics. It is intended in that context. And you can define context in any way you'd . it's important to learn that this for that reason, too. You instead record your term context, matrix. , it's just some   a score or some  a log ratio of ratio of likelihoods. if it turns out that w. 1 and W. 2 are totally independent of each other. which that means that this ratio is one which means that the log of it is 0.  that's   the chance probability. then you have a negative value. , that's , because it's the probability of both of those things happening. It's because here it's  the probability of both events happening. you can just use that ratio as the probability. Another thing that people often do is that they often discard negative values in the Pmi. , if you get a negative Pmi, often they discard it, and they just record a 0 instead. , , people often just discard them. this is one tweak to just doing the raw counts. But you can use Pmi, and you can get pretty good levels of performance if you implement some  ir system  some  basic ir system. And as it's applied to information, retrieval sometimes called latent, semantic indexing, or something or latent semantic analysis. which has lower rank in the linear algebra sense. What you do is you pick a K which is much smaller than it. K, you can prove that the matrix you get by applying this as truncated. Maybe it'll be helpful to look at this graphical intuition of what truncating, truncated Svd does, or principal component analysis. And then you plot all of your word vectors in this two-dimensional plane. graphically, this is what's happening when you're doing this approximation and this one dimensional approximation of the original 2 dimensional make. And finally, the other thing, of course, is that people also train neural models of word embeddings for lexical semantic for lexical semantics. Then you sum them up. Longer does the script skip ground have a lot fewer parameters? you just have one set of. This has been how distributional semantics has evolved. You can prove that they're equivalent to each other in some sense. Is it good enough just to know,  the high level idea that we're essentially suppressing it, and only keeping the relevant information or the ones that don't. And it does this by applying this matrix factorization algorithm. remember, what does Lexical refer to? Lexical refers to that? ,  that and or just or, more broadly speaking, the meaning of a phrase depends on the meanings of its parts. if I say to you, you may disagree, but comp. 5, 50 mean? In the last lecture or 2, when we were talking about lexical semantics. And just look at how they do things and then apply it to natural language. You turn this into some logical form, and then you can evaluate it against  a database that's connected to some meteorological service, and then you can return. it's this  a sense of meaning that we're going to be talking about. The bucket means to die. You're talking about. Interestingly, though it's not  arbitrary , there are still some commonalities in the meanings of  each of these parts, and the overall meaning. The bucket is  a thing that happens. it's and it would be very unusual if there was an idiom where the event type is something  I don't know relaxing at home, and then it means to die, because then one of them is  a it's  a state of being , whereas  die is  an event that just it's  a point in time. usually there are still aspects of meaning that are shared between the idioms and the overall meaning. ,   this is the idea of co-compositionality, which is that this is still  compositionality in the sense that the meaning of the whole is derived from the meaning of the parts. Imagine in your mind what the meaning of red is  on its own , , just think about Red! Might be your prototypical red. But  think about red in the context of all of these words,  a red rose. What  red is that? And red wine is red wine. Really the same  red wine is very dark, ? That color I don't know what to call it. or  red cheeks,  red cheeks. , it depends on their skin tone, but it's almost certainly not red. Or red hair. People who are said to have red hair are also, , it's not exactly red, either it might be there's variation, but it might be more orangey, or whatever, or brownish. then the idea here is that the meaning of red  is influenced by what you're composing it with. Instead, you have to. There, it's contextual. the tradition that we're going to discuss today and  class is to use logic to model sentence meaning. And this was a tradition that started in 1970 with Montague. Indeed, I consider it possible to comprehend the syntax and semantics of both kinds of languages, with a single natural and mathematically precise theory. , after reading this quote. but this is the assumption and the approach behind Montegovian semantics a dash we can use that natural language can be made as precise as  logic, logical languages of our logic. there is an advantage of doing things this way. which is that you can then talk about natural language inference as applying logical rules of inference. if you pick a particular logic as we will, then, whatever inference procedures are defined by that logic, you can also apply them to natural language after you've converted natural language to that logical form. And I would also say that,  the other reason to pay attention to this stuff is that we can never get away from something symbolic and logical, at least not completely. Maybe you have to write some SQL. You're still going to have to translate between that and natural language. ,  then, what is inference? and  these are cases of semantic inference. this is  natural language inference. But we're gonna still turn this problem into perhaps a problem of logical inference by defining a procedure to convert the natural language sentences to some logical form. in that sense. The the idea of city, the city of Montreal. And then you have logical connectives. please tell me you've seen these . you've seen  not, and or and implies and bi-directional entailment. it might be easier and more concrete if we look at more examples. this sentence and this formulation is asserting that there's a something called Rome. We can convert it to this form, for all X log. And  here, wug and blog are these are predicates. you're for all X means that you're . And then, when you're doing for all x, you're  checking for all 10 critters and for all 10 critters. And that's what Wugex is doing. It's a it's a logical connective, ? And you might have to review logical connectives. but in general there will be. And then we'll also define a procedure to combine those pieces of logic together in order to form the overall meaning of the sentence. here arity means valence. because we're talking about students. are we talking about? , because we're talking about some general property that should apply. think there's a way to have equations. ,  for all x for all students, how we do that is we write. And they'll get an A, how about we do sometimes it just. , wedges and end. ,   this is a logical, this is a Boolean connective ? A, by itself gives you the abstract idea of a. this would be wrong. But if you define it with a truth table. You just have to define what it means. And we also need the abstract concepts of  the grades. But once again, , it's important to , distinguish between  these 3 and the names of A, B and C here. these are constants. whereas,  A, B and C, here are the abstract notions of those grades. ,  year Mcgill decides to change the names of all the grades. This is wrong. And for all the abstract notions of  grades that also doesn't apply, because,  they're not students. And and  it's a constant that gives you a name. C are just names of the abstract notions of  those grades. Is there a way to define functions  that , if that are defined beyond this input gives this output. Are there ways to define functions that are not just  this input, gives this outputs. For  just think of it that way that you can define functions. You define functions as this input gives this output. Maybe in some other cases there'll be some regularity in the data or in the in the patterns, such that you can check with in through some other means. You can look at logical rules of inference and derive conclusions by using logical rules of inference. , we need to have some algorithm for constructing these logical formulas at the sentence level from the its parts. and that will help us to define a precise algorithm to do . But we need this because we need to be able to store partial computations as we're building up the meeting representation. S, and what it does express. When we have the entire logical expression. we'll have some fragment of the meaning representation with things that need to be filled in. And then it's still missing the semantic agents is the thing doing the disdaining. They have pieces of logical representations with things to be filled in, and then you, you express those things to be done using lambda expressions. they all have this form where they need. I'm going to post the answer to these exercises. You're also correspondingly doing some function application in lambda calculus, in order to simplify in order to in order to take these pieces of meaning representations, and  merge them together to make a larger, a meaning representation of a larger bit. Here, I'm using dot sem to refer to each of those the partial the pieces of logic representation there, and you're combining them in some way in order to . gets the overall meaning representation of the entire constituent, the larger phrase. Comp, 5, 50 is a constant and , for reasons we're gonna type, raise them  that they're gonna be lambda expressions apply to constants. Comp, 5, 50 will be lambda XX applied to the constant Comp. 5, 50. Rewrites to proper noun would just be the take the semantic representation of the proper noun, and that will  be the semantic representation of the Np. and there's a ruler which is the X to be filled in. Rewrites to a proper noun, and then proper noun rewrites to the word comp. 5, 50, because you get that from this, from its from the subject. ,  you take this entire. the way you do this is, you take this entire expression, you plug it into where X is. And then you take Comp. 50. And this is the logical representation of the sentence. you take those semantic attachments, and then afterwards is a mechanical process of looking at your augmented Cfg with those semantic attachments and then running those procedures. there's some logical representation. that's part of designing this augmented grammar. , I think I'll stop there and we'll continue  class."
    ]
}