{
    "Topic 1": [
        "we have some language models that they try to scale, to  500 languages, but in terms of performance we I don't think we have a good luck with that. and they have a little bit more on label data and a few label data. And even with attention, is still trying to do the same job. you can generate a new, a new text. And there's a way you to also tie them together. Because the vocabulary space is very large, even if you use a word. But it  , you could. Only modules have,  internal representation, that we don't have access. It could be a birth model. This is an example of a birth model where you want to predict Max token. and then you cannot fine tune this on a label example ? Tasks  sentiment, classification, or question answer. it's more on multilingual nlp and crosslingual transfer. Because  we are, we're in the multilingual space. and this is what we do. you do mass language model on, on a larger number of text, and then you cannot find some little sensory classifications which is  different tasks. the transfer is really difficult. But  there are 2 ways you can  add there are 2 ways to fine tune. if you have labeled data, you could just  fine tune this multilingual bits. And  we move to multilingual bits. if you want to do multilingual transfer posting transfer, you need an encoder that is multilingual. And then, if you have trading data, you can find some directly in Americ. This is very, very, very common. this is more for classification tasks. you could just do the research transfer if the multilingual model is really good. Another issue is that you have languages that  may use different scripts. previously for the birth model. , if you have 500 Md of , it was a size  500 MB. And then you have this bio tags that you're  familiar with. And  we have this  collaborative projects where we have native speakers and Mls. And then we have 2 versions of the Masaka Project. What is the root word. But if you can think of what would. Is the cost always as ? One word, and they're a lot more complex and translates to a phrase, , but our models are smart enough again for that. Of personal name is capitalized  at the beginning, is capitalized in Zulu is not capitalized the beginning as something. the even the most the tokenizer, we treat them differently. but there are results also showing this also. , in the paper, where we show that the performance drops very good. The 1st word is more  a business news the second one is talking about an entertainment award which is  an entertainment news, and the 3rd one is  on health news, and the last one is sport news. and that's why we have Masaka and then Masaka 2.0. And then we have some local website where we called news articles, and we label them. , health news is we? you can just give your cry everything from health news, , likely to get mostly else related news. Or we have business with politic news culture sports and all that. and also we are  religion because it's very prevalent in the African newspapers. I had this very, very small idea. And we just have issues of not covering many low resource numbers. and then you can have a sentence where some people will label this as geography, and another person will label it as travel, because someone is going for hiking, or something  this. we have sip easy and hard where we have the 1st sip had,  7, , 7 categories of labels. Congo,  Indo-europia, was the largest, followed by, , Atlantic, Congo. and Austronesia, and all that. They have not been seen directly training. and then  the effect of transfer learning in naturally  performing on language that are not seen during pre-training. And then you see that the region of Europe depend regardless of their classification. Languages also have this  effect. And  then we talk about factors affecting performance. We talk about the language coverage. If a language is not covered during pre-training, they often have lower performance, and then we have categorization,  is the language already seen? And the word embedding is learning. What may happen is that it will take more tokens to encode the information there. the issue is that initially. we need some analysis to see. But it has not been widely adopted. last part 2  I'm I'm very certain we'll not finish this. you often have worse results for these languages. this is  due to what is called cost of multilinguality. Cost of multilinguality is that if you want to cover more languages. you also need to scale the capacity of your money. If you have a word based model tokenization. And then for a non-sympathy scope  Americ. For some, , our language is not covered in multilingual birth, but the script is covered,  you still have very good performance, and Yoruba language is covered in multilingual birth. the lesson is, I try to cover as many scripts during pre-training,  that you will be it will be easier to adapt to new languages in the future. you agree with me that one of the limitation of this approach of language adaptation will be, it's difficult to adapt to non-supported scripts. And then you can copy all these tokens. And then you can copy all these tokens into the   you can copy all these tokens into the original model. the words  tune the model for that new vocabulary. And then things  learning rates also matters. The tokenizer is not in your network. What is the what is the  what is the minimal amount of tokens that can serve a language  , and then the number of token, the number of vocabulary token is a hyper parameter. , the original monolingual birth only has 32,000 , while the multilingual birds. But later versions of best had,  250,000, because they believe this is a better multilingual vocabulary to serve many languages. And you try to get as many tokens, but not too many tokens but we are a bit guided, based on what other people have done previously. someone knows it, and this language has very low performance and the indig bets. there, there will still be some catastrophic forget. And then we just do something  continual training on Mc for Corpus and news corpus. I told you that it was trained on very diverse scripts, very, very diverse languages, 250 k token, but because it was trained on these 250 k. Tokens and many languages. do you really need all this vocabulary? you'll find out that the performance of this knowledge distillation model was the worst and just doing this vocabulary. And here you find out. And the model is a login face. and the 3rd part is very, very short, which I can rush through, and the short part is, the 3rd part is cross lingual transfer learning. The question is, how do you choose the best transfer language? I'm going to demonstrate this with any R model, and  that if you want to choose the best, any the best transfer language. how do you do this? , and then you have . And this is very easy to understand for Nar, for any hour you have things  personal name. they are talking about the Governor of the States, the President,  they will share the same entity. when you want to choose a transfer language, you have to. the question is, how do you  scale this to new languages. How the sound are they related to each other. We have the futura distance, which is a combination of all these distance measures. for you to do transfer learning. This is a result on part of speed tagging."
    ],
    "Topic 2": [
        "How many unlabeled data are available on the web? How many was the size of Wikipedia. That's an example of a labor data labor data. Do you have,  part of speech data sets for this language? which is a Sys class categorization of languages based on how much unlabeled data are there on the web and labor data are there on the web? They have some unlabeled data. And then on the left hand side, you have,  encoder holy models. Why, on the  hand side. we discussed that last time where you could. You want to encode your text properly. And, interestingly, all of them must be based on the transform architecture that we discussed. the left hand side part of the architecture of the transformer only. You have to also construct a task, an example of a task used to pre train and then go down. , I'm not focusing on the sub branch. But what we do typically in an OP. and then you can use it for different classification. But  that you can transfer the knowledge of a model to another task to another domain, to another language. B, , that is, focusing on a different task or domain. you can have same task, and then you transfer it. you can train a model on the same task, and then transfer it to the same task of a different domain and of a different language. then, it's very easy to transfer another one that is even more popular is different tasks. you can train your model on a different task and then use it for another task. You can modify the Internet structures to do transfer learning. And then use this for classification. that's why we call 0 short transfer. it could be any task. But you don't have anything in Spanish. and then you just evaluate some Spanish text. , , but you're not generating it this time. And I've seen a lot of text in that language, even though there's no labor data, you can still have a very good transfer, especially if the languages are very related,  could have a very good transfer from Spanish to French, because they're Latin based. and the languages are very similar. the 1st challenge which we'll discuss  all the rest of the talk will focus on these challenges, and I'll try to present some of my past work on this. the 1st challenge is, labor data sets. But once you find zone, it is another copy. You're going to have 500 MB. And there are also issues of what would be the best source language. or can I use languages that are more similar to the target language. First, st I will start on what we did for African languages back in 2019  we don't really have a lot of data sets that were human generated for African languages. And by working with different African communities, we're able to create different data sets that covers different tasks from question answering text to speech sentiment, classification, machine translation, news topic, part of speech, name, density, recognition. Mascani, , this is an African community I collaborate with which is a Grass group, Nlp community for Africans and by Africans. But I must also tell you that there are other communities around the world. I give you an example of some of the languages and the different linguistics structures. You see a language with a different script,  this can already pose some challenges. and also who can tell me what would be the challenge for? But I didn't put it in the slides. But the models are smart enough to bypass that. What can I pose a challenge. But the models also fits on this  feature. Is there a different word embedding, or a word that starts with a capital? this one I will just really brush through about it new topic classification. Even if you don't speak this language. you're  able to record this. We also did very something very similar to the Masa Project, and we create a new topic classification for these languages. And then we have this new topic classification. this is based on BBC articles and voice of America. Also in the interest of time. when you work on low resource languages, one of the major challenges that there's no evaluation data set. the available evaluation data set  we have taxi, 1,500 from Lmu university. This project has been on for more than  5 to 10 years. and  it will go on for the  20 years. here we  develop Sib 200, which is based on topic classification. And what we did was to leverage an existing data set called Flores, that is based on machine translation. and then we annotate them into these different categories, and then we project that into Arabic. We project it into Yoruba. But that's not the focus from this time. ,  we have this different category. the language families that cover any data set Atlantic. the one of the good thing with Florence data, say, is that it's  cover a lot of low resource languages, ? and then we also fine tune 2 models Xmr was created by Meta. But if you go to some low resource language families. and if you go to topic language, you also have very good results. But if that language is similar to a high resource language. the transfer still work very . if you have a language what can I ? And here you find out that , models  Chatgpt just 1 min had  much worse results than just training on English. It's not a generation task. even though  it's just very randomizer. The Tokenizer also doesn't work very  for this ? if you, Tokenizer doesn't work very  for a language. it's , if a language is not same, and there's no relative or causing then it's gonna give you a poor result. And we have bite level models, which I did better. Can you speak louder,  can you one? Oh, not really if you have a more popular, I would say popular language, I would say a language with more resources on the web. Uses the tokens of ex Lemar. Engram uses engram  what you did in your summit. I'll tell you how we develop a new model called office tomorrow by looking at some of the challenges of adapting plms to new languages. Languages are not covered during the pre training,  when you evaluate it. a result of Amaric  what I told you. , the 1st time I saw this. And then I have to think about what's happening in the model. you can train a tokenizer. you trade the tokenizer on. if the original model has 250 K. And then you can train a new Tokenizer, and saying that it has to be. give you at least 250 k. Vocabulary tokens, and then you can redo the alignments by further training. because then  we I tried a very high learning rate. I use the original, it doesn't work. It's just it's it's How do I explain this. It's a heuristics to determine. you can train a Tokenizer to say, I just need 50,000 tokens that can serve this language, and then you have the heuristic that gives you the best 50,000 tokens. subword tokens that is good for that language. how to address this limitation of lofts which we call left? But this is resource intensive, ? this is an example of evaluation on Indian languages where you have Smr, the original model. But for this language, which I forgot the name of the language which is the only Austro Asia Asiatic language in India. But what we did is study different. to address the audio restriction. But what we did then is. , there's another trick you can do, which is what we call vocabulary reduction. And  this, think about Xmr. the question is, if you just want to specialize the model to a region of languages. we also did the same thing, just removing  focus that are not really African related. unlike that we are the number of parameters of our model went from  270 million parameters to  140 million parameters without. There's still some drop for some languages  Americ and Yoruba, but we will tell you how we address them , even for languages that are not standard for training. but they have a related language. compression or reduction  had better results on average, than the registration. for languages that , we're not able to improve with this multilingual adaptation, we found out that by scaling the size of the model. if you do the same adaptation on 500 million parameters, the performance is much better than the 270 million parameters. Although America was struggling before. Similar for Yoruba,  for Yoruba. We even had better results, Kate. you can have an empty 5 model, and then you can specialize it also to region of languages. And also, you have similar effects where you can also improve the performance even for generation task. I know someone has asked for generation task. you asked for generation task. this approach will still work for generation task by just adapting the model to more, to a region of languages. by doing additional pre-training on just more Chinese tokens, and  they  did Chinese token and English token together. and by this they were able to have Chinese llama, too. and when they did instruction tuning based on Apaka, they are  Chinese Apaca. What you have is you 1st have for training, and then you have instruction tuning. this approach would still work ? this is interesting, because then we are  21 African data sets with any our data sets. They have training development and test sets. And we have 21 non African languages. And then we want to see what would be the best source, transfer language for each of these languages. you train a model on Chinese, and then you evaluate on the rest. model on English, and then you evaluate on the rest 41 languages. and then you evaluate on phone. They're in the same region. Then we also have languages  I told you that we have very portrait. and  that you can train. that you can train a ranking model, a ranker model to  predict what would be the best transfer language for a new language. Given your previous data sets, you have trained . that you train a set of Nlp models  what we have done. You have all these different scores ? and then you will  use this to train a ranker to predict what would be the best transfer language this is based on boosting. What is the data size that's available. And what we did is that we did this, and then, if we do, the brute force approach, you have the top 2 predicted transfer languages. We have,  Nigerian pigeon and Yoruba with predicted if we use the brute force approach. But we still have Yoruba. That was also predicted for hours, even by the language RAM model. if you compare the brute force approach. You will find out that most of the time. That means if you train on the top 2 predicted languages, you'll find out all the time. And if we compare this to the top, one best transfer language, you found that co-training the top 2 from language rank is very similar in performance than using the top one in the brute force approach. why not just use this and just train on the top 2 languages, ? and you can train this for different languages. and then you can  use this to transfer to another language. you have to train an adapter for the source language. You train an adapter for the target language and then for the task specific adaptation. You  train a model that works for the source language, and you are also works the for the source task. in this setting, we believe that the language you're adapting to is also interested in the same source task. and by doing this you can use the remaining models you have trained in many weights. You have trained for the task. and this seems to work very . There's another approach which is very similar to that which is luxury, ticketing, approach. and  that it's very similar to adapter. If you do not use this framework at all. And the source language also matters, and you find out that some source languages, if,  Arabic . regardless of the approach you use if you use a better source, language is already better ? the best transfer language here seems to be wall off for are not English for Euroba. What is the best transfer language for the language?"
    ],
    "Topic 3": [
        "David Ifeoluwa Adelani: Not . that means there's still a lot of work to be done. we could also speed them. what's the size of if you crawl all the available web tests in that language? Oh, better tax specific one. class 0, these are . , and class one would be languages that have few texts. ,  there's a Bible Corpus, or some linguistic activity documentation available in this language. And  I know if you increase the Joshua class Joshi Class 2. And then the winners are   languages with sufficient amounts of labor data and legal data. win us in terms of the amount of data that is available. there are different kinds of language model, as  here. And also we have a bat model is another example, and decoder. , in the case of sequence, to sequence Lstm. there's no way to skip this process ? the way they did the categorization more is the current chart based instruction to Russian are  categorized separately from the others. This is the validity today. It could be water vec, ? It could be Elmo, it could be T. 5. but you can leverage what is called transfer learning, which is very popular in the last 3 years  less popular , because we're prompting. that's why it's called transferring. It has learned some information on a large amount of text. this is inspired by what happened in computer vision, where image net has been trained on a lot of images of cats and dogs and everything. And then you cannot transfer this to what? There's a way to do parameter, efficient transfer line, and  on. And this is one way you can do. Transferring one example will be worth future instruction. All the weights are frozen. and it has been shown that this often leads to better performance than doing feature extraction  and selling it to other architectures  Mlp. suppose we want to transfer the knowledge from English to another language called Americ. You could also have the same experience with transferring to Arabic or some Indian languages that use very different. you concatenate the language of German, English Swahili and combine them together. You can find some language that has training data, , English. And then just do 0 shot transfer to the language. you train a model for English. And then you run prediction for another text which is in another language. And then you have a lot of text Amazon reviews in English. and then you train on English. there's a way you can do transfer learning on monolingual model. But that means you have to 1st  modify this monolingual Lm before you can  adapt it. But this should be clear. And then recently,  last year we had 500 from Lmu. You don't have a lack of data set for many of these languages, and also this virtual language model. transferring from English to American is really difficult because they use different script. We also have issues of parameter inefficiency. If I want to transfer from English to Americ. But if there are other languages to have legal data most, I always use English. , there's a comment in India, AI for Bharat. There's a Southeast Asia initiative. First, st we created Nameless recognition for 10 African languages, some in West Africa, some in East Africa, and later on we find out that , we know we're excluding the southern part of Africa. But , more compelling reason is because they have very different linguistic properties which I'm going to show you. Imagine you want to do a transfer from English language to Americ. we did an example of Swahili in class where you're trying to decode, where you have different prefix. And then you want to try to decode. Nr, if you want to go from English to this language. What can happen with any our model train on English work for? There's no translation of  some words in English language. Just looking at a linguistic structure. You have something  in Nigeria,  one feature of ner is capitalization of entities. this capitalization is absence just makes the annual model fail, or personal names, because it  fits into this idea that if you have capitalization it's more likely to be an entity. And , when you go to a language that doesn't have this feature, it's just   fails very quickly. If he's going to split Kuzang. And then  issues where  it  fails to capture this information correctly across different languages. you have when you pivot and decode. This is  a clear language, and then you have because it's very similar to English. 2023  we had this very. But these are not human annotation. But this is a very expensive annotation process, ? It it would take them  one year to have a huge benchmark. Oh, it requires a lot of expert annotation. The closest to our work is Beli. here we have text in English. And that's how we created a benchmark very quickly for 200 languages. the  when we did the annotation. And  it's a strain on other languages. and we also have got 500 that's already seen a lot of these languages  I've seen  1 77 languages in its portraying. one thing you'll see is that different Mlp. which shows that the transfer learning works but  is not better than a single most layer perception. They often have very high performance. I know Livonne is  endangered  Luxembourg. It's very close to German and some French. Is the script already seen, and the performance will be higher than if the language is unseen. if the language is unseen, but that language is very similar to the same language  Moroccan, Arabic still has a high performance done. Because it's close, closer to the modern standard. , you still have a high performance while you have some languages where the language is unseen, and also the script is unseen,  the Tamashek, which is a Berber language in North Africa, and then you'll see that the performance is really poor, , because the script is unseen, the language is unseen, it's really difficult. 0 shot transfer to all the languages. And that means it's just  you can relate this to compression. That means it's not able to get any useful information. byte level was still  encoded, but the very poor way. And then this still gives you poor results. you won't have folder site optimization . the performance of the language, languages that were not seen by how similar those languages are not seen, or how are they themselves composed of languages that we have seen during training. And that's why I could give example, , look at Arabic way, because it has,  other Arabic scripts that are seen  Egyptian, is very prominent and modern standard Arabic, and then  better results. , for the languages that are unseen and script unseen. , there's some that are still very accurate. Oh, , and what is accurate there. you find out what is accurate is the Mlp. you just train using circuit line. And on the text, that's it just  your assignments. Oh, which we need more? Mlp, , that's a good. And is there a way, we can  find a way to adapt models that in a more parameter, efficient ways. , I trained the model many times as I was, how can I keep getting 0? and after swapping the vocabulary that you adapt the model by training on more on label text. this is already good boost in performance. that means, if a language has already seen that script going to training all your adaptation, you're still not able to reach the performance of the model. Another issue is that it's not parameter efficient because you create a new copy of this model when we are doing adaptation. and also, once you have adopt adapted the model, you can also reuse it for  , the model you have only adapted it for Americ going forward, this will only be useful for Americ. You cannot use it for English again. It's a heuristic way that a guardian of tokenizing the text ? , you have to worry. we have a new model from Africa called Africa. another approach would be to use what is called parameter, efficient approach, where you can use things  adapters, and  that you had an adapter which is  a new weight to the different blocks in your transformer model. and then, instead of modifying all the weight of the transform, but you only modify this new weight, you have added. and this new weight are smaller and more parameter, efficient. And this is the only thing you have to stop instead of storing the entire monophone. And this works very  for post language as . But for the Muriel bet. Similarly, it has better performance on this Sino Tibetan language. this is very similar to what was done previously for Mbat, which is  the topic of your reading assignment. 25.  to  increase the capacity of Mba. They continue pre-training it on 25 new languages. Here we're trying to specialize the model just for Africa languages. And then we combine Irish, some Irish as language  English, French, Arabic. That also widely spoken Africa with 17 most resourced African languages. One popular approach is you do what is called knowledge distillation. you can have embered, and then we have distilled. if you do this adaptation, which is language by language, adaptation. for every language, you adapt it to the monolingual text of that language, you always improve in performance. The only language we did not see improvement in performance is English, because the model is already good for English,  there's no need to adapt English again. And then, when we did this multilingual adaptation, what we see that we almost match the performance of the individual language adaptation. And  we observe some dropping performance. seems to be still a very attractive approach. There was no difference in performance, ? we also did the same thing for sequence to sequence model. And  this is the era of large language model, and I can tell you also, this approach is still quite useful for large language models. And if you want to apply this approach you. All you have to do is just, do continue pre-training on a large amount of text for that new language, and then you do the instruction fine tuning. And then also on Chinese mmu, you also have some boost in performance by this. You train a model on English, anyhow. and each map transfers call  this. And here I'm just going to display some results that are more specific to Africa. Arabic German, we see some interesting transfer to some African languages. In the Francophone region of Africa, we find that they do transfer very  to each other. That means if you train on Bbj. They have  very good transfer performance to each other. even though they do not belong to the same They do not belong to the same family. this language is  Luganda, Rwanda. They also have very good transfer to each other. You see that the performance is very poor. They have very similar properties. those language they have similar properties also transfer very  to each other. Consider this, , we also have interesting transfer performance from Arabic to Swahili. And , in historically, Swahili has borrowed a lot of words from Arabic due to due to trade, because there's a lot of trade between East Africa and Arabia, and in many words were born into Swahili, and also the German influence is quite interesting, because there were some German people there at some point, and I don't know what happened. Very , but I cannot explain the one for German very , but we  see very good transfer, , from Arabic to Swahili better than an African language as , which is quite interesting. , this might also hold. What if you want to do this for another language? In 2019, there was a paper from Cmu on what is called language rank. You train on English, and then you transfer to all other languages. And then also, you use some other linguistic properties and some linguistic distance. And some of the linguistic features are, there's a there's a linguistic vector features known as language to vec,  that means different linguistic properties have been converted to vector representation. if you have a language Indi to, , Swahili, they're very far from each other. How far are they from each other? I forgot the exact definition of that. This guy you train on all other languages except this language, and then you can  run a prediction for this. and here you found that the language prediction are quite interesting. that means the model is not very accurate, but sometimes it can have a good prediction. You find that the English performance is the one in pink and the top one language rank. It's not very clear for some languages. This is better than using English every time. To transfer from that will be better than just using English every time. you, if you're able to get the best transfer language, you can combine it with this framework called parameter efficient fine-tuning, and this will  boost the performance. and  that you can have adapters to every transformer block. You can train this for English. And , this can boost your performance, and if you combine this with choosing the best transfer language, you can easily boost the performance of your postlingual transfer. if you can combine the sauce choosing the best source language, and then you combine it with this parameter, efficient approach. You can significantly boost your performance. and, better still, if you  combine this with Co training on multiple languages, you can even further boost the performance. If you can get it, this can already boost your performance. I am posting watch as far. If you combine this with a different parameter, efficient approach. you can also further boost the performance. And if you combine this by training on multiple best transfer languages to enable further boost your performance. Fantastic from Oh, , from Wednesday."
    ],
    "Topic 4": [
        "there are over 7,000 languages in the world, and or  over 400 of them, are spoken by 1 million speakers. But we don't have a single technology that works for 400 languages per world. the question is that  the distribution of the languages in the world. , you have more languages in Asia. According to Achnolog, you could also categorize them, based on which languages are institutional, which  they are used by different government activities,  schools, mass media but in terms of language technology, even less are supported by language technology. how do you define under resourced languages? it's a very simple under resource. Languages are languages with various resources, but in terms of resources, it will be classified in terms of the data that is available. Do you have set my translation data set for this language or . and also in terms of the research that have been carried out on this language. Do you have representation models for this language? because nlp is data driven. if you have a lot of data on the web. you'll be able to, , create a better representation models. one of the most popular ways to categorize languages is based on Jewish classification. languages where you don't really have a lot of you don't have any text at all. Those request 3 has more on label data than juicy class 2, but also still less labor data. because most of what we do  is based on what is called language model. You have,  decoder, only model. an example of encoder only models would be  birth model. and then you can also send this to a class classification head or to a classifier to do the categorization of what you want to do. And also we have,  encoder, decoder model  T. 5 model is a very good example. Only model would be  something  Gpt. And I believe the current model is still also based on decoder, only model. When did decoder only models become the most popular great, because, when we discussed the transformer, we said the encoder, because this is based on sequence to sequence model the encoder parts. The focus of the encoder parts is to have a very good representation of the model, to compress all the information into a single vector . But better job of passing a very  encoded information to the decoder is all used for generation. whether you use an encoder only or decoder only, you still have to learn an embedding to project the world into a lower dimensional space. you learn, both embedding both. But since it's also supporting output embedding,  you also have to learn an embedding. you need to   go put them in a lower dimensional space. The model is the Max language model? Can we say the decoder? Since 20 2013, plus, you have, you learn a self-supervised model , based on an on based on the large, on the text. It's just the architecture that is different. There's another paradigm, but it will not be the focus of my lecture today,  I won't be talking about prompting a lot. the problem is that you cannot really do have a good, successful, self-supervised training. If you don't have a lot of data. you have a model here where  that it has stored some information. And then you want to try to pass that knowledge to another model. To order conservation tasks, your segmentation and order? you can extract sentence embedding from the language model  Bert model T. 5, model. And then send this to this because it's , is a rich representation. Send this to another architecture  By Lstm. You don't need to modify the weight. You modify all the parameters of the model. you do the self training. The difference is  mass language model has been performed on multilingual text. Just append, and then train the same. And if you don't have training data and one thing you can do, you can. Say, 0 shot, 0 shot transcript. You just mean very good question. once you just you just predict that's it. Yes, and the reason is because you forced to mass language model on a multilingual text, ? That they try to, . That also can be used as an encoder. , of course, lingual transferring is attractive. Because you have  many,  many languages in the world, and then you are not able to have the data set for everything. And and also interesting question that you can focus on. from, , over 7,000 languages in the world. Where, when you fine tune, it just creates new copies of the model. if you prompt , you don't create new copies of Gpt. And then you train 200 models. the 1st one, where we talk about developing data set for low resource languages. They're also working on similar thing. and many people just collaborate with native speaker to develop a level data set for their languages. Then you have languages  Rwanda or Swahili, which have a lot of morphology. and we also have languages  it's called Size Zulu, that apart from using rich morphology, they also have what is called  classes. there's a different prefix for a personal name, for a location, for different entities. entities here, you have something  using ? Because if you want to do with the crf, if you want to be a features for Crf model. One of the features you would do for any R is capitalization. This is  a more general question. But in transformer based models which I'm word embedding, how is how is something  capitalization represented. there, it's not that we have a different word embedding. one major issue, and why we have decided to do this is that when you try to scale to develop new models for different languages, if there's no label data, there's no way to evaluate how good the models are. and then this is how we  develop this Masaka news data set. I'm not going to present results for this, but I'm going to present results for the last one, which is Srb. And again, then  in 20 what? An existing data set  creates a labor data set for many, many languages. But this is based on the Bible on this part. This will just automatically annotated by some rules and heuristics. you cannot trust the result from this data set. We have ud dependency passing, which also has,  dependency, passive part of speech. and they are slowly adding  2, 3 languages every year. That was used for question, answering task or reading comprehension. Given an article just  reading comprehension in high school, you ask a question about the article, and it returns the answer but here they do not provide any training data. We also have a massive from Billy believe was from Meta, and massive, was from Amazon. these sentences already have translation in over 200 languages. and what we did was, if we label correctly the topics of the text in English, we can just project this to the rest of the 200 languages, and that is how we created Siv 200.  it's very simple. We project it into Maltese, and that's it. The exact translation into this different languages by humans. There might be some translation errors. And this is a distribution of the categories. up to 100 of those languages included in this benchmark are low resource languages. and also we have, this distribution across different regions of the world. We have more Asian languages, which is also nice, because we have more languages in Asia, done all other places in the world. , of course we also do prompting, but  I'll skip that all . the accuracy by language, family. Which is the multi-layer perception you see that it's  inferior to the bed based models. This is what you expect . A larger model are the best results. And on average this preaching language models also give the best result. But if you go to some very low resource languages, you find out that a simple Mlp get better results, , , in the Atlantic ago. we also have accuracy by juicy classification and by region. That means, even if they don't have a lot of representation of text. Yes, we have a question. If the script is unseen, how does the world? , processing it and saying, this is  a business or travel class. , that's a great question. Because it's it's not saying it does not send this. Yes, , that's a good question. which I'm going to show you in the  couple of slides is. people address it by using not what level tokenizer, but they use byte level tokenizer. just  ,  site level quotation. , that's a good question. that means you don't need any language model. that gives better result than using a bird's model that was trained for 2 weeks on a Gpu server. if you are able to learn a self-supervised pre-training for them. You have rich representation and better encoding of the information in the language, and then it will give better performance. But for a language that you don't have a lot of text on the web just using simpler approaches  naive base or something they already appropriate. this is , what can you do if languages are not covered. I already showed you result that things doesn't work very  if languages are not covered,  what can you do about it? people  try to scale a lot  Gpt-four, and that's why it's able to learn different languages very . it has slightly better accuracy than X number of data, hey? You just have to ensure that the size match. But I just want to slowly remap the embedding layer and that work. We started with glass glasses. and then you can also have 200 k. But oftentimes you have to have,  something useful. you just don't do  a million tokens or something. A a very simple way is just to return your own new model. The 1st version had a very good performance on this language because it was covered. and then it doesn't work , and they have a random platforms. And here we're able to cover more regions of Africa with a better multilingual representation, learning model. The model is also very big, and up to half of the model size  is in the embedding part of the model. what we showed here is that  an earlier paper already showed that if you want to specialize to 2 languages, just remove all the tokens that don't relate to these languages. Just remove all the tokens, and then your model. you don't need the performance doesn't drop. we can remove things  in the script and in the tokens, because it doesn't relate to Africa at all that, and then you'll still be able to preserve your performance ? while we are throwing away some tokens that we don't need,  we also throw away some tokens that already have languages  Amarik to perform very . , But in general, what we saw is that vocabulary compression was  better than knowledge distillation. if you compare models that they already perform knowledge distillation, which is a very expensive process. The size of the model is remains the same, but in terms of performance, just reducing the vocabulary  is better. When we have a smaller model size than when we move to a bigger model size. , we  create a better model than previous  really strong multilingual models  remed by Google and Md. and then you can also have better results even for machine translation tasks. On the remaining 41 languages. they share some entities that are more interesting,  that relates to France or something. And they are able to perform. , all the 3 Nigerian languages covered we. this name should be flying around in different text, whether it's in Yoruba or Asa, even though they are not the same family. we have languages  this in East Africa, which have very similar syntactic similarity and entity overlap. the result from English to Zulu. We only have 45 ? , from Shona to Zulu, you have better result. They have this  interesting prefix before capitalization of the entity. Look   generation  a generation is really big you. And then you can measure what is the similarity between these different vectors or their distance, and then you can use this as features to  build a ranker model. the geographical distance will be very wide, and then you have genetic distance in their linguistic family. We have the inventory distance which is . What is the entity overlap? Do you have a lot of entities that  overlap between these 2 languages and based on that, you can train your rank and model. and then we also  use a language rank by using what is called, leave one house. leave one house is a is a cross validation technique. If you don't have a lot of data for the new network. and I will show you in a couple of slides that if you use the top 2 and you co-train them together, you might  have very better, better results than predict, using just one prediction or just using English language. you always have better result than just using a single language or just English language. and for any,  the 2 most important features that we discover are geographic distance. The top one rank is better for some languages just using English is better, but if you do what is called co-training. Will is able to provide you at least stop to language. the only thing you need to modify is that you only need to swap the adapter of the English language to that of the target language, Quechua. in terms of the result. This is the blue bar, and you see that this blue bar is inferior to using things  adapter or Lt. Sft. I don't know if I'm able to convince you that."
    ],
    "Topic 5": [
        "How it asks to the force all the languages spoken by 1 million people. and over 1,200 languages are spoken by under the K people. and with 2,000 languages, you have,  a fewer number of speakers. followed by Africa, the Pacific the Americas, where we are located, and Europe and European languages have been more favored because they  because they are one of the early adopters of the Internet technology. And then they have a lot of materials on the web. Do you have language models that support in this language. And do you have even basic linguistic tools? Some languages are not supported by keyboard spell checkers, morphological analyzers and dictionaries. And that's why you will see it's easier to categorize languages  English, Spanish, Chinese. in a decoder only model, the decoder itself learns its own embeddings. Yes, you also have a power citation. And then, after the fine tuning stage, you can add additional layers   a best model and attach it to Lsta or a single layer. we call this pre trained, fine-tuned paradigm. for you to use this, there's a problem. There's lack of legal data for downstream task and also for many languages. also there's not a large enough on labor data for many, many languages. And  this is the area. you could also do the same thing for Nlp, ? one of the ways to  incorporate as far, and it would be  fine tuning. it's  one of the thing you would do. But it could be more complicated  that you could use. , this was our model was trained. The second approach is what you mentioned, which is fine tuning, and here you do end to end fine tuning. in terms of crosslingual transfer. What's interesting is the language with a different script. I hope this is clear. Suppose sentiment let's think about Amazon reviews. , but that's a little bit more complicated. But what we know that most preaching language model do only cover,  on a hundred languages on the maximum. But  we have people that have tried to scale this more. but, , the 1st version of the models we have  multilingual Belt Xml to support,  100 to 100 languages. Just do adaptive fine tuning to 500 languages. We have Serengeti that was trained for 500 African languages we have. Only cover a few languages,  only languages. And also we have issues where you have catastrophic forgetting, , if you do this fine tuning, and then you forget the previous knowledge while trying to adapt another task. 4, this will be completely unscalable, ? people were looking at ways to make it more parameter, efficient. the task of named entity, which you should be very familiar with, which is classifying entities into personal name, location, organization updates. and that means the transfer from this East African languages to West African languages doesn't transfer very  to the one in Southern African languages, because those ones are more morphologically rich. Remember, I told you about not this long. the answer is on the screen. The answer is , the issue of  classes. Do you have capitalization which is humanified feature? if you want to split zang. How would you split it? he's going to split it into one word or 2 words. it's  will may or split it differently. and we have languages with diacritics. And then the what's our representative? A little bit differently in this language is with the acritics, and then it's tokenized differently, and then it also fails for the Ndr task. that this is a language in Nigeria called Nigerian pigin. , , this is BBC, and they are predefined categories. And  that is it possible to repro repurpose. A little bit biased to the religious domain we have any which covers 1 76 languages. and  the community has grown it. The phrase Kappa score was pretty low, just because we're dealing with very short sentences, and people disagree a lot, even for these short sentences. We have more science and technology articles, politics travel than some other categories. we also distribute this by language family. we also distributed by Joshi class. The interesting thing I told you about Joshi class,  class 0 has no data on the web plus one they have few texts, ? And then we have European languages and African languages, and  on. we also then try to find tune some baseline models  classic Mlp. many languages are excluded in our benchmark. for Indo-european you have Slm large. if your encoder doesn't support many of these languages, the transfer performance is also stable. but it will still have a higher performance, even though it has less text on the word ? But if you go to the African languages, you have much lower performance in terms of transfer performance. we also compare prompting with 0 crosslingual transfer. if a script is not supported. the 1st version of Maslinga Bert is going to give you f. 1 score accuracy of 0. , but this is not interesting. Yes, we do have character level models. I'm wondering how that's  even a little bit possible. One of the issues is, many. but also there's other restrictions where models are just getting bigger, bigger, and then nobody can serve them. if you train a embed model, you're gonna have a 4 score of 0 because you're just using one level. And then I understand that the tokenization just fails. , and then, if the language is supported. if the language is not supported, but the script is supported, it still works. But one thing you can do about this is to do what is called adaptive fine tuning. , you can replace the vocabulary completely. That's why what we did there was just to replace the original vocabulary of X number of beta changes to that of Americ,  trained on new Tokenizer and just replace. And if you do this, it's going to correct this problem, and you redo the alignment. And then you are still able to use the knowledge in the upper layers, and then you can improve the performance. by that we're able to improve the performance. You see, for Americ we went from 0 to  60 something. If it has already seen that script during pre-trained. Yes, you have to worry about  changing the model too much. And then I use a very, very small learning rates then what it  did was to slowly adapt the original model  that you don't have catastrophic forgetting it doesn't forget all the knowledge, because I want to retain the original rate as much as possible. just trying to get as many importance as possible. We have another one for India languages called Muriel, and it was just trained on  is it 22 Indian languages? but it didn't cover Sino Tibetan language, and then it has worse performance. just because and it's very important in India languages, because they it's  almost every language uses different scripts. if you miss a language. You also miss the script. what we did then is what is called multilingual adaptive fine tuning. You pick an encoder model  Xnemaro, and then you initialize the weight of the model and just adapt it to the region of languages. here we adapted it to African languages to several African languages. and they show that it doesn't affect the performance of the original model ? here they are trying to extend the model to cover more languages. And here the choice of Xmr was because it covers multiple scripts in Africa. we didn't choose multilingual birds, because it wouldn't work for the Amari case, I should I already described. you have a teacher model, and then you want to distill the knowledge into a student model. But nowadays those models are  too small that nobody cares about distilling embers again. there was some drop in performance, but it was not significant at all. It  still have those languages. And for vocabulary compression, when we compress the model, we see that , we have a little bit drop in performance, and the biggest drop is for languages that use a different script. doing the adaptation on a bigger model. We have the small version, 270 million parameter. We have a bigger version, 550 million parameter. But also we have an additional benefit that the there's no gap in performance between a single language, adaptation and multiple language adaptation. , there's gotta be more weights in the model. and currently, at least for the African languages. how to adapt an English Llm. we have some Chinese authors that  adapted the 1st llama 2, and they adapted it to Chinese llama, and then we had. ,  this is a second part. You just have to train an end by end model. , , here we have languages that are more. we also find,  languages that are geographical proximity. but because they are in the same geographical location. They have the same script. If you transfer from Cosa to Zulu you have better result, because these languages are in the same Uguni family. , , for wholesart Zulu, very similar syntax or linguistic property. let's assume you have this  you already trained this for 40 languages. Do you have to repeat the same experiment and all that. some people have been thinking about this problem. we have different linguistic distance measures  geographical distance. Our geographical distance are these 2 languages. And also we have data dependent feature. And the last thing is which is, you can also use what is called parameter, efficient, fine tuning. You can check, you can get what is the best spas, fine tuning, or what is the best of network for the source language, what is the best of network for the target language, and then you can compose them together. And this is what we have here, which is  our final slide is that for a language  phone. but if you go to Zulu, all of us transfer very  to other languages, did not transfer very  to Zulu, because they have very different linguistic property. And those little details are important. ,  that's the end of the ledger. Thank you for waiting till the end. , Jackie, with the go ahead."
    ]
}