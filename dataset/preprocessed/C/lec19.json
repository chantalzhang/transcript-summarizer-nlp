{
    "Topic 1": [
        "we could also speed them. According to Achnolog, you could also categorize them, based on which languages are institutional, which maybe they are used by different government activities,  schools, mass media but in terms of language technology, even less are supported by language technology. Some languages are not supported by keyboard spell checkers, morphological analyzers and dictionaries. Oh, better tax specific one. class 0, these are . , and class one would be languages that have few texts. And  I know if you increase the Joshua class Joshi Class 2. and then you can also send this to a class classification head or to a classifier to do the categorization of what you want to do. 3. . ? . . Input. . But it  , you could. The model is the Max language model? . Only modules have,  internal representation, that we don't have access. Only. . . This is the validity today. And then, after the fine tuning stage, you can add additional layers  you could have a best model and attach it to Lsta or a single layer. There's another paradigm, but it will not be the focus of my lecture today,  I won't be talking about prompting a lot. Because  we are, we're in the multilingual space. you have a model here where  that it has stored some information. . Is that ? and this is what we do. Trust me. Find that? . . You can modify the Internet structures to do transfer learning. . you can extract sentence embedding from the language model  Bert model T. 5, model. You modify all the parameters of the model. and it has been shown that this often leads to better performance than doing feature extraction  and selling it to other architectures  Mlp. The difference is  mass language model has been performed on multilingual text. And then, if you have trading data, you can find some directly in Americ. And then just do 0 shot transfer to the language. This is very, very, very common. Say, 0 shot, 0 shot transcript. But you don't have anything in Spanish. and then you just evaluate some Spanish text. Yes, and the reason is because you forced to mass language model on a multilingual text, ? ? . . But what we know that most preaching language model do only cover,  on a hundred languages on the maximum. Why? ? ? Mascani, for example, this is an African community I collaborate with which is a Grass group, Nlp community for Africans and by Africans. the task of named entity, which you should be very familiar with, which is classifying entities into personal name, location, organization updates. And then you have this bio tags that you're probably familiar with. Platform around the world. and we also have languages  it's called Size Zulu, that apart from using rich morphology, they also have what is called  classes. there's a result. And why not? . . Remember, I told you about not this long. No. ? if you look at entities here, you have something  using ? ? And , when you go to a language that doesn't have this feature, it's just   fails very quickly. . Is there a different word embedding, or a word that starts with a capital? , . But if you look at these 2 words. ? ? ? but there are results also showing this also. and we have languages with diacritics. Even if you don't speak this language. I think you're probably able to record this. ? this is based on BBC articles and voice of America. ? you cannot trust the result from this data set. It it would take them  one year to have a huge benchmark. and  the community has grown it. . That was  probably this. And we just have issues of not covering many low resource numbers. ? . all . We have more science and technology articles, politics travel than some other categories. The interesting thing I told you about Joshi class,  class 0 has no data on the web plus one they have few texts, ? . . Which is the multi-layer perception you see that it's  inferior to the bed based models. if you have. ? I know Livonne is probably endangered  Luxembourg. but it will still have a higher performance, even though it has less text on the word ? ? We talk about the language coverage. ? And then do you? 0 shot transfer to all the languages. And the word embedding is learning. What may happen is that it will take more tokens to encode the information there. the 1st version of Maslinga Bert is going to give you f. 1 score accuracy of 0. , but this is not interesting. All . you won't have folder site optimization . ? ? just  , maybe site level quotation. . . Oh, , and what is accurate there. that gives better result than using a bird's model that was trained for 2 weeks on a Gpu server. . . Can you speak louder,  can you one? But for a language that you don't have a lot of text on the web just using simpler approaches  naive base or something they already appropriate. We have. Engram uses engram  what you did in your summit. . But . ? let me show you a result of Amaric  what I told you. if you train a embed model, you're gonna have a 4 score of 0 because you're just using one level. , and then, if the language is supported. And then you are still able to use the knowledge in the upper layers, and then you can improve the performance. by that we're able to improve the performance. You see, for Americ we went from 0 to  60 something. ? . and also, once you have adopt adapted the model, you can also reuse it for  , the model you have only adapted it for Americ going forward, this will only be useful for Americ. . And then you can copy all these tokens. And then you can copy all these tokens into the   you can copy all these tokens into the original model. ? Yes, you have to worry about  changing the model too much. And then things  learning rates also matters. Very . . Cool? No. It's a heuristics to determine. I think it originally had  100,000. just trying to get as many importance as possible. you just don't do  a million tokens or something. . . We have another one for India languages called Muriel, and it was just trained on  is it 22 Indian languages? But this is resource intensive, ? and then, instead of modifying all the weight of the transform, but you only modify this new weight, you have added. And this is the only thing you have to stop instead of storing the entire monophone. But for this language, which I forgot the name of the language which is the only Austro Asia Asiatic language in India. just because and it's very important in India languages, because they it's  almost every language uses different scripts. ? if you miss a language. You miss the language. You also miss the script. to address the audio restriction. ? . ? Don't modify anything. Just remove all the tokens, and then your model. Still. we can remove things  in the script and in the tokens, because it doesn't relate to Africa at all that, and then you'll still be able to preserve your performance ? . this is the result. ? ? for languages that , we're not able to improve with this multilingual adaptation, we found out that by scaling the size of the model. But also we have an additional benefit that the there's no gap in performance between a single language, adaptation and multiple language adaptation. . . And here you find out. ? ? for current land models. . how do you do this? , and then you have . and each map transfers call  this. even though they do not belong to the same They do not belong to the same family. And this is very easy to understand for Nar, for any hour you have things  personal name. Swahili. Then we also have languages  I told you that we have very portrait. you can see the result from English to Zulu. let's say, from Shona to Zulu, you have better result. If you transfer from Cosa to Zulu you have better result, because these languages are in the same Uguni family. And , in historically, Swahili has borrowed a lot of words from Arabic due to due to trade, because there's a lot of trade between East Africa and Arabia, and in many words were born into Swahili, and also the German influence is quite interesting, because there were some German people there at some point, and I don't know what happened. this is for name. , this might also hold. . We have the inventory distance which is . How the sound are they related to each other. Do? if you compare the brute force approach. Will is able to provide you at least stop to language. And . cross language as . in terms of the result. This is a result on part of speed tagging. I've been able to. Probably I don't know if I'm able to convince you that. . . , Jackie, with the go ahead."
    ],
    "Topic 2": [
        "we have some language models that they try to scale, to  500 languages, but in terms of performance we I don't think we have a good luck with that. followed by Africa, the Pacific the Americas, where we are located, and Europe and European languages have been more favored because they probably because they are one of the early adopters of the Internet technology. And then they have a lot of materials on the web. how do you define under resourced languages? Languages are languages with various resources, but in terms of resources, it will be classified in terms of the data that is available. How many unlabeled data are available on the web? what's the size of if you crawl all the available web tests in that language? and also in terms of the research that have been carried out on this language. if you have a lot of data on the web. which is a Sys class categorization of languages based on how much unlabeled data are there on the web and labor data are there on the web? For example, maybe there's a Bible Corpus, or some linguistic activity documentation available in this language. They have some unlabeled data. win us in terms of the amount of data that is available. there are different kinds of language model, as you can see here. You have,  decoder, only model. I think we discussed that last time where you could. And also we have a bat model is another example, and decoder. And, interestingly, all of them must be based on the transform architecture that we discussed. When did decoder only models become the most popular great, because, when we discussed the transformer, we said the encoder, because this is based on sequence to sequence model the encoder parts. For example, in the case of sequence, to sequence Lstm. For example. it's also embedding. whether you use an encoder only or decoder only, you still have to learn an embedding to project the world into a lower dimensional space. you need to   go put them in a lower dimensional space. Can we say the decoder? I think it's the same branch. I'm just focusing  decoder. It's just the architecture that is different. and then you cannot fine tune this on a label example ? for you to use this, there's a problem. the 1st problem is. There's lack of legal data for downstream task and also for many languages. If you don't have a lot of data. it's  one of the thing you would do. Transferring one example will be worth future instruction. And then send this to this because it's , is a rich representation. Send this to another architecture  By Lstm. this is very popular. For example, this was our model was trained. You don't need to modify the weight. The second approach is what you mentioned, which is fine tuning, and here you do end to end fine tuning. Lstm, and  on. if you want to do multilingual transfer posting transfer, you need an encoder that is multilingual. you concatenate the language of German, English Swahili and combine them together. Just append, and then train the same. And if you don't have training data and one thing you can do, you can. Suppose sentiment let's think about Amazon reviews. And then recently, I think last year we had 500 from Lmu. Just do adaptive fine tuning to 500 languages. We have Serengeti that was trained for 500 African languages we have. Nllb. the 1st challenge is, labor data sets. You don't have a lack of data set for many of these languages, and also this virtual language model. previously for the birth model. It's  nice. , if you have 500 Md of , it was a size  500 MB. Types, 200 checkpoints stored. It's  no privacy efficient. For example, there's a comment in India, AI for Bharat. I think we did an example of Swahili in class where you're trying to decode, where you have different prefix. But if you can think of what would. Is the cost always as ? , , thank you. You have something  in Nigeria,  one feature of ner is capitalization of entities. One of the features you would do for any R is capitalization. Do you have capitalization which is humanified feature? But in transformer based models which I'm word embedding, how is how is something  capitalization represented. the even the most the tokenizer, we treat them differently. And then the what's our representative? The idea is that this is a language in Nigeria called Nigerian pigin. For example, health news is we? you can just give your cry everything from health news, , likely to get mostly else related news. I had this very, very small idea. the available evaluation data set  we have taxi, 1,500 from Lmu university. This will just automatically annotated by some rules and heuristics. We have ud dependency passing, which also has,  dependency, passive part of speech. It's just an evaluation. and what we did was, if we label correctly the topics of the text in English, we can just project this to the rest of the 200 languages, and that is how we created Siv 200.  it's very simple. We project it into Yoruba. the  when we did the annotation. and then you can have a sentence where some people will label this as geography, and another person will label it as travel, because someone is going for hiking, or something  this. ,  we have this different category. we have sip easy and hard where we have the 1st sip had,  7, I think, 7 categories of labels. Congo,  Indo-europia, was the largest, followed by, I think, Atlantic, Congo. They have not been seen directly training. Lustenbourgish ? It's very close to German and some French. you have a summation. But if you go to the African languages, you have much lower performance in terms of transfer performance. , processing it and saying, this is  a business or travel class. Oh, which we need more? Oh, not really if you have a more popular, I would say popular language, I would say a language with more resources on the web. 1st I will. this is probably due to what is called cost of multilinguality. Cost of multilinguality is that if you want to cover more languages. which is not possible. but also there's other restrictions where models are just getting bigger, bigger, and then nobody can serve them. If you have a word based model tokenization. , I trained the model many times as I was, how can I keep getting 0? And then I understand that the tokenization just fails. But one thing you can do about this is to do what is called adaptive fine tuning. And if you do this, it's going to correct this problem, and you redo the alignment. this is already good boost in performance. What they say challenge. Did you mention? It's very small model. You just have to ensure that the size match. , yes, pretty late. , you have to worry. But I just want to slowly remap the embedding layer and that work. We started with glass glasses. It's just it's it's How do I explain this. you can train a Tokenizer to say, I just need 50,000 tokens that can serve this language, and then you have the heuristic that gives you the best 50,000 tokens. For example, the original monolingual birth only has 32,000 , while the multilingual birds. And you try to get as many tokens, but not too many tokens but we are a bit guided, based on what other people have done previously. A a very simple way is just to return your own new model. and this new weight are smaller and more parameter, efficient. And this works very  for post language as . this is an example of evaluation on Indian languages where you have Smr, the original model. Maybe someone knows it, and this language has very low performance and the indig bets. But for the Muriel bet. and then it doesn't work , and they have a random platforms. what we did then is what is called multilingual adaptive fine tuning. this is very similar to what was done previously for Mbat, which is probably the topic of your reading assignment. Here we're trying to specialize the model just for Africa languages. And here the choice of Xmr was because it covers multiple scripts in Africa. That also widely spoken Africa with 17 most resourced African languages. But what we did then is. And the idea is this, think about Xmr. you don't need the performance doesn't drop. we also did the same thing, just removing  focus that are not really African related. , for example. there was some drop in performance, but it was not significant at all. And then, when we did this multilingual adaptation, what we see that we almost match the performance of the individual language adaptation. seems to be still a very attractive approach. we did. We have the small version, 270 million parameter. Although America was struggling before. When we have a smaller model size than when we move to a bigger model size. Similar for Yoruba,  for Yoruba. and currently, at least for the African languages. we also did the same thing for sequence to sequence model. What you have is you 1st have for training, and then you have instruction tuning. And if you want to apply this approach you. ,  this is a second part. I'm going to demonstrate this with any R model, and the idea is that if you want to choose the best, any the best transfer language. this is interesting, because then we are  21 African data sets with any our data sets. They have training development and test sets. Arabic German, we see some interesting transfer to some African languages. , for example, here we have languages that are more. Maybe they share some entities that are more interesting, maybe that relates to France or something. and then you evaluate on phone. For example, all the 3 Nigerian languages covered we. we have languages  this in East Africa, which have very similar syntactic similarity and entity overlap. They have this  interesting prefix before capitalization of the entity. Very , but I cannot explain the one for German very , but we  see very good transfer, for example, from Arabic to Swahili better than an African language as , which is quite interesting. This recognition. I forgot the exact definition of that. We have synthetic distance. We have phonological distance. What is the entity overlap? Do you have a lot of entities that  overlap between these 2 languages and based on that, you can train your rank and model. And what we did is that we did this, and then, if we do, the brute force approach, you have the top 2 predicted transfer languages. For example, for ours. We have,  Nigerian pigeon and Yoruba with predicted if we use the brute force approach. But we still have Yoruba. and for any, I think the 2 most important features that we discover are geographic distance. an entity overlap. And the last thing is which is, you can also use what is called parameter, efficient, fine tuning. you, if you're able to get the best transfer language, you can combine it with this framework called parameter efficient fine-tuning, and this will  boost the performance. And , this can boost your performance, and if you combine this with choosing the best transfer language, you can easily boost the performance of your postlingual transfer. You can check, you can get what is the best spas, fine tuning, or what is the best of network for the source language, what is the best of network for the target language, and then you can compose them together. if you can combine the sauce choosing the best source language, and then you combine it with this parameter, efficient approach. You can significantly boost your performance. and, better still, if you  combine this with Co training on multiple languages, you can even further boost the performance. I think wallopsy transfer very . If you can get it, this can already boost your performance. I am posting watch as far. If you combine this with a different parameter, efficient approach. you can also further boost the performance. And if you combine this by training on multiple best transfer languages to enable further boost your performance. Thank you."
    ],
    "Topic 3": [
        "it's a very simple under resource. How many was the size of Wikipedia. What's the size? That's an example of a labor data labor data. Do you have,  part of speech data sets for this language? Do you have set my translation data set for this language or . because nlp is data driven. data is the key. and they have a little bit more on label data and a few label data. Those request 3 has more on label data than juicy class 2, but also still less labor data. And then the winners are   languages with sufficient amounts of labor data and legal data. And then on the left hand side, you have,  encoder holy models. Why, on the  hand side. And even with attention, is still trying to do the same job. But better job of passing a very  encoded information to the decoder is all used for generation. Embedded and output embedded. And there's a way you to also tie them together. there's no way to skip this process ? Because the vocabulary space is very large, even if you use a word. the left hand side part of the architecture of the transformer only. , I'm not focusing on the sub branch. It could be Elmo, it could be T. 5. we call this pre trained, fine-tuned paradigm. it's more on multilingual nlp and crosslingual transfer. also there's not a large enough on labor data for many, many languages. B, , that is, focusing on a different task or domain. you could also do the same thing for Nlp, ? given the same task. But it could be more complicated  that you could use. And this is one way you can do. Or Cnn. All the weights are frozen. in terms of crosslingual transfer. that's why we call 0 short transfer. it could be any task. That also can be used as an encoder. And and also interesting question that you can focus on. But once you find zone, it is another copy. Size. and many people just collaborate with native speaker to develop a level data set for their languages. First, st we created Nameless recognition for 10 African languages, some in West Africa, some in East Africa, and later on we find out that , we know we're excluding the southern part of Africa. What is the root word. the answer is on the screen. The answer is , the issue of  classes. Of personal name is capitalized  at the beginning, is capitalized in Zulu is not capitalized the beginning as something. this capitalization is absence just makes the annual model fail, or personal names, because it  fits into this idea that if you have capitalization it's more likely to be an entity. And this is known. This is maybe a more general question. The 1st word is more  a business news the second one is talking about an entertainment award which is  an entertainment news, and the 3rd one is probably on health news, and the last one is sport news. in terms of. one major issue, and why we have decided to do this is that when you try to scale to develop new models for different languages, if there's no label data, there's no way to evaluate how good the models are. And then we have some local website where we called news articles, and we label them. and then this is how we  develop this Masaka news data set. And again, then I think in 20 what? An existing data set  creates a labor data set for many, many languages. when you work on low resource languages, one of the major challenges that there's no evaluation data set. But this is based on the Bible on this part. But these are not human annotation. But this is a very expensive annotation process, ? This project has been on for more than maybe 5 to 10 years. and maybe it will go on for the  20 years. That was used for question, answering task or reading comprehension. Given an article just  reading comprehension in high school, you ask a question about the article, and it returns the answer but here they do not provide any training data. We also have a massive from Billy believe was from Meta, and massive, was from Amazon. It's general domain. And what we did was to leverage an existing data set called Flores, that is based on machine translation. We project it into Maltese, and that's it. There might be some translation errors. The phrase Kappa score was pretty low, just because we're dealing with very short sentences, and people disagree a lot, even for these short sentences. we also distribute this by language family. you can see the language families that cover any data set Atlantic. and Austronesia, and all that. we also distributed by Joshi class. the one of the good thing with Florence data, say, is that it's  cover a lot of low resource languages, ? and then we also fine tune 2 models Xmr was created by Meta. if you look at the accuracy by language, family. But if you go to some low resource language families. for Indo-european you have Slm large. But if you go to some very low resource languages, you find out that a simple Mlp get better results, , for example, in the Atlantic ago. we also have accuracy by juicy classification and by region. And then you see that the region of Europe depend regardless of their classification. Livonne? Because it's close, closer to the modern standard. Is it modern standard? we also compare prompting with 0 crosslingual transfer. It's not a generation task. the issue is that initially. And then this still gives you poor results. that's a good question. it's , if a language is not same, and there's no relative or causing then it's gonna give you a poor result. we need some analysis to see. And that's why I could give example, , look at Arabic way, because it has,  other Arabic scripts that are seen  Egyptian, is very prominent and modern standard Arabic, and then you could have better results. , that's a good question. , there's some that are still very accurate. you find out what is accurate is the Mlp. This is multilayer perceptual. Gpu. That's not good. Mlb,  we have. Mlp, , that's a good. you also need to scale the capacity of your money. And then for a non-sympathy scope  Americ. , you can replace the vocabulary completely. It's a heuristic way that a guardian of tokenizing the text ? Transformer. how to address this limitation of lofts which we call left? Beta. it's already good. but I was resortful. We have Mbat. we didn't choose multilingual birds, because it wouldn't work for the Amari case, I should I already described. I told you that it was trained on very diverse scripts, very, very diverse languages, 250 k token, but because it was trained on these 250 k. Tokens and many languages. The model is also very big, and up to half of the model size  is in the embedding part of the model. Because it's  very big. it's good. The size of the model is remains the same, but in terms of performance, just reducing the vocabulary  is better. , there's gotta be more weights in the model. Beta, by Microsoft. you can have an empty 5 model, and then you can specialize it also to region of languages. And also, you have similar effects where you can also improve the performance even for generation task. I know someone has asked for generation task. I think you asked for generation task. and then you can also have better results even for machine translation tasks. and the 3rd part is very, very short, which I can rush through, and the short part is, the 3rd part is cross lingual transfer learning. They're in the same region. We only have 45 ? Consider this, for example, we also have interesting transfer performance from Arabic to Swahili. Look   generation you might have a generation is really big you. some people have been thinking about this problem. Given your previous data sets, you have trained . the idea is that you train a set of Nlp models  what we have done. We have the futura distance, which is a combination of all these distance measures. And also we have data dependent feature. What is the data size that's available. leave one house is a is a cross validation technique. language is cheaper. and the idea is that you can have adapters to every transformer block. You have trained for the task."
    ],
    "Topic 4": [
        "David Ifeoluwa Adelani: Not . that means there's still a lot of work to be done. Do you have representation models for this language? Do you have language models that support in this language. And do you have even basic linguistic tools? you'll be able to, for example, create a better representation models. one of the most popular ways to categorize languages is based on Jewish classification. And that's why you will see it's easier to categorize languages  English, Spanish, Chinese. because most of what we do  is based on what is called language model. an example of encoder only models would be  birth model. And also we have,  encoder, decoder model  T. 5 model is a very good example. And I believe the current model is still also based on decoder, only model. The focus of the encoder parts is to have a very good representation of the model, to compress all the information into a single vector . you can generate a new, a new text. , there's no encoder. You have to also construct a task, an example of a task used to pre train and then go down. It could be water vec, ? It could be a birth model. This is an example of a birth model where you want to predict Max token. and then you can use it for different classification. but you can leverage what is called transfer learning, which is very popular in the last 3 years maybe less popular , because we're prompting. But the idea is that you can transfer the knowledge of a model to another task to another domain, to another language. that's why it's called transferring. And  this is the area. And then you want to try to pass that knowledge to another model. this is inspired by what happened in computer vision, where image net has been trained on a lot of images of cats and dogs and everything. And then you cannot transfer this to what? you can have same task, and then you transfer it. you can train a model on the same task, and then transfer it to the same task of a different domain and of a different language. you can train your model on a different task and then use it for another task. one of the ways to  incorporate as far, and it would be  fine tuning. There's a way to do parameter, efficient transfer line, and  on. And then use this for classification. suppose we want to transfer the knowledge from English to another language called Americ. the transfer is really difficult. You can find some language that has training data, for example, English. I think once you just you just predict that's it. you train a model for English. And then you run prediction for another text which is in another language. And then you have a lot of text Amazon reviews in English. and then you train on English. there's a way you can do transfer learning on monolingual model. But that means you have to 1st  modify this monolingual Lm before you can  adapt it. but, for example, the 1st version of the models we have  multilingual Belt Xml to support,  100 to 100 languages. you could just do the research transfer if the multilingual model is really good. And I've seen a lot of text in that language, even though there's no labor data, you can still have a very good transfer, especially if the languages are very related,  could have a very good transfer from Spanish to French, because they're Latin based. and the languages are very similar. from, let's say, over 7,000 languages in the world. And also we have issues where you have catastrophic forgetting, , if you do this fine tuning, and then you forget the previous knowledge while trying to adapt another task. We also have issues of parameter inefficiency. Where, when you fine tune, it just creates new copies of the model. And then you train 200 models. people were looking at ways to make it more parameter, efficient. And there are also issues of what would be the best source language. But if there are other languages to have legal data most, I always use English. or can I use languages that are more similar to the target language. They're also working on similar thing. There's a Southeast Asia initiative. And then we have 2 versions of the Masaka Project. But , more compelling reason is because they have very different linguistic properties which I'm going to show you. I give you an example of some of the languages and the different linguistics structures. this is English. there's a different prefix for a personal name, for a location, for different entities. But I didn't put it in the slides. What can happen with any our model train on English work for? But the models are smart enough to bypass that. Just looking at a linguistic structure. , it's very similar. But the models also fits on this  feature. For example, in the paper, where we show that the performance drops very good. this one I will just really brush through about it new topic classification. This is  a clear language, and then you have because it's very similar to English. We also did very something very similar to the Masa Project, and we create a new topic classification for these languages. and that's why we have Masaka and then Masaka 2.0. And then we have this new topic classification. Also in the interest of time. 2023  we had this very. A little bit biased to the religious domain we have any which covers 1 76 languages. The closest to our work is Beli. here we have text in English. We have more Asian languages, which is also nice, because we have more languages in Asia, done all other places in the world. and then you can see the effect of transfer learning in naturally  performing on language that are not seen during pre-training. A larger model are the best results. and if you go to topic language, you also have very good results. And on average this preaching language models also give the best result. which shows that the transfer learning works but  is not better than a single most layer perception. if your encoder doesn't support many of these languages, the transfer performance is also stable. That means, even if they don't have a lot of representation of text. But if that language is similar to a high resource language. the transfer still work very . if you have a language what can I ? If a language is not covered during pre-training, they often have lower performance, and then we have categorization,  is the language already seen? if the language is unseen, but that language is very similar to the same language  Moroccan, Arabic still has a high performance done. And here you find out that , models  Chatgpt just 1 min had  much worse results than just training on English. It's classification task. even though maybe it's just very randomizer. The Tokenizer also doesn't work very  for this ? Because it's it's not saying it does not send this. if you, Tokenizer doesn't work very  for a language. people address it by using not what level tokenizer, but they use byte level tokenizer. byte level was still  encoded, but the very poor way. the performance of the language, languages that were not seen by how similar those languages are not seen, or how are they themselves composed of languages that we have seen during training. Yes, we do have character level models. And we have bite level models, which I did better. But it has not been widely adopted. I'm wondering how that's  even a little bit possible. that means you don't need any language model. you just train using circuit line. You have rich representation and better encoding of the information in the language, and then it will give better performance. Uses the tokens of ex Lemar. this is , what can you do if languages are not covered. I already showed you result that things doesn't work very  if languages are not covered,  what can you do about it? Languages are not covered during the pre training,  when you evaluate it. you often have worse results for these languages. And is there a way, we can  find a way to adapt models that in a more parameter, efficient ways. , the 1st time I saw this. And then I have to think about what's happening in the model. For some, for example, our language is not covered in multilingual birth, but the script is covered,  you still have very good performance, and Yoruba language is covered in multilingual birth. That's why what we did there was just to replace the original vocabulary of X number of beta changes to that of Americ,  trained on new Tokenizer and just replace. Just swap the vocabulary. and after swapping the vocabulary that you adapt the model by training on more on label text. that means, if a language has already seen that script going to training all your adaptation, you're still not able to reach the performance of the model. If it has already seen that script during pre-trained. you agree with me that one of the limitation of this approach of language adaptation will be, it's difficult to adapt to non-supported scripts. Another issue is that it's not parameter efficient because you create a new copy of this model when we are doing adaptation. You cannot use it for English again. Any other language. you can train a tokenizer. It's a model itself. It's a model itself. if the original model has 250 K. And then you can train a new Tokenizer, and saying that it has to be. give you at least 250 k. Vocabulary tokens, and then you can redo the alignments by further training. I guess the words  tune the model for that new vocabulary. It doesn't work. I use the original, it doesn't work. And then I use a very, very small learning rates then what it  did was to slowly adapt the original model  that you don't have catastrophic forgetting it doesn't forget all the knowledge, because I want to retain the original rate as much as possible. subword tokens that is good for that language. But later versions of best had,  250,000, because they believe this is a better multilingual vocabulary to serve many languages. we have a new model from Africa called Africa. another approach would be to use what is called parameter, efficient approach, where you can use things  adapters, and the idea is that you had an adapter which is  a new weight to the different blocks in your transformer model. The 1st version had a very good performance on this language because it was covered. the Austriatic. You pick an encoder model  Xnemaro, and then you initialize the weight of the model and just adapt it to the region of languages. 25.  to  increase the capacity of Mba. They continue pre-training it on 25 new languages. and they show that it doesn't affect the performance of the original model ? there, there will still be some catastrophic forget. And then we just do something  continual training on Mc for Corpus and news corpus. And here we're able to cover more regions of Africa with a better multilingual representation, learning model. One popular approach is you do what is called knowledge distillation. you have a teacher model, and then you want to distill the knowledge into a student model. you can have embered, and then we have distilled. But nowadays those models are  too small that nobody cares about distilling embers again. , there's another trick you can do, which is what we call vocabulary reduction. do you really need all this vocabulary? if you do this adaptation, which is language by language, adaptation. for every language, you adapt it to the monolingual text of that language, you always improve in performance. The only language we did not see improvement in performance is English, because the model is already good for English,  there's no need to adapt English again. but they have a related language. while we are throwing away some tokens that we don't need, maybe we also throw away some tokens that already have languages  Amarik to perform very . , But in general, what we saw is that vocabulary compression was  better than knowledge distillation. if you compare models that they already perform knowledge distillation, which is a very expensive process. you'll find out that the performance of this knowledge distillation model was the worst and just doing this vocabulary. compression or reduction  had better results on average, than the registration. doing the adaptation on a bigger model. Xmrs 2 versions. We have a bigger version, 550 million parameter. We even had better results, Kate. And the model is a login face. this approach will still work for generation task by just adapting the model to more, to a region of languages. how to adapt an English Llm. And  this is the era of large language model, and I can tell you also, this approach is still quite useful for large language models. All you have to do is just, do continue pre-training on a large amount of text for that new language, and then you do the instruction fine tuning. this approach would still work ? And we have 21 non African languages. And then we want to see what would be the best source, transfer language for each of these languages. You just have to train an end by end model. On the remaining 41 languages. You train a model on English, anyhow. And here I'm just going to display some results that are more specific to Africa. In the Francophone region of Africa, we find that they do transfer very  to each other. And they are able to perform. you might have better results. we also find,  languages that are geographical proximity. They have  very good transfer performance to each other. but because they are in the same geographical location. They are linguistically related. They also have very good transfer to each other. You see that the performance is very poor. But if you transfer. They have very similar properties. those language they have similar properties also transfer very  to each other. Why do you transfer? Yes, oops. You might have, for example, for wholesart Zulu, very similar syntax or linguistic property. let's assume you have this  you already trained this for 40 languages. Do you have to repeat the same experiment and all that. In 2019, there was a paper from Cmu on what is called language rank. and the idea is that you can train. The idea is that you can train a ranking model, a ranker model to  predict what would be the best transfer language for a new language. You train on English, and then you transfer to all other languages. And then also, you use some other linguistic properties and some linguistic distance. and then you will  use this to train a ranker to predict what would be the best transfer language this is based on boosting. And some of the linguistic features are, there's a there's a linguistic vector features known as language to vec,  that means different linguistic properties have been converted to vector representation. we have different linguistic distance measures  geographical distance. Our geographical distance are these 2 languages. if you have a language Indi to, let's say, Swahili, they're very far from each other. the geographical distance will be very wide, and then you have genetic distance in their linguistic family. How far are they from each other? and then we also  use a language rank by using what is called, leave one house. This guy you train on all other languages except this language, and then you can  run a prediction for this. and here you found that the language prediction are quite interesting. Sometimes they're very similar. that means the model is not very accurate, but sometimes it can have a good prediction. and I will show you in a couple of slides that if you use the top 2 and you co-train them together, you might  have very better, better results than predict, using just one prediction or just using English language. you always have better result than just using a single language or just English language. You find that the English performance is the one in pink and the top one language rank. You will find out that most of the time. The top one rank is better for some languages just using English is better, but if you do what is called co-training. This is better than using English every time. that means language rank. To transfer from that will be better than just using English every time. And if we compare this to the top, one best transfer language, you found that co-training the top 2 from language rank is very similar in performance than using the top one in the brute force approach. why not just use this and just train on the top 2 languages, ? and you can train this for different languages. You can train this for English. and then you can  use this to transfer to another language. for you to do transfer learning. you have to train an adapter for the source language. You train an adapter for the target language and then for the task specific adaptation. You  train a model that works for the source language, and you are also works the for the source task. in this setting, we believe that the language you're adapting to is also interested in the same source task. the only thing you need to modify is that you only need to swap the adapter of the English language to that of the target language, Quechua. and by doing this you can use the remaining models you have trained in many weights. and this seems to work very . we tested this approach. There's another approach which is very similar to that which is luxury, ticketing, approach. and the idea is that it's very similar to adapter. If you do not use this framework at all. This is the blue bar, and you see that this blue bar is inferior to using things  adapter or Lt. Sft. regardless of the approach you use if you use a better source, language is already better ? And this is what we have here, which is probably our final slide is that for a language  phone. the best transfer language here seems to be wall off for are not English for Euroba. but if you go to Zulu, all of us transfer very  to other languages, did not transfer very  to Zulu, because they have very different linguistic property. And those little details are important. What is the best transfer language for the language? , I guess that's the end of the ledger. Thank you for waiting till the end."
    ],
    "Topic 5": [
        "there are over 7,000 languages in the world, and or  over 400 of them, are spoken by 1 million speakers. But we don't have a single technology that works for 400 languages per world. How it asks to the force all the languages spoken by 1 million people. and over 1,200 languages are spoken by under the K people. and with 2,000 languages, you have,  a fewer number of speakers. the question is that if you look at the distribution of the languages in the world. , you have more languages in Asia. languages where you don't really have a lot of you don't have any text at all. Japanese, French, German, Us. You want to encode your text properly. Only model would be  something  Gpt. One Gpt. 2 Gpt. Yes. Yes. in a decoder only model, the decoder itself learns its own embeddings. you learn, both embedding both. But since it's also supporting output embedding,  you also have to learn an embedding. Yes. yes. Yes, you also have a power citation. Yes, yes. I think maybe the way they did the categorization more is the current chart based instruction to Russian are probably categorized separately from the others. But what we do typically in an OP. Since 20 2013, plus, you have, you learn a self-supervised model , based on an on based on the large, on the text. Tasks  sentiment, classification, or question answer. the problem is that you cannot really do have a good, successful, self-supervised training. It has learned some information on a large amount of text. To order conservation tasks, your segmentation and order? then, it's very easy to transfer another one that is even more popular is different tasks. you do mass language model on, on a larger number of text, and then you cannot find some little sensory classifications which is  different tasks. yes, that's . What's interesting is the language with a different script. You could also have the same experience with transferring to Arabic or some Indian languages that use very different. you do the self training. But  there are 2 ways you can  add there are 2 ways to fine tune. if you have labeled data, you could just  fine tune this multilingual bits. And  we move to multilingual bits. I hope this is clear. , yes. You just mean very good question. , is that clear? Yes. , , but you're not generating it this time. this is more for classification tasks. , but that's a little bit more complicated. But this should be clear. But  we have people that have tried to scale this more. That they try to, . , of course, lingual transferring is attractive. Because you have  many,  many languages in the world, and then you are not able to have the data set for everything. the 1st challenge which we'll discuss  all the rest of the talk will focus on these challenges, and I'll try to present some of my past work on this. Only cover a few languages,  only languages. Another issue is that you have languages that  may use different scripts. transferring from English to American is really difficult because they use different script. if you prompt , you don't create new copies of Gpt. 4, this will be completely unscalable, ? You're going to have 500 MB. If I want to transfer from English to Americ. the 1st one, where we talk about developing data set for low resource languages. First, st I will start on what we did for African languages back in 2019  we don't really have a lot of data sets that were human generated for African languages. And by working with different African communities, we're able to create different data sets that covers different tasks from question answering text to speech sentiment, classification, machine translation, news topic, part of speech, name, density, recognition. But I must also tell you that there are other communities around the world. And  we have this  collaborative projects where we have native speakers and Mls. and that means the transfer from this East African languages to West African languages doesn't transfer very  to the one in Southern African languages, because those ones are more morphologically rich. Imagine you want to do a transfer from English language to Americ. You see a language with a different script,  this can already pose some challenges. Then you have languages  Rwanda or Swahili, which have a lot of morphology. And then you want to try to decode. and also who can tell me what would be the challenge for? Nr, if you want to go from English to this language. Yes, . There's no translation of  some words in English language. What can I pose a challenge. But , yes. these languages. One word, and they're a lot more complex and translates to a phrase, , but our models are smart enough again for that. Yes. Because if you want to do with the crf, if you want to be a features for Crf model. Yes. there, it's not that we have a different word embedding. if you want to split zang. How would you split it? Maybe he's going to split it into one word or 2 words. If he's going to split Kuzang. it's probably will may or split it differently. And then you might have issues where  it  fails to capture this information correctly across different languages. A little bit differently in this language is with the acritics, and then it's tokenized differently, and then it also fails for the Ndr task. you have when you pivot and decode. , for example, this is BBC, and they are predefined categories. Or we have business with politic news culture sports and all that. and also we are  religion because it's very prevalent in the African newspapers. I'm not going to present results for this, but I'm going to present results for the last one, which is Srb. 200. And the idea is that is it possible to repro repurpose. Some some languages. and they are slowly adding maybe 2, 3 languages every year. Oh, it requires a lot of expert annotation. We have belly. here we  develop Sib 200, which is based on topic classification. these sentences already have translation in over 200 languages. and then we annotate them into these different categories, and then we project that into Arabic. And that's how we created a benchmark very quickly for 200 languages. This is clear. , yes, . The exact translation into this different languages by humans. But that's not the focus from this time. And this is a distribution of the categories. up to 100 of those languages included in this benchmark are low resource languages. and also we have, this distribution across different regions of the world. And then we have European languages and African languages, and  on. we also then try to find tune some baseline models  classic Mlp. And  it's a strain on other languages. many languages are excluded in our benchmark. and we also have got 500 that's already seen a lot of these languages  I've seen  1 77 languages in its portraying. , of course we also do prompting, but  I'll skip that all . one thing you'll see is that different Mlp. This is what you expect . They often have very high performance. Languages also have this  effect. And  then we talk about factors affecting performance. Is the script already seen, and the performance will be higher than if the language is unseen. , you still have a high performance while you have some languages where the language is unseen, and also the script is unseen,  the Tamashek, which is a Berber language in North Africa, and then you'll see that the performance is really poor, , because the script is unseen, the language is unseen, it's really difficult. Yes, we have a question. If the script is unseen, how does the world? , that's a great question. And that means it's just  you can relate this to compression. That means it's not able to get any useful information. Yes, , that's a good question. which I'm going to show you in the  couple of slides is. if a script is not supported. Yes. Yes. , yes. , for the languages that are unseen and script unseen. And on the text, that's it just  your assignments. if you are able to learn a self-supervised pre-training for them. Yes, yes. Mlp. Smr, mlp, snmr. Why, Mlp. last part 2  I'm I'm very certain we'll not finish this. I'll tell you how we develop a new model called office tomorrow by looking at some of the challenges of adapting plms to new languages. One of the issues is, many. people  try to scale a lot  Gpt-four, and that's why it's able to learn different languages very . if the language is not supported, but the script is supported, it still works. it has slightly better accuracy than X number of data, hey? the lesson is, I try to cover as many scripts during pre-training,  that you will be it will be easier to adapt to new languages in the future. Yes, . you trade the tokenizer on. because then I think we I tried a very high learning rate. , yes. yes, yes. The tokenizer is not in your network. What is the what is the  what is the minimal amount of tokens that can serve a language  , and then the number of token, the number of vocabulary token is a hyper parameter. and then you can also have 200 k. But oftentimes you have to have,  something useful. but it didn't cover Sino Tibetan language, and then it has worse performance. Similarly, it has better performance on this Sino Tibetan language. here we adapted it to African languages to several African languages. 25. But what we did is study different. here they are trying to extend the model to cover more languages. And then we combine Irish, some Irish as language  English, French, Arabic. the question is, if you just want to specialize the model to a region of languages. what we showed here is that  an earlier paper already showed that if you want to specialize to 2 languages, just remove all the tokens that don't relate to these languages. unlike that we are the number of parameters of our model went from  270 million parameters to  140 million parameters without. There's still some drop for some languages  Americ and Yoruba, but we will tell you how we address them , even for languages that are not standard for training. It  still have those languages. And for vocabulary compression, when we compress the model, we see that , we have a little bit drop in performance, and the biggest drop is for languages that use a different script. And  we observe some dropping performance. if you do the same adaptation on 500 million parameters, the performance is much better than the 270 million parameters. Yes. There was no difference in performance, ? , we  create a better model than previous  really strong multilingual models  remed by Google and Md. we have some Chinese authors that  adapted the 1st llama 2, and they adapted it to Chinese llama, and then we had. by doing additional pre-training on just more Chinese tokens, and I think they  did Chinese token and English token together. and by this they were able to have Chinese llama, too. and when they did instruction tuning based on Apaka, they are  Chinese Apaca. And then also on Chinese mmu, you also have some boost in performance by this. The question is, how do you choose the best transfer language? you train a model on Chinese, and then you evaluate on the rest. model on English, and then you evaluate on the rest 41 languages. That means if you train on Bbj. Maybe they are talking about the Governor of the States, the President,  they will share the same entity. They have the same script. this name should be flying around in different text, whether it's in Yoruba or Asa, even though they are not the same family. this language is  Luganda, Rwanda. when you want to choose a transfer language, you have to. the question is, how do you  scale this to new languages. What if you want to do this for another language? You have all these different scores ? And then you can measure what is the similarity between these different vectors or their distance, and then you can use this as features to  build a ranker model. If you don't have a lot of data for the new network. That was also predicted for hours, even by the language RAM model. It's not very clear for some languages. That means if you train on the top 2 predicted languages, you'll find out all the time. And the source language also matters, and you find out that some source languages, if,  Arabic in this case. Fantastic from Oh, , from Wednesday."
    ]
}