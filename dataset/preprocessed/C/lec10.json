{
    "Topic 1": [
        "Can you see the slides? . and then. we'll be taking the course for the  couple of weeks. Just if you remind us. Also. also we have some tutorials on our end. Please do attend it. All information are on Ed. I also hope you learned something that is useful for the research and for deepening your understanding on Nop, and they discuss very trendy topics that you can focus on for research. And then we discuss recurrent neural networks. and also the most performance Rnn  architecture. And at the end we are going to talk about Lstm Crfs, which is also the topic of your reading assignment. ? Hmmm, focuses on a generative task. Given a set of parameters, theta which you can compute based on your coppers and for the linear chain crf. formulation. And you can say, I also have a feature  that. I think some things were not clear, and I think there was a few errors in my formulation. I. . . If it's convenient for you. . How many checked? , I can see some note. The second week of the 3rd week we're talking about different classifiers. We discuss perceptron. , there are some proofs. But the initial formulation, at least for the fitful neural networks, is more biologically inspired. And each neuron takes scalar inputs. And the input very similar to the logistic regression. in logistic regression, we have this  formulation. That means the output of all logistic regression is being passed to another logistic regression, and you're not  stack them together into layers. . you can have. ? you can have a matrix based on the dimension of the input and the dimension of the eating layer. Then you add it with a bias term and if you look at this formulation to that of logistic regression. You can see the connection that's in a logistic regression. , you have a matrix of parameters. ? 1.  the output of h 1 we serve as inputs to the  layer, and then you can compute H 2, and then the h 2 h. 2 can also be passed to multiply the last weight matrix, and then you can have the output for different application. you can have something  a softmax that the result of H. 2 W. 3 will be passed into the softmax. You can also have a simple a simple, a simple, logistic function. all . And how do we find it? And if you propagate the errors, you can update the weights matrix   your w. 1 w. 2 W. 3. For if you have a binary classification, I think cross entropy can also be generalized to multi-class classification. , if you go through, for example, land language models. This is already enough. What happened in the past what happened. And then you can incorporate this information into a simple fit for learning tool. You can also do that using a simple, fit forward network. around the center. let's assume this is a simple fit for our network. And , this should be  a connection. there should be a connection from Wt, maybe I can do it very quickly. . This. Minus one, and the  word Wt. And you can also do this using the simple what do you call it? a better way to  do this is recurrent environmental. This gives you an opportunity to  incrementally incorporate previous information into the current model. , you have something that is recurrent. at the end of the model, once you have trained this model, what you will have learned is all the States information. This is better approach than . 1. ? the idea of recording analytics work is that you have all these different cells you have cell one cell, 2. you have the Rnn. . y, 1 to N, this is a simple idea. Of course, you can design recurring network with different form. . Shidan Javaheri:  I just wanted to confirm in the previous slide is the theta the parameters shared by every hidden state? all the parameters units will land our data. We just  say, theta for everything. . . Term dependency in language. And you also need a connection between this word. how do we do this? our linear chain crf, a more linear model. because this is more of a linear model. Why recurring neural network focuses on nonlinear architecture of neural networks and is more expressive, is more expressive. Of course, you need more data to train a recurring neural network in terms of feature engineering for the Crf, you can always you need to do feature engineering. You need to  do a lot of feature engineering. Why, for Rnns, you do need to do feature engineering. you have to say, n squared T. That was the time complexity. we can have one to one if you have one to one. ? you have many to one, you have many, too many, you have many, too many. And why is many to one? And the final is , we just need a classification. you but you need for the documents. ? And . Because you. ? ? . for the Lstm which is probably the most popular Rnn architecture or Nlp,  the model includes what is called a memory cell. It's for, for we have different kinds of Rnn, and this cell. this cell that we have here for Lsta, we have different cells. we have different Lstm cells. Maybe, for example, the terminal information is important in predicting the  word. within. And all these are just  either element, wise multiplication or veto multiplication. ,  we have different basic operation. We always need nonlinearity because we are moving. If not. And then you need to really take care of this. . because when all this architecture came, it's  they have developed all these architectures in the nineties, 1993. And then we have some very interesting tutorials. . one of the problem I told you of that you need to take care of. I have. . And then the other information which other information needs to be propagated to the  stage will flow  this. Apart from that, you can also not only have a single Lstm. Lstm. the idea of bi-directional Lstm is very, very simple in your standard neural network. What if you reverse it ? ? you have information progressing forward. And then you have another information progressing backward. you can go ahead from just a simple, by Lstm, to have what is called a is, you can move ahead from just a standard Lstm to what we call the bi directional Lstm. . this is an example of if I Lstm. you go in the reverse direction. But if you combine both the forward direction and reverse direction, you can  have a very powerful model. one example of the model is there's an architecture called Elmo that came out just before birth. By Lstm. , the more interesting thing is that you can, because crf is just  another linear model. If you train a by Lsdr model with a simple linear layer, and you might achieve something  88%. And the reason is because Crf  capture relationship between the output levels. For example, the tax? Both? . And this is coming from a popular architecture in 2015, 2016. And one of this paper, I think is one of the papers for your reading assignments, but if you have time, I will encourage you to also read the 1st one which is also cited in the Lampler paper. ,  in the last one. What do you call it? End to end you can land, you can back, propagates all the errors from the Crf to the Lstm model and update your weight. ? this shows you how we can do inference and training using the Lstm Crf and here, what you do is that for every epoch? Of course, you select your batch and you  compute the bidirectional Lstm. You have the bidirectional Lsm Crf model backward pass. and then you can update the whole, the parameters of the model. ? And that was it. and Lstm Crf. And I would argue also that it's competitive to transformers. This by Lstm, to have a portrayed language model. , Elmo, you can also fine tune Elmo, for your downstream tasks just the way you fine tune events model for a downstream task. Of course we'll be looking at the article structure of a language which will be taught by. I prefer to first.st Do we have questions? Let's easy. . ? The one in the chat is asking, will we talk about Bert or transformers. . If that's . And why. you have x 1 x 2 x 3. x 2 x 3 x 4. Do you get what ? this, it's very easy to. it's you. You have x 1 x 2 x 3. you have x 1 x 2 x 3. in the many. To many this is more appropriate for machine translation, because you need to encode all this information and then pass it into a vector here and then here, at the last stage, you need to decode  for a different language. . I will still be back at some point  after every 4 weeks, of course. and maybe to talk about things  machine translation and crosslingual transfer. And  on."
    ],
    "Topic 2": [
        "David Ifeoluwa Adelani: . Can you hear me? , , , I think we can get started. lecture 10. , today is the lecture 10, and from  class we have another professor taking it, Professor Jackie Sean. You can raise up your hand if you have questions. Workshop. a couple of students from the class. And I hope the workshop was useful for you. We are just interested in the output, and we can formulate it this way where we multiply the parameters by a set of features. This set of features are user, defined features. You can define your features for different applications. for part of speed tagging. The features are different. You can also, define features for other tasks  any other tasks. this  gives you the flexibility to define different features. And Z of X is our normalization constant. And I think we also showed how you can find the close form. we're talking about different  features. we can  compute some crf features very similar to the  probabilities where we just can't compute the indicator function. We can check in our corpus using this indicator function if a determiner proceeds in . and the indicator function is as the is defined as the following, where you have one, if the condition is true or you have 0, if it's false. and then you can  expand these features, for you can add additional features apart from the   features for the , we have what is called the emission probability, and we have the transition probabilities that can be converted as features in Seattle. We can also use the same forward algorithm and the a verteb algorithm for the Lccrf in the forward algorithm. Why, for the Lccrf. For example, I did not put ZZ of XI. We only have Z of X on the board,  I have modified it to have Z of XI. I think, , it's fine. And also it can land from a large amount of data. , you can always connect any directed graph into a matrix. And you can form a matrix based on this. And then and then you can have values for all these weights initially, when you're learning this, you randomly initializing with metrics. if we understand that all these connections are weights, and then can be expressed in terms of metrics,  we can have it in this form. x, 1 is a scalar. x 2 is a scalar, x 2 is a scalar, x 4 is a scalar. if you want to have,  a multi-class classification. Also, if you only have 2 values. We use an algorithm called back propagation. You have a better parameters for them, and then you can use this to improve your model. And if you do this over several iterations. If you go through this over, for example, a million samples, of course you don't need a million samples for every application for some application, even if you have to  a thousand examples. You have an activation function. And at the last stage you have another activation function. This G 3, for example, if it's a multi-class classification can be a softbox function. and you compute the loss function between what has been predicted and your gold label, and then you can do back propagation. instead of. Plus one. And then you have an impute one to hand. Which is the initial state. Or is it different for every hidden state. David Ifeoluwa Adelani:  the parameters? Shidan Javaheri:  it's different. David Ifeoluwa Adelani: Yes. , if you want to do a summarization of a document, you need to be able to connect the information that is at the beginning of the sentence to the one at the middle of the sentence, even if 20 words separate. you'll be able to connect the information that is far behind to the one that is appearing in the future. if you last class, Lccrf. if you are building a classifier, as you will see in the assignments, the reading assignment is that you can  just replace your linear layer at the last layer and replace it with that of the Crf. You can learn everything directly for from the data. , we have different  Rna architecture, . David Ifeoluwa Adelani:  for our end. if you use the softmax function, then you need,  an algorithm to  decide which one,  that you can improve diversity. ,  looking at these different architectures, you have one to one, you have one to many. David Ifeoluwa Adelani: Yes, it's 1 output. I think many to one is correct for language modeling. Which one would you use? You will use many to many that's correct. Shidan Javaheri: I feel  you would capture the context of the entire thing that's being said, and then give the output in a response. David Ifeoluwa Adelani: Why can't the second one work for also for that? Shidan Javaheri: I. David Ifeoluwa Adelani: And then you predict the  one ? Shidan Javaheri: I think it could. David Ifeoluwa Adelani: Who has a contrary opinion. What of for machine translation? Shidan Javaheri: I  think the 1st one would be best for machine translation. David Ifeoluwa Adelani: But but , this one would be better for machine translation. , that's correct. , for part of speak tagging. , from all the diagrams of the Hmms you have seen. For example, in the part of speed tagging. you can have  an activation for her that just gives you 0 1. And we have some activation functions that are very good at this. We also have operations where you do a component wise operation  just multiply the components for one vector, to the other. It's been  close to 10 years old, but it's still  relevant. And then, when deep learning revolution came. What happened? But Lstms has a way to fix that. I hope I have. instead of just using a simple fifall neural network at the hand. The interesting thing is that you can remove that. and then replace it with crf, and if you replace it with crf, crf, gives you additional information because you can. Crf is good for sequence tagging because you can  learn what is the relationship, not you can learn the relationship between the tags, not only the relationship between the words. for the crf, you learn the relationship between the words and relationship between the text. And this gives you an additional information to  boost your performance. and then you can move from 88 to 89, or to 90. Yes, we can combine that, because crf  gives you some boosted performance. They add backward information, and for the crf, you have you have relationship between the different tags. And you can add this information to  improve your performance. You can combine this with the Crf by using what is the transition probabilities between the tasks? Because this is what you get for free from Crm. end to end, and you can  combine this information by combining all the Lstm scores with those of the transition probabilities. and after that you can do back propagation from the element, from the Crf layer back to all the Lstm layers. And you have to do this over many iterations to  learn this Lstm Crf model. I think we have a very good understanding of this. And then you can decide to have  crf to improve performance. Everybody uses Cnn,  every vision paper they just stick with Cnn every Nlp papers they stick with Lstms. Initially, it was for language demonstrated monitoring task and machine translation. If you have time, you can read a hell more paper which I can type here. Shidan Javaheri: Awesome. David Ifeoluwa Adelani: Think slightly. Shidan Javaheri: Thank you. David Ifeoluwa Adelani:  I think you can. It's just a continuation. Shidan Javaheri: . David Ifeoluwa Adelani: That is one way to think about it. David Ifeoluwa Adelani:  for machine translation, you need to encode all the sentence information for language, modeling for every word you have seen. Shidan Javaheri: Thank you. David Ifeoluwa Adelani: , thank you."
    ],
    "Topic 3": [
        "Can you also see my screen? One has been posted on head  the deadline is Friday this Friday, October 11.th If you have questions on the reading assignments, please post on it. I will try to answer. Do you have questions  far on this? I think that's how it's gonna work today. Thank you  much. and after that we'll move to long, short term memory networks, which is the most popular and the most used. Where you try to for the generative task. we are interested in a discriminative task. Of course some languages may not really keep this capitalization as an important feature before English language. Capitalization is important in determine part of proper nouns, for example. If it's beginning the sentence you can ask. We are interested in learning the likelihood or probability of X given theta. Our major interest is in  a discriminative task. Once we have the task, we're . Do you have questions? No questions. Thank you. I think we have some. it's easier for me to  get some feedback if I have, some people would cover us on. How many people check the corrected version of the formulation on ad, because I post a corrected fashion. And if you remember, in enough, I think. We talk about naive base. Then we discuss a support vector machine. And also we discussed artificial, neural, network, artificial neural network. It  is the  learning model which automatically learns nonlinear functions from impute to output. That shows that if you have for any nonlinear function. There's nothing really connecting that to something more biologically inspired. you have network of computational units called neurons. look at neural network, at least the feed forward neural network as a generalization of logistic regression across multiple layers. as a whole, the network can theoretically compute any computable function given enough neurons. for the feed forward neural network, all connections flows forward. , you have the input x 1 x 2 x 2 x 4. You multiply with the weight matrix which the weight mattress  describes all these connections. , all these connections  shows you. all these connections have weight on top of them, and then you randomly initialize them. and then you can do computation from x 1 to X.  if you want to compute the 1st leading layer, you have connections from every impute, and then you can compute what will be the value of the 1st one, and then it goes on from one layer to the other. once you learn all the weight matrix, all the weight matrix, for example, from impute to eating layer. That is, one weight matrix. From the 1st eating layer to the second eating layer. You have another matrix, and then from the eating layer to the output layer. You have another eating matrix, you have another weight matrix. think about impute. where every input here there are scalars. If you combine all the scalars into a, vector, you have a vector,  X is a, vector, and then you multiply by a weight matrix W one, which is the 1st width mattress connecting the input to the inning layer. You can ask, , pass this last output. Do you have questions  far. We discussed gradient descent and stochastic gradient descent. This is a revision for stochastic gradient descent. for the logistic for the stochastic, gradient descent. You have an impute sk, yk, you have a function you want to learn which is F, and then you have a lux function. You multiply the input by the 1st weight matrix plus the intercept. And then you take the output as an impute to the second weight matrix. and the idea would be, you're not only interested. here I think there should be. there should be a line that  connects. And of course you can even increase the context window. And if you increase the context window, you can have 2 words after or 2 words before. this doesn't. The 1st output is given an input x 1. the 1st output can be a y 1. The cell is all this ro. Then you pass this information of your x 1 to hen, and what you will get at the end of the day is that you will get all this state vector, information s. 1, to Sn, and also you get all the Y information, which is your output. If you look at the 1st example, I will look the world that you have described that doesn't make sense to her hub. you see that there's a you need to have a connection between look and hop . You want to look up. Or even crf, but because, Rnas, , in this architecture can pass the information from the 1st impute using this state vector to the second, impute. some of this information in the 1st impute are being transferred to the second impute. the final outputs. these are -defined ways to  select the  one to be predicted. Given a document, you need a single output, which is the class of the document. For part of speed tagging, but which menu to menu would you use for longer? The 1st one or the second one? Why is the 1st one? , I feel  the 1st one would be more performant. , , cool. I think you have an idea. We can try to store a lot of information there to  be able to capture long dependencies between words. And inside each memory cell. we have a vector of weight in the evening layer. maybe, what's the attacks that have occurred? Because you feel this is not important. and the last one that is very important is the nonlinearity. , we have a lot of computation, and then we want to keep them within a certain range. You need to be careful to have all this information within a certain range. And I think you can look up. And I think this one is really good. When you have this  recurring network is what you need to take care of what is called vanishing gradient and exploding gradient. and if the number is near 0 and you multiply them too much, then the gradient is 0, and then you cannot really make a lot of improvements over your network. in Lstm we can propagate a cell state directly to fix the what is called the vanishing gradient program. I haven't shown you this diagram. it's really, you see what is inside. I should have given you a whole diagram on this, but you can find the entire diagram in this tutorial. But a diagram  this, the 1st part of the information. And once you have forgotten that information you have this ft. Here, which is representing which information that needs to be forgotten. But if you have the problem of vanishing gradient, you can just pass the information directly from one cell to the other. That is one way to  fix the problem of vanishing gradient. Is that clear? Here you have the forward layer where you move forward, and then you have the background layer  as you can see. it's  for the forward layer. I think birds became more popular for classification tasks. And this is very important in capturing all the context. Whether it's happening before or the ones that is happening after. it's very important for tax  part of speed, tagging name extra recognition, chunking any or any  task that require you to look into the context of information around you to make an informed decision. and once you have all this information from the forward layer and the backward layer, you can concatenate them together to make the final prediction. you can have once you concatenate them, you concatenate the forward layer information and the backward layer information. You need to pass it to a last feed forward neural network to make the final prediction. Lastly, . And if you replace this linear layer with a crf, your accuracy can jump one or 2 points. and both of these papers. They use this by Nstm Crf model. How how do you  formulate this? and then you can  make what is the final decision on the task based on this. And lastly, here. And after you pass this information to the Crf, you can also do the forward and the backward at the crf layer. I'm really excited about this bi-directional Nstm, it used to be a very strong model before birth came. and I think some people are still trying to revive Lstm to see if it can be competitive with transformers. I think. It was  this when the planning came. End of the task. For name, method, recognition, participant chunking. Hell more which  combine. You have questions. everything is clear. I'm trying to look at. Thank you. You can use both , but I think the second one on the left is better, because think about it. But if the context is one,  this will be x 2 x 3 x 4. for the second, for the 1st one here. Shidan Javaheri: Thank you and thank you for all the lectures. , , thank you  much. for people that are not able to join. , thank you."
    ],
    "Topic 4": [
        "Everything is good trying to change the charts? Unfortunately, we are not able to have this in person, due to the email we got from the university. and today I'm going to be rounding up with recurrence neural networks. but there will be lecture on Wednesday. ,  ,  going to the outline of today. today, we're going to review crf, that we discussed last time. today, we will review the Lccrf. And then, after we're going to review some neural networks because we  touched on some of this concept previously. , you try to learn the joint probability. , we are interested in the probability of Y given X. Unlike the Hmms. Where, you have to try to learn job probability for any  feature, you decide to add, and this makes it very difficult to extend makes it difficult to incorporate new features. For example, if we are interested in the previous tag of P, part of speech to determine what will be the current act. But also we can also, we can had new features for other tasks,  part of speed tagging, you can say, , capitalization can be important. For example, if you want to detect things  proper noun in English language. You can have a feature , was the hands with Edie. Why, in the Lccrf, we're interested in Z of X for the vertebra algorithm, we're interested in the joint probability we are looking for. That will help us to learn the joint probability. , what is the probability of what we're interested in the task? And at the end of the lecture. We try to compute the mle for this by taking derivatives, and we have this formulation in the last slide. And I've posted the corrected version on Ed. Do you have questions on the last lecture? If you can on your camera, we can. Then it's easier for feedback. , that's better. I believe there are no questions, and some of you already checked the corrected version. , going through this lecture. We want to review artificial neural networks. it's very interesting. attention, neural networks can learn it. There's another proof that you can learn any function theoretically. If you have enough data with neural networks. it's really an interesting architecture or method, because number one, it's biologically inspired. But nowadays I think the models we use do not really have a lot of connection to biology or cognitive science. And I will say, Given enough data. I'm checking those people with their videos on, , . for the training of neural networks. We are only interested in one training, example, and then we can find gradients of the loss functions with respect to the parameters. It's a very old algorithm. But we have not been able to find a better algorithm. L. An example of the lux function would be something  a cross entropy cross entropy loss. That's an example of a loss function, a very trivial loss function. That's an example of another loss function. And then we can construct it using a context window. if you want to predict,  a language model, what is the probability of the ? Or let's say, for part of speed tagging, you are interested in the tag. You're interested in the tag of every word, but you want to incorporate the previous word or the  word. We  have a context widow to incorporate previous words, or the  words. What you want to predict you want to predict the path of speech for  one approach is this, , this is what I'm trying to say. what we're interested in is computing the tag from Wt and the tag here is. But , you could use timing information to incorporate the previous word Wt. This is what you start with. Number one. For example, if you're just interested in classifying, if something is spam or not. , if you want to do part of speech tagging every tag is associated with what with every impute. , , there's a question. Yes, that's a good question. We cannot easily model this with Hmms. as you will see which is also the topic of the assignment is that some Crs requires. And , this  reduces the time you will spend in doing feature engineering  and the last one is in terms of polynomial matter, if you remember. And then for Rns, you can have an approximate inference. Shidan Javaheri: I'm   on the previous slide. Approximate inference, only greedy or beam search for Rnn. There are many based on probability you could have 2 words that are equally probable. If not, you will just be predicting the water is most likely, and that's why you need this  approximate inference. if you want to do document classification. Is it one to many, many, to one, many to many, many to many document classification. Yes, I think someone is trying to type something many to one. Can you justify that if you want to say it. Vivek Verma: Is it because the document text has , the input, is  many words. Yes. Yes. Yes, because. the last one here on the list. we can  unveil what is in this cell. And then you can predict this at every time. the sigmoid function, it always gives you a value between 0 and one. there's a very nice tutorial that I would recommend. I think around 2014, 2015, everybody was  just lost. You want to predict the  word given the previous. What if you decide to predict the previous word given the current world. Information is also important in determining what will be the tag of the world you're trying to predict for. You're going from xt minus one to xt to xt plus one, and for the backward layer you move from xt plus one xt minus one. here where you have this sigma. from practical experience. if you take the outpost cost of the Lstm you concatenate. You can learn this joint alike. it's really important to try to connect the old architecture we have been using for long in Nlp with a more new architecture, which is a transformer. To summarize today, we consider Lstm, which is the backbone of many modern Nlp tasks from language modeling in the 2,015 for any Nlp task. No, you use Lstm for all the tasks from pathosp, tagging to nameless recognition, to language, modeling to what sense disabilization. But when transformer case came, , we found that you can use the same transformer also for both language and for vision. I think the Vision Committee were curious if they can use the same architecture also for vision tasks. You're doing this crf layer because you just want to predict the  word. But if you need a tag for every token  very good for a very good architecture would be to use. From from the  lecture. Yes, there's no question. Yes. Shidan Javaheri:  I have a question, and there's also a question in the chat. And then I also had a quick question on Slide 22. Too yes. And for every x you want to predict a word that follows it. if you have x 1,  there's no board here. But I  on this red, you have x 1 x 2 x 3, and you want to predict. And you want to predict x 4 x 5 x 6. Depend, if you have the context window. Shidan Javaheri:  I had imagined that would also be helpful for large from language modeling. Because then you have all the context of the whole sentence, but because it depends on the context window, you're predicting the  word given. The one on the  is better. You need to try to predict the  one. that's why the last one here is the better one. That makes a lot of sense. And I will put a video online. Have a nice day."
    ],
    "Topic 5": [
        "I don't know if  already, but we have reading assignments. We have a reading break  week,  of course, probably  already about the Thanksgiving on Monday, and then we have the entire week. We don't have any lectures. that will be offered by one of the tutors. We skipped the lecture on Wednesday because of the Nrp. I'm very happy that some of you came in person to attend the workshop by. last time we talked about linear chain crfs, and then we talked about this formulation that here we are focusing on a discriminative task. Using mle. and also determine. You can have features in terms of lines, and  on. it's very flexible to have new features for crf model, and just to  relate it, we can also use the forward in terms of computing the inference. What is the parameter of the model that we can take? , it's a bit difficult, since I cannot see your face, and I don't know your reaction. I don't know if you're following. Otherwise, of course you can. efficient run. For example, the concept on transformers attention is all you need. And then you have,  the product of the parameters, you want to learn. You want to learn parameters, a 1, a 2 to a N, and of course, the intercept. you can  compute what would be the value. You only have,  a vector of parameters. And then, if you compute that, you pass it through an activation function g. 1, you have the output of GH. H, 2 W. 3 into another activation function it could be a soft Max function if you want to have a classifier. , I believe this one. good. let's proceed. And the idea just boils down to an efficient way to use chain rules of derivative to propagate the error signal from the loss function backwards to the network. If you have a regression task can also be your mean, square error. if you take the num of your output and your prediction, you can compute your mean, square error. you can repeat for a while , you sample a training case, and then you compute the loss, and then you compute the gradients before you can  update the parameters of Theta. , and the example is here where we can  compute the forward pass. And then you do the multiplication. Another thing approach which might be useful, since we are more interested in language is what is called time, delay, neural network. Which is another fit forward neural network, but you want to  incorporate  a timing information. However, you need to have some timing information and that timing information. Qt, that's what we're interested in. Fit forward and we have architectures  what embeddings have been lent using this way, where you can incorporate previous information and  future information. And you can do this over a time period. the time information can be easily incorporated into a recurrence information without using a time delay neural network. The simple idea of recurring run network is that you have different states. , we talked about Hms,  you also have different states. and in each State you have an initial state, vector X 0. And the idea is that you want to be able to compute all the States and all the outputs. and you will have also be able to compute all the outputs information. Learning. Via a time delay neural network, and all the States are an O are very important to compute what will be the  output. in the recurring neural network there are 2  outputs. Once you use all these parameters, R. And O, you can compute the y 1 output. for you to compute y 1, you need . And x, 1, to compute y. And also you have to output from every recurring run network. But also, you need to pass order, state, important State vector, information to the . you need to pass it to the  cell in the recurring manager. and then you have the state vector and the outputs. and at every cell you can always produce and outputs, and also an additional modified state information to the  one. and this is the idea of the recording random talk. You can even design a recurring network where you don't need outputs at every cell. Then you probably you don't need output information at every cell. And that's for that  task you would need to output at every cell. the data involves all the States parameters that you need to save. Parameters for every State are all incorporated in detail. long. that means you need a way in language  that you can connect dependencies that occur within a very long context and imagine it. are being transferred to the second cell , and some of the information in the second cell are being transferred to the 3rd cell. , but  just to compare the Lc Crf and Rnns. the forward agreeing is polynomial time. it's polynomial time. I told you that every cell doesn't need to have an output ? You don't need every cell to always have an output. This is very similar to , , if you forward a talk yes, there's a question. I didn't quite understand what it means. let's assume you want to output a word here. What would be the best approach it  by  I will know who is following. it's  one output. You still need to analyze them, because you have words following each other. , someone is typing. for language modeling. And you have the state information to predict the  one. But I don't know. This should be trivia. then, this cell. And then we want to learn all this information. We want to forget some old information as you move from one to the other as you move from one state to the other of moving from one cell to the other. You want to forget some information that would not be necessary. You want to forget some information and you want to keep some information. But if you are  you, you want to forget. for we want to forget terms  tags, or whatever code, more than 5 times away in the in the context. you can forget new information. You can extract new information using all the cells. And you can also have an activation function to determine what information to keep, what information to  forget. And then, if it's 0, you forget that information, if it's 1, you keep that information and you cannot integrate this relevant information into memory and then pass it to the desktop. Step. We have masking where you can multiply by a vector between 0 and one and depending on the value between 0 and one. This will determine if you would forget that information, or you're returning that information. if the value is near one, you should keep that information. If it's near 0, you will likely forget that information. You can add information from one vector to the other. You can multiply that information. And you can also concatenate this vector, information. And  we always need that. And I'm gonna tell you one of the reason why you need to. you can have problems in your recurring neural networks  what is called exploding gradients and then vanishing gradients. Think about it when you have to do a lot of multiplication in time and across different layers. You have a lot of multiplication to do. and then you can also have what is called the problem of either vanishing gradients or excluding gradient if the wait  if the values are big and you multiply them over a period of time, you're gonna have a very big number, and that's what is called export ingredient. these are very critical problems that our ends are not able to handle properly. These are  all the different information in a standard recurring way network. I don't have a very good. when you are doing the multiplication in the Lstm is, you have to say, what information do I need to forget? there is no repeated wait application between the Internet State across time. But you can have bi-directional. That is a way to do bi-directional. that is one way to  capture different context of information. you can call, you can capture the information before the current works. , for example, in the case of part of speed tagging by Lstm is very, very popular for this task you always need the previous information. and also the information that is in front, which is the . But Elmo was  using this  idea to build a language model that is going forward way and backward way,  by Lstm model. It's a good model for it. And the answer, of course, can we combine? and you see they have the forward information. If you're using a by Lstm, you concatenate the forward information, the backward information. And of course you need to learn this. And this is how you can combine the Lstm with the Crm. Crf, you do a forward pass from every state, every cell to the other in the Lstm. And also you do a forward pass in the backward direction, because you have a bi-directional Lstm. and the last in the 3rd step. And of course,  we have a transformer before transformers. Everybody used Rnns. crf did not completely disappear then, and for vision task. Maybe at some point they were reaching eating a bar, and for the Cnn they were not able to improve performance, and then they try to integrate transformers also for the vision task. Lstm Crs is very important for many tasks, especially if it's a sequence labeling task  a language model in predicting the  word, of course, for language modeling. Later, in the course. David Ifeoluwa Adelani: , I think we will talk about I don't think we'll go into a lot of details, but there will be some aspect of Brighton transformers. I just wanted to confirm for large, for language modeling, which of the too many to many architectures is best. do you understand what . And then it goes on  that, ? this is one way to think about it. Either the previous or some variation of that. Did I understand correctly? David Ifeoluwa Adelani: Your role is in time, you are predicting the  word also alongside. To have some more advanced talks."
    ]
}