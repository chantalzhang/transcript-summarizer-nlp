[
    "David Ifeoluwa Adelani: Hi, everyone airports. we have very few number of people today. How was the midterm exam was good. it was not enough time some people finish before time, . When do we expect that we'll get the grades back?",
    "We depend on the availability of the Tas. It's a conference you have an OP. we will not be working on it this week. I don't know the exact timeline. hopefully, before you go on break?",
    ",  today we'll be talking about the machine translation machine translation which is a very interesting direction. It used to be a very, very difficult problem. but  I'm not sure it's very difficult. even the current models doesn't solve the task. It's just very good at the general machine translation domain but there are some specific domains that even if you try it the current models may still fail.",
    ", it will still fail for low resource languages, it will still fail for some domains, ,  medical domain finance domain, and very, very specific domains that  they care for. but machine translation has been there for  long. It was an interesting military project. In the era of the World Wars and also it became an interesting projects. , there was some pioneering work that happened also in Canada, , Canada was bilingual is bilingual and some of the earlier couples that was used for bashing translation is  based on Canadian parliamentary proceedings, which by law it should be in French and in English.",
    "you have this couples that could be used to train machine translation models. Similarly, we have some European coppers  European Parliament also have. , which is very similar idea, because in the EU they have some official languages  English, German, French. , I'm not sure if English is still part of it, or it's just a language that they cannot do away with but  they have this official language, and then they need to translate between all this set of languages. But since it's a very, very old project one of the earlier versions is the Ibm model.",
    "this should not be very surprising to you, because Ibm is also a very, very old organization prior to Microsoft, Google and Facebook and open AI, . I wanted to inform you that we'll have another reading assignment that will be released today. and it should be due in 10 days time. and this will be on machine translation. hopefully it  bridge the whole methods with the new methods.",
    "And it can be quite interesting. And you populate new things from the renewal cycle. the outline of today's talk will be Oh, there's another announcement. If you are supposed to take a makeup examination. This will be on Wednesday.",
    "oh, 4, 30, , that means you are taking it. machine translation, we'll talk about why, it is a hard problem inferability and superior wharf hypothesis. Then we talk about the vocros triangle. Then we'll start thinking about the problem by discussing the noisy training model for empty on the Ibm model. Ibm model is quite interesting because it's based on statistical model,  just thinking about the probabilities that the word can be translated to the order by having something  a bilingual dictionary.",
    "What can you do with this? if you have a bilingual dictionary that you can map everywhere to another. and then can you create  a statistical model for machine translation. This is a German text, automatic text verified. this is an example of a machine translation?",
    "And why is this a difficult problem? in this lecture we have been taught about concepts  morphology syntax. pragmatics, discourse, and   why is empty difficult? because  languages with different syntatic structure, with different morphology, with different semantics, and  on. one good example is, considering the lexic lexical gap that could occur between different languages,  lexical gap,  the meaning of the word may not have a translation in a language.",
    "that entity is not even translated. , you have commonly cited examples will be colors us in English. we have different colors, and you can even have different combinations of colors. ,  a color  bonds, orange. which may not have a good translation in another language.",
    "or  the translation of Orange does not even exist. not to talk about mountain range, or do you just describe it somewhere, somehow. and you can have things  kingship terms and slacks. It does not distinguish between mother and father. it's difficult to translate an entity or not an entity , but a proper.",
    "moda or father, or in  rather mother or father in Chinese. there's no meaning of broader. And also in my  I'm looking at my native language, and I was thinking, oh, do we even have translation of Broader? We have a translation of Elder by Yonga. but we don't have a term for brother.",
    "We also don't have a term for grandmother or grandfather. You have to say the mother of the mother, the mother of my mother, or the father of my father. if you speak Chinese,  you can relate to this and then also some languages does not  They don't have plural and some languages they don't have. You cannot even say  for plural, , in our language you have to really specify the number to indicate Laura popularity. you have to say 2 items or 3 items.",
    "all these  things can have confusion if you are working with a very a statistical model. and this might make machine translation challenging. Also, there are more advanced concepts ,  formality. which is also important, is this is former term some languages are very strict. , in German formality is a very strict concept ?",
    "And if your translation model has only seen data that are very, very impolite, it could be a problem  that is an example. we also have issues with pragmatic duration. some words such as again stop, or more. you suppose, or contain an assumption about the world. if you have the word  mark called again.",
    "this already presupposes that my account. This is very clear in English, ? because in English you can use this  words which already assume the presupposed information in the common ground between the speaker and the air. But in some languages this concept does not exist. , if you have phone Mac phone again, I didn't know a phone.",
    "This this word again,   mean slightly different things depending on the context. In this language our call starts me sets, which is, , a Canadian language such usage usages do not elicit a challenge from the air. But in another language, this may  have a problem. ,  we also have other examples, , in morphological examples where you have different levels of requirements of for inflection. You have a lot of prefixes, and I don't know.",
    "in Zulu you can also have suffix. And then this  would change the meaning of the world, and then the prefix might  change present tense to past tense and future tense. we also have languages with noun classes which a good example, would be the Bantu languages in Africa, where or something you can call grammatical gender. every items  nouns can be categorized into different classes. the way the prefix you will use for a person will be different from the prefix you will use for an animal.",
    "It'd be different for the preface we use for a car, we different for the preface you will use for a house. it's  just different categorization of the nouns. and also we have Syntac differences  what other differences? some words,  we have the X view. Are you aware of those things  as your subject verb objects.",
    "but you also have languages that use. Does Arabic use vso or sov vso , correct. different languages use different structures. And sometimes,  you can write from left to . And then Hebrew Arabic could do  to left, and then you have this.",
    "Things can affect the new it can affect  your machine translation model. We have a semantic which  is talking about the meaning. Our special relationships are grammatically distinguishable. And then you have pragmatics, where, depending on the context, . you might be interested in something  politeness.",
    "And then the translation will be different. there is a marketing translation tax that is called formality, where you could give a text in English and then give it a formality, level, formal or informal, and then it will produce the translation based on this information. It will produce different kinds of text. and if you're interested in this, you can check this website called Walls world at philosophy. language structures which you should be aware of has a lot of information about many, many languages of the world in terms of their linguistic structures.",
    "in there are different languages require or allow different morphological, synthetic, somatic discourse properties. And they interact in different ways. We have different other linguistic aspects and non-linguistic aspects that  refer the overall culture of the speakers of the language. the 1st hypothesis we want to consider is a pure wolf hypothesis. I must comment that not everybody believes in this.",
    "the pro, the question is, is it even possible to produce a perfect translation. some people have argued  that for you to work on any language, if you can create a perfect translation system, everything you're able to do in English, you can do it on any single language. should we just improve on machine translation task, and then we can solve all the languages, all the tasks in different languages. it's something you can think about. if you have a perfect passion translation system.",
    ", from English to Chinese or English to Hindi. Every task you can perform in English. Can you also do the same thing in English? Or there's some cultural differences that will make this impossible. , is it possible to have perfect translations within subgroups of languages?",
    ", , between all 9 languages to have exact translations. And, , between French and Arabic, it's, is it possible? Is the problem they'll come perfect is the problem. you can have high accuracy, no highly accurate translation. but saying something is perfect.",
    "Because, depending on the context. do you have a high quality translation between English and German. The answer is, yes,  . But if I do translation of Lego documents. and if you read it,  a native German speaker.",
    "You might not agree with the translation ? you get a gist of it. But the formality is not there. The the way the Lego language supposed to be written is different. And , Lego law is  Lego proceedings.",
    "Legal documents are different in different countries. there's I forgot, there's common law. There's all this  law that different countries adopt. And it might be a problem. But it's possible to have a highly accurate translation in a particular domain for saying, parfait translation.",
    "But nowadays, if you ask many people. They will tell you, ,  for some languages, machine translation is solved because  on 100 cases. if you can have 98% of the cases, you must agree with it. But , cases that . it was just something wrong is going on.",
    "The language you speak affects your thoughts. The strong version is that language determines and constrains all human interaction and thoughts which is really strong, which I will personally not agree with this. The language determines and constrains all my actions and thoughts. Which version language may influence human actions and thoughts slightly in highly specific ways. very few linguists  believe in a strong version of this.",
    "some language even just reject everything. in terms of spatial organization, in a language called Quotayore use uses an absolute system. , they don't have things  left  ahead, and everything just is not eastward sound. how do you do a translation  that? if something says, there's a sentence, says the coffee southwest of the dinner plate, this is  that's strange.",
    "because you would typically say, check the left side or . And then you said South West. and  this coming from culture or historical, that everything they want to do they have to use the navigation, go left,  to the river, or get the fruit or something. and also in some languages in English, you do left to  in Hebrew you do  to left, and in and this other language good. Your you will use east to west.",
    "Even , unfortunately, we are not at a point of worrying about Sapia worth identity. Because this is a very , there's a theory. Theory is, can you even have your offer translation? But what we can do is to  just minimize the complexity and say, can we just achieve translating about events and participants or just focusing on eye level concepts rather than worrying about having a public transportation every time. And the question is, how do we measure progress?",
    "If you're aware of text generation a bit, you might be aware of this metric called blue score. this is one of the metrics that you can use to measure the quality of machine translation. You have a text in English. You have a text in other language, say French. and you want to check Oh, .",
    "You have the text in English. You have the text in French, which is the reference. and then your machine translation model produce another one which we can call the hypothesis or the output. And then you want to compare this hypothesis with a reference. And how do you compare?",
    "You want to do  a matching of n-gram matching of the words . and to see if they  correspond to what you have in your reference. And one way to do this is using the bluescope   the metric was introduced in 2,002, and  that if you can have a metric  that just doing the counts. You can fix the engram, Count, to  2 or 3 or 4, and then you can check how many of the engrams  do much. And based on this culture, you can have a blue scorn.",
    "And then the way we are going to use to measure. How good is the metric is if it aligns with human judgment. Why is it important to do endrams , why can't you expect a number of words that are differentiated. the number of words unigamp. But ,  what we , you can do unidram of unigram diagram trigram.",
    "but typically,  people use  public 2, 3, 4. And the reason is that. the way you evaluate how good the metric is to  give it to humans to judge how good the metric is. and the way you do that is to produce a correlation between. This call that the metric gives you and that of the human.",
    "if you ask human to rate the quality of a transition from 0 to 100, and then you have a metric giving you a value from 0 to 100. Do they correlate with each other for every sentence you have tested. if they correlate, that means the metric is good. if they don't correlate, that means the metric is bad. when blue was introduced, they also do the discretion surprisingly.",
    "it has a high correlation with human judgment. But this doesn't scale to if you transfer this to other languages. it's worked very  in English. it's  the 1st size was created was created for mostly Latin based languages. if you move to a language that use a different script  Hindi even siri exclude  Russian.",
    "Then  something doesn't work again. blue is  focused on what is called precision. It's precision oriented for each engram in the proposed translation you have to check if it if it is found in the reference translation. and this is why it's called a precision based  and in practice, blue incorporates an additional brevity, penalty and a geometric mean over several values of N.  in practice, what we do is that we don't just focus on one end , which is a question you mentioned? We we do for N equals 1, 2, 3.",
    "And  we do a geometric map over it. but me blue is not the only metric that is available. We also have other metrics another one is called material score which is  a 1 to one match very similar to what you're talking about. and between the output and the reference. Then we have another one called translation Error Rates which is the number of edits required.",
    "Are you familiar with edits? Which signifies, how many edits do you need to make to this text  that it's going to look  the other one. if I edit the hypothesis. What's the distance to that of the reference? This is what you're trying to measure.",
    "and a more popular one these days is what is called character. Because for some languages you need to measure things at a character level. especially for morphologically rich languages. If you use blue score, it doesn't correlate at all with human judgment. And then you need to use metrics to  consider the character, level information.",
    "and also languages that use their critics, languages of different scripts , if you use languages that focuses more on character that works  Chinese. They even have their own specific character and tokenizations. And more recently, we moved on to an embedding, an embedding based evaluation you have  seen a bird's model in this lecture. I don't know if you have seen this  very similar to the birds. Architecture you use  a multilingual birds model, and then you can train an estimator model.",
    "an estimator is you share the model, giving a pre-trained model  Bert's model. you send in your hypothesis, you send in your source text, and you send in your reference text. You extract the different embedding. If you send in your hypothesis, you extract the embedding you send in your source text, you extract your embedding, you set in the reference text to extract the embedding, and then you concatenate. The embedding together are connected to the linear layer using the feedborn neural network, and then you can train based on the miss.",
    "this is a regression task. That's why it's changes in Miss Square Arrow. And based on this, you can  create an estimator that you can use for estimating the performance of a machine translation task. ,  this is useful for some of the projects that you'll be working on. If you're interested in machine translation.",
    "you typically expect you to evaluate a statistically based metrics  blue score or Chrf and an embedding based metric  Novid scope. the number of edits one is it where it is when you're making the edits, the edits that you're making? it could be word by word. ,  our developers are triangle. this is  a triangle to describe different ways.",
    "You can do translation at the bottom. You have the soft surface to the target surface. How can you do a direct translation from a source text to a target text and for direct translation. This is  a lower level way of the machine translation. This is based on things  using bilingual dictionary.",
    "if you have words in French, you have the words in English, can you just do an alignment to the words and then create a machine translation. what if you have languages that have very they're very similar or very similar synthetic structure, if it can modify the synthetic structure a bit. Would you be able to do a better translation ? Or do you need to include semantic information? And this is the higher level.",
    "if you can get, if you can take the source language to an Interlingua. then you'll be able to do a translation to retarget cycles. this is the theoretical concept, and we try to see if we can have what? What would be the interling in our case if you want to use  a neural based machine translation. or it's a statistical based machine.",
    "The early efforts early Mt. Researchers developed a set of bilingual dictionary rules to map from one language to the other. Some of these dictionaries have been created by many linguists as , , even English to including very low resource languages  people. Just create this bilingual dictionaries that  if you go to linguists language, libraries, or something  this, you are going to find a lot of these bilingual dictionaries. And the question is that can you use this to translation?",
    "the Interlingua is  a conceptual space common to all languages, that if you can take the source text to this intelliga, you'll be able to do the translation and the advantage is that you can use to develop a general empty system. a system that is trained on and works for a set, a set of specific pairs of languages for direct translation to happen. You need this bilingual dictionary, because without this information you cannot do this alignment with Interlingua, adding a new language only requires translating into the Interlingua. , , if you have it costs a space that you can map it to one example in deep learning is. if you want to do translation, you have to convert every the entire sentence into a single vector and then send it to the decoder which is going to decoded to your, to the language of your interest.",
    "And if you have a multi-way machine translation model. That means all these different languages will be sent to the same space which by which you cannot be decoded. the disadvantage is, what should an interval look  they? This is just  a theory that has not been implemented. This is prior to the planning ?",
    "it might be difficult to work with such an expressive. but   it's a little bit possible. for the statistical machine translation, which is what we're interested in. The Ibm model is a statistical model. and  less as though we want to translate from English to Russian.",
    "This was developed by assuming a noisy chatter. and I don't know where this is coming from. When I look at an article in Russian. I say this is really written in English. but it has been coded, is some strange symbols.",
    "I will  proceed to the code. just think about the time of war, or something that you want to decode something using a noisy track channel. you can look at a machine translation  a form of encryption and decryption. you have the original text, and then you encrypt it, and then you send it through this channel. and then you need to.",
    ", you need to decrypt it to get information. This is  a question to you. Suppose we are translating from Russian to English. Which of the following is correct. using your knowledge of base?",
    "the 1st or the second one. Why I was speaking about  the model of the previous page, that. , he is the prior. , because we don't know. the pov is your prior, which is also your land model.",
    "and then the P of F given E will not be your translation model. if you combine this, you can  use  a very basic statistical model also for translation. the key thing is that we need to do an alignment. You need to do match one word to another. and then you can  count the number of times the world in the.",
    "suppose you have a lot of text in English and in German or French. and then you want to count the number of times that this ward is associated with another world because they always call together. You can use this to  create an alignment model to do the translation. you want to train a model probability of F given he with probability of the source given the target. We call this the word alignment model.",
    "you have a text in English And another text in French, which consume the same text in 2 languages. Canadian answered, which is  a parameter debate in English. And then you have a text in English. You have the text of French, and then you want to see ,  Canada is equal to Canada here. and then  my French is not good, but you have some.",
    "And then you can do count? it's  you're creating  a generative model. you have to do all the counting. something  you remember the way we did part of speech. Yes,   when you do the counting, that's the way you will do the training of the model.",
    "it's  supervised because you need a lot of text and a translation. But it's  accounts based model  that you use to train the alignment model. There are a lot of tricks you can use. You can make use of various tricks to get sentence alignment. the sentence lines, the cognate words.",
    ", if language use a similar autography,  an example of a cognate word is , , reference and reference. you will know this is reference, ? It's just that it's it's  in French. because the autograph is  different. and then you have something  metric and metric.",
    "metric and metric is just written differently. but they have the same roots. Because most of this language, , the Latin days English is not Latin based, but it borrows a lot of words from Latin. and then you have metric in French as  has something similar in English. these are examples of cognit words.",
    "You can try to use tabs to send sentence lines longest common subsequence of characters. and then you can define a similarity function between the sentence and some of the words. Can you can  do one to one translation based on this similarity. And before you  do any fancy account of mines. and of course, you can also use some advanced dynamic programmability.",
    "the dynamic time working, which is for any distance. even after the sentence alignment we do not have words that are aligned. and then factors to consider for the word alignment. You have to think about things  possibility of translation. Many, too many mapping is impossible to do the mapping directly.",
    "Another thing that is very important, especially if you move to different languages is the word order. you can have a word that is beginning in the sentence in one language. another language is in the middle was on the hand. this  may make word alignment a bit difficult. How many of you are aware of the linguistic Olympiad?",
    "we also have linguistic Olympia. the idea of linguistic Olympia, this is very simple. , they can give you 2 languages. and then you based on your understanding. They can give you a language or 2 languages.",
    "And then they give you rules of the language, and they ask you to perform a task in a language  given some rules. can you translate from English to  a Canadian indigenous language, that you have no information about, and they give you. They provide the rules based on your on your linguistic knowledge and the rules that have provided. And you saw the task. this is a very simple one.",
    "he is language in East Africa that is morphologically rich. that means you have prefix beginning the world. And  you have a single word. Which can I try to translate a phrase in English? If you have all these words in Swahili, and this is the translation in English.",
    "What are the Swahili morphines for play? This is this should be straightforward. you have to decode the rule here, ? And they told me what display? she's a , that's it.",
    "That's seems to be correct. And cook, I don't speak soilly. we can have fun today. why not papika or Apica. I'm trying to confuse you.",
    ",  Peter might be correct. I also don't know the precise answer for this one. what is high, based on what you can decode what? And and I, , that's looks correct should be correct. ,  or ,  it's just you.",
    "And then you have we? It will only have one example, but it would  be included. , it may be truly . you pass this Olympian exam. I provided some for a solution, but not for everything.",
    "And then,  the because morphologically, return is, they just keep attaching,  you can have a very long world, and they just keep attaching, prefix, depending on what has been added. but the roots what will still remain constant . Even if you have understand, understood. And then it's not very clear what changes if you move from understand to understood. Decode is what we change from, understand to understood this.",
    "Think  the past tense uses we, and the current tension is . does everybody agree with that? , your answer seems very possible, because that's the only thing we can say here. And you have to remember. I can ask my soil, speaker friends.",
    "if you can decode this. for the Ibm model one. Ibm developed a series of 5 differential models that make increasing powerful assumption. we are going to do more than one or 2, and then we'll move to more neuro-based, empty. which will also be the topic of your reading assignment.",
    "And model one is mostly basic. each source word is aligned to 0 or one target world. you don't try to model different distortion of water. You just say, , every word maps another word. and then you don't try to model likelihood of fertility  some phrases , Take a walk you don't try it, but  this is more complicated for a morphologically rich lover  Swahili.",
    "because everything is just attaching it. And if you do this  an alignment,  you have a no node allows. What's an F to align to nothing in E. That means it's possible that you don't have a word to word matching, or a word in that sense. d is  in the French. And then you have these 2 words that  mean the same thing, and calling and the mandate  I didn't.",
    "I cannot pronounce French words very  at this point, and then you have different word to word mapping, and sometimes you find out that the order is different. you see that standardized in French is coming before the containers in French. you can do the world toward mapping. And then, after you have known the alignment,  you just produce the indices. you say one is aligned to 1, 2 is aligned to 2.",
    "But , the  alignment in French goes from 3 to 4, and then you have. or Lee the L with apostrophe is null, because there's nothing is attaching to it. And then you have another word standardized. which is common the 5th world is  going to the 9th world in English, and  on. And since each word is, you're aligning 0 to one,  the length is the same.",
    "the length of your alignment model is the same as your target model. ,  as your target text. if you want to formulate it. which by  you should be familiar with this  formulation. Probability of, , a French.",
    "What given English  you can materialize over the alignment which is a and this one to one based on probability theory. And and then you can also oh. use the Bayes rule to say. this is a joint probability given E. And this will give you the probability of F. Given Ea. Multiplied by probability of a given E. Joseph is in the past room.",
    "But the question is, how do we compute probability of a given E,  of course we are make we are. We make a very simple assumption. And one way you can do this computation is just computing what is the uniform probability of the translation lines. or you can also compute the uniform probability for each possible alignment. And this is one way you can use to compute it.",
    "this is a very strong assumption. I cannot give more details, because this is just the I'd be a model that I'm trying to split at the moment, which is just the formulation. probability of a given E is mostly concerned about the translation lines at the moment. because the assumption here in the other slide is that you want to align every word to 0 or one target. the length has to be the same.",
    "the  one is probability of F given e comma a. And here we can  use compute this using Emily. the probability of F given E. And the alignment model. You can just compute it if you have a bilingual corpus using mle. and the idea of using Md.",
    "Is just counts the probability that this word occurs both in English and French, and then you normalize by the count in English. using the number I made. and after that you can multiply it. But the question is that let's assume you don't. What are you going to do?",
    "Also part of speech, when we are not able to compute it? The counts, what did we do. with which you can start with the initial counts in, run at random, and then you try to modify the accounts until you are able to find a true distribution. for the expectation maximization, you initialize the parameters. Probability of A. F given E. Randomly, and then you do the SE.",
    "Step and the M. Step for the E step. Given the current parameters to compute the expected value of the Count of F of E over the training data. And for the M step, you try to compute a new value of probability of F given. E, ,  what's the probability of the alignment? we have probability of FE, which would have the composed probability of F given e comma a given E. We have computed the probability of a given E. We have computed the probability of FE.",
    "What is the probability of a given ef  probability of a given ef possible can be decomposed this way, using the Bayes rule which is the pro the pro the product of probability of a EF. Given E. I can try to write it on the board. But I'm just gonna write a second  of. And after that she can marginalize over a.  if you imagine a light over here, you're gonna get the one I'm talking about. ,  I should just write it.",
    "If you want to compute this, this will. This will   ef and they will pass to each other. this one can be expressed by marginalizing over a do you have question? when we have a sentence. And we're aligning English word or away from the line already have .",
    "what does it mean to marginalize over all the alignments? there,  there are different ways to do the alignments, . you can follow different rules in doing the alignment. And you can marginalize over different rules by which are doing the alignment. in order to do this, we need to predify all possibilities of ways of doing.",
    "You have different ways of doing this. it would be a language based rules and not dependent on the length of the sentence. The length of the sentence is also one of the things we should consider based on what we talked about. a and all its possibilities consider both the language and the length of the site. it possible to give an example of how?",
    "there might be an . This is  just using em a guardian. And here you have an English sentence and a French sentence. and then you want to compute this translation model from F given E uniformly  here. What is the probability that mansion or mason is the translation of red?",
    "This is what you want to complete . What information is the translation? Or is the translation of red? What's the probability of this? But this is a very , this example is  too simplistic, because  we have exactly the same value.",
    "But if you run this multiple times using the em that you might be able to achieve to go to the to the more plausible counts that you're expecting. , here, what , if you just want to compute mle for this. This is the count of how many times you have Mason and Red together. both in English and French. and you'll see that they are called together in the 1st example.",
    "that you just have a sentence French. You have another sentence in English. You don't know the translation. You don't know which word is the translation of the other, and then you just can't. How many times do you call?",
    "think about having a lot of sentences. 2 million sentences with different translation. At some point you are going to see a different count for what you are expecting and based on the count. this word is the transition of the other is going to have a higher probability. and then you'll be able to do the translation based on this.",
    "and then you can create different alignment rules to  match to compare the sentence in English and sentence in French. and then you cannot apply the Ibm model, which is just based on the statistical model. but in practice you don't initialize the translation model of F, given E. Uniformly given reasonable sizes of lexicon. too much memory and computation. Rather, you tend to restrict it to what Ps based on how it is aligned in the set experience.",
    "But as  in the  class, there's a way we can adapt this  that you can have something. There's more  there's slightly better. And this would be Ibm model 2. And after that we're going to examine the last model which would be neural based. the neuro base is a little bit more interesting than this Ibm world.",
    "because you are likely to use that  than the Admin model. and also because the Ibm model often gives very poor performance compared to genero base model."
]