[
    "I'm I started a bit late. ,  today we're going to start talking about lexical semantics. But 1st we're going to wrap up a few things from the last lecture on parsing and syntax. And, , I don't know if you saw, but I posted an ed for the readings for this lecture. The latest version of the draft of Durafsky and Martin. 3rd edition seems to have taken out this material, but  it's  very important. I posted a previous version that still had this material. first, st here's a review. What are the components of a Pcfg. what are the components of a Pcfg.",
    "And what do they, , do? Is this a table of, ,  derivation of the words, and then the probabilities associated with each cell that's definitely related to Pcfgs. But that's , once you already have the components of a Pcfg, how you use it to  parse? Yesterday I said tokens, and I was wrong. it's a description of transitions between tokens other than not tokens. Yes, those are definitely part of it. ,   instead of tokens those we'll call them non terminals. ,  we have non terminals. That's definitely one of the components. And then you mentioned that there are rules which is  a left hand side, non terminal which rewrites to some combination of  hand sides, which are term non-terminals, or terminals with probability.",
    "this implies that terminal symbols are also part of a Pcfg. we're still missing somebody else, . , yes, a starting symbol. And I  that this is  it. I would just add that a room  a reminder that,  each non-terminal symbol forms a categorical, you define a categorical distribution for each non-terminal symbol over all the rules with that non terminal symbol as the left hand side ? ,  I need to explain some more. ,  what this means is that ? ,  what are your non terminal symbols? Each non terminal symbol is a syntactic category. a non terminal symbol might be noun phrase, or it could be verb.",
    "what this means is that for each of these non-terminal symbols they're non-terminal, which means that they need to rewrite it into something else. That that's by following a probability distribution. for  there might be, say, say, you're looking at verb phrases. rules that where the verb phrase rewrites to just the verb, because it's an intransitive verb. or the verb phrase rewrites to the verb plus the direct object. If it's a transitive verb. there are a whole bunch of different options you can have for what's underneath a verb phrase, node. and what this last condition property is saying is that process follows a categorical probability distribution. ,  I hope that makes sense. Otherwise, the  bit is gonna not make any sense at all.",
    "if you still need me to clarify, please ask. then the and at the end of the last lecture we discussed probabilistic, context-free grammars in their most basic form, which sometimes people call vanilla Pcfgs informally. And the idea here is that you have a particular way of estimating the probabilities of rules through a maximum likelihood estimation. And that's by looking at a tree bank that has been annotated with all of these syntactic trees and looking at the structure of that and using that as a way to estimate parameters, the parameters of these categorical distributions that I just talked about. ,  if you have the probability of Alpha rewrites to Beta, then the way you derive the Mle estimates for that is to take the number of times you observe the rule. Alpha rewrites to beta divided by the number of times. But what I claimed at the end, and I didn't elaborate on because we ran out of time is that even with smoothing and regularization and all that, this approach doesn't work very . And it's because this approach doesn't model the context very  around the rules, and it's also pretty bad in that. The rules themselves are very sparse in the in the sense that there are many, many possible rules, and it's really difficult, even with a very large corpus. to get enough statistical to get enough data to have reliable statistical estimates of the parameters of these distributions.",
    "I mentioned this last time, too, but I'll go over it in more detail. we know in language that there are different syntactic positions within a sentence. with noun phrases, , in English you can have a subject noun phrases, and you can have object noun phrases. and it's very clear that the distributions of them are not the same. We mentioned the case of pronouns. in English you have differences between I and me. in English all the pronouns that you have to change their form, you have to decline them according to the case, whether it's a subject or object. or generative, and all that. But and in other languages they have a lot more of these cases. ,  if you speak Finnish, , then there's  15 or 16, or I've lost track of how many cases they have.",
    "But what this means is that if you have a subject noun phrase versus a object noun phrase. then they should not follow the same probability. Distribution in terms of what you Gen. All the internal parts that you generate with a Pcfg. And that can't really be captured by a standard vanilla. Because all you see is the Np on the left hand side. This  problem, this  issue is a less obvious one. But it's  even more important, which is, that certain classes of nouns are more likely to appear in the subject position versus the object position, and vice versa. And this is not just in English, but it might be a cross lingual thing as , just because of,  how the world works and what people tend to, how people tend to describe what happens in the world. in particular, in the subject position. the fillers of the subject position tend to be nouns that are animate.",
    ", , humans or animals, or  other moving objects or things that are perceived culturally to have some  , , intentionality or movement, or something  that. and it's more likely cross linguistically, but in English and cross linguistically, that they appear in the subject position than the object position. because there's a correlation between subject position and some semantic role of  entities that tend to do things or  have actions in the world and cause changes in the world, whereas the object position, there's a correlation. Again, it's not perfect that those tend to be entities that receive some action or suffer some consequences of something and  forth. And because of this, then again, you would expect the distribution of words that appear in these impacted positions to be different from each other. Is there not a way that we could create  a subject  phrase, and then an object,  phase and have different rules for all these to account for them. the question is for is for the 1st case, is there a way to create subject, noun, phrase versus object noun phrases, and account for those that way. And, in fact, that's exactly the solution that we'll explore. But we'll do it in a way that doesn't require linguistic knowledge. Because if we take that approach.",
    "then you really have to be  really good at linguistics and take many linguistics classes in order to decide which are the distinctions that matter and how to separate them out, and how to identify them in a corpus  instead, we'll take a more automated I'll call it , a more algorithmic approach to do this  that we can get a lot of the same effects without needing to learn too much linguistics. although you should learn linguistics . And there are also many other cases of obvious dependencies between very distant parts of the syntactic tree. Which which have effects on the word distributions, you would see. , here's a second problem. consider the subcategorization of verbs, but this time with modifiers as . we already talked about subcategorization of verbs, and that verbs can take different numbers of arguments. they can be intransitive and take 0 arguments, in which case  a rule  Vp rewrites to Vbd, vbd, just means  the past tense form of a verb get in the country bank. but you can add adverbs. You can add prepositional phrases.",
    "On the other hand, you can also have,  other subcategorizations. the verb eats, , is  unique or not unique, but it's special, because it can be intransitive, but it can also be transitive,  you can eat a sandwich,  eat a sandwich. and you can likewise add  adverbs and prepositional phrases, and  forth, ate a sandwich, quickly, ate a sandwich with a fork. You can even change the order. The orders around sometimes  quickly ate a sandwich with a fork. in this vanilla Pcfg, there's no relation between any of these rules in terms of their probabilities. They all have to be estimated separately, just by counting the number of times you see each rule in your tree bank. first, st that means you need a lot more data to get enough samples to estimate the parameters. And second of all, clearly, that's not really factorizing the problem in the  way. it seems intuitive that  that we rather we should have,  some probability that a verb is modified by an adverb.",
    "and some probability that a verb is  modified by a prepositional phrase, or it takes on some object. we would  to be able to factorize this probabilities . in order to both get more samples for each to estimate the parameters, and also just intuitively, it seems to make sense for this particular problem. the general solution for both of these problems of , not enough context and too much sparsity is  going to be the same. I'm going to present this paper. This work that  is really elegant and really nicely solve this problem within the this setup of probabilistic context, regrammars. really, the main problem with vanilla Pcfgs is that they make independence assumptions that are both too strong and too weak in different ways. remember what our independence assumptions is , what you choose, the relations between different random variables that you choose to model or ignore. We already talked about how Hmms. Make this really strong independence assumption that, , you, only model some local effects, but overall, it can still be quite powerful.",
    ", it's the same  thing here. with Pcfgs in that the vanilla version. It makes independence assumptions that are too strong, going vertically in the tree. if you talk about  subjects versus object. Nps, what you're saying is really that, , you want to model more context up and down the tree around where the Np is, because that's how you can figure out if it's a subject Np. , , the subject Np. Will tend to be closer to the top of the tree where the node above it might be an S. Node. Would be the way we've been drawing syntax trees, it'll be closer to the bottom of the tree. where its parent will be a verb phrase node. And  that means we need to change the independence assumptions vertically to model more of the context.",
    "On the other hand, this other problem, with  this sparsity issue with  too many rules is because we're making independence assumptions that are too weak horizontally. horizontally across the  hand side of a production. , we're modeling every single combination, and in every single ordering of  hand, side symbols, non-terminals, and terminals, as independent of each other in their generation. which is too weak of an independence assumption. instead, we, we can again change our assumptions and make a stronger independence assumption,  that we can more reliably estimate those parameters. what are we gonna do? , we're gonna take that what was already proposed. But do it algorithmically, which is, we're gonna split up the categories vertically and split up the rules horizontally. vertically, we're at, we can add more contexts by annotating parent categories. whereas before you might only model Np.",
    "Rewrites to a personal pronoun node, or whatever. I'm going to use this notation where I use this carrot. and then you also indicate the parents of that node. With a parent of S,  this would correspond to a subject position for a noun phrase. and in the second case you can talk about an Np. With a subject  with a parent of Vp. Which might correspond to the object, position. and then otherwise everything else works the same. think of this as  a new symbol, a new non-terminal symbol in your vocabulary of your Pcfg. And it has its own distributions, which are different from the ones of just the Np.",
    "And again, you don't need to learn any linguistics to do this because this can be read off from the structure of the tree in the tree bank. In your supervised learning procedure in the Treebank data, you can directly see what was the parent node. And  you can algorithmically add all of these annotations and expand your set of non-terminal symbols. And you're good to go. Yes, but having  many additional . Would that further decrease the individual probability? would it still work out? Or yes, that's a great point. it's always a trade off. this is again, this  complexity versus data expressivity  this, this expressivity versus  a ability to estimates  sparsity trade-off.",
    "the   we're making the model more complex because we're weakening the independence assumptions. you would expect that you would need more data to learn reliable estimates of the parameters. that we think this trade-off might be worth it because it allows the model to be a lot more expressive. With the parent S would have its own categorical distribution at some point. With the parent of S would have its own categorical distribution that sums to one. one thing you can try to combat this sparsity issue is to try to do some  interpolation or smoothing. that you still allow everything that is a empty node to share information with each other through some interpolation scheme. But but then you still model them as a separate probability distribution. this is the 1st sentence of the Penn Tree Bank. Very sadly Piravincin passed away, , , last year or 2 years ago.",
    ", time moves on for everybody. But , , but how this would work is , we would annotate everything. here the S node at the root. You can just annotate that its parent is the root of the sentence. and then np, here is parent would be the S. And then this Np. Its parent would be an Np,  you don't do this recursively, ,  you could do it for more than one level. But you won't , but you don't do it   Np, Np. and s, or whatever, unless you are planning to go 2 levels out. we should do this  bottom up. And  then you would keep annotating and hopefully, you  get the idea.",
    ", I'm going to stop here. You get the idea ? this, oh,  one thing I didn't explain is this is  just a tree represented in a different way using bracket notation. But since you're a computer scientist hopefully, you can recognize this is really just a treat . where all the children are expressed within the parentheses. ,  , you just look at what the parents category main category was, and you annotate it. And then  you can run the same procedure to do supervised learning. ,  this is a quick fix for the vertical problem. Then for the horizontal problem. , we're gonna pretend that every  hand side is a Mini Markov chain that we need to learn.",
    "for every non terminal symbol on the left hand side. It there's a on for the it's  hand side. It's  a mini little language model there, involving those symbols that would appear in the  hand side of a rule. ,  we're gonna we're gonna break down the  hand side of the rule when estimating its probabilities. , , whereas I'm going to add,  explicit start and end symbols to represent the beginning and the end of the  hand side. whereas before you have,  Vp rewrites to this entire sequence as a single unit which is atomic, which is not broken down, and you have  one parameter for its probability. I'm going to break it down. if I choose a Bigram model, then it would be broken down in this way it would be factorized in this way. the probability of this whole thing is  equal to the probability of Vp going to start advert phrase, and then Vp, going to advert phrase Vbd, and then Vp, going to Vbd, Np, and  on and  forth, until you get to the end again. Here there's conceptually it's the twist is that you're deciding to break down how you model this complex thing.",
    "But then, the technically, we've already covered all of these, all of this technique, because this is just an end ground model. And  you can again look into your tree bank and you have all of those trees that are already annotated. And then you can solve that learning problem. You have , you have to solve one n-gram modeling problem for every left-hand side symbol. , what is the star stand for? and how would this still ensure that the sum of probabilities for the for every Vp on the left hand side is one. How do you still ensure that the sum of the probabilities for everything for one left hand side symbol still sums to one. it's through adding the start and end symbols. You can show that if you add those, then you can get it to all work out  that it's still all the possibilities still sum up to one. ,  I'm going to skip this example, because here it's really not about  redoing the annotation.",
    "It's more about how you break down the rules. for  the interesting rule here is , Np, here rewrites to bt, jj, and n, ? here,  this notation here originally, we have Np rewrites to Dt. jj, and n. ,  , what we're gonna say is that this is  going to be equal to Probability of Np. And N. Times probability of Np. Rewrites to an end and then end. If you're doing a diagram model. you can also do a trigram model or a unigram model. You can interpolate, do everything, everything we talked about with N. Gram language models you can do. interpolate interpolating means that you have multiple models.",
    "But you don't trust any single one of them very much. you can take a linear combination of them. that you're  taking the in between of 2 models as your model. , you can do this, you can interpolate between a unigram model, a background model and a trigram model. And then you can take  I'll weight the unigram model with  a 0 point 5, and the background model with 0 point 3, and the tracking model with  0 point 2 or something  that. Yes, in this example,  it's saying the line , Mp, Dta,  the 3rd bottom . Are we saying that it's  a proper noun, and then you're choosing one of those 3 brackets, or it's in that order that it's  I got lost. can you say this again? that line doesn't mean that we have a proper noun that is followed by one of those 3? Or is it those 3 in that order,  the Gta,  the Jj, non executive.",
    "this is not a, this is not a proper noun. this is a common noun. Np is  here the original tree structure is, you have an Np subtree, and then within the Np. Here there are 3 words, and each of them has a tag. there's a determiner, an adjective, and a noun, a single, a singular common noun. then it would be  Npm in , at least it would be  a non executive director. I can also show it in. , let me show it in a more easy to read tree form. you have an Np at the root. and then you have a Dt, and then a jj.",
    "and then and n. and then below that, you have an a non, exact it is. that's what that subtree looks . all we're saying by doing the factorization is rather than modeling the entire thing all at once, we're not gonna do that. We're gonna model we're gonna  there's a start and end symbol. start dte 1st and then Np. and then the end symbol. That's that's just what we're doing. this process is called markovization. because we're making Markov independence assumptions. vertically speaking, that was called that would be called vertical markovization.",
    "Because you're adding ancestors as contexts. the vanilla Pcfg would be called a 0 order vertical markovization model because you're not considering any context. and then the scheme we just described would be considered 1st order. and then you can go further. You can have,  second order, 3rd order and  forth. But of course, usually in practice, people don't go that far because of your model is  expressive and  complex that it's you don't have enough data to estimate that. And then the other thing we're doing is horizontal markovization, which is breaking down the  hand side into parts. the standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter. The scheme we just described with the background model is called 1st order, and again, you can do any other order and can interpolate, and  forth. this was proposed 20 years ago.",
    "and then it's it was these are the results on parsing on the Wall Street Journal through doing the scheme. You have the vertical marketization. Order of , , V equals. V less than equals to 2 is  they have a scheme to only annotate some parents, but not others. You can do all parents you can do selected grandparents, you can do all grandparents. And then here's the horizontal Markovizations markup order of , here's the standard Pcfg with infinite context. And then here's the background model Unigram model. No, this is the background model. H equals 2 is a trigram model, and then H equals. 0 is  the unigram model, and H less than or equal to do is again, they have some heuristics to select, only some things to break down and not others.",
    "and  it makes a difference in terms of the pricing performance. the number here represents the accuracy of parse. How many constituents a parser trained in this way gets . And  with the standard Pcfg assumption that would be the top  of the table. because that would be infinites, Markov order horizontally and no annotations vertically. you would get  a 72.6 2 performance that way. That shows the number of parameters in the model. and  you can do a lot better than that. , grandparent annotation with  a diagram model. Then you get  a improvement up to  79, and your model is a little bit more expressive , and it has more parameters to learn, but not  that, much more .",
    "This has  a real effect on Parson. wanted to clarify about the order, because you described that,  the one we did the 1st order. the question is about, what does the horizontal markup order mean? , the one we just described is H equals. One is 1st order, which corresponds to a diagram model. you look at every pair. If you do H equals 2, it means you look at every triple. , I just want to know if from this data, if this  set oh, God. the question is about  whether this research sets some  gold standard methodology or protocol, or whether the results are specific to the Wall Street Journal. it is somewhat specific in that.",
    "each data set with each annotation scheme has its own characteristics and distributional patterns. you would expect that  a different hyper parameter setting of , what markup order to use would result in the best performance. in that sense it's specific. But the general idea,  still is, holds across different data sets, and  even in other cases beyond syntactic parsing. just for context, the current best parser, which is a neural parser, would be getting a score of around 95.  these are 20 year old results. I'm presenting this, not because this is state of the art by any means, but  it's really nice in terms of how they take these intuitions from what we know about language, and transfer it into algorithms that we can. And  that we can build into the computational side, into the formalism and into how things work. that's really nice, plus I don't have to introduce too much extra in terms of new technical approaches. And Gram language models which we already know about. Even in their paper they  go beyond substantially beyond 79, through additional linguistic insights.",
    "there they do need domain expert knowledge. They can do further category splits and merges to get to around 87 f. 1 performance. any other questions about syntax and parsing? Yes, you have the horizontal markovization. It also still works, even if we do. In Cnf with just the 2  hand side values. how does horizontal markup position interact with Cnf. that is a great question. Cnf is  doing 1st order horizontal markovization. If you think about it.",
    "although  in a slightly different way, and the probabilities are all messed up. , to make things distinct and clear, I would keep it separate from this. this method and this procedure is about training of the model and learning the parameters of the model. And then the Cnf thing is for the parser. It's for the cky parser and what it requires. you can do this 1st and come up with  a model. and then once you get to the point of  parsing. Then you can convert all the rules to Cnf. And then, since this is a Pcfg, you have to think about  how you do the conversion. and that might require a bit of thought.",
    "But what if we started with something that was already in Cnf. We have to reconvert it to. Not Cnf,  it's something if you start with something that's already Cnf,  you only ever convert things to Cnf in order to run the parser,  you wouldn't  there are some linguistic theories and some approaches to syntax, where they demand everything to be binary branching. which is one of the key Cnf assumptions. in that case, then,  you don't really need to do mark horizontal markovization, because everything is already binaries. , the process we  to convert the Cnf wouldn't apply on their probabilities, because you have to make sure the probabilities are mean. Oh, the question is, when you convert things to Cnf, then  that wouldn't be. That wouldn't apply here at. I would have to do some thinking about it, but I'm pretty sure there's a way to still convert things to Cnf. And make sure all the probabilities of the entire tree are preserved.",
    "but we don't know how to  I'll assign it to you as an exercise. You can work it out. I don't think it should be that hard. because if you think about it. And  each non-terminal symbol only is involved in one rule. I'm pretty sure it's . you can work it out if you spend a few minutes thinking about it. ,  where are we in the course? we've wrapped up structure and parsing. And  , we're gonna talk about another really big topic which is semantics.",
    "And this is also this is arguably super interesting for both theoretical and practical reasons. because semantics is about meaning. And if you're a language, technology, enthusiast, and you  you are. Since you're in this course, you want to do things in the world. and things mean things  words mean things in the world. we want to be able to have some formulation and representation of meaning in the world,  that we can represent and understand things and use Nlp systems to derive these representations. and then from there you can make draw inferences,  through some approach that's not necessarily only specific to Nlp in order to derive new conclusions and that can help you make decisions or do whatever you want to do. , semantics is super important because it's a, it's   the representation of something in the world  that you can  interact with the world with your technology. That's what I'm trying to say informally. , it's just a stretch.",
    ",  then let's start off with a philosophy of  language. if semantics is the meaning, if it's if the study of meaning and language, then we have to 1st ask, what does meaning mean? And here I'm going to give some. Very, I would say,  superficial answers. but that these superficial answers will at least give us some beginnings of  guidelines for how we might want to model things computationally. ,  what does meaning mean? I'm gonna give at least 2 answers. And the 1st answer is that meaning is about the relationship of linguistic expressions to the real world, or at least to some world. And the second answer I'm going to give is that we can talk about meaning in terms of the relationship of linguistic expressions with respect to each other. And we're going to start by focusing on the meanings of words.",
    "And this is called lexical semantics. lexical means, it's related to words. And later on we're going to start talking about the meanings of phrases and sentences, and also how to construct these representations from the meanings of words. These these ideas a little bit more. relationship of linguistic expressions to the world and to each other. can we at least have some examples. ,  I need to change my slides, because, , telephone is not a word that's used very much anymore, . But what does the word telephone mean. one way you can think about the meaning of telephone is, you can think about it as you're picking out all of the objects in the world that are telephones. and  these are called its reference.",
    "And then this is also called the extensional definition of a word. And note the spelling here extension with an S.  think of it as , the meaning of telephone is that it's a function. It's a function that takes in an object from the world. and then it gives you true or false. ,  here's a set of items that are telephones. And then there are many, many other  infinitely many other objects in the world which are not telephones. And that's the meaning of the word telephone. or if you can also, think about the meaning of words in different ways. suppose you had to define the word telephone to a 3 year old, or to a friendly Martian. one thing you could do is you can say, telephone, telephone, not telephone, not telephone, not telephone, but that would take a really long time.",
    ",   another thing you might do is if you're if the 3 year old you're talking to, or the friendly Martian you're talking to already speaks some language that you share in common. you would try to give a definition with other. , you might look up the dictionary definition of a telephone. thought there was a way to follow the link. ,  here's the dictionary.com definition of telephone. , an apparatus system or process for transmission of sound or speech to a distant point, especially by an electric device. You can characterize the meaning of telephone. You already know what all of these other words mean,  apparatus, or sound or speech,  that could be a problem. But , this is the general idea behind the intentional definition of a word, which is that you're  listing the necessary and sufficient conditions for something to be a telephone. you can , express it using some logic, in which case you don't have to rely on other language.",
    "in the , whereas before you had,  a function that  takes in an object and returns. Whether it's a telephone or not. It's almost  , you're still taking in an object. It checks a bunch of conditions of ? Are these necessary conditions satisfied? And are these sufficient conditions satisfied? And then it returns to recall? it's not by an enumeration anymore. But it's by a computation of  properties or in practice, we're gonna if you use a dictionary definition, then you're defining words in terms of other words. And  then you do need to know what everything else means, and you already have to have some  language capability to do to give this definition.",
    "Yes, what happens if dilutionary definition? what happens if the dictionary definition mentions a function that needs to be fulfilled? That's just part of the dictionary definition. It's just expressed that function is expressed in language that's all. The intentional definition is different to the fictionality. The intentional definition is  talking about the conditions, the necessary and sufficient conditions for something to mean something. you can express that using some computation, some logic, or whatever you can express that with it, can take the form of a dictionary definition. But they're related to each other. ,  this is not a new idea. this distinction between sense and reference.",
    "one of the 1st people to propose it. The 1st was frege in 1892  he was one of the 1st to distinguish between the sense of a term, and its reference where the sense is more  the intentional stuff about the. the, the properties associated with a word, whereas the reference is about the objects in the world that it points to. at some point long in the past they saw this bright object in the sky this thing that looks  a star. and it appeared in the morning and in the evening. And  then they called something the Morning Star versus the Evening Star. And then eventually, somebody was smart enough to figure out it's  the same thing in the world. And it was just Venus. And , it's not even a star. but But here you have an example of something where there's the same referent we  know.",
    "It's the same reference in the world. However, there are different senses, because the morning star might be that bright thing in the sky that appears in the morning. The evening star is that bright thing in the sky that appears in the evening? ,   they have different senses, but they have the same reference. And   the logical thing to ask is, can you have something with different reference with the same sets. and that I have to think about? ,  the over, especially over time. the Prime Minister of Canada. That has a certain sense. But then, at different points in time.",
    "You can evaluate that, and it points to a different referent in the real world. ,  that was our very superficial look at detour into the philosophy of language. , ,  we talked about how to relate words to each other, but also how to relate words to the world. But we can say more about how words relate to each other. this is the second  a main general area that people work in. And it's much more popular in the computational side  to work in this as , which is to think about how the meanings of words relate to each other. And  all of these, a lot of these. You can express these in some logic as . I assume that most of you have heard of synonyms, ? Things that mean the same thing.",
    "You can express that  in a logical way as  about how things that about  it evaluates the same set of objects to true, , ? And there are many others that you can do. You can express them logically as . But  I'm going to define a whole bunch of terms. and we'll just go through them. ,  one way in which the meanings of words can be related to each other is through hypernomy and hyponomy. And this is simply an is a relationship. If you prefer to think of it that way. , a monkey is a mammal. and Montreal is a city, and red wine is a beverage.",
    "these are is our relationships. the smaller thing,  the thing that denotes fewer things in the world is called the hyponym. and the word that denotes more things in the world is called a hypernym. You just have to remember these terms. The the way to remember,  is hyper is  higher up. it's  higher up in some taxonomy tree. And then hypo is  lower down. it's lower down in your taxonomy. , then, here I assume you've heard of these ones. These are synonymy and autonomy.",
    "synonym means that 2 words roughly mean the same thing. such as offspring and descendant and spawn. and or happy and joyful and merry. There are linguists and people who say that true synonyms don't really exist. and that there's no 2 words or expressions that truly mean exactly the same thing. I'm gonna close the door. there's some people who say that their true synonyms don't really or and true paraphrasing doesn't really exist. because there's always some slight differences in, if nothing else. The connotations of the words . It would be very strange if you go up to  a friend of yours and say, Oh, Hello!",
    "See how are your offspring doing  ? You don't say that ? you say, children in typical scenarios. There's a whole comic based on this. I forget the name of this comic, but the whole humor from that comic comes from rephrasing common, everyday expressions, using paraphrases that are supposed to mean the same thing. But they sound humorous if but if you do a synonymmy replacement exchange planet or something I forget, . But , we can pretend that synonymous exists. And it's  it's it's still useful to come up with this idea of some synonyms of words that roughly mean the same thing. Then antonyms and autonomy is words that roughly mean the opposite thing. it's really hard to say what does opposite mean?",
    "but  there's some dichotomy, or there's some spectrum. And these words appear at opposite ends of the spectrum or opposite sides of the dichotomy in some . , , synonym and antonym are antonyms of each other. or happy and sad or descendant and ancestor. they're they're opposed to each other. we'll come back to synonyms versus Antonyms, because it turns out that it's it's  quite difficult. It's very difficult to separate synonyms from antonyms using many computational techniques. , you need a lot of data in order to be able to do this. ,  this, this will be a problem for us later on. , another lexical semantic relation is something called homonomy.",
    "but different and unrelated meaning. the way to memorize this is to understand,  how to break down these words  homo, means same. Antonym, , , , it's  same. It's about same and difference. there are different kinds of homonyms. There's a homophone which again break it down. That's why telephone means telephone, because it's tele means far away. homophone here means same sound. If you speak a widely spoken dialect of English, . whereas homograph means same written form.",
    "But they're written the same. these are both subtypes of homonyms. They're both hyponyms of the hypernym homonym. This one is again very interesting and comes with lots of problems for computational linguist to solve, which is called polysemy. , here poly means many and sem means meaning. polysame means the phenomenon of a word having many meanings. And in particular, polysame involves multiple related meanings. and sometimes these are  ingrained and  natural to us that we don't even realize there are slight differences in the meanings of in these different situations. the example here is the word newspaper. It turns out that newspaper has many, many different senses.",
    "It has many different related meanings. A newspaper here can be a daily or weekly publication on folded sheets containing news and articles and advertisements   that physical object that nobody  ever interacts with anymore. A newspaper can be a business firm that publishes newspapers. You see how it's different. this newspaper is about the company. The 1st one is about that physical object with things printed on it. But it's somehow different from this 3rd sense. I'm not exactly sure how  the newspaper is the physical object that is the product of a newspaper publisher. When it began to rain he covered his head with a newspaper. here the idea here is that the 1st one is more about the concept of that newspaper  New York Times is a newspaper in the 1st sense.",
    "What other newspapers do people read? Montreal Gazette is a newspaper  as in  that. or firm that publishes, that produces regular articles for consumption. which is different from the business itself, but which is also different from the printed version of that Of that construct. these are all very subtly distinct from each other. and then these are all different from the 4th sense of the word. which is the cheap paper made from wood pulp used for printing newspapers. In the 1st sense, . , or  the 3rd sense, the 1st and 3rd sense are very close to me. they use bales on newspaper every day.",
    "these are all very subtly distinct from each other. But we just we don't even think about this  when we process language. because it's clear to us in context. What the intended meaning is in which sense of the word is intended. It can be very difficult to distinguish between harmonymy versus Polysemy. and I'm sure that the boundary is a little bit fuzzy. one example is with a word such as position. and there are many different senses here of position. and I'm sure we can reconstruct some chain such that, , it seems  they're related to each other. But then, at other points.",
    "It must seem  they're very unrelated to each other. the 1st sense of position given here is  the particular portion of space occupied by something. and then the second sense. Here is a point occupied by troops for tactical reasons. that clearly seems related to the 1st sense. The 3rd sense is  a position in terms of a way of regarding situations or topics. and here it might not be immediately obvious whether these are related or unrelated. To me it seems  they're  related. It's just that one is about physical location, and the other is about  a location in some more abstract sense. You're creating this abstract space, and you're  locating points of view in that abstract space.",
    "And then there's position with about the arrangement of the body and its limbs. Here's another one which is position to do with the relative standing of people in a society, again, is some  metaphorical extension of the physical idea of location to some abstract space. And then this last one is really interesting of  position as  a job in an organization  at 1st glance, it would seem to me that it's just entirely distinct from  the physical location, ? One is about physical locations, the other is about your job in an organization. But then,  it's through this  a again, it's through this metaphorical extension of that. You can think about jobs within an overall abstract space of  a bunch of jobs in the organization. And they're they're they're related to each other, and they're  supervisors and supervisees, and  forth. And in that way, then it makes sense that this is a metaphorical extension. I would argue that all of this is  these are instances of felicity. But sometimes it's really not obvious when you 1st think about and look at the words.",
    "and it's even possible that there's originally a chain of  of this reasoning. You can do to create this. the connections between senses, but then,  some sentences are lost, some senses are lost over time, and those sentences are no longer used in the language, and then it really appears to be unrelated words. ,  I would say that these are not always as clear cut as you might think. , more lexical, semantic relations. If you're trying to write clearly because I find that  If you're within a particular subgroup or subculture where you're used to talking about things in a certain way and taking shortcuts in your expressions, and it's really clear within that subgroup when you're trying to write for people from outside of that subgroup, you don't realize that you've made those shortcuts. And then  you start to. Then you use the same shortcuts and other people misunderstand you. But  some of these are very conventionalized, and it's  used by everybody  you say we ordered many delicious dishes at the restaurant  clearly. You don't mean that you ordered the plates .",
    "In fact, you don't get to keep the plates most of the time. You you ordered the food that comes on the plates. there's been a substitution there. you're substituting one entity for another related one another. One is related to the example we just saw. I worked for the local paper for 5 years. You don't mean that you work for the  the actual piece of paper, the print, ? You work for the organization that produces that paper. Here's 1 that's very relevant to us. Quebec City is cutting our budget again.",
    ", here Quebec City is not referring to the city. The city itself is not capable of cutting budgets, because it's just a it's a city. It's a concept of  some geographical location or something. Instead, here it's referring to the governments, the provincial Government. whose capital is located in Quebec City. The loony is at an 11 year low here. This requires a lot of background knowledge and real world knowledge to fully understand  here the loony is not the physical coin. It's standing for the Canadian currency. and what it means when you say it's at an 11 year low is that you're comparing it against another, its exchange rate with another currency,  the Us. these are relatively  standardized examples.",
    "But if you start chatting with  your friends, or in a particular subgroup, or you'll you'll develop these very naturally you'll substitute words for other related words without even noticing it. and then the other people will understand, and then eventually there will be  a norm. and then you'll have,  some way of speaking, that's very convenient and short. But outsiders would not understand. ,  this is one of the mechanisms in which these things form. , if metony becomes  popular that it becomes  a dictionary definition, does it stop being metronomy at that point. Oh, that's a great question. , some of these are  ingrained that they appear in dictionaries. at that point, is it a meta? Is it a metonym, or is it  a polysimous word?",
    ",  these are just our interpretations. And the way we impose  try to impose some order on the phenomena we observe in language. I would say it's both a metonym and also that word is felicimous with multiple senses. , there's a specific  synec of metonymy, which is  funny because it usually involves things they cannot say in class. it's a specific  autonomy involving whole part relations. here the metonyms in general is just about substituting with related words, but if the relation involves whole part relations, then it has a special name, for whatever reason. if you are in the situation involving sailing boats or whatever, and the captain says all hands on deck the captain doesn't mean just literally, all of your hands must be on the deck. , they mean your entire person has to be on the deck. or many swear words involve sensored body parts. or many, many offensive expressions.",
    "don't be a censored body part. The  relation is Hallonomy and meronomy. ,  we just mentioned whole part relations. yes, it has its own name, too. And then maronyms are the part. the way I remember this is , Hall sounds  hope. I don't know if that's etymologically correct. But that's how I remember it. and there are different subtypes. you can have groups and members  a class consists of students.",
    "You can have,  physical whole part relations  a car has a windshield. and then you can also have,  a composition,  the substance composition of something physical,  a chair, is made of wood. these are all whole part relations of different kinds. Yes, all of these definitions are not necessarily mutually exclusive. are all of these definitions mutually exclusive. we showed here that it's  not. Something can be meton  a metonym, and it can be a polysemis or . And , with Haronomy and Morontomy, is that similar to  we saw on the 1st slide about this. , , that's a great question. is Hallonomy versus meronomy, the same as hipernomy and hyponomy.",
    "while they the structure is similar, because it's it seems to be about  greater things and smaller things. They they are distinct from each other because hypernyms and hyponyms is about. here, a student is not a class, and wood is not chair and windshield. A windshield is not a car. it's about is our relationships  in programming, in object oriented programming. It might be something  inheritance. whereas a whole part relationships. It's more  the elements you put in a class. It's  it makes ups that thing. they are still distinct from each other, even though I agree that structurally you can represent them both with,  some  tree struck  to talk about  what's what's the whole?",
    "It's just a different style of hierarchy. ,   if you already have remembered all the terms yet or not. What relation does that exhibit? , yes, these are synonyms there and there. Yes,  there are, which is a some  homonyms. yes, and which one is which head of the forecast? Yes, the head is the whole thing. the head is the holyms, and the hair is the marinym enemy. And above you just said it. You just said the 1st 3 letters.",
    "cut as in hair versus cut as in bread. Someone anyone who has not. And you see how these are just distinct, ? cut hair is, you have something that's attached to something else, and you're shortening it . Cut bread is  you have something, and you're slicing it. which is distinct also, if sometimes you can tell if something is polysemous, if  another language, and you try to translate it into another language, and you have to use a different word. That's usually a clue that it's  it's  there are different senses. I  in French,  I have mixed. I don't know how you some. The word I would use for cutting.",
    "I don't speak French natively, but the word I would use to cut bread is  Tranche. But I don't know if you can coupe a bread. ,  that it's also business in French. and the other one is called. Yes, I put him great. And you've got one part of the midterm down. people have come up with the attempts to systemat systematically, annotate all of the senses of all words in the language. they started with English, because, . and  they came up with this resource, which is called wordnet, which is a lexical resource organized around this idea that words can have multiple senses. And in fact, they take it a bit further.",
    "They talk about these relationships, these lexical semantic relationships. and they organize everything in terms of these senses rather than in terms of the words themselves. the primary organization is in terms of senses. They give a particular name to these senses, and they call them syn sets, which are, which is short for synonym sets. the idea behind Wordnet is that in the language. Then you have nodes, and these nodes are called SIM sets, and they can be expressed in through many different realizations  with actual words. And then the edges in this graph correspond to lexical semantic relations between syn sets. and they have a different hierarchies for different parts of speech, but the one that's by far the most  developed is the one for nouns, and then the other ones they happen to. But it's not as extensive and not as , not as connected. I hope this works, there is a web interface.",
    "Oh, , it's still there. Give me a word that I can safely type here. here, , let's just focus on the nouns. in Wordnet, there are 6 different synsets associated with the word table. each of this, each of these think of it as  a concept with a certain way that you can realize it in language using some words. , , this 1st inset can be expressed using the word table or using the phrase, tabular array. And here's a dictionary definition of it,  a set of data arranged in rows and columns  C table one. Here is a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs. It was a sturdy table. The  one is a piece of furniture with tableware for a meal laid out on it.",
    "I reserved a table at my favorite restaurant. this is funny, because,  the second and 3rd sense is , it's the same physical object. But it's  a different sense, because the 3rd one is specifically in the context of a meal. , the  one is flat table land with steep edges. The tribe was relatively safe on the mesa, but they had to descend into the valley for water. that here cable is not the most common word used to for that sense, and it's usually a mess up the  1. This is  a this is a metonym which has become  conventionalized that it's  listed as a separate sense. it's it's  a blissimous thing. A company of people assembled at a table for meal or game. He entertained the whole table with his witty remarks or the arrangements.",
    "She sets a fine table room and board. ,  this is how it's organized. If you click on one of these  if I click on the first, st since that it gives you its connections to other synsets  it has a direct hyponym or a full hyponym. You can look at  Member maronyms. You can click on any of these and find,  other sunsets that this is related to in this network. ,  , table, there are subtypes of it that you can have a correlation table. You can have a table of contents and actuarial table, a calendar file, allocation, table, periodic table, and  on. And then each of these is its own synset. this was just a backup in case the things didn't work online. ,  if you're using nltk, you can  interface with this.",
    "and then there are some useful functions you can use,  you need to import it and download it. It's not that big, but it's still takes a bit of time. And then you can look up synsets in Wordnet. And , these are all of the synsets that these lexicographers have annotated in within this project over the past 2 decades or something. I'll just introduce the problem. And we'll continue this  class. arguably, one of the most obvious things you can do. Once you have those resources, you can try to disambiguate a word into which word sense was meant by that word. figuring out which word sense is expressed in context. His hands were tired from hours of typing.",
    "This corresponds to a synset with this identity of hand. which is a noun of the 1st sense, the 1st sense of that. and it corresponds to  your actual hand physically. The  example sentence is due to her superior education. Her hand was flowing and graceful. This represents another sense of hand. which is, it's not listed here. But it's  the handwriting, the handwriting style. And  one computational task that you might want to solve is called word sense disambiguation. this received a lot of attention in Nlp, which is to figure out which sense of a word is met in a particular context.",
    "And the general idea here is that you can use the words in those contexts to help you disambiguate. , in the 1st context, here. the fact that you see the word tired is  informative and typing. , whereas in the second context,  something  flowing and graceful would help inform that this hand is referring to the handwriting style. Another question that you can ask, or  you should always ask, is , why do we want to solve words and disambiguation? one reason might be, you think it's  in inherently interesting to figure out, how do we figure out the intended sense of all of these words that have multiple senses? But if you don't think it's inherently interesting. You can still try to come up with application oriented reasons to work on this task. it says,  , I said before, one obvious application is in machine translation. And that often different senses of a word should be translated differently when you translate to a different language.",
    ",   I'll stop there and then  time we're going to look at algorithms for word sense disambiguation."
]