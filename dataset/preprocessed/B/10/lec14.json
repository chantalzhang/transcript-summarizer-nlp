[
    "Jackie Cheung, Professor: But good evening. How are you do some fucking thing? It's usually the cost center. Oh, , let me fix the lights. we are going to continue our discussion today about lexical semantics. you remember what lexical semantics is? It's the study of meaning as they relate to the lexicon. lexical comes from lexicon, and the lexicon is  this idea that we have some mental storage of entries related to language which typically store words, but  also stores, phrases sometimes, or  also morphemes. something in our mind that holds words and their meanings, and also  their characteristics and behaviors. But ,  lexical semantics and last class we talked about.",
    "We started talking about wordsense, disambiguation. where  that you have a repository of words and their senses associated with them. and we talked about Wordnet in particular, which are primarily organized in terms of the word senses, in fact, which are these syn sets. there's they have funny names   this, where you have a synset. And then there are multiple words that can realize that sense. And then it's named after the most common word that realizes that sense  hand, or something. Disambiguation, is the task of figuring out which word sense is expressed in a particular context. here, in the 1st sentence, here. this sense is meant to indicate the physical hand. whereas the second sentence here is meant to indicate the style of handwriting  and where we left off last time was to say that in general we have this very good clue about how to disambiguate for the intended word sense by looking at the other words in the context that would help you to disambiguate it, which  makes a lot of intuitive sense.",
    "here, tired, is something that is a property of some physical part of your body. whereas in the second sentence, flowing and graceful might be words that indicate the style of handwriting. what if all the words in the sentence need context to disambiguate them? what if all of the words in the sentence will need context to disambiguate them? In fact, I would say that is typically the case. today, we're looking at each word in isolation and saying, given all the other words, figure out which sense. But really you can imagine a version of this task which is called all words word census ambiguation, where you try to do make all of these decisions together. it's   all of the words help disambiguate each other. It's rarely the case that you really can't tell which sense it is at all you  can figure out with  there might be one or 2 sentences you're not sure in between, but usually you can figure out in communication and commonly found communication. But  in some kinds of genres,  poetry or something,  it's harder to say.",
    "I'm going to go through some algorithms again, not state of the art algorithms. They don't use any neurotechnics. But the reason that I'm presenting them is that they give you the flavor of how you can use knowledge about the problem and your intuitions and build them into computational algorithms. the 1st algorithm is called Lusk's algorithm. And it fits under this category that I'm going to call heuristic algorithms, which is that you design a bunch of heuristics based on your knowledge about the problem, and you directly build them into the algorithm itself. Then there's another class of algorithms which I won't discuss, because hopefully by , you can imagine how this would work which is to apply supervised machine learning. to turn word, sense disambiguation into a classification task. hopefully, you can imagine how this would work. You have a word you want to disambiguate  hand. You collect a corpus involving lots of instances of the word hand, and then you ask annotators to label each of those contexts with which sense of hand is indicated there and then.",
    "After that you feed that into some classifier. and you, then you have a word census configuration system for the word hand. and this is definitely possible. But the downsides are pretty important there. it requires a lot of work to annotate word, sense information. And in particular, in this in this formulation of Wsd, I'm going to start saying Wsd, , instead of word science, disambiguation that would mean that. you need to train one model for every word you want to disambiguate. And  the other general strategy I'll illustrate instead is something is to use a minimally supervised or sometimes it's called unsupervised machine learning methods. And  we're going to introduce a new idea there using this simple algorithm called Yarowski's algorithm. which  will be generally useful in different problems that you might want to solve involving natural language or otherwise.",
    "we're going to look at that as . But first, st  Lusk's algorithm. Lusk's algorithm is from the 80 s. And  that you use dictionary definitions of a word senses in order to help you disambiguate in context. here are the steps of Lesk's algorithm. 1st you construct a bag of words representation of the context where you're trying to disambiguate the sense of some word. and then for each candidate sense Si of the word W. You calculate a signature of the sense by taking all of the words in the dictionary definition of Si. and then you compute an overlap between the context and the signature. and you select the sense with the highest overlap score. It'll be much clearer once you. We look in an example as usual, but the high level idea is each sense can be represented by its dictionary definition.",
    "and then each context is represented by the context,  the words in the context. And then you find the overlap and find the sense that has the highest overlap. suppose we want to disambiguate the word bank here which is underlined and the 2 senses that we're trying to pick between is the Financial Institution Bank versus Riverbank. Intuitively, the words in the context that will help us figure this out will be words  deposit and check, and closed because the chances are there. ,  you can have a deposit in a riverbank  it's in the sand deposits, but you won't have check at least . and  the riverbank can be closed to the public. But that's relatively rare compared to a financial bank which is always closing before you need to get there. ,  then there will be some statistical correlations between all of these words. And   your context, then, will be represented by your contexts. There are different choices you can make regarding how to represent the context itself.",
    "one choice is to directly use the word itself. then your contacts can directly be deposit and check. and before and close, and  forth. you get rid of stop words. or what you can also do is you can take the dictionary definitions of all senses, of all of these context words themselves as , and that gives you,  a more expanded set of words that you can plug into this formulation, and then you have an expanded set of words, and that becomes your bag of words in your context. B, ,  the and high level idea is, take all of the context. Words extract some information from them to come up with some beg of words, representation of it. then  for each sense of the word bank. we have bank one and bank 2,  bank. One corresponds to the Financial Institution Bank and Bank 2 corresponds to the Riverbank Bank.",
    "You take its dictionary definition. and then you put, and then you find overlaps between that and B, and likewise for the other one, and you figure out which sense has a higher amount of overlap, and then you pick that one. that's the high level intuition. Again, this idea of Lesk's algorithm, there are lots of variations you can play around with. And  each of them is a slightly different algorithm. And then you might want to use a development set to figure out which version is the best. , , how exactly you represents the context, and whether you use the or only the words or the words. Also from the definitions of all senses of the context words, how you compute, overlap. , do if you, if there are multiple words, if there's  a word type that overlaps multiple times. Do you count it as  multiple overlaps do you do any waiting in terms of the rarity of the words , there's something called tf, idf waiting.",
    "explored that in assignment one ? then you can use that you can filter out. There are all sorts of things you can do, and they each yield a slightly different algorithm. But the general high level idea remains the same. Yes,  what I'm thinking of is, say. should we think about this in the way that we're looking at the definition of deposit? See if bank shows up there and then we add that to the bank. the question is, are we looking at the definition of the deposit and seeing if bank shows up there and added to the out to the bank. You could do something  that, because  you want exactly the senses of the word deposit that relates to bank and not the other ones. But the simpler version just puts all of them there.",
    "I already said this, which diction also. Which dictionary do you use? That's  a very obvious initial choice. You have to make ? Oh, and do you lemmetize and all that  I'm gonna for the sake of time. I'm gonna skip this because we're slightly behind schedule in the lecture. But you can definitely check that the algorithm yourself. , you can run less algorithm using Nltk for the dictionary definitions. a wordnet, nltk and wordnet for the dictionary definitions and for the census. And then it tells you to ignore stop words and include example words and count them overlap.",
    "you can check this yourself. how do you do overlap? That's a design decision for the algorithm. it would  make more sense to go by token. , , you can compute the minimum number of times that a word that word appears between the minimum between in the b versus in your signature. That seems to me to be a reasonable thing to try, because if something overlaps multiple times,  it's more important. Can you do partial overlap by characters, or is it always by whole? Can you do partial overlap? It might make sense I don't know depends how you do it. it's a design decision as .",
    "this idea of  lemmatizing and counting lemmetized Lemma overlap is, is  implementing that idea. But in other cases the partial overlap might be  overlap in terms of  the prefix, which might not be that meaning ? And there was a question there. pictures to what exactly does the bundle here represent? if you can go further  back we have. And then you see the overlap between those. If I understand, high level question is, what does B represent something  that. It's an approximation of the kinds of words you should see in that situation where this word appears. here this word, this word bank appears in this context, and you need some approximation of  the  situation you're in, and what other words may appear and also,  there are things that are not directly mentioned, but that also might appear  money here doesn't directly appear, but may  it appears in the definition of check ? there might be additional context words for that particular topic that might appear.",
    "By looking beyond the words that directly appear in this context, but also other words that may co-occur with those words, that's  what you're approximating. You're just approximating a and a scenario or situation. and the kinds of words that might appear there. then you can get a better overlap score between that and the overlaps in the definitions of each of the senses. Yes, all the little dots would be words, and then the ones in color. ,  that was algorithm number one, today, we're gonna cover  3,  4 algorithms. that was algorithm number one for word, sense, disambiguation. use heuristics to define some ideas about  what you want to do, and in particular, to design these overlap scores to computes and do disambiguation. And algorithm number 2, we're going to introduce an idea called bootstrapping. this is a really cool idea.",
    "And bootstrapping, , comes from some saying , Pull yourself up by the bootstrap, which is that you should rely on yourself to get things done. and that's  the idea of bootstrapping as  in general. bootstrapping appears in multiple contexts in machine learning and lp,  don't get confused between them. the sense that we're going to use today is that you're going to use some weak signal to train a weak model. and then the model is going to pull itself up by its bootstraps by using the weak signal to iteratively improve itself. it's gonna , keep improving itself and get becoming a better and better model. it's a model that is independent and can do its own thing and doesn't need help. , except at the beginning of October. ,  here is the idea behind Yayrowski's algorithm. once again, you have to gather a data set with the target were to be disambiguated, such as bank.",
    "Oh,  the example we have is with the word plant. Then, here you need to do something first, st which is to give it. A little bit of information which is to automatically label a small seed set of examples. then you repeat the following process, you train a supervised learning algorithm from the seed set. Then you apply the supervised model to the entire data set. and then you keep the highly confident classification outputs to be the new seed set. and you repeat this step 3.  this step 3 is iterative, and this way you get. This is how the model pulls itself up by its own bootstraps, which is  it gradually improves itself through multiple rounds of training on automatically labeled data potentially, automatically labeled data. And finally, you use the last model as your final model. ,  let's look an example and go through the process to make sure this makes sense.",
    ",  we're going to disambiguate the word plant . our data first, st to understand what is are the possible senses. we can have the company set. The plant is still operating. here it means that plant is  some  factory ? The second sentence is, although thousands of plant and animal species. here Plant refers to the type of living organism that photosynthesizes and all that. here in Jarowski's setting there are only 2 senses. and  all of these contexts will involve the word plant in one of those 2 senses. Either the factory sense or the living being sense car and truck plant, manufacturing plant, animal and plant life fluoride, monomer plan, blah, blah, blah.",
    "we need to have a cheap automatic way to label all of these examples. in a very noisy fashion. the heuristic that Jarowski proposed in his algorithm is to pick one other word, just one other word that  will correlate and co-occur with that sense with very, very high probability. for the life form he picks the word life and for the factory you picked the word manufacturing. by doing this you can automatically label some of your data set involving the word plants. then, all of these cases are sense a, with  the living being plant. And all of these contacts have a sense B, because it's manufacturing plant. And then most of your data is  still unlabeled, because most of your data  doesn't contain either of those 2 words. Question just to clarify is the word selected to distinct between the 2 types manually selected? Is there some , or is it.",
    "the question is, to clarify, are these words life and manufacturing? you do need some  initial data. otherwise, you're in a completely unsupervised setting. And then you have to apply some  word sense induction algorithm, which involves clustering and all that which we won't talk about today. Yes,  in on the . The words being used are live and manufacturing are those derived from life, form and factory or . How are the words life and manufacturing selected? Are they derived from life, form, and factory? you have to know what are the senses you want to disambiguate in between. And then, based on that, you have to be clever and select a good seed word to create the seed set.",
    "the seed word is life in manufacturing yes, that's . The seed words here are life in manufacturing. , once you have automatically labeled data. The  step is to train a classifier. in Jarowski's algorithm, this was back in the early nineties when a different set of classifiers were popular. you train something called a decision list classifier, which we didn't cover in detail in this class. But , it doesn't really matter. You just need to train some  classifier. You could also train a logistic regression model. Here you can train a naive, base model.",
    "You can train whatever you'd . just train something as there are 2 requirements. first, st it has to produce an output  it has to say for these new samples, is it sense A or sense B, and also it has to produce some  confidence score. , , it might produce a likelihood score  a naive base would produce a likelihood score ? And the  does logistic regression. you just need to produce some  confidence, for even if you use Svm  support vector machines. If you remember how that works, you have some notion of  distance to the margin which you might be able to interpret as  a confidence score. you just need some  score to tell you how confident the classifier is. and then you can use that in order to rank all of the labeled samples and in terms of their confidence that the sample belongs in each of the senses. And I'm gonna close the door.",
    "I'm gonna shut the door. the reason this is really clever is that by this process lets you discover other words that co-locate  that are found near each of these senses. ,  1st of all, as you would expect all of the samples that involve the original words. Life in manufacturing should have very high confidence, according to this new classifier that we trained  because that was the cue we used to create those sets. But what is more interesting is that the classifier finds other words that co-locate with each sense. this classifier found out the word animal. that if it co-occurs within 2 to 10 words of the word plants. then it's likely to be of sense a which makes a lot of sense, ? Because that was the life form sense. whereas if it's equipment, then it belongs to sense.",
    "B, an employee also belongs to sense B, and  on, and  forth. and that there are also direct diagrams that it finds that correlates with each sense. And this is great, because that means this new classifier is  useful, even for new samples that don't contain the original seed words of life and manufacturing. How did how did we know, , that animal within 2 to 10 words just based on the log likelihoods works is that is that manual analysis. how do we know that this case holds. this is from the decision List classifier. this classifier gives you some interpretable features that says that,  animal within certain number of words is one of the criteria, with a locked likelihood for classifying something into one sense versus the other. But, , in logistic regression, we wouldn't have that. in logistic regression you would not have this directly, not in the same form. Instead, you would have a feature weights.",
    "then you can interpret it by looking at the magnitude of the feature weight, and whether it tends to correlate with one sense or the other. Manually, if the outcomes are ? And the question is in this step, are we checking? If the outcomes are correct manually. here, I'm showing this to help convince you about why bootstrapping works and why it might work. But in practice you don't have to do any manual checking in between the iterations. You still  want to make sure your model is not learning something very unexpected and weird. But but you don't have to do it if you just are run, if you're just running the algorithm. ,  then, in terms of how the algorithm goes. you take this outputs these outputs.",
    "what you do is you then expand your the samples in your seat set to add new samples, where your model was highly confident after one round. whereas before you only have the cases involving lice. you add additional samples, because the model was confident on them. For ,  a question mark to be a I'm wondering if that should be in the second round, because  it's , if it predicts question mark to a and then a to a again, then that's a confident. But this one is,  it's still just a confident yes. the question is  the question mark to a are we premature? And adding them to our seat set? I would say that no, because you have to add something to change the model   after one round, you have to add some new samples in order to get the  iterations model to be different from the previous iterations. , because you had some pre labeled samples, ? you had some pre-label samples, you use them to train the model, and then you label everything.",
    "why can't you just take the ones that are correctly labeled in the 1st round, and then in the second round, you can take the ones that are that were unlabeled, and it predicted it correctly both times. the things that you get correctly labeled the 1st time. That's the question mark to a. Oh, , , . question mark here means unlabeled in iteration 0. And then a means it's it was labeled as a after one round. After one round you take your highly confident labels and you add them already into the csat. it's possible to define some algorithm where you look at you have more history. And you might consider that in the later iterations. But in Yaroski's algorithm they only look at the current round. and also remember, in our setting we don't have any correct answers.",
    "We don't  have any correct answers while running bootstrapping  . It's not a supervised method. We only have the confidence scores. just keep that in mind as . We don't do any annotations in the middle of an iteration to annotate for the correctness of the samples. We're only looking at model confidence. the question is, are we going to use the model on all the data? Yes, we're going to use it on all of the data. But we're going to use the highly confident ones to grow the set that we pass into the training for the  round. that's the main idea behind bootstrapping.",
    "And someone had a question over here. Oh, , over there  last longer to look at. Can you think of your Oscis algorithm as having an automatic dictionary. ,  unless the algorithm, we're using the dictionary definitions quite directly. that was algorithm number one. This is algorithm number 2, Jarowski's algorithm. we're not using the dictionary definitions. we're, only we might be inspired by it to pick our initial seed words. And then throughout the process, the iterative process of training and retraining the algorithms. Then you're not explicitly constructing a dictionary definition.",
    "But you are finding other cues and other clues that help you label each sense. it's a little bit different. when this paper came out, it was really great, because it showed a very, very high level of performance on binary word sense disambiguation. that's that was pretty impressive because it didn't need a lot of human intervention. It just needed,  one seed word per sense per word. and in fact, it was achieved the same level of performance as the supervised methods of the time. which was great, because previously people  painstakingly annotated these senses on a lot of data, and then you train a supervised model on it. And you're asking showed that through this bootstrapping process that was not necessary. And then, in modern times, though we solve harder problems , and not just the binary word sense disambiguation problem. In which case, , then, the numbers drop because it's a harder problem.",
    "Yes, is this considered an unsupervised or sending school. Great question is this considered unsupervised or semi-supervised depends who you ask? It depends who you ask. I would say that sometimes people propose this type of approach, and they call it unsupervised because it doesn't need this huge labeled corpus. If your definition of supervised is, you need a huge labeled Corpus, then this is unsupervised. But other people, , in more modern times people may call it semi-supervised because the learning algorithm itself in the in the training in the loop is a supervised learning algorithm. It's just that you have automatic labels at the beginning. then, in that sense, it's it's slightly supervised. Whereas in that definition, unsupervised, would really be something  clustering. , is there a way to apply this to non-binary cases by treating this ambiguity between each case as a binary decision?",
    "Is there a way to apply this to the non-binary setting by treating each pairwise thing as a binary decision among the all of your senses. there's nothing here that is restricted to the problem being binary other than the classifier itself. you can either have a classifier that is itself non-binary. or you can do something else  what you say, which people do with Svms. there are standard methods to use. ,  2 algorithms done, and then  2 or 3 more to go for today. what we covered then  far is worth sense disambiguation. which is to figure out which is the intended sense of the word. But they're  a lot more tasks within lexical semantics which people have worked on, and which are popular. And  for the rest of today's class, we can switch gears, and then we can talk a little bit about detecting lexical semantic relationships overall.",
    "But the   is a short transition break. I invite you to all  stretch and . We'll take  5 seconds, and then we'll continue, , stretch. mindset shift, we're no longer working on Wsd, , we're working on detecting lexical semantic relationships. ,   remember last class, we went through all of these lexical semantic relations. a rabbit, is a mammal, remember which one that was. for the algorithms that are coming up, the goal is to figure out. how do we find hypernym pairs, or,  other words, in some lexical semantic relation. And this is called using Hearst patterns. that there are certain patterns in terms of the contexts in which word pairs occur.",
    "And we can use those contexts in order to help us figure out the lexical semantic relationship between words. ,  do people know what a Bambara dang is me, neither. Yes, it's a ball loot. that means that , is this bow or bow,  bow, because it's  bow and arrow. And it's plucked and has an individual curved neck for each string. What's a what's an instrument, , ? the idea here is that we figured that out, even though  most of us. including me, before I started teaching this course,  didn't know what a Bambara and Dang is. And that's because of this phrase, such as. ,  here we can identify that there's a hyponym hypernym pair because of the phrase such as,  here the hypernym is the bolut.",
    "and the bombarandang is the hypono. and that was also the picture on the title slide. That was a Bambara and done . The idea behind Hearst patterns is that if we can identify expressions  this  such as then we can discover hyponym, hyperym pairs automatically. And that way, if you're constructing Wordnet. you don't have to , just sit down and think really hard about what are hyponyms and hypernyms of certain senses of words. Because you can use a corpus based approach to help you expand that. that's   I should have started with that. That's  one of the motivations. For why you want to discover words in certain hyponym hypernym categories.",
    "it relieves the effort of just trying to think about this and think about what are the taxonomic relations or other relations? these are Hurst's original patterns. ,  Np, such as whatever such Np as blah, blah, blah, Np, or other. Np, np, blah and other np, np, including blah, blah or Np, especially blah. here's a quick exercise we can do together. for each np, can we label them as indicating the hyponym or the hypernym? I remember, the hyponym is the smaller one, which is  the more specific one, and the hypernym is  the bigger one, which is  the more general. is this hyper or hyponym? And then how about this case? we can just shout at me, Joseph.",
    "is this hyper or hypo? Yes, hypo app out here. Yes, and then this one? Oh, , is it clear what the stars mean? They're  from regular expression terminology. ,  here, it's 0 or more. star is 0 more because you can have a whole list. , how about this headbook? Do we need a vote? once you have these patterns.",
    "You then go to your corpus, and then you do a graph, a really expensive graph. and then you get all of these pairs, and then it might be noisy,  then  to do some extra work to  clean them a bit. But this is how you can get a lot of hypernym hyponym pairs. But then a reasonable question is . How do you find these patterns? curse was just really smart. And she thought of them. , but can we do an approach that allows us to be less smart, . ,  in particular, we can be Spartan in a different way, and  use our idea of bootstrapping that we just saw from earlier this class. What do you guys think, how would you apply bootstrapping?",
    "who were there, and what? Whatever the models in it works together? to repeat in case people didn't hear. What you can do is you can have an initial seed set of words. it's 2 words in a hyponym hyperym relation. Or  it's  you need more than that. But , and then you go into a corpus. and you find all of the context in which those 2 words appear, and you can get the context around those, you can extract the particular context around those 2 words. And then, once you have those contacts you can find which contexts are very common. and then you can apply those contexts more generally and extract a whole bunch of new words that could potentially be in the same semantic relation.",
    "And then and that's 1 iteration. And then you can go back and forth and back and forth this way and again, that is bootstrapping. It fits in the same general pattern. You have an initial seed set, which is some  cheap. semi-automatic way of labeling data. And then you have some more expensive step where you do a little bit of learning, and then you redo the whole iteration. , if there was human feedback on a set of samples that came from one iteration, would that make it a reinforcement learning algorithm? If there was human feedback after one iteration, would that make it a reinforcement learning algorithm. reinforcement learning is a learning paradigm for training a model where the assumption is that you have some reward signal that helps you modify the model's parameters. it's it's a different thing compared to what we're talking about, because what we're talking about is the general strategy of automatically label things, labeling things in order to derive some signal to improve the algorithm itself.",
    "Learning is more  the particular machine learning, optimization, paradigm or technique that you apply in order to change the model parameters. I can believe it's totally possible to combine the 2 of them together. The idea of bootstrapping with reinforcement learning. But it's just a different thing that makes sense. And and if you don't know what reinforcement learning is. It's not a major part of this course. but sometimes it's used in Nlp. and you can take on other questions about it or read up about it. why don't we just find the flavors, such as including instead of, we need to eventually buy. Because you just say it's Hi.",
    "You want to find hypoten and hypoten? I detect the keywords such as including that. why don't we just do that? ,  the question was, what we already have these words  such as or including, and especially , why do we need to go through this process of training a classifier doing anything else. this set might be incomplete. this set was  using our expert knowledge or our hearst expert knowledge. We figured out these patterns. but we have no clue, and we have no guarantee or clue that the set of patterns is comprehensive. I might miss a lot of other pairs of nouns in that relation. and  these set might not even be the ones that are the most specific to that relation who knows?",
    "by using some automated procedure, we can do a better job and better get all of the taxonomic pairs that are in some data sets. it's  to do a better job than using  a static lists of patents. And, in fact, you can even use the same strategy for other relations. not just hyponym and hypernym. , , using this idea. These Hearst patterns have also been discovered and used for other relations, such as between cause-effect relations. this is Gizhou's work in 2,002.  earthquakes caused tidal waves. The the pattern here is Np. then, np, which is the cause, causes the Np. But then, by using that as  your seed pattern.",
    "She found other verbs that also indicate causal relations , induce or give rise to or stem from, and  forth. ,  we see that we've seen  2 examples of applying the strategy of bootstrapping. in general, it can be very useful if you just don't have a lot of resources or time or data. But you, if you don't have a lot of  time and resources to hire experts. And  the problem, the phenomenon itself is not very  understood. that doesn't exist experts for the thing you're trying to solve. And in that case you can consider using some   bootstrapping strategy where you start with a small amount of insights to derive some training examples to start the training the process. ,  that was 3 algorithms. , we're going to cover the 4th algorithm or task area for today. I know it's a lot.",
    "But you guys are troopers. for the bootstrapping, what is the smallest size that you would need. By that do you mean  the smallest seed set or something? It assumes that you have access to unlabeled samples that are relevant to your problem. there,  that's 1 area where you do need data. It wouldn't work very  if you have a very small amount of unlabeled data  what's small there, you should just get as much as you can that you can afford. And hopefully, it's cheaper because it's unlabeled. he can scrape something from the Internet involving all of the sentences that contain words that are relevant to your problem. bootstrapping reduces the annotation costs. But it doesn't really reduce the data cost.",
    "in fact, you can even think of this bootstrapping approach. Even if you have labeled data. if you can use your even if you have a label data set, and it's only on  a thousand sentences. You can use that to train initial classifier. and then you extract a much larger set of unannotated data and apply your classifier then, and then you can still do this bootstrapping in that setting as . And , meaning you have,  all the data. You take a small sample, and then you use that. And then you put it back. Oh, ,  , ,  here's . in case other people didn't hear.",
    "what you're talking about sampling with replacement. Remember how I said that there are multiple senses of bootstrapping in statistics, and that's the other one. Yes,  you have to run a wst on the word bootstrapping. 1st to figure out in a context whether you mean the Statistical Bootstrap, which is for statistical testing. which is different from what we're talking about, which the other sample I remember it being used in machine learning as , yes,  . The other sense of bootstrapping is also used in machine learning for statistical testing. , but that's unrelated to bootstrapping in the current sense, which is a unsupervised or semi-supervised strategy program. ,  we covered hyponomy and hypernomy. And we  we cover  word census integration. And hyper and  flexible semantic relations of some types.",
    "But it turns out that the hearse pattern strategy doesn't work very  for synony. and it also doesn't work very  for autonomy. that's because there's a difference between the different lexical, semantic relations which change how they affect, how words appear near those words. we've looked at the relationships between 2 words that co-occur and their intervening words. here we have another canonical example. We have extinct birds, such as dodos, moas, and elephant birds. And  it's about the relationship between these words or noun phrases that co-occur in the same context. But there are some lexical semantic relations, such as synony. where the words and those relations don't tend to occur co-occur directly. and it is exactly because synonyms are supposed to be substitutes of each other.",
    "you wouldn't use both of them in the same sentence, because you just need one of them. And if that's the case, then our strategy of using Hearst patterns would be unlikely to succeed. because Hearst patterns depends on both of those words occurring in the same context. we here the we have. The synonym is between the verb phrases we have went extinct and died out. The dodo went extinct in the 17th century versus the Dodo died out in the 17th century. You would not be able to use Hearst patterns to extract a relation between went extinct and died out because they would not co-occur. it would be redundant to say the dodo went extinct and died out in the 17th century. I suppose you could say it, but it might be rare. instead, the other signal that we're going to use is  that the words that tend to occur co-occur with the target words themselves.",
    "the context should be similar. the dodo went extinct in the 17th century. these 2 sentences, what do they share in common? They share in common that all of the other words around the phrase that is a synonym pair are the same. , is this  similar on, , , slide 18. But you don't have any text there in terms of how we're using the context of 2 words and then looking at stop gap and boss? ,  you're using the context. that is, that is a shared part. It's just that the way you're using the context is different because the problems are the structure of the problems are different. And this idea is really powerful.",
    "and its most general instant, , realization is  what powers large language models. ,   really, all of this stuff comes back to this idea of distributional semantics. which is the idea that you shall know a word by the company that it keeps. the company that it keeps the company that one extinct keeps is  Dodo and 17th century, and  forth. And that's similar to the company that is kept by the phrase, Dido. that's how  that those words are related to each other. ,  the general idea here is that you understand a term by the distribution of words that appear near that term. and large language models based on predicting mass tokens in context are implicitly relying on the same notion that the context around which a word appears is useful to getting the meaning of that word. It's just that large language models do it in a much larger scale. in a different, in a slightly different way.",
    "with a different machine learning architecture. ,  but we're going to go back to the basics and look at some of the earlier versions and earlier instantiations of this idea of distributional semantics to  see the progression of the field over time. the basic idea here the simplest version of how this might work is to go through a corpus of text. and then, for each word, you keep account of all of the words that appear in its context within some context window of here,  arbitrarily, 5 words. for John, you would gather the counts of  birth was an English and  linguists. And then for and you keep doing this, and for  oops for a figure. You would look at the 5 words before and the 5 words after. and then you would accumulate all of those counts. what this means is that for every word it has 2 roles. either it's in the target position.",
    "when it's the center of your attention and you're looking at all the words around it. That's that means it's the target word. And you're accumulating the counts for that target word. or it can itself serve as the context word for other words. it's the context word, in the , the target. And then it becomes counted in the, in the counts of the other word. And  what you end up with is something called a term context matrix. where each row represents the counts that are accumulated for a certain target work. here, 1st and figure and linguists and 19 fifties in English these are all target words, and the representations are the counts where each column is the context word and the numbers here are the co-occurrence counts within your corpus. position of the ordering of the words in the context.",
    "Is there any way to incorporate the position of the and the ordering of the words? there are a variety of ways that people have tried to do this and done this. You could, , do some   a wait waiting,  that if it's directly  to it. You have a higher count compared to. you could define your notion of context differently. here we define a symmetric context of  5 words on each side, but you can instead define a context of  5 words to the left, only, or 5 words to the . or 3, or whatever you could define your context in terms of syntactic relations. another thing people have tried is they 1st do a dependency parse of a sentence, and they consider something in context. Only if they're in, if there's a dependency edge between them. , all sorts of ways.",
    "Is there a reason why we're including software? Is there a reason why we're including stop words? It's a design choice again. I would say that stop. They're usually useful to include , because it gives you stop words. They do still correlate with some kinds of semantic meaning. , if you see a word that occurs a lot with  in or at or something,  it's a time, word or location. No and it also correlates with syntactic properties. , if you see a word that correlates with B and A a lot, then  it's more likely to be a no. Is this going to be   a square in person,  a sparse radius?",
    "the question is on a large purpose. Is this gonna be a square matrix that's very sparse. yes, in the original version, yes. in practice, what people do, we'll get into some of that. But one thing they do is they don't keep all the columns. They only keep the  the . 5,000 or 50,000 most common words as context words, . And there are, we'll discuss ways to automatically compress this large term context matrix and do something smaller. there were 2 more questions, that 1, 2, , 1, 2, 3. And then we should stop.",
    ", , the column, vector, you can consider the vector representation, the context that's . The column vector is you can consider it as the representation of a word as a context word, . And then in the back. other main targets on complex one. how do we get the main target and context words? the target words are just the words you see in the Corpus that you want to model   every time you see a new word, you can add it to a new row for it. For the context, words, you can do the same thing. But, as was pointed out, this can get very expensive  often. What people do is they 1st pre compute a list of the most common words that they see in that corpus, or some other corpus. and then they keep,  the top K. That they can afford.",
    "and then they only count words that are in the context and that fit that are within that context. and the way to read this, , would be in the context of English linguist, shows up 21 times. , in the context of English language shows up 21 times according to the this notion of context. that we've counted things, this is useful, because  you can talk about. You can compute some that . every word is  a point  in some high dimensional space. you can start computing similarities and things between them. and the most common function that people computes to compare the meanings of 2 words is by computing their cosine similarity. and this is defined to be  if you have 2 word vectors. 2 rows in this matrix.",
    "they're the same dimension, ? that means you can compute cosine similarity between them, which corresponds to the angle between them, . and is defined to be the dot product of a dot B divided by the norms of the 2 vectors. And ,  this corresponds to the cosine of the angle between the 2 vectors. And this gives you a range of values. if you have 2 vectors that are pointing in exactly the same direction, then they have a cosine similarity of one. If they are orthogonal to each other, then they have a cosine similarity of 0, and if they're pointing in the opposite directions, then they have a cosine similarity of minus one. If your vectors are positive. It's easy to see that the similarity scores will be between 0 and one, because the numerator here is always positive. And then these norms that they always give you positive numbers.",
    "then, one naive thing you could imagine is that, ? we wanted to text synonyms. then, we'll just gather a large amount of text. We'll we'll find a feasible way to compute some version of that term context, matrix. And then we can compute cosine similarities between word vectors, and then we can use the ones that have high similarity scores and call those our synonyms ,  pairs of words with high similarity scores, and we'll call them our synonyms. that works to an extent. But it turns out that things are in practice some a bit more complicated than that. And the issue is that cosine similarity and word co-occurrent,  similarity of context in general. give you a lot more than synonymy. ,  any words that tend to share context, words will have high cosine similarities.",
    "certainly synonyms and near synonyms are one reason for words to have similar contexts. , , antonyms also turns out to often share similar contexts. ,  if you have something  this bowl of soup is blank. You could put hot there, but you could also put cold there. Or I saw a movie. It was it could be great or it could be. And for that reason a major problem with the approach of distributional semantics has been that it's very difficult to separate out the different reasons why words or phrases may share similar distributions of context words. and  that this means that we need to distinguish a little bit between the abstract idea of similarity versus the abstract idea of relatedness case. Similarity in this sense is specifically about taxonomic relations. semantic similarity is about taxonomic relations.",
    "It's about synonymmy and hypernomy and hyponomy. , , chair is similar to furniture, because a chair is a hyponym of furniture. however, cat is not similar to scratching post, because in the taxonomic tree they're very far apart. A cat is an animal is a living being. which is an a physical entity, and at that level physical entity might give you  a inanimate objects, and  man-made objects and then and then  household goods, and then furniture, and then scratching posts, or something  that, ? they're very far apart in the taxonomic tree. and  they are not similar, but they are related because you might often find context in which you see cat and scratching posts in the same context. abstractly, similarity is only about taxonomic categorizations and hierarchies, whereas relatedness includes anything that might be associated in some more general sense. and  good is related to bad. But they're not similar, because the kinds of things that are good and bad are very different from each other in any particular context.",
    "this is useful as a distinction to keep in mind. But unfortunately, people are often very loose with their language, and in terms of how they talk about things. they don't know about this distinction? cosine similarity is a measure of relatedness. , it's not directly a measure of similarity. It's a measure of relatedness, because all cosine similarity gives you is the cosine of the angle between 2 vectors, and they could be. they could have high cosine similarity for any reason. Are we missing some notion of the magnitude of the vectors? Are we missing some notion of the magnitude of the vectors. What does magnitude correspond to in this term?",
    "Yes, I would assume it's , if things 0 point 1 0 point 9. comparing figure and 19 fifties where the vectors are just , there's nothing really related. ,   the way I interpret that question is magnitude in terms of the vector magnitudes. it's not about the magnitude of it's not about the score, whether it's 0 point 1 or 0 point 9. I agree, it's difficult to interpret what counts as being close or not. But what is the magnitude of the vector here represents. , , it's just about the frequency. that matters because,  frequent, more frequent words,  have some kinds of semantic meanings which are in general different from less frequent words. But magnitude here is just about how frequent a word is, and cosine similarity is more. It factors that out in its computation.",
    "Context, matrix is this   analogous to  a word, that is Aha is this term context, matrix analogous to word embeddings? We will return to this in  10 slides or . What are there any measures of similarity which are of similarity directly and not relatedness? Yes, but it  presupposes that you already have your taxonomic tree. one thing that people have done which has been really popular in the past is, if you already have something  a wordnet. then you can use wordnet structure to define a measure of similarity by looking at where, in the wordnet, hypernym, hypernym tree the 2 concepts are, and how closely they are related to each other. there's a whole bunch of metrics there that you can use. But then it depends on your goals, ? Because,  a lot of what we're doing today is, we assume that we're trying to construct something  a wordnet or construct some  hierarchy that makes sense and defines  the relationships between concepts. Whereas that approach you already have that.",
    ", we should move on. ,  how do you  evaluate these vector space representations? Because it turns out that, , you can think of this high dimensional space where all of these word vectors live as a vector space. And I'm not sure there is  a really great way to do this. But I'll talk about the things that people have done. And then we can critique this if we'd . the problem is that word. Vectors have no objective inherent value that we can evaluate  if you had 2 vectors for the word linguistics,  is point 4.3 negative point 2 better? Or is 0 point 2.5 point 1 better? That's not a question that we can answer.",
    "All we can do is we can evaluate the similarity of vectors to each other by similarity. Here  some  computation of a function to get a score. I don't necessarily mean taxonomic similarity. we all we can do is we compute the similarity of vectors in that sense to each other, and correlate that against some gold standard. And there are many possible choices of this gold standard. But one option is to just directly ask people. ,  that's what Finkel Stein did in 2,002. I don't know how I feel about this evaluation, but I'll present it . And then you can tell me why I'm skeptical about this. what they did is they got a list of word pairs.",
    "their data set had 353 words. And then they went and asked people how related these 2 words are to them and to, and they ask people to give a score between  0 and 5 or something  that. And then they took the average. And that's  their gold standard. this is the average amount  this. on average, this is  people's judgments on , how close these 2 words are. And then if you have a trained word vector model, you can get your word vector to also give you similarity scores with cosine similarity. And then you can do a correlation. You can compute a correlation between them to check whether it's the case that the higher the similarity, according to your word vectors, the higher the similarity, according to people. why do you think , I'm not super fond of this evaluation.",
    "Yes, it could depend on people's interpretation of the word. It depends on people's interpretations of the word, . Did you have a different point? Or was it the same point? , and the average is paid for each vendor for the average before the decision from each readers to all the that's a great point. you're getting at the  things. the average is  over the different annotators that definitely makes sense. ,  you're getting at one of the another key issue, which is that different annotators might have different ideas of what a 3 or a 5 means  in the scale. when you just take the average across multiple annotators  it's really not clear what that's doing. you might want to , readjust and calibrate each annotator against each other.",
    "Another issue could just be  a word might have multiple senses. how are the annotators handling that? That by considering the most similar sense that they can think of. In that case it might depend on  which sense they happen to think of at that moment. or are they  averaging across all the senses. , this is not a great evaluation for many of those these reasons. but people still do it because it's once you gather this set of annotations, then you can run it. And you can just check what scores you get. , , there are other methods for constructing better word vectors. I won't go into them, because I know I've overloaded you already with too many algorithms.",
    "Today I'll just speak at a high level about that question about word embeddings. ,  word embeddings are these trained vector space representations of words to predict words in context. we're up to  just a decade ago word 2 vec. Was a really famous model that did this. this was   the beginning of this whole deep learning revolution and nlp, . I would say that word 2 vec is   the moment when deep learning based methods, or at least neural network based methods. These days you might not consider that deep learning really, took over Nlp and show this promise, and everyone was really excited about it. the idea behind word 2 vec. is that you set up a neural network architecture where the neural network. Architecture is trained to predict words and contexts just  a large language model.",
    "But this was  a simpler version. and there's some intermediate hidden layer in the neural network that you interpret as the word embedding. You interpret that as the representation of the word just  in the simpler count based methods. You have a target word vector which is  the row vector in the term context matrix. And then there's a lot of math and structure behind it, and a lot of  hyper parameters to tune, and all that. to answer the question from before the key results. Was proposed, it made a big splash, and the key result was that,  a year later. researchers,  Omar Levi showed that the skip ground model in particular, is  equivalent to a version of the count based model, where you apply a singular value decomposition to that. there's  these matrix compression matrix factorization algorithms that you can run in order to compress the matrices and try to preserve information in the matrix. And you can show mathematically that one version of the word 2 Vec model is equivalent to one version of the matrix factorization algorithm which I thought was a really cool result, because on the surface they seem to work very differently.",
    "One method seems to be about  creating this neural network architecture to predict words in context. the other is about counting words and then running some -known matrix factorization algorithm to compress that to , to compress that in order to extract some statistical information. And it turns out they're doing the same thing. that was a surprising and cool result. And it also just goes to show you that,  having different views of a problem and thinking about things in different ways. can really get to  get you very far. it turns out that thinking about things in terms of  a neural network. it exposed a lot of other hyperparameter decisions. And it was a lot more flexible. And that's why you can find those hyper parameter decisions that give you very high levels of performance.",
    "Whereas thinking about things in terms of matrix factorization, those tweaks were not obvious in the matrix factorization setup. , there were lots of things that you would have to do in the during the factorization and compression process to achieve the same high level of performance as with the Skipgram based approach. , , , I thought that was a really interesting historical , not,  historical, not historical, . I lived through this, but  an interesting   a result  in the in the literature about  how the progression of the models went. ,  I, , I don't want to introduce any more today because we've gone through too much. what do we do today? the 1st half of the lecture was about word sense, disambiguation, ? given a word, figure out which sense was intended. and we covered the less algorithm which is  a general , which is  a heuristic strategy to do this. And we also cover Yarowski's algorithm.",
    "which introduced this very general idea of bootstrapping which is applicable across multiple contexts. And then, in the second half of the lecture we talked about how to have methods that can extract word pairs that fit some lexical semantic relation. for some of those relations  hypernomy and hyponomy, you can use the context surrounding the word pairs in order to identify that relation. but for others  synonymy and autonomy. then you cannot do that. And  you have to use the con. Word contacts themselves is the signature to . Figure out the some representation of those words. and that introduced a whole bunch of other confounding factors involving relatedness versus similarity. And then this whole distributional semantics approach with  the training of these term context matrices and  neural approaches after that.",
    "that's a summary of what we covered today. And we will continue  class, and I will release the  programming assignments  hopefully by the end of today."
]