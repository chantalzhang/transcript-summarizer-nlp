[
    "David Ifeoluwa Adelani: North campus. be discussing Engram language model. I will skip the review of last time. I'm on Slide 2 . the view of language,  far,  in the in the past lectures we have examined. How can we model language  far in context of test classification? That is what we have been doing. But you would also notice that we do not fully care about the context of the of the words,  we only treat them as bag of words or bag of engrams. And then are we also recording the lecture? Some people are able to join.",
    "there is a there's a zoom table. far, we have only cared about how to. how, how to do , on downstream tasks. And then we have examined. , could you please not join on Zoom? If you are already in class at least difficult here. or you at least mute yourself. ,  common feature instruction strategies destroy much of the information in the text message. Because we use  bags of engrams. in the in the case of knife is very easy to explain.",
    "We don't really care about the order. We just care if the world East in the what do you call it? If a word is in the sentence. we can use and grams instead of considering one word, we can consider 2 words that follow each other. But we don't really care about what is the context we don't really care about does it depend on another world? And here we are going to try to move to something more realistic in how do we do language modeling  that we can also factor in the context or the previous words before the word we're trying to predict. does that word depend on previous words or not. the 1st thing we want to look at is how words are distributed. What are distributed are following what is called Zip's law. and then it's a very simple law, which I'm going to show you is just trying to say, what is the relationship between the frequency of the word and the ranking.",
    "Then we're going to examine more statistical language modeling based on n grams, and then we'll do or maximum likelihood by relative frequency, and then we'll touch on how to evaluate our language model. One of the most popular technique is what is called a publicity. Until today publicity is still being used even for the big models. I want to remind you of what is the word which you  know? Award is the smallest unit that can appear in the context. I forgot to say, we are at Slide 4.  what is the smallest unit that can appear in an isolation. , but the problem is that it's not very easy to determine sometimes. Especially if you move from one language to the other. If you have a world  football? Is this 1 word or 2?",
    "Is it foods plus ball? Also we have peanut butter? Is this one word or 2 words, and if you translate it, , to German, you have something  Foosball or endless person. and then that's that's the. It's always difficult to have a clear definition on how to separate. it's even gets more complicated if you move to some languages when slide 5, and when we consider the Chinese example. and I don't speak Chinese. But if you're a native speaker. You can clearly see what we're trying to say in the 1st world we have  3 words. And but the problem is that they are not really separated by space.",
    "This is more character separation. And this has caused a big problem in Chinese. And I'll be because it's a 1 of the 1st acts they have to tackle is how to do word segmentation in Chinese language. and if I speak of the language will be more obvious, because things are  stuck together by characters. But do, is there a notion of war segmentation that you can  agree on for Chinese? We go to autography, word types and tokens. One of the best way to separate your words in the sentence is just to use what's the space. by  this is very obvious. But when I started working on Nlp, this seems to be very trivial, ? But there are some times where you have things  a poster fee, and then you have to determine how do you separate them?",
    "if you will have a word , and then most of the time. The apostrophe and tea are often separated from the can. Because also the apostrophe and tea appears very frequently in our corpus on how do you even ask a question? How many words are there? , the cat sat on the mat. and then we have the following word, tokens, which, if you count every instance. then you are going to have,  6 words or 6 word tokens. But we could also differentiate between token and types where types are unique occurrence of the world. And here, in the 1st example, cat marks on Saturday you have 2D's, and then and then, when you're trying to say, what are the, what types that will  go to your vocabulary. and then you can separate.",
    "You can ignore every other occurrence of the. And then you have  5 war types. There are some cases that are not very clear. In Slide 7, we have the Fuzi cases. and the question is, how do you separate? Let's assume you want to build a vocabulary. I have the word  run and runs. They're very similar, all , but they are. They're going to have different ind index in your vocabulary. The question is which we also treated.",
    "the second class is, how can you merge them? And one of the ways you can match them  that you don't have very similar worlds that are talking about. The same thing is to do what's called  lemmatization and stemming. And by , who can remind me about. If you want to combine, run, and runs. What are you going to do? Would it bematization or stemming? Or , if you want to convert Rons to run. What are you going to do without a dramatization of studying? to be honest, it could be both in that case.",
    "For for the 1st example. Because then we are just  truncating the suffix for the second one. Is that is that scaling or lemmatization? What it says of democratization, because happy and happily are different. , this gets more tricky. if you want to go to the root word. , what if we have  happiness? Happy if you have happiness if you want to cover happiness to happy. To be honest, I'm not even sure. Again, if Apple will cover, because we even have different  rules.",
    "We have different kinds of stem up. we treat this as a stem. and then we also have the way where you could combine and normalize words  realize. Is it the American way of writing it versus the British way of writing it. I'm doing the same, for happily we just remove either way. Yes, you just  to chunk it. The last few words at Carras. and also you have cases , apart from,  what is British, English, or American English. We can also have a way of capitalization. if you take  the sets of all rewards you have we, which is, which is a calculator.",
    "W. And smaller W are going to be separated. if you before you do before you compute your. but before you build your vocabulary, if you lowercase everything, it's better than if you don't lower case. Similarly, you have things  fragment and fragment on our cases. In slides number 8, we can compute what is called word frequency, or we, the popular word that we use is what is called time frequency, where you want to compute the number of words in your compost. S. And for this to happen. how many times does, , in this example, how many times does the word cat occur in the card sat on the mat. This is very simple, it appears just once. but you could also have different occurrences. how many times does the word d or call in the sentence, if you are still following.",
    "I haven't lost you 2 times, and for the relative frequency you have to just normalize this by the size of your sentence. which  is correct from the world. for you to do any calculation. You always need a corpus, and of course you could also have several copperam that you will combine to form one gigantic text. Let's assume you want to analyze words in general in the English language. The question is that what  corpus can you use? What is the representative coupons for English? If I throw you that out to you? What is the representative corpus for English? If you want to analyze all words in English.",
    "Someone says the Oxford Dictionary. another person will say it can be dictionary. But you could also just have a very big compass. A very good example that can be representative will be  the English Wikipedia. English Wikipedia has been contributed by  many people. and  it has  the gigs of text that is big enough to  capture different cases in English. Of course this will not capture all the social media slants and words that have been used. it's not the best, but it's still one of the one that is representative enough or English. we have other examples that are previous previously used  a brown corpus. And the Wall Street Journal couples are  many of us.",
    ", this one is very simple. You just want to connect the frequency of what type to the rank of the world type. we are saying, the frequency of war type is inversely proportional to the rank of what type. and it's very easy to see  in slide Number 11, you have the rank which is one, and then you have. That's run for one also is connected to the world with the largest number largest occurrence of the largest frequency in our campus. and  by  you understand the relationship inside 10 that will stay. The frequency is inversely proportional to the rank. I believe there's no question. And then, if a word is infrequent also, what will happen? That's what is going to have a lower rank.",
    "you have the word deed that is around one. The word that is infrequent might be around 30,000. we can generalize this to see if mandible it's law which here they are  adding a constant. if you have this inverse relationship, that means you have to introduce a constant. you have to introduce a con, a constant that  relates F to R, and that is B,  that should be clear in slides 12, and after that for us to have this Mandel Broth's law, which is a generalization of this law. We had 2 more constants. which is a file and B, but typically what is being used is that B is equal to one. And some people just say, this file is depending on the language. because it's  you are trying to model how every language, what is the a law that gets every language. And I'm going to show you in the  slide that it.",
    "It differs for different languages. because different languages have different structures. and if you add a log,  F equals to P over r plus 5 raised to power B, and you are going to have this notation? We should reclaim from the law of login. what was the motivation between adding the additional parameters  this? , that's a great question. initially, when they did this, since there is work perfectly for English. and then when it moved to other languages, you find that? And then we have something that can analyzes that if you can provide the  file and . B, you may be able to model any language.",
    "Practical implication of this most war types are very rare. And the problem is that some words we only appear once, why? we appear 10,000 times ? we have to know that, and that's why we have a long tail and sometimes this long tail are very important that if you miss them you will not be able to. There's no way to recover this if you missed that. a small number of waters make up the majority of waterkins that you see in any compost. these issues will often cause problems for us in terms of designing models and evaluating their performance, as we will see. this is what I was talking about. Thanks for the other question, where we are trying to see what  equation works for different languages. 40% of the words appear once in a corpus.",
    "and then you have a handful that come very, very many times  you have a lot of stop words, a lot of preposition that appear  many times, and you have other words that appear fewer times in Bulgaria which I don't speak. You have same number of water types, or call for fewer tokens. which is very different for that of English. and in lips you have 80% of words appear all at once. That's on the Sgm end. And why is because the world is morphologically rich. Do you have an idea of morphology? , you have a root word, and then you can append to the left or to the   how that some language is, just keep, append it to this suffix, just skip. adding more words to the , and then it becomes longer, and there are many, many languages  that in the world, , subun languages in Africa, , you just keep adding. And then for this  thing, it's really difficult to.",
    "you will just have many words that are purchased very few countries. here I want to talk about the phone to work ratio, which is very simple to how many phonemes are in this sentence? For the 1st one,  it's clear that we have 6 warnings. and then you can also divide it by one. and for the second one Cantone east. You have  7 phone names, but here you only have what? also we have several, what types! And similarly for the French word. And you see that for the initu, too, which is very, very different. Here you have 8 phonemes.",
    "but it seems  it's the same work. But when you pronounce it. Why do we count words when you come to what's very important for  building? What is called Lovewood model? Because statistical language model is just by counting how many words appear in your couples. And then you use this to build probability distribution. ,  this, what frequency are important for test justification, as you have seen in previous class. When we are trying to classify whether something spam or not in information retriever, and in  many tasks  remind you the task of language. The task of language model is , you're trying to predict what is the probability? you could also see it.",
    "As what's the probability or trying to find what's the probability of the entire couples. if you are asked to put it in  world, given this context, may we add a little. you find that both alarm and accidents are  good consternation of this one. to view this problem realistically. , where we define a random variable W equals a small W. Given a context C,  the random variable here is W, which can take any word in our dictionary. the smart of you represents that the value it can take. And then we have a context. What is the probability of the world equals Lamb. Given the context merely at a little. you can compute this probability, but sometimes, when we are working on this, we often ignore this random, variable which the statisticians may not be very happy with us about.",
    "But anyways, we would do. We do this , equivalently. You can say, language model is the probability distribution over a sequence of words. Just imagine you have a sequence of words. Combine all the text in English booking period into a gigantic text and then complete. What's the probability over all the words that have been that's not worth it. And you can decompose this probability. Using this chain rule of conditional probabilities which I can explain. using the book and maneuver of the country with all those things abandoning CPU of w. 1, even W. 2 W. 3 WN. And then we can also decompose this from W to.",
    "you can multiply by a little confusing. I need to review then, this topic that I realized $4 ready to, . If you apply the chain rule continuously, , you just use the law of conditional probability. probability of the blue one. and you can also reverse it because it could be probability of WN. and then you can also do it this way. the way our group expressed, this is  not the best. because using the law of probability again. using a lot of probability. Express the hospitality on document I know you are.",
    "You hang on as . hold on your phone 2, 1. in bidirectional oil, you can do it both in the forward direction and the backward direction. , this is  the this is the better formulation going down here for what you are describing. Yes, per student, the product term be from I of one to N, and then WI, plus one that I minus one. did I miss something it's starting from. for the 1st formulation, I did, that is correct. But for the second formulation we have to modify. , you're going to see it should be W.  , it's with Wk, and then you start from one QWK. Minus one to your hospital.",
    ", example, a good language model should assign a higher probability to a grammatical string of to a grammatical string of English. , you're wearing a fancy hat and a lower probability to ungrammatical strengths. , fancy you our hearts be wearing sometimes you can have something that is. has a good probability in terms of language model, but it's not very grammatical,  the example here in Slide 21.  also the length, the length of the sentence, and the reality of the words in the sentence affect the probability. you all agree with me that the probability of I had the would be greater than the probability of. because I heard they might, I would appear more frequently because it could have more combinations than I had in cake. what would language model capture. They do capture some linguistic knowledge. Interestingly, they also capture many facts about the world, because if it's a correct fact. then it should be able.",
    "It should be easier to predict. , if it's a correct answer because it will occur, it will co-occur with the context, multiple times in different sentences. and then you cannot refer to this as a good fact. And that is why, a longer model  Gpt. It's able to predict a good fact and a correct spot. but sometimes it can also produce incorrect facts. just because it seems to it, as if it has a good probability. and I  believe this is the topic of your assignment to  determine if the fat is true or not, and then we across fire for it. The longer model can also predict an incorrect fat. Also you can use it to capture what is the correct syntax, and also to it can also be used.",
    "There are many applications of language model. When you use your mobile phone. you prefer that it suggests what's for you when you're typing. It's an example of a simple language model which is sentence completion. automatic speaker automatic speech recognition. that the way I'm speaking  is supposed to transcribe the text, and if it's a good ASR model one of the models, there is a language model after you have really acoustic model. You also need a lot of model to be able to go one after the other to generate what is the  thing after the purpose. ASR is a very a big application of language model and also machine translation. , typically we find a solution that maximizes a combination of tax-specific quality. or they'll add the language model probability.",
    "If you are just shooting for tax specific quality. But also that task may not realize. , you may not be able to generalize across many tasks. and if you just focus on having a very good probability distribution, it may not work  on different tasks. there's a trade off for this. building models giving lots of data from real world. that we want to find what is a set of parameter that describes the data. in terms of status quo nlp. if we are looking at what we are talking about the parameters. It just means you have to estimate every single probability for that text.",
    "And if it's a neural network that means you have to find a set of parameter that  can model everything, the entire text. ,  the model is, what is the probability of the world? and you can mathematically,  calculate this for either a unigram model or a diagram model. how do you build your statistical language model the 1st thing you are supposed to do is to gather a large representative training purpose. Something  the English will give you there and then you learn the parameters from the couples to build a model. If you just want to build a statistical model. That means you have to estimate all the probabilities. , all the unigram probabilities, diagrams, triangle probabilities, and then use this to build a language model. That means you have to learn a parameter theta that  is able to predict the  word given the focus work. and once the model is fixed, you can then evaluate it on the text data.",
    "what's the probability of the world? we make use of this assumption, which is called conditional independence. It's very similar to what I've explained on the Board for unigram probability. What is the probability of the  word given the context for Unigram. Because it's a unit of distribution. What's the probability of every single word? , there is no quantum system. What is the probability of the  one given one single. What is the probability of Wn. Minus 2.  if you say the cats is sitting on the mat.",
    "And then you want to predict the math. The last 2 words that proceed are . The last 2 words of the format. that is  a trigram distribution. And if you say the cat is sitting on the mat, if you just say what's the probability given just one previous word. That means you'll say, what's probability of Mat? I have an exercise for you to processing very soon. an example here is, what is the probability of cuts that's in Slide 29. That will be the count of all the times. Hats appears divided by the count of all the words in your compos.",
    "that's a unit gamble of religion. the byground probability of cats are given. D,  that's the count of how many times the cards or call. Don't change the order, please. How many times they can cut or come  that. That means you have D, followed by what the cat divided by the Count of B. That's the background distribution and the diagram probability of the cat given P to D.  that is, if you want to model feed of the cats, that will be the probability of cards. Give 1 50, and then that will be the count of the feeding cat. How many times you have feed the cats  that occurring in a couples divided by the counts of feeding. all these are what we call the embali estimates.",
    "Do you have questions on this? Because you wouldn't need this for the exercise. word data in no context, no context. , that's the unit distribution. Oh,  you're counting one types. How many times the occurs multiple times. I thought this was    in part of space. the web  would be  a type. It depends on what you're counting. The type are in your list of words.",
    "unique words that appear in your in your couples. world of the world, just our own. that's what they can occur 10,000 times. , you're just saying, how many times? , this is your exercise. Can you compute a unigram and background language model, using the following sentence. let's start with the unigram. what is the probability of that's what is the probability? I just have a question about unicrons. if we count the probability of each word based on all the words in the purpose, aren't you always going to put those V on every word, because we use no context and use the most frequent words, which is the , but usually for a standard language model.",
    "You combine the diagram, the diagram, and the unigram together? I know that when you tokenize we can tokenize into that. the matrix into the model. you said, if you are using a tool, a library, . Oh, for the, for the assignment. you're supposed to generate from . No, no, for any level, , , once you have the data. Yes, you have to tokenize it , and the choice of tokenization can be. You can split it word by word, or you can do some single one. And then once you have those grams, you have the matrix, and then you feed that into the model.",
    "But you don't really have to do anything ? You don't need to do it because it works. , because you're focusing on just classification. Here, we are focusing on a more general language task. what is the probability of that? What's the probability of is. what's the probability of what else it? you have completed your Gram language model. you want to estimate the probability of that's that ? it should have been easier if I'm projecting.",
    "But  you have probability of that. That is probability of is that  you are going to take them 2 by twos. That's that, that is that is. is not, not, is, is not, not, is that? It's it's it is . And then, if you want to compute the probability for every unique types there and graph types, what are you going to get. What's the probability of that? How do you compute over 5? And  by 5. ,  it's the probability the count, of the number of times you have that which is 2 times divided by the what the count of that . And how many times do you have that?",
    "You have that 5 times ? And that is how you have 2 over 5.  what's the what's the background probability of that is 2 or 5. . 1st word, or the second word. in the case of that is, are we dividing it by the occurrence is, or the occurrence of that? , it's not divided by the occurrence of the 1st one before the second. The 1st word these words, it depends on the second, the second one. The second word always depend on the 1st one. and then for that is, that will be the count of the number of times you have words that's East. divided by probability, or the count of the times you have that ? And that will be again.",
    "Also 2 or 5. , what of is this? what happens whenever the denominator  that's in regards nothing. happy end of the sentence. at the end of the sentence, if nothing is happening,  this is more  a unigram. you can skip the last discarded, because there's nothing. 6 out of 5 not out of 6. No, it doesn't affect the calculation. It just affects how you group damage to tools. We always have,  another end of sentence token that will always support that. what's the probability of is one over 6. , probability of is that 2 of us should be part.",
    "Probability of, if not 2 of us. A probability of it's eat one of our 2, and the probability of it is. it's that we are having is always something. Is this, is that, and say,  included. But I felt  a factor identified   as the vibrant is, and then and 7 isn't  an additional option with  one or 6 or 12.  you mean for the background? that if you have the end of the war token. there's something where the cover which is back off. if there's nothing you can compete again, you can back off to from diagram to Unigram. and then you can back off from diagram to buy ground bylaw. But this basic knowledge is  enough for this lecture before we make this more complicated.",
    "I don't get a question. if you add end of end of sentence token, you already added for the entire purpose, and then you compute it. I  all of you are more focusing on what's the problem, what we have go to the  one. , our goal is that we want to just estimate every probabilities we can estimate from that text. that, can you estimate the probability of every word. every background and every diagram that  in this in the couples. that is the goal of language here. ,  the last sec section is that once the model is fixed. we can  use the model to evaluate it and test the data. ,  normally, if you if you're trying to test very similar to test classification.",
    "You need to divide your data set into training data and the tested data. But you could also have the validation data. for us to test , if you remember, what we are trying to do is to estimate what is the likelihood of generating the testcos. after you have feed the data using parameter theta on everything on your training purpose, how do you compare? What is the probability of the test compost. and how do you evaluate it? there are 2 ways you can evaluate. 1st we can use cross entropy. and then you can also use publicity. And there's a nice connection between cross entropy and publicity.",
    "Because publicity is just too. what do you call it? race to power the cross entropy. consider some random variable X distributed according to some probability distribution. Of course, we can define the information in terms of how much certainty we gain from knowing the value of X. And the more information that you need to know. This is what we want to measure if the information is very trivia. there's no need to waste a lot of that beats to encode that information ? That is the idea from information to. If you are observing a likely outcome, less information is gained.",
    "observing the word gay d. But if you are observing a more rare information, more information is gained, and we can  use this. Go back to information theory to  develop a metric that we can use to estimate how much information in bits. information of X in slide 36 equals the log of 2 of one over B of X. This is by definition, I'm not adding anything. and entropy is just what is the expectation? What is the expectation of Ajax the expected amount of information we can get while observing a random variable. an expectation of Ix would also give you this formulation, which is, you sum over all the pis log of lot of Api. and if you do, the math one over pi will give you negative. If you had a log within. you're gonna have logarithm of 2 raised to power 0, minus log of 2 pi, and then this will give you negative.",
    "I have an example in Slide 38. The plot of entropy versus the coin tos fairness. If there's maximum fairness that means you cannot easily predict what will happen. Then you can have a very high expected value of the information which is the entropy eye entropy. And then, if it's biased, then you can easily determine if the probability will be 0 1.  there's no need to. sweller of this and code information. Do you want to quickly explain how information and the logarithm are related in pivots. the information, why is it locked to a 1 over? you are you talking about slide? 35, 35 or 36. .",
    "no, it's the 6 30.  that's the formal definition of what is information in terms of this is the definition from information to Europe. Why is it one over the 1,002 in the law? Or we don't need to know. you want me to talk more about this. I'm just taking this from definition. Of of how information is defined. And then we're trying  trying to borrow the concept into Stats square of you, . here in the cross entropy, . that entropy is the, of course, entropy is the minimum number of bits in order to communicate some message. but if we already know the probability distribution.",
    "The message is drawn from then we can compute what is the cross entropy. and the cross entropy is also defined this way. But the basic idea is that if you want to know what's the cross entropy between the true distribution P and the model distribution in Nlp, what happened is that we don't know the true distribution. We already train a model cube which is our model distribution. And we want to use this model distribution to determine what is the probability of the test. The starter does it , and because we don't know the true definite, we don't know the true distribution of fee. we come up with an approximation of what is the how to estimate the cross entropy which will be defined as minus one over N log of 2 of the probability of the model you have trained. at test time, what you are going to use is you're not going to use, say. the true distribution at all, because you don't even know it, and you all focus your attention on estimating having a good estimate on the model distribution. And if you do this over if you are able to, , you're trying to say the if you have a good model.",
    "The model distribution should be close to the true distribution. But since you don't know the true distribution, you use the model distribution instead of the true distribution. And then you compute the cross entropy based on this. and from this we, this leads us to publicity and in publicity. We are just saying that this will be 2 ways to power. Some people formulate it differently, which would be if you assume log of E. This can also be the exponential of the cross entropy. You have questions that's the end of luncheon. Every caffeine is beyond my control today."
]