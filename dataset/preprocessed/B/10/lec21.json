[
    "Jackie Cheung, Professor:  the captions button disappeared. I'm trying to look for it. today, we're going to talk about automatic summarization. And then the link to the readings by Nikova and Mccune. There's a different link that I posted on Ed. please use that because  this link no longer works. this week we're gonna look a little bit at automatic summarization and text generation. And we're gonna look at some of the ideas that have been important these fields. And we're going to look at how we got to the current state of generative AI from this line of work. for today, we're going to focus on summarization.",
    "we're going to look at what is the definition of automatic summarization tasks? What are some important sources of signals to determine whether a piece of text is important and should be included in an automatic summary. And then we're going to look at some simple, basic methods from the past for document summarization. focusing on both single document, summarization and multi document summarization. But focusing on the extractive case. I'll define what it means, what extractive summarization means. And then, finally, we're going to end by talking a bit about summarization evaluation. , automatic summarization is the task of turning some source text into a summary. This,  appears all over the place. in news articles, , it can appear because, you can think of both the title of a news article, and this is  this is called the Byline.",
    "in traditional newspapers, when people still printed out newspapers, usually articles they have  this short one or 2 sentence  it's called a byline in between the title and the body of the article, and that sometimes appears, and it's  a very, very short summary of the rest of the article. and it exists as   that everything is online, that it also exists in that you have a short paraphrase of the rest of the whole article, and then you can decide, based on that, whether you want to click on to that article and read the rest of it. And even more than that, there are many other applications of summarization that are  in deployments. This is one that I find quite interesting from the recent literature. it turns out that doctors  spend most of the time not necessarily directly interfacing with patients or doing anything clinical or medical. They spend a tremendous amount of time doing paperwork. , because they have to document every single patient interaction they have to write down  the summary of  what happened and  what they, what course of action they prescribe, and then they fill out billing codes and stuff. I gather it's even worse in the Us. Because of all of the insurance stuff they have to handle there. And  that's a big overhead.",
    "And that's a big cost to doctors. And it turns out that  there are technologies out there for a summarization that claim to do a good job in summarizing these clinical notes. And  this is by nuance. This is a system by nuance called dax copilot. And then they've released a product in that space. and then you can check out whether their demo and  forth. here in this demo, there's a video and all that's very flashy, and it says episodes of hypertension. And  it highlights specific phrases, and then it also generates the overall paragraph,  the patient reports something or other. It's a deployed contacts for a summarization system. Technically, I should say that.",
    "I consult for Microsoft and Microsoft owns nuance. But I'm not at all involved with nuance or with this product. , but this goes to show summarization is in deployment, and it'd be nice to talk about , what is summarization. What are is it just one possible task? Or is it  a range of possible tasks? And because, , my claim is that summarization is just. It's not just one thing. , summarization is  a family of tasks with many different flavors. I'm going to call them . And that's because summarization that very general notion of figuring out what's important and distilling it down to a short passage, a short piece of text that's very under specified.",
    "There are many different possible contexts in which you might want to do that. And  we can discuss this and analyze this. And it's possible that these different contexts will lead to different possibilities in terms of the modeling techniques that you should pursue. And  we can break it down. summarization can be a particular summarization task might involve specifying the purpose of the summarization system. assumptions about the source text assumptions about the format of the outputs. and also assumptions about the users who will be interacting with the system. , if we think about the purpose. there are  multiple purposes you can have for a summarization system. One is to be informative.",
    "an informative summary tries to be a substitute for the original source material. and it tries to express as much of the important points as possible that were in the original source. what would be an example of that? ,  do people use  close notes or anything  that when  reading novels or trying to redo things for English class, is it allowed? , hopefully, it's allowed you can read it  as long as you don't  copy from it and plagiarize it,  it's allowed. or just , , you, you want to read things. A short version of things to understand . what are the important points in the original? Then that's that's considered informative. Indicative, that's the second possible function.",
    "It's an indicative summary provides a link to the source text to help users decide whether or not to read it. in the example I showed earlier with the news article with the byline. That's arguably more of an indicative summary, because that short piece of text is there to help you decide whether you want to click and read the full article. or, if you have , search engine results. And then the search engine results comes with both the link, and, , , a very short extract, that of the some relevant part of the article. With respect to your query, that's also arguably indicative. And then, thirdly, a 3rd purpose might be a critical summary which provides an opinion of the source text. none of these is really they're not disjoint  a summary can have some mixture of all of these different purposes. , the reading assignments that you're doing for this class. That's arguably a  summary of the original article.",
    "We're we're asking you to read. And it  contains a mix of these purposes. I would say,  mostly informative and  critical. ,  a summary doesn't have to serve only just one purpose. But it can be some mix of them. And  different techniques that you want to employ, depending on what you're trying to do. In fact, arguably, you could say that  these different purposes,  some of them, you might not want to automate. to me, it's a little bit It might raise concerns either ethically or otherwise. about automated systems that provide some  critical summary. potentially , depending on the specific context , do you trust an automated system to do a fair and unbiased job?",
    "In providing some critical summary of some material. , another way in which summarization systems can differ from each other is simply the nature of the source. there's there are all the obvious things  what is  the domain of the text you're working with, what is the genre of text you're working with and  forth. And I'm going to restrict myself to texts for the purposes of today's discussion. You could also summarize other kinds of material. You can summarize  structured data  weather data or something you could structure, you could summarize  figures. And I don't know other kinds of content. But let's restrict ourselves to text summarization. But  another basic way in which systems can differ is whether you're doing single document or multi document summarization. And this, this factor has a large impact on the types of techniques that you should consider and the kinds of things you want to do.",
    "The reason for that is that in multi-document summarization there are additional issues you have to handle. If the multiple documents are by different authors, then chances are that there may be conflicting information or contradictory information between them. Sometimes it's just  you have a news event, and then more things happen, and then you get more information. It might be that if you're doing, say product review summarization, then different users might just have had different experiences and different judgments of a product. And  you have to handle that somehow. But have you seen,  , on Amazon? They  have,  short AI generated. it's AI generated reviews of  the overall user opinions. also, there may be redundancies between different documents. And this is  both useful and a problem to handle.",
    "If you have multiple people saying the same thing that may increase your confidence and trust that the thing that multiple people said is  true or is important. On the other hand, when you're in the process of  generating the outputs. If you have redundancy on the input side, then you don't want to say redundant things on the output side. you have to watch out for that and be careful about that. And finally, you just might have to do extra work to combine information from multiple sources, from multiple documents. , a 3rd way in which summarization systems can differ can be in terms of the format of the expected output. This is categorized is whether you have extractive summaries or abstractive summaries in extractive summaries. that you copy and extract parts of the source text. whereas in abstractive summaries  that you need to synthesize the contents and then produce some potentially novel text for the outputs which requires more advanced semantic analysis and natural language generation. Was the 1st thing that the field worked on  determining which sentences should be important.",
    "And it's  still an important task to work on for its own sake. some domains require the exact wording to be preserved. Can you think of examples of this , why might you want extractive summaries, even if you have the capabilities to do a good abstractive summary? , , for quoting research. , what about other very high stakes. In that case, in those situations you don't want to accidentally put words into other people's mouths,  to speak. and  you might want to directly copy and have it exact wording as it is in the extractive summaries outputs, and even then you can get into trouble, because then, if you lose the context,  there are all these things  about. Oh, you're quoting me out of context,  the context is not what I meant, that  thing, but at least the problem is slightly better in the extractive case, whereas in the abstractive case, then, people could really just say that claim that you're miss misattributing what they said. Yet another way in which summarization systems can differ is in terms of the their assumptions about the users. in the field there's this assumption that there exists something called generic summarization.",
    "where there's no particular point of view taken, and that the what, the goal here is to take the source texts. author's views and preserve them, and just , restate them in the output summary, and try to preserve that as much as possible. more and more I would question, and  others would question whether such a notion of a generic summarization system  exists. Because what counts as generic and what counts as  neutral,  neutral, without adding your own point of view. could be very tricky to define. this is more of an artifact that there's some   homogeneity in the population of researchers and the tasks they work on. and that you can make an assumption that  for the general public audience. they might have similar views on what, in terms of , and also similar states of background knowledge that  that you can generate a -called generic summary for them, and then it. It makes sense to them that you're not injecting too much of your own opinion. that this term exists generic, but then  it's worth taking it with a bit of a grain of salt, even though lots of people use it in summarization and  elsewhere.",
    "you can make other assumptions as  that are clearly not generic. you can have user tailored or query focused summaries where the summary reflects a particular specific specified goal or priority of a user. here is where summarization starts to get  the boundaries between something  a summarization system or something  a question answering system starts to get blurry. or even  an information retrieval system , if you type in a search query to on your favorite search engine, and it gives you some summary outputs of some, some of the of, say the 1st link that it returns. Then that seems  a summarization output. But it is query focus. And then it's starting to feel a bit  an information retrieval type system as . And again, within the past few months I've noticed that this has been integrated to in  the popular search engines. Or you might just decide to generate different summaries based on the background of the user in the in the medical space there is a corpus called up to date, not a corpus, a resource called up to date. And , if you're a doctor, you would know all about this, or if you're in med school, or whatever.",
    "the idea behind up to date is that it's a resource where they have reference. , except it's not a wiki. it's   a encyclopedia for doctors, and that they have,  a documentation of  all of the common things that can happen  syndromes and diseases, and  tools and diagnosis and things  that, and then they can look it up. And those articles are written specifically for doctors. And  they contain a lot of jargon. They assume a lot of basic background knowledge in medicine. And it's it would not be that useful, for  a layperson. and that up to date also contains another version of those same articles that are written for lay people  a lay audience,  patients,  to read and try to understand what's going on. but without as much technical terminology and driving. And  you can imagine that, based on the assumed background information.",
    "You might want to give very different presentations, and  to define terms in one case and not in the other, and  forth. And  that is interesting as . that's a type of another type of user tailored summarization system. or , if you have a situation where you have there , I said, there's an evolving current event, and then  one set of assumed knowledge in your readers, and then you can. You might want to just generate an update summary that provides what has happened since then. ,  then, those are all of the different ways in which summarization systems can differ from each other. That's why I claim that summarization is not a single task. And and there's a lot going on and depending on the specific settings. You might need a very specific  model or technique. , I'd  to talk a bit about how do we  determine what content is important?",
    "even before we look at any particular systems. And let's also think about  which of these are easy to quantify and measure with the techniques that we've discussed in class, and  which ones are slightly more difficult. And  how that informs the choices of how the field has developed. ,  1st of all, importance is a very tricky thing, ? All of us  finds different things interesting. And and we might have different judgments about  what kinds of information, we trust, and what we find important, based on what we trust, and  forth. And   the direct way to estimate importance in  a person , if it does exist in  a person as opposed to, if it existing in text, would be , you have some   scanner that  you directly aim at someone, and you can read their brain signals and see  how much it activates or something I don't know. And then you can use that to say, . this piece of Texas would be important to this person, and this piece of text would not be important. However, of course, that technology doesn't currently exist, as far as I know.",
    "there's some way to approximate it, using,  some  fmri scan or something  that I don't know. But even if that's the case, it's not practical. we cannot directly measure it. instead, we have to try to model importance in a different way in text. using  other heuristics or other cues. That we think would correlates with this. here are the major classes of signals that have been explored in the literature. And and again,  it's a lot of the reasons that people choose. These is that they're  easy to compute or easy to , and not. And  some of these, this is not super costly to compute.",
    "the 1st type of signal is the distribution patterns of X within the source text material. And then there's some  something to do with discourse structure. And then also the relation of the text with the background domain inquiry. ,  the 1st is distribution patterns and texts. The basic idea is that we can approximate importance by looking at a notion that I'm going to call centrality. It's it's it's the idea that the most important theme or the most important information. You  say many things about that topic, and you might say many related things about it. And  that means that you can pick up on the cues of this distributionally, because it means that in the simple case you can just look at word identities, and  that,  the word appears again and again and again in  in a passage. And that gives you a clue that , that's an important topic. A more sophisticated version of this might be that, , you have a lot of a bunch of text, and then you can use your favorite embedder.",
    "You can use word embeddings you can use. some  transformer based model or Lstm, whatever. Convert every span of whatever granularity you're interested in into a representation. And then, once you have that just visualize in your mind that this all exists in some high dimensional space. But , for visualization purposes , imagine  a 3D. And then  all of these fans are these points in the space. and then you pick the points that are centrally located, and those would be the spans that if this assumption holds would express the important information. , oh, we're  looking , what does centrally located mean  that you can come up with computational definitions of it? , you can define it as what is the average distance between this point and all of the other points. If this has low average distance to all of the other points in the source, then that's  located.",
    "If it's very far, then it's not . we want to remove , stop words. Do we want to remove stop words? Quite , , assignment one. that's all I'm going to say. You can  make arguments either way that you can, if it's some notion some ways of modeling content would be. It would be such that it matters, but  depending on your stop list and depending on the specific nature of the data set. you should not remove it. let's take a look at  a news website. oh, I  have to reshare the screen for the people online.",
    ", I'm gonna pick a news site. I'll say, BBC, , , is our assumption  true. Is there any happier news? , this is a happy news article. After a hundred years salmon have returned to the Klamath River, following a historic dam removal project in California. what is the main topic here? we assume that it has something to do with salmon and river. I thought it would highlight all the cases. that salmon appears a lot  in this article for obvious reasons. clearly, this article is about salmon, and  salmon appears a lot, but also related words will also appear   fish appears, and  trout appears, and  forth.",
    "What would be something that would be . Less important is, is there any sentence? , you can read it. Are there any sentences here that  you consider to be less important. Here it's arguably less important. It's art is talking, explaining about why dams are not good for fish, because it blocks fish migration, and how dams were demolished, and  forth. But here, this is a subtopic, too. , if you have,  a slightly longer, a bigger budget for your summary. then  you can say a little bit about this. But here there's a subtopic to do with dams.",
    "Here's another piece of information that's a bit less important about  negotiations and failed negotiations. I'm sure it's important to the people who are involved. But again, if we assume this notion of a generic summary exists. For the purposes of this article the process of how this came to be about the dam removal with negotiations, and  forth, that might be to them it might not be as important. And you can look at this because  if you embed this in some information space,  because this is about negotiations and more  political issues. It might be a bit farther away from , what's more centrally located, which is about  salmon and fish and ecological preservation. it works for this article. ,  hopefully, the  screen is being shared. then the  queue would be discourse structure. 1st we talked about centrality.",
    "the  queue that I claim is often used in summarization systems is discourse structure. And what  by discourse structure. Is that the way that we write passages? And there are expectations that we have. About how a piece of writing is written. , how a piece of writing is written, because we're we've seen many instances of that type of writing. And and sometimes we're explicitly taught to write in a certain way. and   we can use those patterns themselves as  to help us inform, find important information. we just saw this example of the salmon article on BBC,  there. just ,  it could be an article about sound, and it could be article about anything.",
    "Where do you think the important information would be contained in a news article? ,  they're in the back. Doesn't also depend on what exactly the office are trying to get our attention on   to put emphasis on something that no might trigger the interest of the audience. , it's not syntactical or semantically relevant to the article. sometimes authors might try to peak interest in a certain way and write something that's not. it's just to grab attention. , it could just be a specific word. that's also part of the score structure. And that's typically called a hook. They're trying to hook you into reading it.",
    ", but  that  that fits here as  within the score structure. But what about for  event based news articles. At the start of paragraphs. , at the start of paragraphs, because that announces the main idea. The end of paragraph at the end of paragraph yep, could be could be. 1st and the last paragraph, the 1st and the last paragraph. And , the more important stuff tends to be at the beginning. , these are all cues good clues for news. It turns out the most important clue. Besides,  the headline, is the beginning.",
    ", it turns out that  all the clues, you said are . and they're all they have. All have an effect for news articles for event based news articles. It turns out that the beginning is the picking that the opening sentences is the best predictor of important information. The reasons, , if you think about it,  it's pretty intuitive. Why, that would be the case. again  the days when things were printed on actual physical newspapers. There's a newspaper,  every day, ? people are working on very, very tight deadlines. and that means editors sometimes have to.",
    "just chop things down to size  that the printing deadline can be met and the newspapers can be printed. Then news article writers were explicitly instructed to write in such a way that the article could be cut off at any point, and it would still be a coherent article just to fit the length budgets. And  that's the way that the news was written. And  then they will always start with the most important, and go less and less and less and less important in order to support that. And even , even today, that still makes sense as a strategy, even if things can be updated online and  forth. Just because we all have very limited attention spans. you might only have,  the patience, or at the time, or whatever to read,  one or 2 paragraphs. And then you either think it's this is interesting. You keep reading, or you abandon and say, , that's it. for news that turns out to be the case.",
    ",  within the thread,  all the posts, are already ordered . either by time or and also by  the number of Upvotes. But , what about inside of a post itself. Post , a lot of times. The last sentence, because that's where the post original poster asks the question after giving a lot of context. , last sentence, that's a good question. just in case some people didn't hear a lot of the times in a long post sometimes, people would ask a question, and then the last sentence, it might summarize, give some context, , , that's ? sometimes I put a tldr  at the end, which stands for too long. What about in a scientific article or in academic writing. And it might contain a lot of the summary of the important information , in the back.",
    "The conclusion, , that's . In fact, if you take courses on  academic reading and writing your advice that for most papers. You don't read it from beginning to end  in scientific articles. Usually it makes a lot more sense to read the title and then decide. Do you keep reading and then read the abstract and then decide. And then  then it's  abstract intro conclusion. and it's only usually when you're in, when you're doing research. you usually to get into an area  to read  hundreds of papers to , understand the lay of the landscape of the work in that in that space. And  you just don't have time to read every single paper in the same level of detail from the beginning to the end. and in that case you would really  skip around ?",
    "You would read the abstracts of everything, and then read the intros of everything and conclusions, and only for the small subset of papers of ,  10 or 20, or whatever papers that are directly relevant to your current project. You read them in detail, and then you go through everything. those are all examples of  using discourse structure, and you use the general way in which a piece of text is expected to be written in order to help you find it and locate the important information. a summarization system can also do that. , in fact, in new summarization. it took,  10 or 15 years for the field to beat this baseline, which is just to take the 1st 3 sentences of an article. It turns out that is a really hard baseline to beat, and it took us many, many years in order to beat that. don't discount the score structure. And a 3rd major source. A 3rd major signal for determining importance is a little bit different.",
    "because then this is about the relationship between our general expectations and understandings of the world and what is written in texts. the previous 2 types of signals are just, you can directly get it by analyzing the article itself. This one is more about looking at how this piece of text is related to other ways in which we structure knowledge in our minds. and  as to think about how this piece of text is related to background knowledge and queries. , , , let me just ask you. And then you can  see the point. What do you expect to see in an article about a natural disaster. Or the doctor kilometer range? I'm also gonna put costs. Where to donate for relief.",
    "Number that's covered in casual. what to do if you're affected  suggestions and advice for those affected. But, , you see that we have very strong intuitions about what  information should be expressed in an article about a natural disaster. you can expect that the summary should also contain this information. and in fact, people have developed templates for this of  here. Here are some of the basic slots and basic thing types of information that are expected. And you can use even use those to check for  comprehensiveness, ,  it's missing something  advice for people who are affected. that's really important critical to include. What do you expect to see in an article about an election? Hopefully, people are making their decisions in a rational way, based on this information.",
    "Yes, , depending on what it is yep. Done properly for you guys. the baseline quote, yep, demographic breakdowns  just let  time and how to vote. , ,  basic information about the process surrounding the vote. , again, we have very strong intuitions here. , ,  then, a summarization system could try to learn this. Either you just directly specify it , here is a domain. Here is the import important types of information, and try to capture as much of it as you can. Or  if you have multiple articles of the same type,  you can try to induce this and come up with  a representation of  the kinds of important information that exists. those are the important kinds of information.",
    "then  how they're  implemented in specific systems. And again, these days,  one, strategy, is to just throw everything into a pre trained, generative model. and  do some prompting or whatever, and try to get outputs. But   older systems, because then they're easier to , think about and analyze and see what's going on. All summarization systems need to perform these steps in some form or another. one is to do some  analysis or content selection which is about determining what to say, and  using our signals just  to determine what is important and novel and interesting and relevance. There might be more than that, though, because then, after you find the important information,  to do some things to do with  aggregating common or contradictory points in order to draw some new conclusions or inferences from text. And finally, a 3rd step, which can sometimes be non-trivial, is to do synthesis of this information, also called surface realization, which is to determine the specific final form of the summary. the specific words you're going to use and how you're going to put the sentences together and all that. let's start with the these 3 steps, looking at these 3 steps for the simplest.",
    "What I would say, is  one of the simpler settings which is single document, extractive summarization. this is simpler because  by extraction. Again, remember what this means is, you take existing spans , say, existing sentences. and then you concatenate these sentences together to form your summary. or you highlight them, or something. this is the simplest, because most of the work has to do with content selection. ,  , all of the work is in content selection. just determining which sentences to select, whereas you don't really need to do very much transformation or synthesis or extra work for surface realization. you need to work on  the ui , how do you present the summary to indicate that this is an extractive summary? there are some incoherencies created by the extraction process.",
    "But then you try to indicate that to the user. ,  one approach is just to frame this as a supervised machine learning problem. this might not work for all of the different signals of importance that we discussed. ,  , ,  , let's think about this. we want to design a supervised system for summarization. we throw in a bunch of words , here is  a bunch of content, words and function, words or whatever. And then here are some discourse, features , is this sentence the 1st sentence, or the second sentence, or  you also throw in features related to  discourse cues , if are there words ? Because or therefore that might indicate Kate some conclusion. and other features of discourse structure. between these 2 classes of features, which do you think would be more successful if we take a supervised learning approach?",
    ", think about this for 5 seconds, and then I'll ask for a vote. if you had to pick one and just one. which class of features would be more successful. Who votes for lexical features? ,  more people voted for discourse features, and  I would agree with you. the reason for this is that this is really tricky, but if you think about it. it depends on the nature of the supervised learning system. But for  simpler kinds of machine learning and simpler kinds of supervised machine learning systems. you have a very direct relationship between the input features. here that would be the words with the output decision of  is this sentence important or not?",
    "And the problem here is that in summarization a summarization system might have to handle articles that are about very different topics, even if they're all news articles. One could be about salmon revitalization. Another could be about  global warming. A 3rd one might be about some election, and a 4.th One might be about the economy, and they're all different from each other, and they all use different words. what is considered what would be an indicator of an important sentence in one type of article might not be an indicator in another type of article. that's why I would expect and work has found, that using lexical features with is less successful. On the other hand, these discourse features about  how an article is written. is more stable within a particular genre of text, and a within a particular summarization setup. And  that's why the discourse features are the ones that are better suited to this type of supervised learning approach. then the question is, would I have the same opinion for unsupervised system?",
    "in unsupervised systems, then things are different. , then it would be. Then then we can look at difference. Then there'll be different trends. to spoil it, because  I'll talk about this a bit later, but unsupervised systems. Then you try to directly implement heuristics that measure centrality. In which case, then, you can use these lexical features. Would it change based on language? because  the arguments I made about how words indicate the topic. But the topic can change.",
    "but  the where, where there would be interactions would be  different languages might involve different genres might be correlated with different genres of text that speakers of those language tend to use, and different expectations. And  then the discourse features might be different across languages. but then that just means you need to do some adaptation or do different training steps for the different languages. ,  , people tried this. , very, very early in the fifties and sixties. the very early work is not really machine learning. It's more , here are some heuristics for  trying to. even back then they had some statistics. They did have statistical models. People back then were very smart as .",
    "it was based on what we can  consider a supervised learning method, using statistical methods. And then in the nineties Lynn and Hobie did something similar. They trained us to a supervised method as . Where the input was the source text plus some human written abstracts. And then for each sentence in the human abstract they find the position in the source article that has the highest similarity to it. this is an automatic way to generate a label. they didn't have to ask summarizers to  or human experts to select the correct. They had a heuristic to automatically generate that as . And then what they found was unsurprisingly that,  in different kinds of corpora, usually the title is the most important sentence. with the highest overlap with the human abstract.",
    "They did have  they did have human written reference abstracts. ,  unsurprisingly, the title is , usually considered the most important according to this notion. And then after that, it's it's it's dependent on the specific type of newspaper corpus. It turns out that you want the 1st sentence of the second paragraph, and the 1st sentence of the 3rd paragraph, because of the particular way that those products were introduced and described. whereas on Wall Street Journal, which is  economic text, it's  the 1st sentence of the 1st paragraph, and the second sentence of the 1st paragraph, and  on, and  forth. both of these cues that  you guys had proposed both of these ? some of you propose,  the 1st sentences of all the paragraphs. that's that was learned here. Whereas for economic news, it was , just what's in the 1st paragraph. And I already discussed this.",
    "But it turns out that in news text the opening of the article  acts  a summary in and of itself. the baseline method in this style of in this genre of text should be just to select the 1st sentences of the article up to the word length limit. We already checked BBC,  we don't have to go back. Another approach that people have taken is to try to implement this idea of centrality. And  this is  starting to relate to the question about unsupervised methods, and that this is an approach for trying to reweight words in order to better understand their centrality, their centrality scores, , or how central they are computational. And this notion is super important, because this also formed the basis of all information retrieval systems. Even  I'm sure it's used in current search engines. Ex,  explicitly, or even if not, then implicitly. It's used and it's learned through the training of current models. And this is the idea of term waiting.",
    "already experimented with this a bit in  the 1st programming assignment, or  in the second one as . which is the idea that,  not all words are equally important. and you can capture this distributionally at least. By important ,  important to determining how central it is. , if you see the word be in an article that tells you almost nothing about the topic of that article or the contents of it. whereas, if you see the word penguin, then that gives you a pretty relatively strong idea that  the article is about Antarctica, or it's about  ecology and conservation. I don't know something to do with penguins. the way that this has been implemented is through this idea of tf, idf. which stands for term frequency times, inverse document frequency. The heuristic here is that a term is important or indicative of a document, if it appears many times within that document, but is relatively rare overall.",
    "And  tf, , is usually just the count of the words in the document, , how many times does this word appear? An Idf is a little bit more complicated? There's usually some  rescaling by passing it through a log function. , one basic version is to look at how many documents does there exist within some reference corpus. and then you divide that by the number of documents with some term T, you add one to avoid dividing by 0, and you take the log for it, or something  that. And  this separate reference corpus, you need some other source of  data for this. And again, this was a really important and basic tool in information retrieval for the longest time. for it, here's a worked out example. suppose the appears in almost all the articles. Then the computation will be  this.",
    "It would be  a 35 times in the current article Times, the log of whatever. And this might, you would expect this to turn out to be a relatively small number, because what's in the log when you divide it out. when you take the log of it, it's close to 0.  you get a very small number. whereas penguin, if it appears very rarely overall, and your whole corpus. but it appears just twice in the current article. The fact that,  the you have this  much larger number inside the log is usually enough to  increase the weight of the penguin sufficiently of these rare words sufficiently that it scores highly in terms of tfidf  the idea here is that you can take this approach or other approaches. and reweight and score terms, and then, after that, you can just directly use those scores, and  score each of your sentences by say, it's average, or its total tf, idf weighting. or I  idf score ? Or you can use this to, you can use these tf, idf, weightings within each sentence or each span and come up with some vector, representation. And then you can find,  the centroid vector.",
    ", the is that called centroid,  medoid. , you can find the most central vector within the collection of vectors that you have and select that to be the most important sentence. Here's another method again by Lynnon Hovey, which is very successful. The idea here is that they use tools from statistics in order to determine the important words and phrases within an article. What they did is they set up a statistical test. where 1st they determined 2 sets of articles related articles and unrelated articles. that's R, and not R. , , if you were summarizing a document about vaccinations, then in your related set, you would have articles in the health domain overall. and then in your unrelated set you would have articles in the finance and Education domains, or something else. and then for each term, Ti. It's the occurrences of that term across the related and unrelated articles.",
    "And  each term is either the term of interest. Ti, or it's not the term of interest some other term. and then you would record all of these. , , in the summarizing articles about vaccination tier. One might be  needle or something, and then you would see how many times needle appears in health articles versus non-health articles. And then how many times words which are not the word needle appears in,  the related in the health domain versus, not in the health domain. Once you've set this up, then you can do a hypothesis test. and you can do a binomial test. what you can do is from the 1st row. You can ask, what is the probability that occurrences of Ti are distributed between R and not R in this way?",
    "this is a binomial distribution, because it's  a you can think about the some theta which is  you model. , it will either appear or it won't appear in each article, and then you have to use combinatorics to account for all of the ways in which the appearance or non-appearance might occur across these sets of articles. and once you have that, then you can set up a statistical hypothesis and run a test. the 1st test is that the term Ti is not characteristic of the domain. that means the distribution of occurrences of Ti between the related and unrelated articles is the same as for all of the other terms. then, the likelihood of your data set given, this hypothesis is given by this expression. where you have a single parameter. P. That you use to model each of the 2 rows, the row one and row 2 in your table of term distributions. this is  the null hypothesis, which is that this particular term Ti is nothing special. You can treat it the same way as you treat all of the other terms.",
    "Is that the term Ti is important to the domain, and the distribution of occurrences of Ti between the related and unrelated sets is different from the distribution for all of the other terms. then the difference here is in your second hypothesis. You compute this likelihood, using p. 1 and p. 2. And then for each of these you can just fit what's the best parameter that maximizes that likelihood. And then, finally, once you have 2 likelihoods. There's a statistical test you can perform, using a log likelihood, the log likelihood ratio. And then this is the statistic associated with the test. And then you can rank terms and sentences by oh,  it should be ranked terms by this statistic, and then select sentences with words that score highly on this. when they applied it to their data, it works very . They show some examples in their paper which you can look at.",
    "they did this for both unigrams and bigrams. the 1st the 1st set of articles, is clearly about  something to do with jails, and  forth, and the justice system, , and jail overcrowding. and then the second one is clearly about  a cigarette, cigarettes and tobacco, and  smoking and  forth. And  this topic signature method works  extremely . And again, this is based on just a very simple notion. This is really an instantiation of centrality. because it's saying that if this word appears more than you would expect according to some background distribution, which is what the binomial test captures. then it would score highly. And then that gives you some notion of  what this article is about in the multi document case. ,   far, we've looked at single documents.",
    "But in the multi document case, then there's additional things to consider. because then you have to consider that there might be conflicting or contradictory information. and there might be redundancy in the documents, and how to combine information between them. But , redundancy is both good and bad. , I said earlier, ? dependency is good because it helps. It's a it's a, it's a computable signal that . If everyone's talking about it, it should be important. But it's bad in the sense that you have to be careful when you're selecting sentences to include in your summary. You don't accidentally select,  the same sentence many, many times.",
    "you have to avoid redundancy in that way. And  one very basic system is appropriately named, some basic by Neiko von Vander Wendy. What they do is that they use unigram frequencies with a simple update term to account for non-redundancy. and this is extremely simple, but it works quite . it's a good baseline for multi document summarization. first, st what they do is they compute the relative frequencies for all words,  they just take the number of times the word appears, and divide it by the length of the article, and then of all the articles. and then you repeat until the summary length is length limit is reached. 1st you rank sentences by their average word probabilities. and then you select the best scoring sentence to add to the summary. by default, it's just  the best scoring one sentence.",
    "what this means is you select the sentence that contains them, the words that are on average the most frequent. And then, after this, to account for non redundancy, what they do is they do this thing, which is not very  theoretically motivated, but it works, which is that they just update the probability of all of the words in the selected sentence by squaring it, and because these are probabilities that lowers it. this means you're less likely to select a very similar sentence in the future, because the new average probability of the words in similar sentences has been decreased. And  you just run this iteratively until you reach your length limit. , , what is it? There is a word that's pretty important in all the documents they  solving. If you talk to them about solving. And  we can reduce the weight of that a very important for it, Solomon. that there's potentially another second very important idea about something that it's on the 1st one. But then this week's gonna go down.",
    "Because you , that's a good question. the question is , what if this down weights  the really important words , if it was a salmon article, then you've down weighted the word salmon. But salmon was really important. that's why the squaring works. This is the genius of the squaring. which is that if it was a very common word in the set of articles  about salmon. that salmon should have a relatively high probability. when you square it, you're decreasing it by less. then you would decrease the probability of a relatively rare word and similarly, for function words. Then, even if you don't remove them, then the function words they won't  selecting them shouldn't be  that bad  selecting function words again.",
    "And  then, by squaring it, you're decreasing it by less. , that's  why the score increase. of course, this was a very basic system. And they're much more sophisticated methods. , some of the later work that people have worked on in this literature include,  updating this, the inference procedure. rather than always selecting a greedy  greedily, what you think is the most important  sentence,  you can turn that into an optimization problem and there. people have current summarization into  a knapsack problem or something from which  encountered. If you've taken a theoretical Cs class where you're trying to select sentences with high informative informativeness score, informativeness score, while with while respecting your overall length budget, and  you also add some constraints. then this breaks the knapsack assumption, but then you add additional constraints about,  the relations between sentences,  that you don't want them to be redundant with each other. you can do all sorts of stuff  that and turn it into some interesting optimization problem and solve for that.",
    "It turns out there are other things that people have tried. , if you're doing extractive summarization, you should avoid sentences with words whose interpretation depends on  the overall context. , with pronouns, you should avoid them, because usually the pronouns refer to something in  the previous context, , if you have an it and you extract that sentence, and you don't extract the context, then you have a dangling pronoun , because you don't know what it originally points to. Similarly, with things , therefore, or whatever things that these words don't make sense outside of context. if you're doing an extractive system, then you should avoid selecting those sentences. or alternatively, you can select those sentences, but then you might want to cut out these discourse cues , therefore, and  forth. and other work on modeling the coherence or flow of  the summary sentences. And in fact, even back in 2,006, I know  these days, there's all this talk about. Oh, AI systems are better than people and taking over the world. But even back in 2,006, we were able to achieve this as a community using these simpler methods on some tasks in summarization.",
    "in 2,006, Conroy et Al. Used the topic signature idea introduced earlier with a sophisticated non redundancy, module, and  some rules just to  eliminate some parts of sentences deterministically. there's something called Gerund clauses that this approach removes also restricted relative clause positives. I don't expect you to know what these are, but . There are certain  syntactic constructions that they remove. She said that they would never do that, he said. She said whatever without consulting us,  just remove that, and also lead adverbs. by doing this they got a very high score. there's there's a metric called rouge, which I'll introduce in a bit, and then their particular system. ,  their particular system is  this system.",
    "And , and  they got . very close to human level scores even back then. I forget which one it was. They had 2 versions of the systems, one with the O system, one without. , 1 of the systems. This was  a query focus summarization task. one of them directly had. ,  this was this was an fully automated system. this was the system where they combined their method with just looking at word overlap with the topic of the summarization that is given, and by doing that their scores were   within the range of,  the humans which are all of these letters. and then some basic and all the other methods are , lower a bit lower down there.",
    "And then these days, of course. then you can do extractive summarization by training a neural method on some large scale data sets. You can do this in any number of ways you can do supervised learning. , if you're able to derive a label for each sentence as  a label 0 or label one. then it's just a supervised learning problem. It's it could even be  a sequence labeling problem where you feed in a bunch of sentences one after the other, and then you have to label them , is this important, or is this not important? And if you have a budget, then you can  rank the sentences by  the probability of being in class one of being important. You can also do this via reinforcement learning. We've done this in my group as . you think of summarization as a reinforcement learning problem, where your action, if you've seen reinforcement learning, the action, is to select a summary sentence or not selected.",
    "and then the reward signal is , how good your estimation of how good it would be to select. the reward signal would just be  how good it was to select the sentence. and then you have to define it a certain way. and if you have not seen reinforcement. Don't worry about it, but it's not the contents of this class, but it's  an alternative to supervised learning. You can think of it that way or unsupervised learning, it's  a different learning paradigm. ,  for the last 10 min, I'll just quickly talk a bit about evaluation. , this is one of usually the 1st questions I get when,  someone asks me, I say, Oh, I work on summarization, they're . But how do you tell if a summary is good, isn't it all subjective? But and also if it's subjective, is not necessarily a bad thing.",
    "But , to evaluate a summary. You have to think a little bit about  evaluating for the contents of the summary, and also for,  the overall quality,  the linguistic quality of the summary. Just  the reading assignments, rubrics, and  forth. But ,   we have the summary content which is ? Does it accurately reflect the original content,  the source content? Does it contain the most important content? Does it include non-redundant contents? And then for linguistic quality. It's  the grammaticality of the individual sentences and the coherence of the output overall. one thing you could do is you could just ask people to rate a summary  from a scale of one to 5.",
    "How would you rate the quality of the summary and this,   all the evaluation approaches. This has advantages and disadvantages. the advantage is that you can You can change the question and tailor your question towards what you care about. and you can have different sub questions on  different aspects of the summary also. This means that you do not require gold standard summaries which could, which could be tricky to produce. But the disadvantage is that this is expensive, because every time you change your system. and every time you want to do a new evaluation and compare systems. You would have to collect new judgments from people also. People are , I don't know if you've met people before, but they're really annoying and difficult to work with for science. , , different people have different interpretations of the scale.",
    "what counts as a 5 out of 5 versus a 4 out of 5, or  a 1 out of 5.  just think about , , you use your phone to check the ratings of a restaurant. Even between cities, or  even between neighborhoods. You cannot interpret the scale the same. I've noticed people in Montreal tend to be very nice. if you see a 4 out of 5 restaurant in Montreal, it's  it's a sign you should avoid the restaurants. But if you go to other places where the scale might be different, a 4 out of 5 could be a very good restaurant. And it also means that results do not generalize across different evaluation runs. if one research group runs this human judgment. Study on one set of summary summarizers. You cannot  take the same numbers and scores and used and compare with them in  a different setting.",
    ", if you have a different research group that tries to replicate that study and  reruns this human judgment study, you cannot take  a 4 here and compare it to  a 3.5. There, it just doesn't work. It's not a good comparison. an alternative to this is something I mentioned a bit earlier called rouge scores. the overall process for this approach is that you ask human experts ideally, or you somehow automatically construct a reference summary. what is  a good reference? You ask someone to write that. And then, when a system generates a summary of the same source. You compute some notion of overlap between the reference summary versus the system generated. And that's expressed in this rouge.",
    "rouge, N means you're computing it over. Not necessarily over individual words. rouge one would be over unigrams. But say, rouge, 2 would be over bygrams, and  forth. And then here in the numerator, you would count the number of matches and N. Grams. and between the system summary and the reference summaries. and then in the denominator, you would count the total number of N graphs in the reference summaries. and here you assume that you have multiple reference summaries. you might ask multiple people to writes this reference summary, because  you believe there's a diversity in the range of possible answers or good summaries that people can write. and , because people are too clever for summarization.",
    "This came after and they called it rouge. The difference between blue and rouge is that blue is more precision oriented in that you, the denominator, is mostly defined in terms of the number of words or n-grams in the system generated translation. whereas for summarization, because , you want to make sure you're comprehensive and capture everything in the source text. Here the denominator is defined in terms of the length of the , that's defined in terms of the length of the source article. I'm gonna skip this because I don't think it's that interesting to compute where it overlaps. But ,  there are yet other evaluation methods. there's 1 called the pyramid method, which is a  structured content evaluation. that you get this is really expensive. It's a human annotation that's really expensive. You ask people to highlight and find chunks of information in the in, in the reference summaries, and then you also ask people to find chunks of information in the system generated summaries.",
    "and then you ask yet more people to link the 2 of them. it's   a structured analysis of for each important piece of information which is called a summary content unit. Here do you find it in both the reference summary and in the system generated summary. , and from there you can derive a score, and you can , you can check whether the system generated summary contained the chunks of information which are considered to be important because they appear in the human written summaries. Yet another very different approach is an extrinsic evaluation. the idea behind these extrinsic evaluations is you don't directly look at the summary in and of itself. But you see whether the summary enables users to do something else better or faster. you can, if summaries are , meant to be useful in the education setting for learning. , then you can directly check. You can ask students and to use a summary  you either.",
    "You either just give them the original material they're supposed to learn, or you give them the original material plus the summaries. and then afterwards you can quiz them and see whether they do better on the quiz if they've if they have access to the summary. or you might be interested in testing whether summaries improve the speed of doing something else. here in the second study, what they did was they had a  artificial task of here's  a hundred documents. Find all of the documents that are relevant to this particular topic. and then they either they had one condition where the people had to just look at the documents themselves alone, or they can have access to both the document and a summary of the document, and that work showed that, having summaries can improve the speed of this process. You can do things  that. far in this lecture, in terms of the methods I focused on the extractive approaches. And then  class, we can talk a little bit about approaches to neural abstractive summarization, which is interesting, because then we need to bring in this idea of natural language generation. And how do you  do a generation test?"
]