[
    "David Ifeoluwa Adelani: Not . there are over 7,000 languages in the world, and or  over 400 of them, are spoken by 1 million speakers. But we don't have a single technology that works for 400 languages per world. we have some language models that they try to scale, to  500 languages, but in terms of performance we I don't think we have a good luck with that. How it asks to the force all the languages spoken by 1 million people. that means there's still a lot of work to be done. and over 1,200 languages are spoken by under the K people. and with 2,000 languages, you have,  a fewer number of speakers. the question is that  the distribution of the languages in the world. , you have more languages in Asia.",
    "followed by Africa, the Pacific the Americas, where we are located, and Europe and European languages have been more favored because they  because they are one of the early adopters of the Internet technology. And then they have a lot of materials on the web. we could also speed them. According to Achnolog, you could also categorize them, based on which languages are institutional, which  they are used by different government activities,  schools, mass media but in terms of language technology, even less are supported by language technology. how do you define under resourced languages? it's a very simple under resource. Languages are languages with various resources, but in terms of resources, it will be classified in terms of the data that is available. How many unlabeled data are available on the web? How many was the size of Wikipedia. what's the size of if you crawl all the available web tests in that language?",
    "That's an example of a labor data labor data. Do you have,  part of speech data sets for this language? Do you have set my translation data set for this language or . and also in terms of the research that have been carried out on this language. Do you have representation models for this language? Do you have language models that support in this language. And do you have even basic linguistic tools? Some languages are not supported by keyboard spell checkers, morphological analyzers and dictionaries. because nlp is data driven. if you have a lot of data on the web.",
    "you'll be able to, , create a better representation models. Oh, better tax specific one. one of the most popular ways to categorize languages is based on Jewish classification. which is a Sys class categorization of languages based on how much unlabeled data are there on the web and labor data are there on the web? class 0, these are . languages where you don't really have a lot of you don't have any text at all. , and class one would be languages that have few texts. ,  there's a Bible Corpus, or some linguistic activity documentation available in this language. And  I know if you increase the Joshua class Joshi Class 2. They have some unlabeled data.",
    "and they have a little bit more on label data and a few label data. Those request 3 has more on label data than juicy class 2, but also still less labor data. And then the winners are   languages with sufficient amounts of labor data and legal data. And that's why you will see it's easier to categorize languages  English, Spanish, Chinese. win us in terms of the amount of data that is available. because most of what we do  is based on what is called language model. there are different kinds of language model, as  here. And then on the left hand side, you have,  encoder holy models. Why, on the  hand side. You have,  decoder, only model.",
    "an example of encoder only models would be  birth model. we discussed that last time where you could. You want to encode your text properly. and then you can also send this to a class classification head or to a classifier to do the categorization of what you want to do. And also we have,  encoder, decoder model  T. 5 model is a very good example. And also we have a bat model is another example, and decoder. Only model would be  something  Gpt. And I believe the current model is still also based on decoder, only model. And, interestingly, all of them must be based on the transform architecture that we discussed. When did decoder only models become the most popular great, because, when we discussed the transformer, we said the encoder, because this is based on sequence to sequence model the encoder parts.",
    "The focus of the encoder parts is to have a very good representation of the model, to compress all the information into a single vector . , in the case of sequence, to sequence Lstm. And even with attention, is still trying to do the same job. But better job of passing a very  encoded information to the decoder is all used for generation. you can generate a new, a new text. whether you use an encoder only or decoder only, you still have to learn an embedding to project the world into a lower dimensional space. in a decoder only model, the decoder itself learns its own embeddings. you learn, both embedding both. And there's a way you to also tie them together. But since it's also supporting output embedding,  you also have to learn an embedding.",
    "there's no way to skip this process ? Because the vocabulary space is very large, even if you use a word. you need to   go put them in a lower dimensional space. the left hand side part of the architecture of the transformer only. But it  , you could. You have to also construct a task, an example of a task used to pre train and then go down. The model is the Max language model? Can we say the decoder? Only modules have,  internal representation, that we don't have access. Yes, you also have a power citation.",
    ", I'm not focusing on the sub branch. the way they did the categorization more is the current chart based instruction to Russian are  categorized separately from the others. But what we do typically in an OP. This is the validity today. Since 20 2013, plus, you have, you learn a self-supervised model , based on an on based on the large, on the text. It could be water vec, ? It could be a birth model. It's just the architecture that is different. It could be Elmo, it could be T. 5. This is an example of a birth model where you want to predict Max token.",
    "and then you cannot fine tune this on a label example ? And then, after the fine tuning stage, you can add additional layers   a best model and attach it to Lsta or a single layer. and then you can use it for different classification. Tasks  sentiment, classification, or question answer. we call this pre trained, fine-tuned paradigm. There's another paradigm, but it will not be the focus of my lecture today,  I won't be talking about prompting a lot. it's more on multilingual nlp and crosslingual transfer. for you to use this, there's a problem. There's lack of legal data for downstream task and also for many languages. Because  we are, we're in the multilingual space.",
    "also there's not a large enough on labor data for many, many languages. the problem is that you cannot really do have a good, successful, self-supervised training. If you don't have a lot of data. but you can leverage what is called transfer learning, which is very popular in the last 3 years  less popular , because we're prompting. But  that you can transfer the knowledge of a model to another task to another domain, to another language. that's why it's called transferring. And  this is the area. you have a model here where  that it has stored some information. It has learned some information on a large amount of text. And then you want to try to pass that knowledge to another model.",
    "B, , that is, focusing on a different task or domain. this is inspired by what happened in computer vision, where image net has been trained on a lot of images of cats and dogs and everything. And then you cannot transfer this to what? To order conservation tasks, your segmentation and order? you could also do the same thing for Nlp, ? you can have same task, and then you transfer it. you can train a model on the same task, and then transfer it to the same task of a different domain and of a different language. then, it's very easy to transfer another one that is even more popular is different tasks. you can train your model on a different task and then use it for another task. and this is what we do.",
    "you do mass language model on, on a larger number of text, and then you cannot find some little sensory classifications which is  different tasks. one of the ways to  incorporate as far, and it would be  fine tuning. it's  one of the thing you would do. But it could be more complicated  that you could use. You can modify the Internet structures to do transfer learning. There's a way to do parameter, efficient transfer line, and  on. And this is one way you can do. Transferring one example will be worth future instruction. you can extract sentence embedding from the language model  Bert model T. 5, model. And then send this to this because it's , is a rich representation.",
    "Send this to another architecture  By Lstm. And then use this for classification. , this was our model was trained. All the weights are frozen. You don't need to modify the weight. The second approach is what you mentioned, which is fine tuning, and here you do end to end fine tuning. You modify all the parameters of the model. and it has been shown that this often leads to better performance than doing feature extraction  and selling it to other architectures  Mlp. in terms of crosslingual transfer. suppose we want to transfer the knowledge from English to another language called Americ.",
    "What's interesting is the language with a different script. the transfer is really difficult. You could also have the same experience with transferring to Arabic or some Indian languages that use very different. you do the self training. But  there are 2 ways you can  add there are 2 ways to fine tune. if you have labeled data, you could just  fine tune this multilingual bits. And  we move to multilingual bits. if you want to do multilingual transfer posting transfer, you need an encoder that is multilingual. The difference is  mass language model has been performed on multilingual text. you concatenate the language of German, English Swahili and combine them together.",
    "Just append, and then train the same. And then, if you have trading data, you can find some directly in Americ. And if you don't have training data and one thing you can do, you can. You can find some language that has training data, , English. And then just do 0 shot transfer to the language. I hope this is clear. This is very, very, very common. Say, 0 shot, 0 shot transcript. You just mean very good question. once you just you just predict that's it.",
    "you train a model for English. And then you run prediction for another text which is in another language. that's why we call 0 short transfer. it could be any task. Suppose sentiment let's think about Amazon reviews. And then you have a lot of text Amazon reviews in English. But you don't have anything in Spanish. and then you train on English. and then you just evaluate some Spanish text. , , but you're not generating it this time.",
    "this is more for classification tasks. Yes, and the reason is because you forced to mass language model on a multilingual text, ? there's a way you can do transfer learning on monolingual model. But that means you have to 1st  modify this monolingual Lm before you can  adapt it. , but that's a little bit more complicated. But this should be clear. But what we know that most preaching language model do only cover,  on a hundred languages on the maximum. But  we have people that have tried to scale this more. but, , the 1st version of the models we have  multilingual Belt Xml to support,  100 to 100 languages. And then recently,  last year we had 500 from Lmu.",
    "That they try to, . Just do adaptive fine tuning to 500 languages. We have Serengeti that was trained for 500 African languages we have. That also can be used as an encoder. , of course, lingual transferring is attractive. Because you have  many,  many languages in the world, and then you are not able to have the data set for everything. you could just do the research transfer if the multilingual model is really good. And I've seen a lot of text in that language, even though there's no labor data, you can still have a very good transfer, especially if the languages are very related,  could have a very good transfer from Spanish to French, because they're Latin based. and the languages are very similar. the 1st challenge which we'll discuss  all the rest of the talk will focus on these challenges, and I'll try to present some of my past work on this.",
    "And and also interesting question that you can focus on. the 1st challenge is, labor data sets. You don't have a lack of data set for many of these languages, and also this virtual language model. Only cover a few languages,  only languages. from, , over 7,000 languages in the world. Another issue is that you have languages that  may use different scripts. transferring from English to American is really difficult because they use different script. And also we have issues where you have catastrophic forgetting, , if you do this fine tuning, and then you forget the previous knowledge while trying to adapt another task. We also have issues of parameter inefficiency. Where, when you fine tune, it just creates new copies of the model.",
    "if you prompt , you don't create new copies of Gpt. 4, this will be completely unscalable, ? previously for the birth model. But once you find zone, it is another copy. , if you have 500 Md of , it was a size  500 MB. And then you train 200 models. You're going to have 500 MB. people were looking at ways to make it more parameter, efficient. And there are also issues of what would be the best source language. If I want to transfer from English to Americ.",
    "But if there are other languages to have legal data most, I always use English. or can I use languages that are more similar to the target language. the 1st one, where we talk about developing data set for low resource languages. First, st I will start on what we did for African languages back in 2019  we don't really have a lot of data sets that were human generated for African languages. And by working with different African communities, we're able to create different data sets that covers different tasks from question answering text to speech sentiment, classification, machine translation, news topic, part of speech, name, density, recognition. Mascani, , this is an African community I collaborate with which is a Grass group, Nlp community for Africans and by Africans. But I must also tell you that there are other communities around the world. , there's a comment in India, AI for Bharat. They're also working on similar thing. There's a Southeast Asia initiative.",
    "and many people just collaborate with native speaker to develop a level data set for their languages. the task of named entity, which you should be very familiar with, which is classifying entities into personal name, location, organization updates. And then you have this bio tags that you're  familiar with. And  we have this  collaborative projects where we have native speakers and Mls. And then we have 2 versions of the Masaka Project. First, st we created Nameless recognition for 10 African languages, some in West Africa, some in East Africa, and later on we find out that , we know we're excluding the southern part of Africa. But , more compelling reason is because they have very different linguistic properties which I'm going to show you. and that means the transfer from this East African languages to West African languages doesn't transfer very  to the one in Southern African languages, because those ones are more morphologically rich. I give you an example of some of the languages and the different linguistics structures. Imagine you want to do a transfer from English language to Americ.",
    "You see a language with a different script,  this can already pose some challenges. Then you have languages  Rwanda or Swahili, which have a lot of morphology. we did an example of Swahili in class where you're trying to decode, where you have different prefix. And then you want to try to decode. What is the root word. and we also have languages  it's called Size Zulu, that apart from using rich morphology, they also have what is called  classes. there's a different prefix for a personal name, for a location, for different entities. and also who can tell me what would be the challenge for? Nr, if you want to go from English to this language. But I didn't put it in the slides.",
    "But if you can think of what would. What can happen with any our model train on English work for? Is the cost always as ? There's no translation of  some words in English language. But the models are smart enough to bypass that. Just looking at a linguistic structure. What can I pose a challenge. Remember, I told you about not this long. the answer is on the screen. One word, and they're a lot more complex and translates to a phrase, , but our models are smart enough again for that.",
    "The answer is , the issue of  classes. entities here, you have something  using ? You have something  in Nigeria,  one feature of ner is capitalization of entities. Of personal name is capitalized  at the beginning, is capitalized in Zulu is not capitalized the beginning as something. this capitalization is absence just makes the annual model fail, or personal names, because it  fits into this idea that if you have capitalization it's more likely to be an entity. And , when you go to a language that doesn't have this feature, it's just   fails very quickly. Because if you want to do with the crf, if you want to be a features for Crf model. One of the features you would do for any R is capitalization. Do you have capitalization which is humanified feature? But the models also fits on this  feature.",
    "This is  a more general question. But in transformer based models which I'm word embedding, how is how is something  capitalization represented. Is there a different word embedding, or a word that starts with a capital? there, it's not that we have a different word embedding. the even the most the tokenizer, we treat them differently. if you want to split zang. How would you split it? he's going to split it into one word or 2 words. If he's going to split Kuzang. it's  will may or split it differently.",
    "And then  issues where  it  fails to capture this information correctly across different languages. but there are results also showing this also. , in the paper, where we show that the performance drops very good. and we have languages with diacritics. And then the what's our representative? A little bit differently in this language is with the acritics, and then it's tokenized differently, and then it also fails for the Ndr task. this one I will just really brush through about it new topic classification. that this is a language in Nigeria called Nigerian pigin. you have when you pivot and decode. This is  a clear language, and then you have because it's very similar to English.",
    "The 1st word is more  a business news the second one is talking about an entertainment award which is  an entertainment news, and the 3rd one is  on health news, and the last one is sport news. Even if you don't speak this language. you're  able to record this. We also did very something very similar to the Masa Project, and we create a new topic classification for these languages. one major issue, and why we have decided to do this is that when you try to scale to develop new models for different languages, if there's no label data, there's no way to evaluate how good the models are. and that's why we have Masaka and then Masaka 2.0. And then we have this new topic classification. this is based on BBC articles and voice of America. And then we have some local website where we called news articles, and we label them. , , this is BBC, and they are predefined categories.",
    ", health news is we? you can just give your cry everything from health news, , likely to get mostly else related news. Or we have business with politic news culture sports and all that. and also we are  religion because it's very prevalent in the African newspapers. and then this is how we  develop this Masaka news data set. Also in the interest of time. I'm not going to present results for this, but I'm going to present results for the last one, which is Srb. And again, then  in 20 what? 2023  we had this very. I had this very, very small idea.",
    "And  that is it possible to repro repurpose. An existing data set  creates a labor data set for many, many languages. when you work on low resource languages, one of the major challenges that there's no evaluation data set. the available evaluation data set  we have taxi, 1,500 from Lmu university. But this is based on the Bible on this part. A little bit biased to the religious domain we have any which covers 1 76 languages. But these are not human annotation. This will just automatically annotated by some rules and heuristics. you cannot trust the result from this data set. We have ud dependency passing, which also has,  dependency, passive part of speech.",
    "But this is a very expensive annotation process, ? It it would take them  one year to have a huge benchmark. and  the community has grown it. This project has been on for more than  5 to 10 years. and they are slowly adding  2, 3 languages every year. and  it will go on for the  20 years. Oh, it requires a lot of expert annotation. The closest to our work is Beli. That was used for question, answering task or reading comprehension. Given an article just  reading comprehension in high school, you ask a question about the article, and it returns the answer but here they do not provide any training data.",
    "We also have a massive from Billy believe was from Meta, and massive, was from Amazon. And we just have issues of not covering many low resource numbers. here we  develop Sib 200, which is based on topic classification. And what we did was to leverage an existing data set called Flores, that is based on machine translation. these sentences already have translation in over 200 languages. and what we did was, if we label correctly the topics of the text in English, we can just project this to the rest of the 200 languages, and that is how we created Siv 200.  it's very simple. here we have text in English. and then we annotate them into these different categories, and then we project that into Arabic. We project it into Yoruba. We project it into Maltese, and that's it.",
    "And that's how we created a benchmark very quickly for 200 languages. The exact translation into this different languages by humans. There might be some translation errors. But that's not the focus from this time. the  when we did the annotation. The phrase Kappa score was pretty low, just because we're dealing with very short sentences, and people disagree a lot, even for these short sentences. and then you can have a sentence where some people will label this as geography, and another person will label it as travel, because someone is going for hiking, or something  this. ,  we have this different category. we have sip easy and hard where we have the 1st sip had,  7, , 7 categories of labels. And this is a distribution of the categories.",
    "We have more science and technology articles, politics travel than some other categories. we also distribute this by language family. the language families that cover any data set Atlantic. Congo,  Indo-europia, was the largest, followed by, , Atlantic, Congo. and Austronesia, and all that. we also distributed by Joshi class. The interesting thing I told you about Joshi class,  class 0 has no data on the web plus one they have few texts, ? the one of the good thing with Florence data, say, is that it's  cover a lot of low resource languages, ? up to 100 of those languages included in this benchmark are low resource languages. and also we have, this distribution across different regions of the world.",
    "We have more Asian languages, which is also nice, because we have more languages in Asia, done all other places in the world. And then we have European languages and African languages, and  on. we also then try to find tune some baseline models  classic Mlp. and then we also fine tune 2 models Xmr was created by Meta. And  it's a strain on other languages. many languages are excluded in our benchmark. They have not been seen directly training. and then  the effect of transfer learning in naturally  performing on language that are not seen during pre-training. and we also have got 500 that's already seen a lot of these languages  I've seen  1 77 languages in its portraying. , of course we also do prompting, but  I'll skip that all .",
    "the accuracy by language, family. one thing you'll see is that different Mlp. Which is the multi-layer perception you see that it's  inferior to the bed based models. This is what you expect . But if you go to some low resource language families. for Indo-european you have Slm large. A larger model are the best results. and if you go to topic language, you also have very good results. And on average this preaching language models also give the best result. But if you go to some very low resource languages, you find out that a simple Mlp get better results, , , in the Atlantic ago.",
    "which shows that the transfer learning works but  is not better than a single most layer perception. if your encoder doesn't support many of these languages, the transfer performance is also stable. we also have accuracy by juicy classification and by region. And then you see that the region of Europe depend regardless of their classification. They often have very high performance. That means, even if they don't have a lot of representation of text. But if that language is similar to a high resource language. the transfer still work very . if you have a language what can I ? I know Livonne is  endangered  Luxembourg.",
    "It's very close to German and some French. but it will still have a higher performance, even though it has less text on the word ? Languages also have this  effect. But if you go to the African languages, you have much lower performance in terms of transfer performance. And  then we talk about factors affecting performance. We talk about the language coverage. If a language is not covered during pre-training, they often have lower performance, and then we have categorization,  is the language already seen? Is the script already seen, and the performance will be higher than if the language is unseen. if the language is unseen, but that language is very similar to the same language  Moroccan, Arabic still has a high performance done. Because it's close, closer to the modern standard.",
    ", you still have a high performance while you have some languages where the language is unseen, and also the script is unseen,  the Tamashek, which is a Berber language in North Africa, and then you'll see that the performance is really poor, , because the script is unseen, the language is unseen, it's really difficult. we also compare prompting with 0 crosslingual transfer. And here you find out that , models  Chatgpt just 1 min had  much worse results than just training on English. 0 shot transfer to all the languages. Yes, we have a question. If the script is unseen, how does the world? It's not a generation task. , processing it and saying, this is  a business or travel class. even though  it's just very randomizer. And the word embedding is learning.",
    ", that's a great question. The Tokenizer also doesn't work very  for this ? Because it's it's not saying it does not send this. if you, Tokenizer doesn't work very  for a language. What may happen is that it will take more tokens to encode the information there. And that means it's just  you can relate this to compression. That means it's not able to get any useful information. Yes, , that's a good question. the issue is that initially. which I'm going to show you in the  couple of slides is.",
    "if a script is not supported. the 1st version of Maslinga Bert is going to give you f. 1 score accuracy of 0. , but this is not interesting. people address it by using not what level tokenizer, but they use byte level tokenizer. byte level was still  encoded, but the very poor way. And then this still gives you poor results. you won't have folder site optimization . the performance of the language, languages that were not seen by how similar those languages are not seen, or how are they themselves composed of languages that we have seen during training. it's , if a language is not same, and there's no relative or causing then it's gonna give you a poor result. we need some analysis to see. And that's why I could give example, , look at Arabic way, because it has,  other Arabic scripts that are seen  Egyptian, is very prominent and modern standard Arabic, and then  better results.",
    "just  ,  site level quotation. , that's a good question. Yes, we do have character level models. And we have bite level models, which I did better. But it has not been widely adopted. , for the languages that are unseen and script unseen. , there's some that are still very accurate. I'm wondering how that's  even a little bit possible. Oh, , and what is accurate there. you find out what is accurate is the Mlp.",
    "that means you don't need any language model. you just train using circuit line. And on the text, that's it just  your assignments. that gives better result than using a bird's model that was trained for 2 weeks on a Gpu server. Can you speak louder,  can you one? Oh, which we need more? Oh, not really if you have a more popular, I would say popular language, I would say a language with more resources on the web. if you are able to learn a self-supervised pre-training for them. You have rich representation and better encoding of the information in the language, and then it will give better performance. But for a language that you don't have a lot of text on the web just using simpler approaches  naive base or something they already appropriate.",
    "Mlp, , that's a good. Uses the tokens of ex Lemar. Engram uses engram  what you did in your summit. last part 2  I'm I'm very certain we'll not finish this. this is , what can you do if languages are not covered. I already showed you result that things doesn't work very  if languages are not covered,  what can you do about it? I'll tell you how we develop a new model called office tomorrow by looking at some of the challenges of adapting plms to new languages. One of the issues is, many. Languages are not covered during the pre training,  when you evaluate it. you often have worse results for these languages.",
    "this is  due to what is called cost of multilinguality. Cost of multilinguality is that if you want to cover more languages. you also need to scale the capacity of your money. people  try to scale a lot  Gpt-four, and that's why it's able to learn different languages very . but also there's other restrictions where models are just getting bigger, bigger, and then nobody can serve them. And is there a way, we can  find a way to adapt models that in a more parameter, efficient ways. a result of Amaric  what I told you. If you have a word based model tokenization. And then for a non-sympathy scope  Americ. if you train a embed model, you're gonna have a 4 score of 0 because you're just using one level.",
    ", the 1st time I saw this. , I trained the model many times as I was, how can I keep getting 0? And then I have to think about what's happening in the model. And then I understand that the tokenization just fails. , and then, if the language is supported. if the language is not supported, but the script is supported, it still works. For some, , our language is not covered in multilingual birth, but the script is covered,  you still have very good performance, and Yoruba language is covered in multilingual birth. it has slightly better accuracy than X number of data, hey? But one thing you can do about this is to do what is called adaptive fine tuning. , you can replace the vocabulary completely.",
    "That's why what we did there was just to replace the original vocabulary of X number of beta changes to that of Americ,  trained on new Tokenizer and just replace. and after swapping the vocabulary that you adapt the model by training on more on label text. And if you do this, it's going to correct this problem, and you redo the alignment. And then you are still able to use the knowledge in the upper layers, and then you can improve the performance. by that we're able to improve the performance. You see, for Americ we went from 0 to  60 something. this is already good boost in performance. that means, if a language has already seen that script going to training all your adaptation, you're still not able to reach the performance of the model. If it has already seen that script during pre-trained. the lesson is, I try to cover as many scripts during pre-training,  that you will be it will be easier to adapt to new languages in the future.",
    "you agree with me that one of the limitation of this approach of language adaptation will be, it's difficult to adapt to non-supported scripts. Another issue is that it's not parameter efficient because you create a new copy of this model when we are doing adaptation. and also, once you have adopt adapted the model, you can also reuse it for  , the model you have only adapted it for Americ going forward, this will only be useful for Americ. You cannot use it for English again. you can train a tokenizer. And then you can copy all these tokens. you trade the tokenizer on. It's a heuristic way that a guardian of tokenizing the text ? And then you can copy all these tokens into the   you can copy all these tokens into the original model. You just have to ensure that the size match.",
    "if the original model has 250 K. And then you can train a new Tokenizer, and saying that it has to be. give you at least 250 k. Vocabulary tokens, and then you can redo the alignments by further training. the words  tune the model for that new vocabulary. Yes, you have to worry about  changing the model too much. , you have to worry. And then things  learning rates also matters. because then  we I tried a very high learning rate. I use the original, it doesn't work. And then I use a very, very small learning rates then what it  did was to slowly adapt the original model  that you don't have catastrophic forgetting it doesn't forget all the knowledge, because I want to retain the original rate as much as possible. But I just want to slowly remap the embedding layer and that work.",
    "We started with glass glasses. The tokenizer is not in your network. It's just it's it's How do I explain this. It's a heuristics to determine. What is the what is the  what is the minimal amount of tokens that can serve a language  , and then the number of token, the number of vocabulary token is a hyper parameter. you can train a Tokenizer to say, I just need 50,000 tokens that can serve this language, and then you have the heuristic that gives you the best 50,000 tokens. subword tokens that is good for that language. and then you can also have 200 k. But oftentimes you have to have,  something useful. , the original monolingual birth only has 32,000 , while the multilingual birds. But later versions of best had,  250,000, because they believe this is a better multilingual vocabulary to serve many languages.",
    "just trying to get as many importance as possible. And you try to get as many tokens, but not too many tokens but we are a bit guided, based on what other people have done previously. you just don't do  a million tokens or something. how to address this limitation of lofts which we call left? A a very simple way is just to return your own new model. we have a new model from Africa called Africa. We have another one for India languages called Muriel, and it was just trained on  is it 22 Indian languages? But this is resource intensive, ? another approach would be to use what is called parameter, efficient approach, where you can use things  adapters, and  that you had an adapter which is  a new weight to the different blocks in your transformer model. and then, instead of modifying all the weight of the transform, but you only modify this new weight, you have added.",
    "and this new weight are smaller and more parameter, efficient. And this is the only thing you have to stop instead of storing the entire monophone. And this works very  for post language as . this is an example of evaluation on Indian languages where you have Smr, the original model. But for this language, which I forgot the name of the language which is the only Austro Asia Asiatic language in India. someone knows it, and this language has very low performance and the indig bets. The 1st version had a very good performance on this language because it was covered. but it didn't cover Sino Tibetan language, and then it has worse performance. But for the Muriel bet. Similarly, it has better performance on this Sino Tibetan language.",
    "just because and it's very important in India languages, because they it's  almost every language uses different scripts. if you miss a language. You also miss the script. and then it doesn't work , and they have a random platforms. what we did then is what is called multilingual adaptive fine tuning. You pick an encoder model  Xnemaro, and then you initialize the weight of the model and just adapt it to the region of languages. here we adapted it to African languages to several African languages. this is very similar to what was done previously for Mbat, which is  the topic of your reading assignment. 25.  to  increase the capacity of Mba. They continue pre-training it on 25 new languages.",
    "and they show that it doesn't affect the performance of the original model ? But what we did is study different. here they are trying to extend the model to cover more languages. Here we're trying to specialize the model just for Africa languages. there, there will still be some catastrophic forget. And here the choice of Xmr was because it covers multiple scripts in Africa. we didn't choose multilingual birds, because it wouldn't work for the Amari case, I should I already described. And then we combine Irish, some Irish as language  English, French, Arabic. That also widely spoken Africa with 17 most resourced African languages. And then we just do something  continual training on Mc for Corpus and news corpus.",
    "And here we're able to cover more regions of Africa with a better multilingual representation, learning model. to address the audio restriction. One popular approach is you do what is called knowledge distillation. you have a teacher model, and then you want to distill the knowledge into a student model. you can have embered, and then we have distilled. But nowadays those models are  too small that nobody cares about distilling embers again. But what we did then is. , there's another trick you can do, which is what we call vocabulary reduction. And  this, think about Xmr. I told you that it was trained on very diverse scripts, very, very diverse languages, 250 k token, but because it was trained on these 250 k. Tokens and many languages.",
    "The model is also very big, and up to half of the model size  is in the embedding part of the model. the question is, if you just want to specialize the model to a region of languages. do you really need all this vocabulary? what we showed here is that  an earlier paper already showed that if you want to specialize to 2 languages, just remove all the tokens that don't relate to these languages. Just remove all the tokens, and then your model. you don't need the performance doesn't drop. we also did the same thing, just removing  focus that are not really African related. we can remove things  in the script and in the tokens, because it doesn't relate to Africa at all that, and then you'll still be able to preserve your performance ? unlike that we are the number of parameters of our model went from  270 million parameters to  140 million parameters without. there was some drop in performance, but it was not significant at all.",
    "if you do this adaptation, which is language by language, adaptation. for every language, you adapt it to the monolingual text of that language, you always improve in performance. The only language we did not see improvement in performance is English, because the model is already good for English,  there's no need to adapt English again. And then, when we did this multilingual adaptation, what we see that we almost match the performance of the individual language adaptation. There's still some drop for some languages  Americ and Yoruba, but we will tell you how we address them , even for languages that are not standard for training. but they have a related language. It  still have those languages. And for vocabulary compression, when we compress the model, we see that , we have a little bit drop in performance, and the biggest drop is for languages that use a different script. while we are throwing away some tokens that we don't need,  we also throw away some tokens that already have languages  Amarik to perform very . And  we observe some dropping performance.",
    ", But in general, what we saw is that vocabulary compression was  better than knowledge distillation. if you compare models that they already perform knowledge distillation, which is a very expensive process. you'll find out that the performance of this knowledge distillation model was the worst and just doing this vocabulary. compression or reduction  had better results on average, than the registration. seems to be still a very attractive approach. The size of the model is remains the same, but in terms of performance, just reducing the vocabulary  is better. for languages that , we're not able to improve with this multilingual adaptation, we found out that by scaling the size of the model. doing the adaptation on a bigger model. We have the small version, 270 million parameter. We have a bigger version, 550 million parameter.",
    "if you do the same adaptation on 500 million parameters, the performance is much better than the 270 million parameters. But also we have an additional benefit that the there's no gap in performance between a single language, adaptation and multiple language adaptation. , there's gotta be more weights in the model. And here you find out. Although America was struggling before. When we have a smaller model size than when we move to a bigger model size. There was no difference in performance, ? Similar for Yoruba,  for Yoruba. We even had better results, Kate. and currently, at least for the African languages.",
    ", we  create a better model than previous  really strong multilingual models  remed by Google and Md. And the model is a login face. we also did the same thing for sequence to sequence model. you can have an empty 5 model, and then you can specialize it also to region of languages. And also, you have similar effects where you can also improve the performance even for generation task. I know someone has asked for generation task. you asked for generation task. this approach will still work for generation task by just adapting the model to more, to a region of languages. and then you can also have better results even for machine translation tasks. how to adapt an English Llm.",
    "And  this is the era of large language model, and I can tell you also, this approach is still quite useful for large language models. we have some Chinese authors that  adapted the 1st llama 2, and they adapted it to Chinese llama, and then we had. by doing additional pre-training on just more Chinese tokens, and  they  did Chinese token and English token together. and by this they were able to have Chinese llama, too. and when they did instruction tuning based on Apaka, they are  Chinese Apaca. What you have is you 1st have for training, and then you have instruction tuning. And if you want to apply this approach you. All you have to do is just, do continue pre-training on a large amount of text for that new language, and then you do the instruction fine tuning. this approach would still work ? And then also on Chinese mmu, you also have some boost in performance by this.",
    ",  this is a second part. and the 3rd part is very, very short, which I can rush through, and the short part is, the 3rd part is cross lingual transfer learning. The question is, how do you choose the best transfer language? I'm going to demonstrate this with any R model, and  that if you want to choose the best, any the best transfer language. this is interesting, because then we are  21 African data sets with any our data sets. They have training development and test sets. And we have 21 non African languages. And then we want to see what would be the best source, transfer language for each of these languages. how do you do this? You just have to train an end by end model.",
    "you train a model on Chinese, and then you evaluate on the rest. On the remaining 41 languages. You train a model on English, anyhow. model on English, and then you evaluate on the rest 41 languages. , and then you have . and each map transfers call  this. And here I'm just going to display some results that are more specific to Africa. Arabic German, we see some interesting transfer to some African languages. , , here we have languages that are more. In the Francophone region of Africa, we find that they do transfer very  to each other.",
    "they share some entities that are more interesting,  that relates to France or something. And they are able to perform. That means if you train on Bbj. and then you evaluate on phone. we also find,  languages that are geographical proximity. , all the 3 Nigerian languages covered we. They have  very good transfer performance to each other. even though they do not belong to the same They do not belong to the same family. but because they are in the same geographical location. And this is very easy to understand for Nar, for any hour you have things  personal name.",
    "they are talking about the Governor of the States, the President,  they will share the same entity. They have the same script. this name should be flying around in different text, whether it's in Yoruba or Asa, even though they are not the same family. we have languages  this in East Africa, which have very similar syntactic similarity and entity overlap. this language is  Luganda, Rwanda. They're in the same region. They also have very good transfer to each other. Then we also have languages  I told you that we have very portrait. the result from English to Zulu. You see that the performance is very poor.",
    "We only have 45 ? , from Shona to Zulu, you have better result. If you transfer from Cosa to Zulu you have better result, because these languages are in the same Uguni family. They have very similar properties. They have this  interesting prefix before capitalization of the entity. those language they have similar properties also transfer very  to each other. when you want to choose a transfer language, you have to. Consider this, , we also have interesting transfer performance from Arabic to Swahili. And , in historically, Swahili has borrowed a lot of words from Arabic due to due to trade, because there's a lot of trade between East Africa and Arabia, and in many words were born into Swahili, and also the German influence is quite interesting, because there were some German people there at some point, and I don't know what happened. Very , but I cannot explain the one for German very , but we  see very good transfer, , from Arabic to Swahili better than an African language as , which is quite interesting.",
    "Look   generation  a generation is really big you. , , for wholesart Zulu, very similar syntax or linguistic property. , this might also hold. the question is, how do you  scale this to new languages. let's assume you have this  you already trained this for 40 languages. What if you want to do this for another language? Do you have to repeat the same experiment and all that. some people have been thinking about this problem. In 2019, there was a paper from Cmu on what is called language rank. and  that you can train.",
    "that you can train a ranking model, a ranker model to  predict what would be the best transfer language for a new language. Given your previous data sets, you have trained . that you train a set of Nlp models  what we have done. You train on English, and then you transfer to all other languages. You have all these different scores ? And then also, you use some other linguistic properties and some linguistic distance. and then you will  use this to train a ranker to predict what would be the best transfer language this is based on boosting. And some of the linguistic features are, there's a there's a linguistic vector features known as language to vec,  that means different linguistic properties have been converted to vector representation. And then you can measure what is the similarity between these different vectors or their distance, and then you can use this as features to  build a ranker model. we have different linguistic distance measures  geographical distance.",
    "Our geographical distance are these 2 languages. if you have a language Indi to, , Swahili, they're very far from each other. the geographical distance will be very wide, and then you have genetic distance in their linguistic family. How far are they from each other? We have the inventory distance which is . I forgot the exact definition of that. How the sound are they related to each other. We have the futura distance, which is a combination of all these distance measures. And also we have data dependent feature. What is the data size that's available.",
    "What is the entity overlap? Do you have a lot of entities that  overlap between these 2 languages and based on that, you can train your rank and model. And what we did is that we did this, and then, if we do, the brute force approach, you have the top 2 predicted transfer languages. and then we also  use a language rank by using what is called, leave one house. This guy you train on all other languages except this language, and then you can  run a prediction for this. leave one house is a is a cross validation technique. If you don't have a lot of data for the new network. and here you found that the language prediction are quite interesting. We have,  Nigerian pigeon and Yoruba with predicted if we use the brute force approach. But we still have Yoruba.",
    "That was also predicted for hours, even by the language RAM model. that means the model is not very accurate, but sometimes it can have a good prediction. and I will show you in a couple of slides that if you use the top 2 and you co-train them together, you might  have very better, better results than predict, using just one prediction or just using English language. you always have better result than just using a single language or just English language. and for any,  the 2 most important features that we discover are geographic distance. if you compare the brute force approach. You find that the English performance is the one in pink and the top one language rank. You will find out that most of the time. It's not very clear for some languages. The top one rank is better for some languages just using English is better, but if you do what is called co-training.",
    "That means if you train on the top 2 predicted languages, you'll find out all the time. This is better than using English every time. Will is able to provide you at least stop to language. To transfer from that will be better than just using English every time. And if we compare this to the top, one best transfer language, you found that co-training the top 2 from language rank is very similar in performance than using the top one in the brute force approach. why not just use this and just train on the top 2 languages, ? And the last thing is which is, you can also use what is called parameter, efficient, fine tuning. you, if you're able to get the best transfer language, you can combine it with this framework called parameter efficient fine-tuning, and this will  boost the performance. and  that you can have adapters to every transformer block. and you can train this for different languages.",
    "You can train this for English. and then you can  use this to transfer to another language. for you to do transfer learning. you have to train an adapter for the source language. You train an adapter for the target language and then for the task specific adaptation. You  train a model that works for the source language, and you are also works the for the source task. in this setting, we believe that the language you're adapting to is also interested in the same source task. the only thing you need to modify is that you only need to swap the adapter of the English language to that of the target language, Quechua. and by doing this you can use the remaining models you have trained in many weights. You have trained for the task.",
    "and this seems to work very . And , this can boost your performance, and if you combine this with choosing the best transfer language, you can easily boost the performance of your postlingual transfer. There's another approach which is very similar to that which is luxury, ticketing, approach. and  that it's very similar to adapter. You can check, you can get what is the best spas, fine tuning, or what is the best of network for the source language, what is the best of network for the target language, and then you can compose them together. in terms of the result. This is a result on part of speed tagging. If you do not use this framework at all. This is the blue bar, and you see that this blue bar is inferior to using things  adapter or Lt. Sft. And the source language also matters, and you find out that some source languages, if,  Arabic .",
    "regardless of the approach you use if you use a better source, language is already better ? if you can combine the sauce choosing the best source language, and then you combine it with this parameter, efficient approach. You can significantly boost your performance. and, better still, if you  combine this with Co training on multiple languages, you can even further boost the performance. And this is what we have here, which is  our final slide is that for a language  phone. the best transfer language here seems to be wall off for are not English for Euroba. but if you go to Zulu, all of us transfer very  to other languages, did not transfer very  to Zulu, because they have very different linguistic property. I don't know if I'm able to convince you that. And those little details are important. What is the best transfer language for the language?",
    "If you can get it, this can already boost your performance. I am posting watch as far. If you combine this with a different parameter, efficient approach. you can also further boost the performance. And if you combine this by training on multiple best transfer languages to enable further boost your performance. ,  that's the end of the ledger. Thank you for waiting till the end. , Jackie, with the go ahead. Fantastic from Oh, , from Wednesday."
]