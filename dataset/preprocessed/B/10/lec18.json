[
    "David Ifeoluwa Adelani: , I also have people on zoom. I believe you can all see my screen. , ,  today we'll be talking about neural machine translations. That is  more relevant to our models have been trained  than Ibm models , nobody uses that. today we'll try to finish off with Ibm models. And also we'll touch about we'll touch on some important concepts , how do you  get your Machine translation output, because when you're trying to decode from the decoder side or any other model you use, you need some authorities to decide  what will be your finer translation. if you still remember the large language models, language models that we talked about. When you want to predict the  world they are. there are different probabilities, and there are sometimes that's the same. That's you have different words that are equally probable.",
    "And the question is, how do you decide on what would be your candidates? Words that you will produce? also today we'll talk about attention mechanism, , which  will bridge the discussion from Rnn and Lstms to the transformer. And the interesting thing is, some of these techniques have been developed . we have some work that have been developed also at University of Montreal and And   one of the authors of the attention mechanism is  still around. you can talk to him at some point and then we have transformers and landing models, and we'll talk a little bit about prompting. There's nothing technical about prompting, but there's there's some ways you can do prompting that will give you better results  just to continue from what we did last time we talked about Statistical machine translation. If you have translate if you have a sentence in English, and you want to translate to another language,  French or Russian. and then we say, we can formulate this using the base rule. Using, what is the probability of E multiply by the likelihood probability of F given E, and then we can take the Agmax based on the alignment model.",
    "for the Ibm series one Ibm model, one. It was one of the 1st models from a set of 5 models, and if you remember, from the last class, the most important thing we discussed is that you want to align every word from English to another language, say French, which is our example. You want to align every word to 0 or one target word. and then you need an alignment model to be able to do this. and once you are able to get the alignment model, and you can have more than one alignment model. And then you need to pick the best alignment model to use. And for the simple case of Ibm model one. You don't need to model different distortions of the world. You don't need to care about the world or that. And then also there's no need to care about the likelihood, the likelihood of fertility, .",
    "caring about if a word  translates as phrase in some other language. because  you don't have a direct translation of that word in a language we talk about some languages that do not have concept  grandfather and broader. And then you need to find an  an explanation in a target language for but for the Ibm version one or the model one doesn't  treat these cases. And we said, if you don't have any training data to start with but you have larger labor data, what you can use, you can use expectation maximization. where you start with counting. computing mle over every token in your target language and in your source language, , French and English. and then at the end step, you can  compute the likelihood given a parameter theta and  in practice you don't initialize this likelihood you're trying to compute uniformly if you have a reasonable size of lexicon. what happens is that in practice you try to restrict this only to your training data to available training data. once you're restricted to your training with that, then , this  reduces the number of parameters you need to compete. for all the variants of the Ibm model.",
    "when the sentence length are high, there's a need to come up with an algorithm to be able to  compute these probabilities more efficiently. in Ibm version 2, we don't assume that all possible alignment structures are equally possible. we don't have this street assumption that every word has to go to 0, or one word. in this in this example. you can have a word that is attached to another word. and then you can have a group of words that  mean the same word. you have a single word in English, and  means 3 different words in the target language. Ibm version 2, Ibm model version 2  allows you to model this. Why, Vashawan only allows water to order alignment. version 3 and then this  models it more explicitly.",
    "On how many words can you allow to model a single word. Do you allow just 3 words or 4 words? Then you decide and then model 5  makes use of dependency structure of alignments,  that they don't depend on each other. all these ones are  the series of Ibm models that have been developed. But in practice nowadays what we usually do before the neural models is what is called phrase-based, statistical, empty. And after that we have the newer-based statistical engine. for the phrase based status. that you can leverage the knowledge of your status to put Lm and  use it to do machine translation directly. And there are some times that this is effective, especially if you don't have a lot of data. if you have a pair of language that you don't have a lot of data, then typically people advise phrase based statistical, empty ?",
    "But by the end of this class. I'm going to tell you how to address pairs of languages where you don't have a lot of data even in Europe, based empty. , previously it used to be. You don't have a lot of data used, statistical, phrase-based, empty. You have a lot of data used in your basement because you have a lot of training data to have a good model. here, for the phrase base, you can have different possibilities. You have a word that means another word, and then you can have a phrase that can also be translated to another phrase. rather than having a world based statistic, statistical, empty. that what if you can model immersion translation even with context? That is the idea of the free space smt.",
    "And here  that instead of splitting the sentence into words. you  just split it into phrases. and then you will be able to  compute the probability of the target phrase given the previous source first, st and then you could add some distortion. This distortion is important, because. depending on the language you're trying to translate to the word order might be different. you can have a group of phrases that will. That's that is supposed to be normally at the beginning, and then this is  shifted to the end. you need this  distortion parameter to be able to address this. whether you are using the statistical Nt or you are using the neuro based empty. you will need to do what is called machine translation decoding.",
    "Because when you're trying to decode you're trying to produce the translation of a sentence of a sentence, 1 word at a time in the case of an Lstm and a group of words at the same time. In the case of a transformer architecture, you will still need a decoding algorithm to  search for the best possible translation that you will give back to the user. , if you use the beam search. it's very common that people will specify a beam size of either 5 or 10, and usually with the with higher being beam size, you'll be able to get  a better translation result. And  that, for , the deal search  that you have,   5 candidate translation. And you need to decide which is the best . And how do you do this? we have a couple of algorithms, such algorithms  sh greedy hill climbing beam search  for the Greedy Hill climbing. that you  want a single translation from different candidates. But this  start by creating a 1 complete candidate translation.",
    "When you translate word for what the question is, can you get what is the best candidate translation for this word out of the different, many possibilities. you have this word in German. and then this sentence is being translated to English. What the English translation it's not very grammatical, as , because you're translating word by word you have this week is the grain which at home. But  the question is, how do you make it? And that's where you have the E climbing. and for the hill climbing, the idea would be is that, can you change the translation of a word to a phrase? And then you can  combine them. , you're trying to formulate a more grammatical sentence. and when you combine the translation of 2 words into a phrase, you can split up the translation of a phrase into 2 subphrases, and rearrange the part of your translation.",
    "and instead of having a very strange grouping of sentence because, after translating by word, you can then have a better translation. , what we discussed in the Ibm model is still true. , you evaluated by computing the probability of E, a probability of FA. Given E, where your translation depends on how you are able to align from the source to the target ports. Do you have questions before we move to the neural machine translation. for the neuro-based machine translation. neural network, one of the most appreciated successes of neural networks is application to machine translation. because machine translation is a very, very difficult problem. Because you need to translate from one language to the other. and these languages can have different preferences.",
    "May I ask a question about hill planning? The greedy step is to take the change that produced the maximum probability. Yes, but I don't understand. Are they applied systematically in a certain order. or is it random until you stop improving. , you can have something  that. You try this, and then if this is not a proper match, then you replace it with another one. and  it's you're keeping track of everything you've already tried. , it's  a heuristics, is a heuristic way of  getting the best translation. for the neural machine translation this often leads to the current state of the art model.",
    "But also this requires a large amount of data to be able to train immersion translation previously for any single pair of language. You need at least a million parallel sentences between. The initial language you want to translate from to the target one. and which you agree with me that this is not available for many languages of the world. one very successful approach is to use recurring neural networks. which by   you're familiar with it. On we use the recurring neural network in an encoder, decoder manner. Where you have the encoder, you try. You have the memory for every input that you can have. you can have input, a BC.",
    "And then you have the Rnn States. you use this state and memory information to compute what would be the outputs that will be sent to another state, and then you can compute. every Rnn state can  output something. But you can design it in such a way that it doesn't output anything at the except at the last stage. in the earlier lecture we gave different forms of Rna, that is available because it's very, very flexible. You can use Rnn for different tasks. You can use it for a for a topic classification where you just need a single output,  it does not need to output values at every single state. And then you can have another task  language model that you need to output of every State, because you need to produce what will be the  word. What you need to do is that you need to understand the entire sentence ? for you to understand the entire sentence.",
    "you do need to output at every state of the Rna. which is C, you can  produce. you cannot produce what is called  a context. Vector this context vector produce  summarize all the information in the of the sentence as a. Vector  once you have this information, you can pass it to the decoder side. And when you cannot  generate a translation of the sentence. Same vector passed to every cell in the decoder? the encoder, when once the context vector is encoded by the encoder. Yes, it passed to every cell in the decoder. Vector no, it's passed to the  cell in a decoder. and then once you produce the  the outputs add the  cell, which is W. They need to also pass it to the  one.",
    "Yes, of course you modify as you go. , that's the way it works. But I'm going to show you another approach where you don't. You need to send every outspot. And this is where we'll talk about the attention mechanism. But for , all you have to assume is that you are only interested in a single vector, that summarizes everything in the source text. and that single vector is  partially decoder for decoding. But  you can reason about some issues with this approach. And what can get wrong? And we'll talk about a modification of this, we where  an attention is introduced.",
    ", you encode every single sentence. And then you have this context, vector, Z, and then you pass it to the decoder side, where you  decode it. One word at a time. , there are some tricks to improve where you can reverse the order of input sentence. Or you can use a bi-directional r and n. to have a better encoding of this single vector. and then you  pass it to the decoder side. Also, another one is that you can train an assembly of translation models and decode by averaging their output probabilities. because you can have all these outputs and then use one of those algorithms that we talked about to  determine what will be the translation. But what is the problem with this? The problem is, if you go from one Rnn state to the other.",
    "As you move across the memory stage, you tend to forget one or 2 things . And once you forget some of this information at the decoder side. the problem is that the decoder doesn't know what is the most important part of the sentence that needs to translate. There are some sentences that if you don't get the say the  word or the anchor word correct correctly. it doesn't make any sense again, . we had this thing a project that we're doing recently. And then the translation was an apple. A day will keep you away from a doctor or something  this. What is the most important word there? It's  apple and a dog.",
    "if your mother doesn't know what to pay attention to  we tried it for a language West Africa, and then it gave banana  the translation was correct. But the only thing that was missing is I changed the word apple to banana. I'm  just this gives you a very useless sentence, because it's not a banana idea that would. That will keep you away from the doctor. the question is, how can the model learn to  focus on the most important one, which is apple ? And and you can sometimes you also have a sentence where you can use a pronoun. and then you use it to refer to another word. and when you will be referring to it when you refer, when you use a pronoun for a noun? The model already completely forgot about the word you are trying to refer to. the question is that is there a way, we can  model this?",
    "And the answer is, Yes, we can model this. And the short answer is that from this encoder decoder instead of just  throwing away the outputs at every encoder memory cell. Because  we keep just one output. You need to keep the output of each memory cell. And then you use this to compute what is called the attention. and this is why you have this interesting diagram. I didn't know I was blocking. that means you need to gather because the X is your input. You need to gather the output at every. And here they are using a bi-directional.",
    "that the encoding is stronger. And then the outputs you use this output at every  every Rnn memory cell to  compute the attention  you can think about attention as trying to compute what is the dot product between 2 vectors. and then it's a measure of similarity. And then, if you multiply them together. what is the most important  where you do  a dot products. And this is the way it works. this attention will  be used to multiply the final output you got. and then what will happen is that because you run the attention over a soft Max is going to give higher weights to some important information and give lower weights to information that not very relevant. and by this once it gets to the decoder side. The decoder knows what is the most important information to focus on.",
    "I hope you get the idea. It's , of course, you can read the paper for the more details. attention as something as a salt retriever. in a retriever system, you have, you want to query. And then you need a key  to access the information, and then you add the value. attention can be seen as a soft version of a retriever system from a memory which has a key. They used to unlock the value. you have the query which is a representation of what we are looking for. you're looking for a document. and then you have a query, and then you want to check the similarity between the query and a document.",
    "You have a representation of what you're looking for as a query, and then you have the key, which is a representation of an entry in memory. and then you have the value, which is the information that is being stored on general, what we use to formulate. These are just  random mattresses that we give different rules and one random matches we serve as the query matrix and another mattress will serve as a key mattress. Another one will serve as a value mattress, and then by construction, because of the way the attention has been computed, you assign different rules to the different mattresses to  give you to model what is called the attention. you can visualize the attention. If you have the sentence. This is one of the most famous example that you see in attention. And this is from the newspaper, and here you have, what is, what is the model supposed to focus on? the agreement on the European economic area was signed in August, 1992, what information are important and what information is not very important. , and  the matches, the visualization of the of attention.",
    "you will see that The agreement seems to be very important. and then you have European seems to be very important, and then you see that even though it was swapped in the French sentence. You see that the model was able to pay attention to this, even though there's some reordering happening. the model is able to pay attention to it. And the model sees that August, 1992 is also very important. And you see that you see that it's very bright. And you have another example here. And by this people started thinking about, oh. that means we can interpret our neural network models by computer attention. Does it give you a good interpretation of what is happening in the neural networks?",
    "But this  gives you an idea of what the model is trying to do. this visualization of attention gives an idea that the motor is paying attention to the most important part of the sentence. You might repeat what you said about whether or not attention tells us what's happening in a network. I'm I'm saying it's not an interpretability tool. if you just analyze the attention on what. you can have an idea of what's going on. But it doesn't give you a complete picture of what's going on in the neural network. But it's a black box, ? But at a certain step it shows you what the model's paying. , there are some times that if you analyze the weights.",
    "you  have an idea of what's going on. And then sometimes that's just completely random. does it depend on the model or it's random within a model. every model has some structure going on. , , if you take a birth model and you analyze the lower layers, it has been shown that, just analyzing the lower layers,  that it's capturing some synthetic information taste  part of speech. But there's sometimes, if you analyze some other parts of the layer, you don't see anything going on. that's why I'm saying that you cannot use it as a complete interpretability tool. Sometimes it helps to give you some additional information. But in the case of Martian translation, . I'm sure if you visualize for some other sentence, you may not be able to see a very plain trick.",
    "And with his attention getting popular. we  have what's called a transformer architecture. And the transformer architecture is very simple. Let's do away with recurrence. Let's do away with what is everything. We still fix the encoder decoder part. But let's do away with recurring network, and let's just use attention. And it seems to work. , we're still using attention since 2,000. And we're still using transformer architecture since 2017, and the title of the paper, which  attention is all you need.",
    "and I remember there are many papers that says, this is all you need. This is all you need. But at least we see that, for  it still seems that attention is all we need. And the interesting thing with the attention, architecture, or the transform architecture is that it has been applied to different modality, to vision. It seems to work to. Although we have been trying to replace it. far no one has been very successful. you can try to replace it in that . to  replace Rnas because Rnas has a lot of problems one of the bigger issues of Rnas are things  vanishing gradients, ? But there's a way you can address that which we already discussed in previous class.",
    "Because if you have a really long context. Rnas, which they struggle because, , you're passing information from one cell to the other, another cell, and at some point it's the length. The context length is just too long, and it has forgotten a lot of the information. it still has some issues in modeling long context dependencies. because you are passing information one time, one step at a time is also very difficult to train. and it's difficult to paralyze what attention you can paralyze everything in training. and then you can allow flow of information from one world to the other, because everything is  you're computing just mattress multiplication. they are very cool ways of paralyzing matches multiplication. And then it's  a computing those multiplication trying to understand how with vector, is similar to the order. all this can be easily paralyzed, parallelizable compared to Rnns.",
    "for the attention in transformers, you have the sentence. and then you have an embedding. and the goal is to compute the  layer of water representation at Layer L. And what we are trying to produce is that given X and the embedding of the words  given the word and the embedding of the word . and by  you should know how to compute the embedding of the word, even if you don't learn it yourself. A lot  using what's back and all that. You, the motor, can also learn it by itself. you have a sentence, and every you have a sentence, and every word in the sentence also has its own embedding. And then the  thing is, how do we compute Z.   that we can  just do mattress multiplication. using the structure of attention and  learn what is Z.  we want to learn a distribution over words to decide how important each word is in order to compute the representation of the  layer. It might be that when you compute the  the  word representation is just going to tell you the same word is important, but sometimes it's going to tell you that another word is more important than the word.",
    "I'm  imagine that you have a representation of w. 1 w. 2 CW. Head, and then you want to know which of these is more important to w. 1, and then you do the multiplication across the embedding. and sometimes it tells you, for w. 1 w. One is the most important. Then sometimes it tells you, W. 8 is the most important ? And then you want to learn this. And the way we learn this is using the query, key and value. in the query you want to use of this word as a query. because we want to compute the representation at the  layer. we always need a query for the key. We want to use this vector to decide how important the world is to another world as part of the attention computation.",
    "This is the value that is being stored. This value associated with the key. And this is how you compute a value. each view is associated with its own. What we do is just we just randomly initialize. And then we just do the matches. Multiplication based on the attention computation. , this is same attention. You because you save attention is that you're multiplying it by itself. The queries and the keys are those words from the same sentence, , the query.",
    "the key values are for the sub centers. that means giving a sentence. ,  imagine that you represent an entire sentence with a vector. That is your vector then, you  need to initialize different information. You have  a vector representation of query, vector representation of keys, vector representation of values for the same. And then you multiply them together. in practice, query, key and values are just randomly initialized vectors. Initially, they don't mean anything. until you train the model, they don't mean they don't capture any information. what the embedding do capture information because the embedding is connected to the word.",
    "But these other values queries, keys, and values. They don't mean anything at the minute. At the beginning they're just randomly initialized. But once you have the structure of computing the attention which  I can pull up the formula. I want to recommend reading the Illustrated Transformer. , when it 1st came on, nobody understood it. But this tutorial was very, very helpful. and it's still relevant to today of , more than 6 years ago this was prepared. The and the guy  had a very good visualization of what they're trying to do. But in practice is that you have a good representation for the, for the, for every word.",
    "and then you have a representation for what it means for the query keys and values, and then you just multiply them together. we want to know that if you have thinking machines. what is the most important thing? Is it thinking or machines? This is what we want to compute. and it's safe attention, because you are doing attention to the same sentence. You want to know which part of this sentence. or which word in this sentence is  the most important. And here, when you compute, you multiply the query with the key and then you have this value. and then you do that for machines.",
    "You do that for thinking you do that for machines. and then you  normalize it. and then you apply softmax. This model is telling you. This calculation is telling you that thinking is attending to itself. It believes that thinking is more important, ? this word, when attending to itself. It believes that the same word is more important. But there are some cases where  he believes that this, that itself, is not that important. , , if you have a pronoun  it, it might mean that.",
    ", although you are trying to attend to me. But I have another word that was referring to me. And you are able to get this just doing this dot product computation. You multiply the softmax with the value. and then you take the sum you sum. Yes, how important is the quality of the embedding of the word to the results of it. , because you need to be able to encode the word very , and it's either from a separate model,  word to back, or the model  starts with one hot encoding and learns its own. But what we do normally is just to learn everything. everything is lined is the initial input, one hot encoded, then everything. everything, including embedding the queries.",
    "keys and values are just lined. there are 3 parameters that you have today. Do you have other questions? Yes, wouldn't it be faster? Instead of starting with one hard encoding, we start with the embedding of a different model. , initially, it might be faster. But people find if you just learn everything, it's it's better because, the 1st application of this is machine translation. And  I told you, machine translation is trained on very large amounts of parallel sentences. In your training data to  learn a good representation. there's no need to, .",
    "Try to improve another thing. and then you have what is called multi-head attention. you have self attention, you have multi-head attention. the idea of multi-head attention is that you? The way you have done it for a single sentence. do this n amount of times ? you can do this 8 times instead of one time. and then you just have everything is just  master's multiplication. And and then the model figure out what is the most important. it's , you are having multiple ways of viewing the data.",
    "you, you have one way, if you do a single self attention, and if you do it 8 amount of times, then you have 8 different ways of viewing the data and then you can  concatenate all these results together and pass it to the  leg. And this is the idea of multi-year edition. Are the weights shared between each head? you're learning 8 different key value queries. Yes, what about the embeddings? we have transformers in Mmt,  initially, Google translate was based on Rnn end. And then and they've moved to transform our architecture. And they achieve very good performance. , they didn't change everything. The I try to just change the transformer encoder.",
    "And then  this is transformer. This is what you'll see. you have the encoder parts. and then you have the decoder part. , the batch model only use the encoder parts. They don't use the decoder part. They never use the encoder badge. because you see that every model, whether you're using encoder only, or the decoder. Only you always have the embedding. You have to learn the embedding,  you can just ignore the encoder path if it's not necessary.",
    "He wants to understand what is in a sentence, to be able to make a prediction, to focus on the encoder path. Gbt wants to make a prediction and generate text. They focus on a decoder part T. 5. Model tries to use both. It wants to have a good encoding and also do a good prediction,  they use words. for machine translation, you need a good encoder to encode the entire sentence,  that you can pass this information to the decoder side to translate. we need a good encoder. Google translate, they did away to just do away with the they added an encoder, and then they have a better encoder, which is a transformer encoder. But they still retain the Rnn. Based decoder, who has an idea why they retained the Rnn.",
    "Do you have an institution? ,  they  just use transformer everything. But then why they do this? Do you have an idea? But it comes from the line that it's less expensive. why is it very expensive? I know you can say, but why is it very expensive? this computation is N squared. it's really expensive at training, you can paralyze things and just wait on different gpus. But at the prediction time you still need to compute all these attentions, ?",
    "But Rnn is very cheap. You just need to predict one world at a time. what they did initially, just to ,  decode Rnn is cheap for decoding,  they  just use the transformer encoder opposed to the decoder to save cost. But   we are making a lot of improvements in terms of developing more powerful gpus. This was really a big problem, because it's very expensive at decoding. And there's a lot of research  focusing on how to improve the efficiency of at the decoding time. How to minimize this N squared to something. n, times log n, , to  reduce are this expensive inference. Because Rnn will feed one token and predict the  token untransformable. We're just feeding the entire context in order  far to predict the  token.",
    "And this brings us to another architecture that's really famous. it's  we had this transformer era we have. and then we have the bet era that's  rain for  4 years. And Chatgpt came, and then everybody forgot about that. And  we can use the same architecture of the transformer. We're  focusing on the encoder only side which is on the left. and the way this model has been trained is also a language model. But instead of predicting the  token, what you  predict is a Max token. This is a world Maxed, in this sentence, ? And then you have to train a model to  predict what is the Max token.",
    "and it will also train on what is called  sentence prediction. Does this sentence follow this sentence, and the way it was trained is very simple. and then you have an entire paragraph. You have a sentence by sentence, and then, if 2 sentence follow each other, you'll see a  continuation. It's very easy to predict if this sentence will follow each other. if someone is talking about a sentence in economics, and there's another sentence in out. it's not very likely that you follow each other. And then with this task we are able to know. to predict if the sentence follow each other, and combining this Max language model with  sentence prediction, you can  create a very powerful encoder that can be used for many natural language, understanding tasks from sentiment, classification to topic, classification to any classification task based color. and it was strained on a large amount of data.",
    "Then it used to be large, which is  800 million watts. which is called books, covers and Wikipedia and it's up to 3 340 million parameters. and apart from this, they also train a multilingual model on, , over 104 languages. which we also have a multilingual birth model. But people have made different arguments that you don't  need this  sentence prediction. the Roberta model, which never got accepted,  just did A did not use this nest sentence prediction, and they were still able to have similar performance. But they also increase the amount of data they train. that means if you train on more data you don't need. You don't need this second task of  sentence prediction. Why didn't Roberta get accepted?",
    "Because, sometimes reviewing is a very noisy process. Sometimes you have a bad review, and then the fact that's rejected it. but with the multilingual version got accepted. we also have,  a Gpt. 3, which, you  know by , is from open AI. And what they did was to just scale the amount of data I told you about Roberta scaling the amount of data, and they had better results. And they can even do away with one of the pre-training tasks, which is the  sentence, prediction for Jupiter 3. They  train on more data, 500 1 billion Watts this was published in Europe's, and it has  175 billion parameters. It's really a big model. And at that point it was too big to be released.",
    "But nowadays we also have,  more than a 75 billion models that have been released. It's just very difficult to run them. And  they put it behind. in terms of the successes  that if you train on what we call a self supervised pre-training. self-supervised training means that just using an unlabeled text, you construct a supervised tax to train your model on unlabeled text, and you have what is called self-supervised training, and the task for the for the birds model is just predict the missing token, which is a Max token. And if you're able to do this, you can create a very powerful encoder that can be fine-tuned on many downstream tasks. part of speech, tagging anyhow, sentiment, classification, and  on. 3 came up with another idea which is really cool, which is, you can do what is called 0 shot learning. , you can just predict. the model has been trained on just simple  sentence prediction.",
    "But if you prompt the model. you can try to get a you can try to solve in natural language, understanding task by just prompting a generation model. the idea would be if you want to know what would be the sentiment of a text. you can say, this movie is great a name you. You can tell the language model that the sentiment of this will be positive, and then you give it another example. Then language model just with that single example which we will call one shot. The language model will be able to solve the task of sentiment classification. You give it an example, then it has an idea that oh, the movie is great means positive. if you give it another example. this movie is just a waste of time.",
    "They knows that this is a negative sentiment. the interesting thing is that almost all Nlp tasks, I would say, oh, I don't see any tasks that cannot be converted. To just prompting the language model to give you what will be the performance to give you the result. You can do this both for 0 shot and few shots. with a few examples, and the model will be able to answer him. other interesting architectures that  it would be nice to mention would be  architecture  T. 5, model. And T. 5 is quite interesting because they use both the encoder, part of the transformer and the decoder side, and it's very similar to the bets model, because they also use what is called span corruption. You corrupt some part of the sentence, and then the model  reconstruct it. you  apply a noise function to your text to mark something out. and then the model is supposed to reconstruct the entire sentence.",
    ", however, for the T. 5, they are  not reconstructing the entire sentence. What they are doing is  just providing what is missing in the input  you have this example in the original paper. And then it also retained the sentinel tokens. but  predicting what was missing. and also retaining the another sentiment token  the Y, and predicting what was missing. And then they have a final token. this was how T. 5 was created. strain on the C 4 corpus. and it has, , over 34 billion tokens. and they scale it up from a very small model to even 11 billion parameter model.",
    "And  you can cast any task as a text generation task. you have a translation task translates English to German, and then you send it to the model it can translate. Then you can give it another sentence. You have 2 sentences, and it can give you what would be it can give you the rating. , this one is  similarity tasks. you want to know if sentence one and sentence 2  similar. And then you can have a grammatical check task which is a caller data set. And it's gonna tell you if this is acceptable or not, and you can have another generation task  summarization. And it's gonna give you  every task. And it will give you it will generate what will be the outputs.",
    "But one issue is that it's sometimes it's tricky for a classification task. You can model it to just give you , 0 1, 2, 3, 4. If you have 4 classes. , , you force the model to always generate what would be the prediction? if you have a topic classification , categorize this into business news sports, news. entertainment, news, or political news ? Sometimes it can generate different things. It can generate political or politics. and then you have to be able to Co. To convert what has been predicted back to the class you are interested in. But this is very easy to address.",
    "we also have a multilingual version of this that was trained on, , one on one language and a lot of data, 6.3 trillion tokens. And, , you can use this for the machine translation task. because it's a multilingual model. it can do what is called crosslingual transfer. , you have a language in English, and then it will be able to translate it to German. Some, the pre trained model can already do that, but it's just that. It's not very good, but if you fine tune it explicitly to do this, it will give you  better result. , this was very interesting to people working on multilingual nlp, because  you can do this for different languages by starting with the multilingual model. we have other text to text models that will be interesting to mention, but model, which is  the topic of your reading assignment, which is similar, objective as the birds model, but with an encoder, decoder, transformer architecture. And we also have the multilingual variant of those models.",
    "We have the bat model with 25 languages, and we have another bat model with 50 languages. And  you can check the 2 architecture and see what is the difference? , there's no difference in these 2 architecture. The only difference is . There's no difference in the pre-training task they are doing  a Max language model. , the difference is that bass is in an encoder, decoder architecture. and they also try to do different things. Interesting things with the noise function to see what part of the sentence should they, Max? Should they, Max out just one word or a phrase, and then the model reconstruct everything. And  we also have another branch an another idea.",
    "models  embat or T 5 has been trained on only on label text. which is no supervised learning. This is just on label text. But you can also train on massively amounts of data. you can train on if you have a lot of data on  many languages. This can also be trained ? And  you are not training on just not you are not training on monolingual text. But you're training on parallel text. M. 2, m. 100 was created. I forgot the citation, but  they were created by  Meta, and discover,  over 100 languages which they have trained on parallel examples in all these 100 languages.",
    "5, which is just based on which is not based on parallel data, . And then you have another B 200, which , they scale it from 100 languages. to 200 languages, and this seems to still be the state of the arts for machine translation since the last 2 years. and they also release an evaluation data set. That also is very popular, which is called a Flores 200, that covers  200 languages. And this picture shows you an example of the different language groups or regions that are covered. lastly, I wanted to share one interesting finding, which is even if this pre trained model has not been trained on a language. there's a way you can  just quickly adapt it to that new language. if you have a few 1,000 high quality translation compost for a new language, even though it's not trained during self-supervised pre-training or large machine translation per training, you can still quickly adapt it to a new language. this is an example of a language rule.",
    "In,  East Africa, where  hilly has been covered in the pre-training blue has not been covered. But if you take,  an original pre-trained model,  empty 100 for Swahili. it already achieved 20.1 blue score. and for  that's from English to Swahili and Swahili to English achieve 25.2. But you cannot use this model to generate for a language called Luo, because it's not covered valuable training. But if you fine tune it on high quality compost you can  just have. you can use it as a translation model, ? , which is very cheap. Suppose you have,  5,000 parallel sentences, and then you can fine tune this pre-trained model. whether the one that has been massively portraying on unlabeled data or massively portraying on parallel data, and then  some huge improvement in performance, whether you use Mt.",
    "5 or you use Mbat. And oftentimes, if you use a machine translation model you often can get slightly better performance than if you use or models that have not seen parallel sentences. it was almost  wrap up there. The last thing I wanted to talk about is current trends  for Nlp, including for machine translation, is that you can just prompt a language. Model this guy, you can come to language model. And  sometimes there are interesting things you can also do is that you can provide instruction, you can provide a demonstration which   an example. and then you can  provide a query, and then it will give you the results, and I believe all of you are already aware of this. Another thing that can boost the performance is what is called chain of thought prompting. And it's  you ask the model to  explain the thought process and trying to predict the output. for the standard prompting you.",
    "You give it  an example. And this is the answer. You can also prompt it where you provide an example, and then you provide an explanation on how it arrived at the answer, and the explanation doesn't have to be much. You can provide just one example. the already give the model an idea on how to solve the problem. or you can provide more than one example. , this Mgsm was . Short example is  8 example. and by this you can have a better performance of the model ? Because the model also will try to replicate this chain of thought idea to be able to produce better results.",
    "this is  teaching the language model to reason before it produces. ,  , you have questions. , , and on Monday we're gonna have another class on multilingual nlp or crosslingual."
]