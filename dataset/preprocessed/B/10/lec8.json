[
    "but , I would go there. , can you hear, me, , , I, guess, we can start   Lecture 8 where we'll be talking about. We'll continue our discussion about part of speech tagging. And today we are going to be going through some algorithms that are popularly used for this task. And  last time we are trying to compute,  Emily. I have to remember Emily for any Markov model where we assumed just a background model and today we're we're going to try to give you more general framework that has been used because people have developed a couple of algorithms for Hmms, and this is what we will be describing today. last time, we defined path of speech as a synthetic. that's a glory for every word in our text. , we have things  nouns and examples  restaurants, dinner. We have verbs, we have adjectives, we have preposition adverbs and determinar.",
    "And also we talked about different ways of being part of speech different schemes, ? who can remind me of the different schemes? We  examine 2 popular schemes? yes, there was a pantry. Scheme, and then there's a universal one. Here we , this is an example of the pantry bank  scheme. where we use  nlp for proper nouns and for universal dependency. We use, , , pr ovn  they have various different symbols. And why, what was the motivation for having a more universal dependency scheme? try to generalize across other languages.",
    "And here we also talked about stats for when you're trying to build a model for this  task. you need to consider the current world if you want to compute the mle. you also need to consider the previous context. And similarly, for the part of speech. The  part of speech depends on the previous part of speech. we established that, using a very simple diagram example. where the  part of speech we always depend on the  on the previous part of speech. And also, , we made that  assumption which we call the Markov assumption. It's a general term that is used. Markov assumption, where you have to reduce the context instead of saying, you have all the context, saying, when you make this conditional independence assumption.",
    "We also call it the Markov assumption. here we  relate this to Markov chains. And in Markov chains we talk about how we can decompose the joint probability which by  has been removed from the board. we have the probability, the judge probability of all the observations and all the States. what is the Gedian variable? They are the Union variables and the o's are the observations what you observe. the States would be the part of speech. Tax the different ones that occur in your sentence, and then the observed variables which is your all, would be the words. And here we make an assumption here that, given the State one. With some initial probability you can then generate what will be the observed word which would be, o 1, and also, if you go to Q.",
    "2, you can generate with initial probability, O. 2, and then with state transition probability, you can go from q. 2.  in the diagram face that we described. if you want to say probability of OQ, where all and q are random variables, and all are all your observed. Observe observations, or observe variables. Are all the States which represent your part of speech. You can say, this will be the product of the q. 1, which is your 1st part of speech which you can compute, based on uniform probability, that how many times does this part of speech occur, or how many times does this part of speech starts my sentence across the entire compost? This is the way we competed. And then you multiply that with the State transition probability for it to move from one part of speech, from the previous part of speech to the  one, and then you multiply this with the emission probability to  generate a word given the tag because there are many possibilities.",
    "And here we also talk about the model parameters which we can estimate using an mle. And today we are going to try to have something  a  a generalized, how do we  compute parameter theta that is more general. this is what we want to. Consider today, I must warn you. Today's lecture is highly technical. we have the initial probabilities. And then we have the transition probabilities from Qt. Plus one which we can also compute using mle. And we also have the emission probabilities. which is the probability of generating award given.",
    "And to compute it,  it's very important to do this revision. Just because we'll leverage on this in this class and here to compute your pi high height. And here you're just concerned about which part of speech can begin. The sentence is as simple as that. and then we have the count of this. High equals the part of speech divided by all the possible sentences that are in your training couples. This Emily, is often calculated over your corpus. and then you have the a high J, which is computed that given. If  the previous State, can you predict the previous? And we can also compute this with very similarly, assuming this is  a diagram.",
    "distribution where we can count how many times you have in our couples, how many times do we have a tag? I, followed by a tag. J, and then we normalize by the number of counts of act. and the last one would be the emission probability which we can also compute as number of times you have one K. Contact. but I'm dividing it by how many times we have qt equals I in our compass. this is where we ended the lecture the last time where we talk about , after we compute the likelihood. in the last time we have. And  we want to consider how to  compute the maximum a posterior. which is  the map, we want to  estimate what will be the theta? we want to estimate .",
    "What would be the likelihood of a sequence of observation which is the probability of all given Theta and this Theta. But this is what we want to calculate. and this data can be calculated by examine examining all the possible states that can happen when you're trying to go from one part of speech to the other. Also, we want to compute what will be the best States. On the observation which is  we are trying to compute the judge probability given the parameter of theta. and it's also possible to compute this if you don't even have any level. This is in a very unsupervised way. And here what will happen is that you 1st initialize the state with random probabilities. and then you can  try to improve on this state values iteratively. we are going to examine things  the em algorithm that is very popular in mushroom.",
    "Do you have questions from the last class. Because this is  what we are looking for. Eventually, the part of switch. why didn't we include the new one. the one no one doesn't consider the but of course, , it's not that it doesn't consider it. But you can marginalize to give you one. Do you remember the auto marginalize? if you marginalize over all the queues, you are going to get payable? yes, and that's what we have here. how do you compute the likelihood?",
    "And the simple answer is that you just have to marginalize over all the state sequences. And this is a very simple law of liability  you have the joint probability, and if  all the joint probabilities. You can just marginalize to get the other one. the problem is that if you try to do this. you have exponentially many paths and risk to priority. Imagine you have 2 states, and then you have 2 observations towards to be generated. That's already  4 possible bots that you can have. imagine you have 2 states. and then you have 3 observations. this is , it's possible paths that you can take.",
    ", and the solution is that we can try to solve this using forward algorithm which is coming from dynamic programming to avoid unnecessary recalculations. we're trying to create  a table of all possible state sequences. And then we calculate the probability. we  say, the  state depends on all the values you have computed in the previous States. And then we go step by step. we go from one state to the other. And then  we  try to create a lot a table of values  which can be referred can be compared to  having a trellis. having a trellis of possible state sequences. And here, this is  every possible combination that can happen. here you have all the States, and here your States are the part of speed tax.",
    "and then you have all your observations that happen via time. you have the 1st observation, and you move to the second observation, and you move to the 3, rd to the 4, th to the 5, th and  on. and once you get to the hand of it. once you have calculated all the state values, you can marginalize the entire values to calculate what is the probability of all given theta. the values inside the States can be referred to as what is the probability of observing from one to T, because for every State take a random one there for every State we are saying that just think about every value you select from a colon. We are saying that it depends on the previous state at time. T.  if you want to compute for time, t plus one, it depends on everything you have on time. T.  if you want to compute for  Alpha, determiner of 2,  it depends on all the values you have from Alpha Vb. In the time step of one. ,  and here we want to copy what is the probability of the observation from one to the to that time where you are to that time step.",
    "And then what is the state that you have? the queue which refers to the state values the part of speech, and then given the theater. What's the probability of the current tax. And what's up to ? Because this is everything that has been observed from one to 2. ,  here we can compute what will be the initial state probabilities. what is the initial state probabilities? We I gave you an inch of that last week. this is how you can compute the initial state probabilities which depends on. If what is by J. . that's the initial state probabilities,  initial state probabilities.",
    "Then you multiply that you  observe this? this is how you compute the Alpha G's. for every Alpha they can be computed as this. Yes, I don't understand why we can multiply by zoom. look at what you're trying to compute probability of 4 1 wants to see. and then key of T, ? you need what is the initial state vulnerability. which is the 5G, and then you need to multiply this by  observing forward with this conditional independence as a machine. Have we already marginalized for queue? we have not marginalized for you.",
    "The probability of Dt beginning this sentence, . the probability of a meeting, the what you observe? if you were , let's go to last. This is what we did the last lecture, ? We have the initial probability. if you remember the calculation, I can open the slides, but it's very, very similar to what we did in the last calculation. where you have an initial state, you don't have transition probabilities,  there is no way you multiply. Do you remember we factorized stuff? we have this big equation here. You have probability of q 1.",
    "You have probability of q 1. Which is the pi eyes, or, , that would be your pi, j's. and then you don't have state probability that there's nothing to multiply ? But you do have a mission probability. , what probability of a meeting that world? and that's why you got this. based on what we have done. , then, the complicated thing is that if you want to  compute for the States of, , Alpha, Dt. We are saying that this depends on everything from time one to time. C.  everything, all the States before that  contributes to the value of the  state, which is Alpha, Dt.",
    "And then if you do this over and over again. this is   the formula,  where you are. These are the previous states. all the values you sum over them. and then you multiply, which is your aids, your aids will be your transition probabilities, and then you multiply by the emission probabilities. you should be familiar with aig multiplication with Bjot, because this is what we have been doing early on. But what we are saying  is that in the middle you don't have the you don't have the initial state probabilities. you only have the values at the previous state that you need to multiply. and then you sum up all the values across all the different States. that means all possible ways  that means you consider all possible ways in your part of speech.",
    "in the last column, once we have calculated everything for distrellis,  you can. just  marginalize marginalized means you sum over all the probabilities to get your Po given data because everything previously contributes. And then you continue to  sum up all the probabilities until you get to the last one. And at the last stage you can just  sum up or everything. You have calculated to have the probability of O given Theta. we can  transform this into an algorithm. I don't know which one works better for you the algorithm or the training. first, st we create a chalice for high equals, one to hand. I, I equals one to hand. These are your different states, which are the part of speech tax, and T from one to T. These are your observations.",
    "you create the States across every part of speech. And then every word in your sentence which are your observed values. And then, for the 1st one, which is the Alpha. This is the way we calculate it, which is the Pi, J. Bj. 1, and then you have an algorithm that goes from one state to the other. And if you want to calculate, what is the time complexity of this? it's very clear that this will be all, and to n squared T. Because you have n twice. 1st you sum over N, and then you have another for loop over N, and then you have T. And once you have completed all the Alpha Tj's you can marginalize to have what is the probability of all given Theta. we can also have a backward algorithm which is the reverse of that. in the backward algorithm, instead of starting from the initial state probabilities that you have.",
    ", we assume we are starting from the last. , from the last cell. And then you go backwards. And that's why it's called the backward algorithm. And here, and  based on because we are just because this is the probability of a joint probability you can do forward or backwards,  I showed you the last time in the language model. You can say, what's the probability of the previous? Given the current, you can also do probability of the current. probability you can do both probability of the current given the previous, and also you can do the reverse, which is probability of the previous. Because the law of probabilities  does allow us to do this. ,  here you want to call instead of completing the alphas!",
    "The alphas are the are the one we computed at the forward algorithm in the backward algorithm, you compute the betters. What's the probability of T plus one to T, because  you're starting from T, which is the last cell and then given Qt equals. R, , here the backward algorithm is  very similar to the forward algorithm. One thing we assume here is that we don't have initial step probability. But all our betters at T, the last stage are given once. you  in the initial probabilities for the alphas, you have to multiply the step probability with the probability of  admitting a word, and here the last one you just give them once. the values are known, and after that, then you go backwards by  completing this. And after that you can  marginalize again. And the marginalization is very similar to what you have in the because when you're when you get to the 1st  you're going from the back to the 1st state. You have initial state probabilities which you need to multiply would be betters.",
    "I will show you the offer, the forward algorithm again, in forward algorithm, everything is already integrated. the last stage you just need to marginalize over what you have at the end. And in the backward algorithm you still have your initial state probabilities, and then you have your probability of emitting the 1st word. and then you multiply that with the benches. that's the back order going there. That's the 1st token one. , that's the  we have an assumption that you just give them one. 1, 1, 1, 1  for the backwater guardian. you need for you to  pro progress backwards. You need to assume, because you need to have something to start your going  to.",
    "You need to have values to start. You need to have some initial values to do your calculations  for the afford algorithm,  this is very clear because you can assume the initial state probabilities. But for the backward algorithm we don't know. we can just assume the maximum value you can get for probability is one. And then you just assume that value. And then you go backwards. for the forward backward algorithm here you just out to multiply what it offers and the betters together. this is what is called forward backward algorithm. And before, after you multiply the alphas and the betters together. There you marginalize our A from for every timestamp, from one to 3. ,  as  here, there's a lot of probability multiplication which  can cause problems.",
    "there's a simple trick that we use. if you have, if you have to multiply a lot of probabilities. And you need to take the Agmas most of the time. It's better to work in Logan, because, instead of multiplying all you have to do is to just sum. and then we can have what is called this log some trick. I can give you an annotation with  a very simple example. if you are multiplying all the probabilities together,  you see here, we keep multiplying the alphas and the betas together. If the values are very, very small, you can have what is called  on the flow. and to prevent that, it's better to work in logarithm. you just sum the properties together.",
    "And, , if you take the log of all these products of pis,  you will have a summitation over the log pis, and if you assume that log pi is plus AI. , if you want to log some trick, is what is the log of sum of probabilities? , what is the log of sum of exponential of probabilities? the formulation you have below is very used, especially if you're implementing. If you're implementing probabilities and then you multiply a lot at some point, you will get some value very close to 0. Is that different from the same A's in the forward propagation? You need to apply Bayes theorem to inverse the yes, , because you are going backwards. they're different from the one the same age in the? Oh, , because, ,  to be honest,  they are the same. because the AI is the way we calculate them quits.",
    "if you compute this, whether you go from Ji to high. , I see what you mean. you're talking about the normalization. if you are going to the backwards, yes, yes, it will change a bit. Yes, there will be distant. I didn't understand the forward and backward multiplication this one. this is an assumption of the algorithm. you have 3 different algorithms. , you have forward algorithm backward algorithm. And then there's 1 to combine both.",
    "I've tried to connect, and there's a reason why they connected both together. Because you can use this, both for the supervised setting and unsupervised setting for the all supervised setting. You don't have the labels. but you can use the forward backward algorithm and combine this with Em algorithm, which I will show you a couple of slides. the log sum trick is  the log of the summation of exponential of probabilities. which I can show with a very simple illustration on the board. let's let's give  very, very, very simple example. Let's assume everything here is  base 2.  where we have log of truth. and then you have the summing. But something very similar to this.",
    "They're serving across all the exponentia. And what they say is that this is stuffed off. it's just the maximum of all the pis. that's a p 1, 2.  hey? And  what is the maximum? let's take a very, very, very simple example. Here, we're saying, this should be involved of the maximum here in this very simple example. and then 2 rest of our heads. And then if you solve across this,  you're saying, pi. , 2 cool and even trying to do very, very simple example.",
    "You'll see that they  books. Is it clear from this illustration. you see that they're  equivalent. , , if you're implementing it, they are not equivalent. Because if you multiply a lot of probabilities together at some point, you're just gonna get 0 because of the floating point limitation of your what do you call it? Of your computer at some point? This is just be very close to 0. But if you take the maximum all probabilities and you do the do it this way, you can  get a signal. this is a very, very common trick that is used when you have to multiply a lot of probabilities. And also we can use it here at implementation stage.",
    "we have talked about how to compute the probability of all we have competed. , we have describe the probability of all given theta, which is the likelihood. And , how do we  label the samples? Which is what is the  queue. If you take the Ag mass over all the probability of Qo, given theta   this is  vitabi algorithm, which is very, very popular. which  you can read up more is very similar to the forward. , instead of as summation, you  just take the maximum instead of summing. you have the States here, and then you sum instead of summing, you just take the maximum. you start with something very similar to this with a forward algorithm. And then you take the maximum over all the datas also you have the same runtime.",
    "But when you  do this calculation, you have to also keep track of where the maximum entry to each cell came from. You have to keep track because you need it for the  to calculate the  you need you to calculate the  column ? You need to keep track of it. we have a short exercise which I hope will help to clarify things if you do it yourself. here,  because of it's a lot of material. We  would do this together. let's assume we have 3 States XYZ. Which also corresponds to your path of speech tax. And then we have 2 possible emissions which are your what do you call it? You have your what's this symbol?",
    "And at and we have the probabilities. we have the 2 parameters? A and B,  what is a. yes, please, what is a. yes. , it's a state transition probability. And the is the emission probability. you have the emission probability. You have the You have your State transition probability for the forward case. even without looking at a solution. How would you calculate it. I may have to write the formulas on the board, because  it's more difficult.",
    "for the time we are guarding. the only difference is that we're going to take the maximum instead of the submission. , I believe this is all we need for the exercise and for the forward backward. We are supposed to multiply. and the better it is together. ,  this is the exercise. I just need to have the values. Think it's  easier to take a picture just all . and what we want to compute is  want to compute the exclamation mark at ? if we want to compute that for every stage for the forward algorithm.",
    "what are we going to do based on all the information you have on the board? What is the formula for the 1st state you have to compute? What is the initial state probabilities multiply by the admission probabilities. what is the initial step of abilities which is your pile here. This will be 0 point 1 times your initial probability, which is B times 0 point 1. you multiply the 1st pi high with the force emission probability, because B is your emission probability. Oh, I'm good is 0 point 1.  this will be 0 time, 0 point 2 times 0 point 1.  for the second one, what should be the value? this would be the second state probability that was 0 point 5 times 0 point 5. You did 0 point 3 times 0 point 7. . Seat is 0 point 3, I agree.",
    "But then to get the app symbol, it looks  it should be 0 point 3 wide scale. No, no, we're not doing the arts. We are still at the what do you call this? because we want to go for exclamation mark at. all the probabilities for exclamation mark is what you multiply with . you are going to multiply each scale probability by the observation. What you what you observe. we have 3 things we are observing. And then we also have a 3 part of speech. 3 states and 3 things we're observing.",
    "at the 1st State we are observing just the exclamation back. which is the this and the probabilities for each of the States. For all this establishment, 0 point 1 0 point 5 0 point 7, and the initial set probabilities are also given as 0 point 2 0 point 5 0 point 3 is the 1st color play. what are you going to do after that? We said, every state in the 1st column contribute to what would go to the what? that means we, if you want to compute. which is the Alpha Gt. Minus one would be 0 point 0 2 0 point 2 5 and 0 point 2 1.  you're going to take each of these and multiply by the Aij and Bj of ot. for the 1st one we take 1st X, we go for 0 point 0 2. , what is the Aij 0 point 5?",
    "you multiply this by dj, of old thing video of Ot, we are concerned about apps. we must buy with the apps. And  we have 0 point 0 2 times 0 point 5, which is the 1st States there times 0 point 9, and then you add that with 0 point 2 5 again times 0 point 2, which is the second value times 0 point 9. ? , it's just you just have to pay attention to the It's really simple, if otherwise,  it's straightforward. And once we have calculated that , we are interested in the 1st value. you only multiply by 0 point 9, because here, this is Bj of Otp. And then if you go to video voting ot. we go to  this is B of one at timestamp of 2 ? And then if you go to Dj of 2, a time step of 2,  the value will change. you can compute the second Val the second cell, which would be 0 times 0 2 times 0 point 4, which is for the Y column times 0 point 5. .",
    "and after you have computed everything on that column. the every values here also contributes to the finer outputs. which is, you multiply 0 point 0 7 2 9 times 0 point 5 times 0 point 9, and then you had it. it's just the same computation. If you observe, you find out that what you are multiplying with are  the same for full of column 2 and column 3. Why was prime by December is because we are supposed to produce art twice ? And the second reason is because you are at the same States. you are multiplying with the same state value. And also you are trying to emit the same word. And that's why we are multiplying by the same value.",
    "But in the case of the 1st one, you'll find out that we are  multiplying by a different value. We don't have 0 point 1.  instead of 0 point 1 in the 1st column we have 0 point 9, because the symbols are different. And at the end of the day you can sum if you sum everything you have your period of all given to them. ,  for the backwater gardening. what you're going to have is that we said in the last column you're going to have once you have once throughout. and  you are trying to do the same calculation. But the formula is slightly different. here, here you have the A high J.  what is your aid? You still have one, because your contribution from the last column is one, then this would be one times 0 point 5 times 0 point 9, because 0 point 9 is what you are emitting there. plus one times 0 point 4.  if you observe in the last column.",
    "for the phone, I've already , we are going this week for the calculation for the back, or I don't think just you're doing this in terms of the values you pick. go to the folder garden. you find out that you multiply 0 point 5, and then the second,  you multiply 0 point 2, then you multiply 0 point 1, and for background it's very different. And then you propagate everything to the very 1st column. and then you do the calculation. And after that you sum up everything together. But while summing the difference for the backward algorithm is that while summing all the values, you also multiply by the initial step probabilities. and also the better one. we may not able to. We may not be able to go through the calculation step by step in a class.",
    ", but I will encourage you when you get home you should try to work it out yourself. They have a clear understanding. just using the formula and then doing the calculation. ,  for the vitabi algorithm,  the difference is  you take the maximum. And what is the maximum of the 1st column? That's 0 point 2, 5, and is the maximum that you use in the calculation. and you continue the game. you always, instead of doing the submission you take the maximum . that is in the supervised setting, where you have all the observed values. And also you have the labels which is your sequences your States.",
    "But you could also have this in a in a unsupervised setting where we don't have the state values. we need to guess them. And for this, what we can do is that we can initialize the States with some values. And then we try to learn this iteratively. and what we are going to do is  doing this, you predict the current state sequence using the current model. And then you update the current parameter. And this is  the idea of the em algorithm. for the em algorithm is that you 1st initialize the parameters randomly. And then you repeat the 1st time you predict the current state sequences, using the current model. and then you update the parameter of your model.",
    "you, you learn a better parameter to be able to predict a better state sequence. in the forward backward algorithm, where? And this is what we want to connect to the Em algorithm em is expectation, maximization. And  that you 1st assume a set of parameter values and based on that you try to estimate what will be the value of theta. and you go through your data to estimate a better parameter of the model. it's ,  that everything is unsupervised. you don't have what's what you should start your calculation with. But you randomly initialize them, and then you just estimate at that state. or at that time step with that value. once you move to time, step one.",
    "What you need to do is that you get a better estimation of the parameters, and then you redo your calculation. at the expectation, you get a split account for the eating structures and using the current Teta K and the eating structures will be your part of speech or your State's values. And for the maximization, then you find what will be the a better, Theta k plus one at the  time. Step that  maximize the likelihood over your training data. the parameters you want to estimate , we call them probabilities responsibilities. Here we have to parameters. We have some epsilon, and then we have some. Our gammas we want to estimate. it's very similar to what we are estimating previously. where we compete, what is the probability of Qt.",
    "Equals a tag given what we observe? And after that we can  compute what we? What is our state, what's the probability of Q equals? I , what's the probability? This is  the transition probability from time step T to type, step t plus one. once we estimate these values,  these values will be corrected at the maximization stage. where you can estimate it's based on your training data. , but initially you have some random values for them. and then you estimate the parameter of the model that you are looking for. and after that you recalculate what should be the exact value for Gamma, T. And epsilon of C.   the idea for the east step we have 2 parameters that we want to calculate.",
    "We have a gamma I of T, which you can decompose to be this probability of Qt. I given what you observe and the parameter you are looking for. Remember, we don't know this parameter of T of Theta K. But we can estimate it with some random initial random values. and we can then use that to estimate what is our Gamma T. Of T. Gamma, high of T, and after that, and the probability of Qt equals. I, because in this algorithm, we are using the forward backward algorithm. we know how to estimate what is the probability of Qt equals. I comma O, because we can estimate it's using the alpha I of T from the forward algorithm and the beta I of T from the backwater guardian, which we multiply together. after we have calculated this one and then we have the normalization constant, which you can obtain by marginalizing over all the possible values of I.  at the Easter we can compute something very similar to the transition probability from one State to the other. Given our observed value of value and the initial value of Theta K, and we do very similar thing. Which is also coming from the from the forward backward algorithm, the forward algorithm you have, you are computing your Alpha J of T, and then we multiply by all the values in the backward algorithm.",
    "and then you can divide by what you marginalize. if you have gotten the initial value of the Gammas and the Epsilon. Given a randomly initialized Theta K, then the  stage is at the maximization step would be  to compute a new value of Theta K plus one. and the new value of Theta K plus one we be. We want to compute new values for, alphas! New values for a better for are betters, and then you continue  at the maximization stage. It's very, very related to having a sub version of the mle. the Alpha ij it's very. You can compare it to what we did previously. where we have the joint counts divided by the count over the initial states before we move to the  State.",
    "and at the maximization stage you can  get a better value. Better values for all the all the teta values you are looking for, and then you will go back to the expectation, to the E step. and then you do this iteratively, and if you do this iteratively over many iterations. you will have better values for your teachers which you don't know. And the question is that when do you stop your Em algorithm you are likely going to stop. When your likelihood doesn't improve at some point it will stop improving, and then you can stop  this is the idea of the em algorithm or another way to stop is you can perform the prediction over your development set to see if things are improving. Where , how are these equations derived? I see the similarity with the in the past, but I don't see why it took  , you see the similarities? Because, , we ,  what we're trying to Co calculate. here you're trying to say, what is the probability of the States?",
    "You are looking for the States. Given what you observe ? And here you are trying to compute what is the transition probabilities. it's very similar to what we did in the in the earlier stage, where we want to go from one State to the other. that is the alpha values. It's  the same idea with what you're trying to achieve. , for the other one, which is the d of IK plus one. it's also very similar to your emission, your emission probabilities. But  of the expectation maximization. But we are trying to just learn everything unsupervisedly from the data.",
    "this is what we're trying to do . There are no labels to train on. we cannot just apply vitabi algorithm directly. And  we want to do everything  our supervisor. for the Em algorithm, we are trying to find the local minimum. And the old idea is that you find out you don't know the value of Theta  the value of teta  will be everything in your trellis, the States all the values of Alpha or the values of Beta. And what we are trying to do is that if I can have a likelihood given. My new parameter of theta improves the previous one. Then that means I'm making progress. the difference between the supervised and the unsupervised is the supervised.",
    "You can  estimate your offers and your betters, which are your offers. That you calculate are your betters. You can, that you calculate is what we call the is our theater. But in the unsupervised setting, you cannot estimate this. you have 1st a random initialization of everything that is there. you have random initialization of the entire state. and after you have a random initialization of everything, then you want, and I want to. I want to see if you are making progress. If you get a new parameter of theta. is it better than the previous one you got, and then you do this iteratively until you, until you find out that there's improvements in the estimation of the likelihood.",
    "as long as you are making progress, that the likelihood over your observe or your training data, which is all your observation. Given the new title you have. Computer is better than the old one. Your offers are your betters as long as it keeps improving. Then  that em is working, but at some point, if it doesn't improve again, of course. That means you have to stop. That's the best em can give you. There's no guarantee that you're going to get very good results. If you're interested in more proofs, there are proofs of correctness above bound Welsh correctness which you can  check with additional materials. there, if there are a few things you need to consider when you are working on things  the emigrating you have what is called random restarts.",
    "First, st you have to train multiple models because this is an unsupervised setting, and if you have very bad initialization. You will not go very far. you have to have random restarts. And  you can pick your best parameter by always computing. What's what is the result on the development set? Another thing that might be helpful is to have a more better initialization by using an initialization based on some external source of knowledge. And , this will improve your performance because you have an initial estimation of the parameters of alpha and betas. This will  lead you to better convergence than having a random initialization. the only supervised setting, which is about vash a guardian with no labor data often gives very poor results, because there are many issues , if you don't have a very good initialization. You're going to likely have very poor performance.",
    "And it's often nice to have small values of label data that you can  estimate better beta parameters for your alphas and betas before you start the emigrating . in practice, Emi guardian can be used for different tasks. And then we have some very popular baselines on the Wsj couples, which used to be a very popular benchmark. But of course most people are not using this against that. You can  estimate how good is the performance. does give you a very good performance. we also have some other sequence modeling task  chunking where you  find syntatic chunks in the sentence. you're not only looking for a word and giving it a tag. But you can have something  noun phrase that you want to tap instead of just a word. And also, you have a very popular task named Entity Recognition, where you want to identify popular categories  personal name, organization, and location.",
    "And this is very, very important for information extraction you want to. When you're searching for an important thing on the web, you want to get the entity   that you can get the  information. and if you can get the entity ,  this will already have an entry on Wikipedia, and then you can extract information about the entity. this is very important for information instruction. But of course, here you need to detect the spans of multiple words. Here you're not only annotating a single word. Again, you need to identify the span. You have this plot, Mcgill. You can view them and concordion allocated in Montreal. And then you have organization.",
    "What is the problem with this scheme? If you just labeled everything as organization? when  we have different schemes for named entity, recognition. But this is a very busy scheme where you just label it as You just label each occurrence of the entity with the Tag organization. it's often better to use a better tag where you can  map. Where is this the beginning of the span of the entity versus? Where is the entity ending rather than just having a single span, because an entity can be a single word. It can be multiple words. in the I/O or B tagging here, we try to know what is the beginning of an entity. What is the ending of an entity.",
    "here we have my view university. It's not a full organization, you have to say, my girl, because  my gear is also a name of a person. if you see my gear university, this makes it an organization,  you can have what is the beginning of the entity, and also what is the  word that follows the entity. and then you also have the old tax, signifying that there is no entity   East is not an entity located is not an entity. We also have other skills , there's another one BIOE. S. Where you can have the start of the entity. You have the end of the entity. You have the intermediate of the entity, and then you have the old tax,  everything that is not an entity will be an old tax  and but  this is  the most popular tagging technique which we call the iob 2 tagging scheme, where you clearly define the beginning of an entity and the ending of an entity. And you can also solve these tasks with Hmms, and as  in the future lectures using crf. ,  we'll stop the lecture here today  week.",
    "October second, we have the end of workshop at Mila. I encourage you to attend."
]