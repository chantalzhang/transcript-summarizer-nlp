[
    "Jackie Cheung, Professor: It sounds  nothing. , we're talking about universities that you're you're pumped and that you're open. It's awesome. Oh. wasn't expecting  team one. The block people here. Oh. probably want to switch about this one second. . outlook is phone.",
    "It's not helpful. . , we'll get started in a few minutes we should leave time for people to get here, because the main entrance was blocked. And in the meantime, if you have lighting preferences, let me know. and I'll try to play around that question. , I'm gonna stick with this setting unless someone tells me they prefer something else. Bye. . Hi, everybody! unfortunately, you'll be spending the  few weeks with me rather than David, and we will talk a little bit about structure.",
    "we'll talk about syntax and semantics. before we get there. it's just some announcements and reminders. reminder that  week is Thanksgiving, plus the fall reading break. that means there are no classes or office hours or lectures. , no classes but that the reading assignment is due later this week. please be sure to get on top of that. The reading assignments are not meant to take a long time there, but they are there, those to help complement the material that we discuss in the course. Also, we're gonna be releasing the final project description hopefully by the end of the week. please watch out for that.",
    "And also and if you need any help in finding final project members, if you haven't started thinking about that yet we will provide help for that as . And you can also post on the Ed of the course for that. . Question. is it doing the 11.th , when I checked and my courses was the top. But you should go with whatever is officially released. , my slides could be wrong. . no. groups of 3, please.",
    ". I'm gonna summarize where I think we're at in the course, and then you can  I'm if I'm off base. from my understanding of what we've done  far in the course we talked about text classification where we treated passages as samples, ? that was  the 1st content topic. And I remember giving a lecture on that a long time ago in September or something. And then with David, you should have talked about text as sequences and looked at sequence, labeling problems. ? , for example, he should have taught you about n-gram models, ? And smoothing hidden Markov models. Yes, good nods.",
    "? And then, most recently, Lstms, ? And crfs, maybe crfs and Lcms,  that's great. for the  part of the course, we're going to look at hierarchical structures. And first, st we're going to look at syntax. we're going to ask, what is syntax? We're going to look at some particular characteristics or properties of English syntax themselves. And then we're also gonna look at a formal system for describing structure called context-free grammars. . if,  many students in the course you were, you got interested in Nlp because of all the recent excitement with e-learning.",
    "That's great. And if,  many of them, you're reading and following a lot of machine learning blogs and  forth, you might be under the assumption that language is a sequence of tokens. ? And my goal in this lecture is to give you evidence to disabuse you of that notion. while language could be described as a sequence of tokens. it's  a lot more than that. ,  it's easy to forget this, especially  that we're in modern times. Often we interact with each other through textual means. , and Texas seems to naturally come as a sequence of discrete tokens ? And in English.",
    "We even have spaces between the each word. then. and that's how you might define a token. we might think that language is a sequence of tokens. But it's important to remember that text is just an approximation of language. It's 1 view of language. but it's a discrete approximation of language. and it's a discrete approximation also of speech,  the speech modality of language. And that means that there  are a whole bunch of other phenomena where the most natural way to think about them and characterize them and describe them and model them, is not to just think about them as a sequence of tokens. Let me give you some examples.",
    "1st of all, language is not just a linear sequence of tokens. because the parts interact with each other quite closely and quite tightly. The pronunciation of words that are  to each other will often affect each other some of the most of the time. It's in these very small ways that it's really hard to notice, especially if you're used to speaking a language. You never notice how the pronunciations of words bleed into each other themselves. But I can give you some examples of this happening in more obvious ways. one example, one obvious example from English might be. how many people here speak a dialect of English? That is non rhodic. , in other words, you don't pronounce r's at the end of syllables , instead of saying car, you might say car or  there, you would say that.",
    "if you're from the Uk. For example, chances are quite high. You speak that a version of English  that. one way in which the pronunciations of words affect each other adjacently, and those dialects of English, for example, would be that in most situations they don't pronounce the r at the ends of words and syllables, but they would pronounce it if the following word starts with a vowel. ,  and sometimes there's even a phenomenon called intrusive R, where there originally wasn't an r in the word, but then you add the extra r in to smooth that out. to add a consonant between 2 words. a. A a common example of this is, if you have , if you want to say, I have an idea of what is happening in those dialects, you might say an idea of what's happening. ? you might notice this if you if you   time you, you chat with someone from the Uk, you can try to pay attention to the speech and see if they have .",
    "not everyone from there has it. But some people do. Or another really obvious example is in French. how many people here speak French? , more people. French pronunciation is  famous for having these effects. And it's a really difficult thing for learners of French to grasp. For example, just the articles that  the. if you have,  a in French, . How do you say?",
    "A  a masculine, singular version of a How do you say it? And yes, that's . At least that's my approximation of how you say it. And if you think about it, there's  no consonant there. But you think there is, because it's spelled UN. But it's really a nasalized valve. ,  air comes out of your nose. However, again, if the following syllable starts with a vowel. suddenly the end reappears. and the vowel is no longer nasalized.",
    ",  if it's a I know it's that you. Suddenly the end appears. . Whereas if it's something else that's masculine, singular. that doesn't start with a vowel. , someone give me a noun gato. Yes, there you go. then you print. Then you don't pronounce the n. but the vowels neither . that's another example of pronunciations bleeding across words also intonation patterns.",
    "They work over entire sentences, or when things are spoken. You tend to talk about utterances rather than sentences. , and that seems  it. It cuts across multiple tokens in the sequence. If it's a question, for example, there are particular intonation patterns for questions in English, and it depends on your dialect. It depends on whether it's a yes, no question or whether it's a question with a question word, and  on and  forth. those are some examples of why it might make sense to work at a level. That's not at the level of individual sequences of tokens. But for today's lecture, the most salient phenomenon here is that we can think about and analyze the internal parts of sentences and find patterns that we can better explain through positing hierarchical structures. the goal of today's lecture, then, is for me to provide you with evidence that this structure exists.",
    "and also to come up with a formal computational model of this structure. then, you can describe it in those terms in terms of hierarchical structures. And  you don't always have to go through linear sequences of tokens, as you might with the Lstms. . . And  then, what we're gonna talk about today is syntax. In the linguistic sense, we're going to talk about how structures in a sentence can be formed and assembled to create larger units, larger structures. And you can do this recursively  you can start off with  the smallest possible units. Maybe their words, you can combine them with other elements to form bigger structures. And you can combine those structures with other bigger structures to form ever bigger structures until you get to the level of a sentence, and that's where we'll stop for  with syntax and in syntax there are some key concerns.",
    "One key concern is what makes up a valid sentence of a language. and this is a notion called grammaticality. one simplification and formalization here is, you can assume that for every single string of tokens. There are some strings that are accepted by the language, say English, and they're part of the set of valid English sentences. And then there are other strings which are not accepted, and  they don't form valid English sentences. this is a valid sentence is a valid sentence, a sentence, this valid is, is not a valid sentence of English, and  you can say that the 1st sentence is grammatical, and the second one is ungrammatical. and by convention, you mark, and grammaticality with an asterisk. This means this arrangement of words don't form a sentence of that language  one goal that we could have with syntax, and by looking at internal structure. is to specify an all and exactly those sentences of a language which are grammatical. The second key concern that we might have, and why we might want to work with internal structures and syntax is, we might want to use the syntactic structure to help us infer the semantic structure to help us come up with a meaning representation of that sentence.",
    "and that comes to  very tricky and philosophical questions about what is meaning and what is a meaning structure have to do and what counts as a meaning structure. And we'll get to that after the art and syntax. Then we'll get to semantics. this idea of syntax and that languages have structure is not a new idea. It's not even an idea from last millennium. , this is an idea from multiple millennia. Ago some of the earliest people to have thought about this that we are still aware of are grammarians from, say, from South Asia. Panini, which I'm not pronouncing  . Panini. From the 4th century BC.",
    "developed a grammar for Sanskrit, which is a classical language of South Asia. And he wrote this whole grammar of it, and you can translate it. and it's very interesting, because even back then. People are just as smart as today. Some people say, even smarter than today, because they don't have  technology or whatever. And and they come up with, and they produce descriptions of the languages that they speak . And you can see that it's it's going to be very similar to some grammars that you can read about today. ? There's some option where you can add certain things, but other things are not allowed, and there's a prohibition, and they talk about affixes, say, and then they always receive some augment. they there are multiple categories of things, and they combine in certain ways.",
    "And that's also a description of the morphology and the syntax of a language. it's really interesting. You can read more about it. But ,  grammar is not a new idea. I will say, though, that there's another usage of grammar which is, which is not the one that I'm gonna be concerned with in this class. there's another notion of grammar, which is that the it's this  a onerous thing that's really scary. And if you don't write in some particular way you get yelled at. ? And it's it can be even captured systematically by books. ?",
    "for example, there's this famous book on grammar called the Elements of Style by Strunk and white. Has anybody heard of this book? No, which is good because this is not the type of grammar we're going to talk about. And also this book is just bad because it contains lots of linguistically inaccurate descriptions. these are style guides essentially, and they're prescriptive. They tell you how you should write. , it doesn't tell you how a language is as and how it occurs. It tells you that it's some normative standard about how they think you should write in order to have something be correct. in this course, we're only going to be concerned with descriptive grammars. I that here's another common misconception, which is that somehow things that are non standard or informal or casual somehow have no grammar.",
    "And that's simply not true. All varieties of language have some grammar. It's quite possible that the informal or casual version of a language has a different grammar compared to the formal version, the written version and the prestigious version. But they can all be described using rule, based systems. The informal version is not even necessarily simpler or worse, or lazier, or anything  that. Sometimes it's even more complex. ? there's no relation at all between the level of formality versus the complexity of a system. and sure we need to have some standards  that people can communicate with each other efficiently, especially in a written form. Perhaps.",
    "however, it's good to try to separate the 2. ,  for our purposes here, we want to build computational models of language as it is. And  we're more interested in descriptive graphics. . I said earlier this lecture, that the main goal I have for this lecture is to convince you that there exists hierarchical structures in language. what does this mean? , if it's a hierarchy that means there are these intermediate nodes and some intermediate level representations. And in the domain of syntax. These are called constituents. what are constituents? A constituent is a group of words or other constituents really that behave as a unit.",
    "Somehow they form some unit, and then there are properties that you associate with a unit rather than with each of the words that make up that unit? you might have heard of some of these terms,  a noun phrase and an adjective phrase. These are examples of types of constituents. , a noun phrase, something  computational linguistics is a noun phrase, the word the single word. It is a noun phrase. Justin Trudeau is a noun phrase. 3 people on the bus is a noun phrase. These are all noun phrases. and you don't just have noun phrases. You can have adjective phrases  very good, is an adjective phrase.",
    "ridiculously annoying and tame, is an adjective phrase. You can have adverb phrases. There are many different kinds of constituents. then the  question is , How is it that what does it mean for these words to form a unit? And  the answer is, they share some similar distributional properties that we can test for. And   I'll go through a bunch of tests to check whether something a group of words is a constituent or not. and the more tests that a group of word pass, the more likely it is that it's  a synaptic constituent. Sometimes there are many factors that go on. And and it can obscure things. But  you should try multiple tests.",
    "But I'm gonna give you some tests. And we can try. . one test for constituency is that groups of words can appear in a similar syntactic environments. And this also gives you evidence that it's the same type of constituents. let me give you an example for examples. Suppose you have a context. this is your syntactic environment. I saw and then blank. , after this.",
    "you can put you can put anything that is a noun phrase,  and syntactically, it would be correct even if semantically, it might not be . For example, you can say I saw it. That's that's a grammatical sentence of English. That's fine. You can say I saw Jean-claude Van Dam. The muscles from Brussels. That's fine. this is also a noun phrase. You can say I saw 3 people on the bus. That's also a noun phrase.",
    "You can say things that are syntactically correct, but semantically meaningless,  you say, I saw computational linguistics. that's technically grammatical, even though we're not sure exactly what it means, and you might struggle to come up with a metaphor where that makes sense. ? However, here are some examples of things that are not noun phrase constituents. And  then they fail. This test. You cannot say I saw a van from Jean. , you can't. You can't just say I saw a van. You can't say I saw Ondi.",
    "that's not a constituent. that's the 1st test . Question. . is this a test for a noun phrase constituent. Yes. technically, this is a test for a noun phrase constituents. That's . that , tests for different constituents. , ,  you have, you have to come up with some syntactic environments where the type of constituents you're interested in could fit there.",
    ". , here's the second test. Constituents can be placed in different positions or re replaced in the sentence as a unit. Suppose you have Jean-claude Van Dam. The muscles from Brussels meet me up. You can also say it was Jean-claude Van Dam, the muscles from Brussels who beat me up  it's the same group of words, and you can put it in different environments. You can passivize it. You can say I was beaten up by Jean-claude, madame, the muscles from Brussels. you can move these around. And this is a slightly different test.",
    "Another slightly different test is you replace it with a pronoun. in fact, that's  what pro words pro forms. That's what they do. You can use them to refer to something else, and often you can use them to replace an entire constituent. But but the type of constituent stays the same. ,  Jean-claude, madame, the muscle from Brussels would be replaced by the pronoun he. because it's singular 3rd person masculine. then it becomes he beat me up. That is translated by Namda solution process. ,  these are all additional evidence that it's these are all noun phrases, the same syntactic type A 3rd test is, it can be used to answer a question on its own, especially in formal language, who beat you up.",
    "? , it's just  as an example of something that fails. If someone asks who beat you up, you cannot answer the muscles from that's not a constituent. That's not a  face constituent. And I already said this orally. But  the type of constituent that you have is going to be called its syntactic category. noun phrase. all of these examples are with working with noun phrases. But you can have verb phrases,  phrases, prepositional phrases, clauses, sentences, and  on and  forth. There's not really a universal list of syntactic categories, because.",
    "as you might expect, linguists don't agree, and there are different formalisms and different languages might  have different syntac categories that seem to make sense for them. But here are some of the most common ones that are in most grammar formalisms, and there are likely others. Yes. Question when you define a constituent  noun phrases, is there a definition, or is it defined by its tests? ? That's a good question. When you define a constituent such as a noun phrase? Is there  a definition, or is it defined through the tests? I think it depends on your theoretical perspective, and where you come from,  for our purposes, we don't have to care  much about that we, for our purposes, we can define them operationally through their distributional behaviors. for us, a particular syntatic category will be defined.",
    "I will say it's useful to have it if it forms some coherent group where all of these things that we call say that they belong to the same category, they all behave distributionally. Similarly. yes. Another question supposed to be applied. is it the . the question is, there's not people online. these tests seem to be qualitative and also are there other tests for other types of constituents? yes, that these tests are qualitative and also yes, these there you have to define different tests for other syntactic types, syntactic categories. for verb phrases, for example. you might need to do something , ask questions , you did.",
    "What? What did you do? And then you have to answer with  going to the park? There are also replacements for verbs. They're  pro forms for verbs as   to do . There might be also tests in involving a phenomenon called elision, where you . remove things and you skip over them. But . ,  it can get very complicated. .",
    "far, we've covered syntactic categories and constituents. , we can talk about the relationships between the different constituents, because the it's not just that they there exist constituents in a sentence. but they also relates to each other and systematic and regular ways. And  we can talk about grammatical relations between constituents. here are some that are as very widely known, and probably you've heard of them, and they're related to verbs in particular. But in general there are many more grammatical relations than this. but when it comes to, when it comes to verbs. you can talk about what are called arguments of verbs,  verbs can have subjects, and they can have objects, and they can have direct objects and indirect objects. ,  Jean-claude Van Damme relaxed. Here the verb is relaxed, and the subject is this noun phrase constituent of Jean-claude Van Damme.",
    "or the wallet was stolen by a thief. Here the verb is, was, or was stolen. and then the subject of it is the wallet. note here that subjects an object here. We're defining them purely, syntactically. purely, structurally. it's not based on the semantics of the sentence. it's not based on. Oh, it was the thief who stole the wallet,  shouldn't the thief be the subject? No, here the thief is part of a prepositional phrase.",
    ",  structurally, the subject is still the wallet. it's about the where it's located in the sentence, not about what its function is. . subject is the most common one, the , most at least in English. The  most common one is direct object. The boy, keep the ball here. The ball is the direct object. and sometimes you also get sentences where you have multiple objects, and one of them is the indirect object. she gave him a good beating him. Here is the indirect object.",
    "and,  before, there are many other grammatical relations. These are just some of the common ones related to verbs that you might have heard about before. but, for example,  adjectives, can also require certain arguments, and you can talk about the relations between a determiner  the versus its noun,   the ball or something. there are many other relations. verbs and usually predicates in general. they tend to impose some  constraints on other syntactic elements that must appear to form a valid sentence. and that's called subcategorization,  subcategorizing verbs into different kinds of verbs, depending on what constraints they add to the other elements in the sentence. You might have heard of terms  intransitive verb or transitive verb or ditransitive verb. Maybe if you've heard of those terms, those are essentially different subcategorizations of verbs. , different subcategories of verbs.",
    "For example, an intransitive verb is one that only requires a subject. relax is an example of that. You can say I relaxed after a long day at work. relax also happens to be a transitive verb. You can say I relaxed my muscle. you have to relax. Your shoulders to do this exercise properly, or something  that. ,  that in that case it would be a transitive verb that takes on both the subject and a direct object. and here, from the examples, before steel is also transitive, and kink is also transitive. But and then something that takes  2 objects would be something , give usually verbs that involve the change or transfer of something.",
    "Usually you need,  the transfer, the recipient, and the thing being transferred . There are 3 things there  often those require 3 elements, and then they're called ditransitive verbs. And there are some regular processes that exist in languages that can systematically change the categorization. , for example, in English, there's this passive structure which changes a transitive verb into one where it only requires the subject, and it changes the mapping between the syntactic and the semantic roles of the arguments of that. And  that's a systematic thing. Other languages have different role, different operations and things  that as . . . Do you mind to give an example of that? I didn't understand for steel, maybe.",
    "Sure. for steel it would be something , . the active voice one would be a thief stole the wallet. ,  there, there's both a subject and a direct object. When you pacifize it, it becomes the wallet was stolen. and technically by a thief. Here it's not obligatory. You can add it if you want, but it's not obligatory. the in terms of what's required by the verb, it's just the subject. .",
    "And then the other the prepositional phrase, you can add it on, but it doesn't count as an argument. Yes. What's the? Oh, these things? . with the number here. I should have explained the number. Here is just the number of arguments that it takes. and then the list here is the relation to  what else it requires,  the list of argument type types. here it means that you need to include a subject.",
    "and you need to include a direct object, and with give, you need to include an indirect object as . you can think of a parallel to say programming languages if it helps where you can talk about functions. And they also have to take a certain number of arguments. that's why they're they're called arguments in both cases. . you might have a function that is , multiply XY, and it takes in 2 numbers x and y, and it multiplies them together. ? Some really simple function. , in natural language, analogously, you can have, you can think of , there are things  this as  where you have predicates which are  the verbs or other things, but mostly verb, for , and they take on arguments, and then those arguments are. They have to appear in certain positions around the sentence.",
    "and you can call them subject and direct object positions and  forth. there's an analogy to be drawn there. And in fact, we'll make this analogy quite explicit in a model later on in the course. , there are many other subcategorization possibilities  with the verb wants. I want to learn about computational linguistics. It takes a subject. And then this is called an infinite table clause, because it's to learn. You can have something  a prize which takes a subject, an object, and also a prepositional object. The minister apprised him of the new developments. and  the minister is the subject.",
    "Him is, I guess, an object, and then prepositional phrase, object of, and it has to be with of you can't say the minister apprised him in something, and you can't simply say the minister apprised him. I don't think that sounds weird to me. and,  before, it's not only verbs, but other things can do this  difference, depending on your dialect. You probably want this course's difference. And then one of these prepositions. I'm not sure which one I prefer. I think I prefer from or to. I'm not sure exactly, and maybe in your dialect it's a different preposition. But but what I expected. this is another subcategorization.",
    ". a subcategorization is a slightly different in that. It's talking about what certain predicates expect. the others, constituents,  there will be a constituent that fills each of these slots that fills each of that needs to fill each of these roles, and they have to appear in a certain syntactic environment which can be called subject or object, or something else, or a prepositional object, and  forth. And that's part of what helps you figure out if this the sentence is grammatical or not. , that's what this is all about. ,  here's an exercise. Identify the prepositional phrase in the following sentence, and give arguments for why? It is a constituents. Maybe we can do this together because we're running behind time.",
    ". 1st of all, what is a preposition? Can somebody identify what the preposition is in this sentence? You raise your hand? Yes. on, on. Yes, thank you. Do you want to see? Did you want to say something else? Or Ed?",
    "Yes, on Saturday, October 12? th Great? . prepositions are these words that help indicate relations between things in a sentence, and they appear before a  phase. You can also have words that function similarly, but they'd be. They appear after a noun phrase, and those are called post positions. But in English, it's overwhelmingly propositions. There are some post positions. But . and other languages prefer to have post positions .",
    ",  why is it on Saturday, October 12.th   give me some arguments for why, this is a constituent as opposed to just on. Why, it's just on, not a constituent. Why is it not  on Saturday or on Saturday, October. think about you. Go back to those tests and think about it . why is it? The whole thing is a prepositional phrases. Yes. for whole purpose. the beginning.",
    "it would still retain its meaning. Yes, you can move the whole thing to the beginning of a sentence. That's a great test. on Saturday, October 12.th The  assignment is due. and compare that against our alternatives. ? you cannot say. On Saturday the  assignment is due October 12.th  you could, but it would be very strange, and it means something very different which doesn't make any sense, I guess, and you cannot say on the  assignment is due. Saturday, October 12.th Great more tests. Yes.",
    "exactly. You can ask, when is the  assignment due? And you can answer. On Saturday, October 12.th I suppose you can also just say Saturday, October 12.th When is the  time to do? Saturday, October 12.th You can do that, too. that's evidence that Saturday, October 12th is maybe also a constituent. But it's a it's a different constituent. And you cannot answer just on , when is the  assignment due on. No, you cannot do that . What else?",
    "Any other tests? How about replacement? Can you replace this with some  pro form? . I don't know. You could say  is due then? Yes, great. The  assignment is due, then, is where then? Replaces on Saturday, October 12.th , great. we have pretty strong evidence that this is a constituent of some kind.",
    ",  that covers  the linguistic elements of what wanna talk about. For  for syntax. And , we're going to talk about the formal computational model that we're going to use to describe these hierarchical structures. Because remember, we're computational linguists. We're not just linguists, we're computational. what is the formal model of grammar that we're going to use to account for these and other syntactic concerns. And 1st of all, maybe I should say, what makes something formal. ,  there's a whole field of research in formal language, theory, formal grammars. for example, who has heard of automata theory? , some people, ?",
    "And maybe some of you are taking that course or another course in formal grammars and formal language theory. here we're going to take that machinery and that theory and apply it to natural language. ? And in that context a formal grammar is a set of rules and other associated things that help you generate a set of strings that make up a language. and in this context language simply refers to the set of strings that you want to accept. , for example, you can. You can be working with arbitrary symbols. You can be working with,  the language of arithmetic. where you have symbols involving plus and minus and multiply and divide and numbers, and you want to figure out what forms . what's the set of strings?",
    "That's a correct arithmetic expression. ? In our case, we're going to take that form of machinery, and we're going to apply it to natural language. where we assume that our goal is again to characterize exactly the set of strings that form valid sentences of English. Again, there are good reasons to question this assumption, but we're gonna run with it. For  I don't know if I said that before. ,  there are good reasons to question that assumption. But we're gonna , go with it. For  to say that we're gonna pretend that we can model English or any natural language as sets of a set of string. And you want to figure out which sets of strings are grammatical and which sets of strings are ungrammatical.",
    ",  why do we want to do this? , a formal understanding helps us to develop appropriate algorithms for handling and dealing with hierarchical structures and syntax plus? Maybe there are implications for cognitive sciences and language learning. ,  there are other formal grammars you might have heard of and encountered, or use finest state machines  finest state automata and regular grammars. Fsa's our finance data are particular ways to talk about and describe regular grammar. there's an exact correspondence between them. And they are  used in Nlp as . they're very practical because they tend to be much faster to , compile and run and process , but  just to use the same terminology again. and Fsa. Generates a regular language.",
    "and fsas correspond to a class of formal grammars called regular grammars. here, then, formal grammars, you can also talk about them as  possible sets of strings that you could describe with that formalism. , in Nlp. They can be used for tasks such as stemming and lemmatization and morphological analysis stuff that we talked about at the beginning of the course. however, it turns out for the syntax of natural languages, where you might have multiple constituents and some categorizations and other phenomena. it's more natural and useful to use a more powerful class of formal grammars. And these are in particular context, free grammars. ,  that's what we're gonna talk about . And before we continue, I'm gonna close the door. I'll be  back.",
    "Somebody is really upset that I'm talking about context-free grammars. . what are context-free grammars. I'm going to start by giving you  the form of it, because I'm guessing that maybe when you're thinking about describing programming languages, you might have seen something similar before. , if you're talking about the syntax of a programming language  of python, or something  that, or if you're in a compilers class, or something  that. ? But essentially, they talk about what? Possibly they talk about our rewrite rules. They're they're essentially a series of rewrite rules where you have something on the left, which represents some elements that you're working with, and it rewrites into something on the , which is  a sequence of smaller parts that form that bigger part, that bigger chunk. in natural language you can talk about.",
    "Oh, we might have a sentence which we'll denote with some symbol  S. And a sentence should be rewritten into a noun phrase, plus a verb phrase. It's made up of a noun phrase plus a verb phrase. and then maybe a noun phrase. Can we write to  a word  this? And a verb phrase? We write to something  a verb, and there are some options for that as . here a vertical bar is just a shorthand. It means, or ,  a verb can be  to is a verb. Can you write to kicks? However, can we write to jumps and over?",
    "Can we write to rocks? this is already a very simple Cfg,  this is, I'm starting off with an example. and then you can use that to generate or to recognize. to generate. You start off with a starting symbol which for us will be the S. The sentence. and you apply the rules of a Csg. To keep rewriting until you end up with a sequence of words. ,  S. For example, generates Npvp, there's only one option in this grammar. It's a very simple grammar. then you get, as we writes to Npvp.",
    "And then Np. Rewrites to this, and then Vp rewrites to V, and then V rewrites to some of one of these sperm. maybe it rules  this would be a string. This rules which is accepted by this grammar. that's part of the language described by this grammar another one is  this, rocks. ? You just use a different rule there. for B rewrites to rocks. then, you have non-terminals, which by convention are written with capital letters. Again, this is just a convention, and the terminals or leaf nodes or words in our case, which are the , which correspond to words, because we're applying it to natural language.",
    "Yes, what makes what about this makes them context free? What about this makes them context-free? you should just try to ignore context, free the meaning of context, free. what? there's a formal definition. here's a,  this is . formally speaking, this is what makes it a context, free grammar. Here's the formal definition. If you want the intuition, the reason that is called a context, free grammar is because all of these rules you can always apply them. As long as you have the  left hand side you can always use them to rewrite, and that's what makes them context free.",
    "There are other classes of grammars. There's 1 called context Sensitive grammar. or you can add conditions. You can be , I'm allowed to rewrite an S. To an Mpp. In the context of something else or Np. Can be rewritten to something in the context of something else. But that's not super important for our purpose. The formal definition is what we should go by. , and this is the formal definition of a context-free grammar. or from  on I'm gonna start saying Cfg, to be shorter.",
    "a Cfg is a four-tuple. We love those they have a N. And Sigma and R. And S. Where N is a set of non-terminal symbols. Sigma is a set of terminal symbols. R is a set of rules also called productions. In this particular form of something from the non-terminal set rewriting into some combination of things in the non-terminal and terminal sets. and the star here means you can have 0 or more of them. technically, you can rewrite to an empty symbol. But that's not very useful most of the time in natural language. we're just pretend it's  one or more, but I guess it's 0 more also works. And also, if you're allowed empty things, then it causes parsing to be much harder.",
    "But that's a that's an aside. ? And also you need a designated start symbol, which is an element of the non-terminal symbols. And within the system. You have a derivation to where it's a sentence accepted by this grammar. If you can start from the starting symbol, and we write to a bunch of terminal symbols. . here, for our purposes. the ends will be the constituents, ? That are larger than single words.",
    "All of the constituent, the syntactic categories. In constituents they will be symbols in the set. N. Whereas things that are words will be the terminal symbols, and then they'll be set part of the set. Sigma. ,  let us use this formal machinery and work with it, and use it to describe a very, very small fragment of the English language. our extended example would be, let's develop a Cfg that can account for verbs with different subcategorization frames. , for example, we have relax and steal and kick and give. . ,  I'm gonna start off by copying the initial grammar from before. But I'll just replace the part to do with verbs for the verbs you want to account for.",
    ",  what was it we wanted? Account for? We want to account for? Relax. relax steel cake give. And , I'm going to put them in a 3rd person just to make things simpler. ? the way to handle this. 1st of all, everything requires a subject. we're good, because,  this 1st rule of Sv.",
    "Rights to Npvp. it takes care of that. The subject is there in  this 1st element of np. the thing we need to change is we need to change what can be a verb phrase. because we  have 3 options. ? We can have the intransitive option, the transitive option, or the ditransitive option. this means we need to change this rule from Vp rewrites to V, to Vp. Rewrites to V, or Vpv. Writes to Vnp.",
    "Or Vp. Rewrites to Vnp and P. And not only that, here comes a subcategorization part we need to subcategorize. We need to come with subcategories of the verbs, because there are going to be some verbs that are that take one that are intransitive,  they only take the subject some verbs that take 2 arguments and some verbs that take 3 arguments. these become 3 separate symbols in our system. And  we can change the subcategory,  that not all of them are simply these. v, 1 would handle relaxes. and then v. 2 handles, steals and kicks. and v. 3 handles gives. And that's how you create a grammar that respects the subcategorization of these verbs. and you might also want to make your nominal structure more complex.",
    ", all this grammar can do is say, this relaxes, this steals, this, this kicks this. And this gives this . That's all of the sentences currently admitted by this grammar. if you want to make it more interesting, you can also work on the Np part and make that more interesting. But that's . That's , that's not what the question asked for. we're done with this. For  any questions  far about the example. . all clear, obvious, or completely.",
    "Unclear fair? , . We also have to draw a tree. Or is this all great question? Do we also have to draw a tree? you have to draw a tree anytime you  work? You want to work with an actual sentence  a real sentence. this is a grammar ? It describes the possible sentences that can be accepted. this describes all of the possible sentences that could be accepted.",
    "But it doesn't work with any particular sentence. if you want to work with a particular sentence  this, this steals this, then you would have to draw the tree. and we can do that. This deals this. I'm going to attempt to use the drawing mechanism. this is a noun phrase, ? Oh, no. I should type that. . this is an Np steals is v, 2.",
    "And this the second. This is also an Np. and then B 2 Np. Gives us a Vp and Npvp gives us an S. And then you can draw lines. Don't want it to be red. , we can draw lines to indicate that they correspond to rules in the grammar  . And you can check that. Each of these is a rule in the grammar, and since the top node is an S, which is a starting symbol, this is a sentence accepted by the grammar and everything that doesn't work . if you had this, this deals, you would not be able to find a derivation  this, where at the top. You have one S. Node.",
    "That's how  it's not a sentence of the scrum. ? But clearly that grammar is not the grammar of English, because we say other things too. here are some problems with the grammar. 1st of all, is under generation  it misses a whole bunch of valid English sentences. For example, there's a whole bunch of things to do with  nouns, and that and it does that. It doesn't know about ,  noun phrases  boys and thieves and balls and wallets, and  forth. that's called under generation. that's something in the actual language that should be accepted. But it's not accepted currently by the grammar.",
    "You can have the opposite problem, which is over generation where it generates sentences that are not grammatical. I think I used a different, a more complex example when I was thinking about this. here the you can even argue  this steals. This is not a very interesting example. ? suppose you add, , relax here. , then we can have overgeneration. you might say that relax and relaxes they should be. They're they're both intransitive verbs, ? However, there's something annoying called subject-verb agreement.",
    "you're not allowed to say this relax in English. at least not in  the standardized formal version. And  then this grammar would overgenerate because it generates it accepts the string. This relax, even though it should not be accepted. If you want to account for what's  in Ingram. ? we can work on this, we can gradually start to modify our grammar to account for more and more phenomena, and to fix issues with over and under generation. it becomes an engineering project, ? it becomes a grammar engineering project. ,   time someone asks you, what did you do in class?",
    "You can say I was a grammar engineer. ,  let's extend our grammar to account for say. prepositional phrases. I'm also going to make the Np structure more interesting. I'm going to say that a noun phrase can also rewrite into a determiner plus a noun and a determiner can be something  V, or maybe a and a noun can be something  a foul or boy or girl, or something else. , , or wallet, I guess. to make things slightly more interesting. ,  we want to account for, say, prepositional phrases. Or, , let's do adverbs first.st ? here it's , softly.",
    "how will we do that? How do we add adverbs to our grammar? Any suggestions? Yes. or yes, that's . to modify this grammar, to add adverbs. , first, st you need to add the adverbs. we can do that. we can do softly or quickly or slowly. And , we need to integrate this into our grammar somehow.",
    "and it seems that the most obvious way is to put it in with the verb phrase. and semantically. This also makes sense, because adverbs, as their name suggests, are associated with verbs,  they somehow modify the meaning of the verbs or the verb phrases. and  we can add it . this grammar will accept a sentence  the girl kicks the balls softly or quickly, or whatever . question can you have  an empty element inside? From the good question, can you have an empty elements inside. the A constituents. It depends on who you ask. if you're thinking about this formally in the from the Cfg perspective, yes, you're allowed to do it by my definition of Cfg, because here this is 0 or more , you can have a non-terminal rewriting into an empty SIM  into nothing from the perspective of linguistics.",
    "It depends who? You ask. I think, in mainstream. , in the  linguistics you're likely to encounter in linguistics courses here. They probably will teach you that there are empty elements, and they give evidence for that. There are some other people who say that there's there's something wrong with that approach, and that there may not be empty elements. But they're they're useful to account for many phenomena from the perspective of parsing. . , I'm just talking about  Cfg and what they do and how you use them to describe syntax. , we're going to start talking about how to systematically recover these trees.",
    "? suppose, given a grammar, given a sentence, give me the parse of that sentence from the perspective of parsing empty elements cause a big headache because you don't know where by definition, you don't see where the empty things are. The the empty elements are  to do, parsing properly with empty elements, becomes  intractable because you have to posit all possible combinations of up to infinite number of empty elements everywhere in your sentence. in practice you probably have to limit yourself to . At most, I will have,  this, many empty elements in a row, or something  that. And then parse with respect to that. But , that's a really interesting question. Turns out to be  everything there are complicated answers. But . yes, in our example we would have deleted the other Vp mapping ?",
    "There can only be one. What we develop. we need to keep the original rules. Is that your question? It's just one mapping, that's all extended. . What do you mean by mapping? , I shouldn't look back. It's the wrong word, but  Vp rights to 2 different things. But it's  just one big rule.",
    "You can't have multiple rules. you can have multiple rules. In fact, this is just a shorthand. This is  6 different rules. ,  this is , vp, rights to v, 1 Vp rewrites to v, 2 nt, Vp rights to that. these are 6 different rules just for the sake of saving space and using the vertical bar, that's all. But literally, it's just a safe space. It's these are, think of these as 6 different rules. ? And I think the question also asks for prepositional phrases.",
    "But I'll let you do that on your own. It's the same idea you have to say, what is a prepositional phrase, come up with the internal structure of a prepositional phrase, and then integrate that into your grammar. , here's an interesting phenomenon. Consider the following sentences. The dog barked. I know that the dog barked. that I know that the dog barked. He knows that. that I know that the dog barked and at infinitum. ?",
    "in general, you can describe that with some  grammar you can have Sp rights to Npvp. and then the Np is  the subject,  it can be  any noun phrase. The Vp is the verb phrase, and then it gives you . you can have one branch of it which gives you barked  the dog barked, and another branch of it that goes to  know that something. But the really interesting thing here is that you can have it. You can write it  that knows that rewrites to S  your initial starting symbol. why is this interesting? It's because we have an instance of recursion where you have a non-terminal symbol rewriting to itself eventually. and assuming that you have no empty elements, and,  you, have actual words there. what this would say would be that it's possible to have sentences of that are arbitrarily long, arbitrarily long.",
    "if it fits within this structure of  this, this described by this grammar. you see why? ? Because,  anytime, you have a sentence of a certain length, you can make it longer by adding, . I know that, or he knows that, or she knows that or something, somebody knows that ? And  this is an interesting property, and which is suggested by Cfgs, which is that in natural language there seems to be no fixed upper bound in terms of the length of a sentence. It's arbitrary, and really the main constraints are due to our processing power. , we have a limited amount of  brainpower and memory, and  forth. And  in practice, the sentences are not that long. but in terms of what this formalism suggests theoretically.",
    "And and this model suggests that you can have sentences that are arbitrarily long, infinitely long potentially. . that's why it isn't. And some people think this is a big deal. and you can contrast it with other systems that maybe are fixed in some way. , for example. supposedly animal community again, super controversial, but animal communication,  with  bee dances, . Have you heard,  bees, dance in a particular way to signal to their colony about the location of  food or shelter and stuff. Also,  chimps monkeys. They they make vocalizations, and you can teach them to sign.",
    "And they have properties and  whales, clicks. And that's the type of communication. , this is all controversial and potentially new, but one way that they may or may not differ from human language, at least many of them seem to differ in human language in that they cannot be arbitrarily long. , they there. There might be a fixed set and fixed inventory of symbols and signs that can be produced, but they don't have the same properties that,  Of recursion, where you can have, . you can in principle have sentences that are very, very long. If you want. . here's another exercise. Let's fix the subject.",
    "Verb agreement issue. fortunately, English has a very limited amount of subject-verb agreements in the present tense, . except for irregular verbs. in the present tense. the only thing we really have to fix is, we need to ensure that if you have a singular 3rd person. Subject. then the verb also has an affix of s or es, depending on the verb. otherwise is just the base form of the verb. ,  let's fix that part of the grammar. .",
    "before we do that, we will add both forms. ,  let's start with this grammar and then fix it. ,  what is our general strategy gonna be any ideas? the main problem we have is   we can generate the ball. Relax the boy or the boy relax the boy, kick the ball. That's currently accepted by this grammar. ? Do you agree? Because, through this branch. We can have the ball the boy.",
    "and then through the Vp branch you can go to Vp, v. 2 np. And then you can have the boy kick because kick is currently v, 2, the ball. And suppose we also want to model , I, . we have both 1st person and 3rd person. . , split them into different categories  that they absolutely great. That's exactly . ,  we have to split them into different categories and rewrite the rules to make it all fit. we have v. 1. For with an S and v 1 without an s. and likewise.",
    "We'll have B 2 with an S. And v, 2 without an XS. And then we'll have. I'll try to make this more distinct. and we'll have 3 v. 3 with an s. and then v. 3 without an s. And that means we have to duplicate all of these rules. we have versions with S and versions without S. Oh, oops! I made a mistake. Versions with S versions without. Oh, no, these are all. S's, , that's . And not only that we also have to pass that information up to the level of the Vp.",
    "if you think about it, the agreement works between the subject and the and the verb , and the subject is  all the way over here on the left. and then the verb is inside the Vp, which means that we have to pass that information along somehow. ,  that means that we also have to indicate that this is a verb phrase, where there's an S on the verb. And that means,  we need 2 versions of this rule. We need one rule, which is  Mps Vps, and one role, which is Mpvp. and I  realize that I hate my naming terminology. Because. , this is going to be . we're going to have 2 types of nps, those kinds of envy Nps where you they have to take an S on the verb, and the  Nps where they have to not take an S on the verb, ? , .",
    "this will be part of Nps. But I, for example, would not. And for I,  you can have debt n and also I. And there are 2 different kinds of noun phrases, the ones that are 3rd person singular, and the ones that are not 3rd person singular. , for example. . then. all the 3rd person singular nouns would be  a ball boy girl wallet. And then all of the not 3rd person. Singular nouns would be  balls.",
    "boys, girls, waltz. . Do you see why I hate my terminology? , this is confusing. I'm gonna say, 3 s. Rather than 3rd person's. Ts, how about ts. 3 s. What do you prefer? 3 Si guess. it's 3rd person. What  by S, here is 3rd person singular.",
    "Oh, no. Something. . Oh. 3 s. no. This is . , this is just a noun. ? the  nouns that are not 3rd person singular will be handled by this rule. nouns, and that our 3rd person singular would be handled by this rule.",
    "? And then you have to ensure that the 3rd person, singular nouns agree with the 3rd person. Singular verb phrases. . I will use 3 s.  . But but yes, hopefully, I didn't make any mistakes. , does this make sense? ? the general strategy then. is that we want to.",
    "Every time we create a distinction we have to think about , , is it? How do we incorporate that into the grammar? Usually it means that you have to split up a category into multiple categories. And then you have to think about where that information needs to go, because if it if it affects something that's very far away in the syntax tree. then the mechanism we have for making sure that the other part of the sentence knows about this is to propagate that information up and down through the syntax tree and through the rules described by the syntax tree. that's the general strategy. and, as you can tell, this becomes very hairy very quickly. then people have come up with schemes to make this a little bit more tractable. , for example, here, we're really talking about features. ?",
    "we have a feature, which is that a noun can be singular or plural. A noun can be 1st or second or 3rd person. and we want to be able to pass that feature information up and down the tree. And we want to have rules that check and ensure for correspondences between the features and different parts of your sentence. then, although we don't talk about it much in this course, people have developed theories of grammar that incorporate that. And that's really cool because it's a computational model. it can be implemented and read by a computer to do parsing and whatever. And you can specify things much more plainly and elegantly than we're doing here. through a feature-based account of a language and properties of words and grammar rules, and  forth. But we won't have time to go into that in depth.",
    "if you're interested in that ask me, and I can point you to references. And they should take she this they should teach this in linguistics, but I don't think they do. But , that's a rant for some other time and for us, we're just sticking with standard context-free grammars, where your only option is to create these very complex symbols. . far, then, we've talked about. Here's evidence for hierarchy and structure in sentences. And then here is one formal computational model to describe said structures. there are  alternatives. There's an alternative which is also very popular in computational linguistics in Nlp called dependency grammar. It's just a different view of the syntactic structures within a sentence  grammatical relations, they induce a dependency relation between the words that are involved.",
    "and we have the student studied for the oh, I should go back to full screen mode. . if you have the student study for the exam you can have, you can talk about how each phrase has a head word, and you can connect the head words to each other. , for example, for the entire sentence, the head word is said to be studied within the constituent of the student. It's student within for the exam. It's 4, perhaps, and within the exam. It's exam. And in a dependency grammar, you  draw arrows directed  directed edges to connect and describe this relation. And here's the here it is for that particular sentence, ,  study will be will not have, will not be the child of anything, because it's the head of the whole sentence. and then from there you can draw a directed edge to student and to 4, and you can label them.",
    "here this is a subject relation, and here this is a prepositional phrase, argument, relation, and from student you can draw an arrow to V, and from 4 you draw to exam, and from exam to B.  dependency. Grammars are just a different view of the hierarchical structure of language. But it shares  this basic property and assumption that it that it's hierarchical. Yes. Question. , how do you identify headwords? That is tricky. you can think about . some of it is through which elements specify the properties for the whole phrase, the whole constituent. for example, in terms of singular and plural.",
    "It's the student that lets  that the entire phrase of the student is singular. that's 1 aspect that helps, . What's the what's the head word more generally, it's the fact that the student is a noun that causes the student as a whole to be a noun phrase. It's not V that causes the whole thing to be a determiner phrase, although there are theories that do that. But but ,  for our purposes, it's the student that causes the student to be a noun phrase. and it's a 4 that causes this whole thing to be a prepositional phrase. for the exam. Again, there's a lot of complexities and different theories that do different things. But let's forget about that for . But the advantage of talking about dependencies as opposed to constituents is that it exposes the syntactic relations, the grammatical relations much more easily.",
    "you can directly tell here that there's a subject relation with this arc. And there's a preposition, prepositional phrase, argument, relation with this park. whereas if you draw things out in a tree. it's not  obvious, ? How do you tell that something is a subject? How do you tell that something is a direct object. The only way to do that is to describe some here, at this portion of the tree you might find the subject as an Np. To the left of some Vp or something, and that's quite complex. whereas in the dependency grammar it's much more clearly exposed. And  if you're interested in understanding the semantic relations between words and noun phrases, and you need it for further processing.",
    "For example, you're solving some information extraction task. People often work with dependency grammars rather than constituent grammars. Also, the other thing is that you can convert between them. dependency trees can be converted into constituent trees. Deterministically. if the dependency edges don't cross each other. and constituent trees can be also converted into dependency. Trees, if  what the head of the constituent is ? , let me do this example. ,  the students study for the exam.",
    "The way to do the conversion is to check. You start from the leaf, nodes, and you see what is associated with it. And then you turn that into a constituent. ? in the student study for the exam here, the leaf is B, and there's a student there. that's a constituent. Similarly, the exam. ,  that you've merged those nodes, then you can check. , for also is associated with that ? Because  the exam is and is essentially a leaf, a new leaf note, because you've merged them  you can.",
    "You can merge it with 4 . then it becomes for the exam. And finally, you have studied, which takes in 2 arguments with  students and for the exam, and  that you can merge those 3 together, and you get a ternary note there. you don't get exactly the same thing as  the  constituent grammars that we talked about before. that's potentially a problem. you might need to do a little bit more work to if you want to ensure that   a binary branching trees with internal structure that we talked about before. But at least there's some mechanism to convert a dependency tree structurally into something that which is a constituent tree. And also note that there are no labels here. you need to. You need to figure out the label you.",
    "You have to define additional rules to convert between from the these, these grammatical role labels into the non-terminal labels in the constituent tree. And it turns out you can do things the other way around, too. But I won't show an example, for . And , dependencies can cross in English. They rarely cross in other languages. They cross much more frequently. Does anybody speak German? Some. , a little bit. Does anybody speak Russian or Czech?",
    "What other languages? Possibly Farsi a little bit. Does anybody speak, Farsi, and they're really into Persian poetry. , , we're out of luck. You just have to trust me. you can have dependencies, graphs where the edges cross each other . And this is an example from German. He tried to reach me. You can say either. It's  he has me tried to reach, or he has tried mean to reach.",
    "These are both sentences of German. and they're both pretty natural, I think, and accepted. and you can draw the arrows using how the syntax of German works. And  in the 1st version there's no crossing dependencies. And in the second version, there's a bunch of crossing dependencies,  that he has me tried to reach  has a crossing dependency because it's reached me. not tried me, but reach me. and in the second one it implies that if you convert that into a constituent structure. you will get discontinuous constituents. You have a constituent that's broken up by other words in between, which is pretty interesting. I think we'll end there for .",
    "time we're going to talk about  our natural languages, context-free grammars. And then we're going to talk about parsing algorithms. enjoy your reading, break and happy Thanksgiving."
]