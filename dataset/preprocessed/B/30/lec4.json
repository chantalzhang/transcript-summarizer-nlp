[
    "OK, Can you hear me? OK.  again for starting late. All ,  today we'll move to Lecture 4, which will be on nonlinear classifiers. who can remind me of what we did on Monday, Lesson 3 or Lecture 3? Very briefly, who can remind me what we do? we still examine different text classification techniques and then we examine at least three. And then we apart from naive base, yes. SVM, yes, there's another one. OK,  there's another one . After perception then , Neural networks. we spent a lot of time on naive base and I want to assume by  you  have gone through the lecture notes and redo the exercise to understand how to do the calculation. Is everyone clear with the exercise, the one we did last time, or do you still have questions with the last exercise? Yes, yes, you can have some bias in a data set. the data set is very similar to the test sets, but you still have to do the calculation. the answer will be very obvious  even. For the time where you have zero, , that's a really good question. It's possible that you ruined the probability that what probability of Y equals spam and Y equals non spam. In the case of binary, when you do the calculation, we'd give you 0.  there's a way to address this in NLP, at least in statistical NLP, and we make use of what is called smoothing. And smoothing is that you had a very, very small term to the probability  that it's non 0.  you can add. OK, let me give you an example. let's assume you do a count. Let's assume you do a count of what is the probability of a certain word in your token? you can add a very small term  10 raised to the power -10 . And then this will make it make it to be non zero. And there are a lot of smotting techniques. There is additive smotting, and there's even more. There's a nice smotting and there's souvenir smoothing techniques that we typically use in NLP. But that's a great point. if you add the smotting, you will still be able to rank the probabilities to be able to decide which one should have the bigger class. All , hopefully,  we'll also treat smolting in this class.",
    "yes, 1 / 3 rather than 2 / 3, yes. the reason is, let me try to open that. I wouldn't display it, but I will try to open it on my computer. the reason is that we are trying to compute the probability of dozen review notes and then we are trying to compute and the probability of dozen review notes. You have no just appearing once out of the top three possibilities of grade equals a. , because they say it doesn't. OK, OK, let's move to today's lecture. The first announcement is that assignment one. programming assignment one will be released. And the good news is that it will be on test classification. you can try out different techniques we studied  naive base, SVM and  many techniques. My guess is that it will also you also require  tools  NLTK or psychic line. if you have not installed psychic line, you should install it and start playing around with it to be useful for your assignment. Will we be building our own model? The best answer is with to see the assignment  that  what exactly you should do. All , yes,  T office hours were posted on edge. if you need help  with programming or some concepts, you can reach out to them. OK,  , tutorials come in  few weeks. they would have there will be a tutorial to guide you if you are not able to install the  packages and some probability basics and review. OK.  we're still continuing with this very simple notation. And  that given an input X, we want to learn a function that will give us Y. And that function can be anything. it could be naive base, it could be logistic regression, it could be neural network, it could be perceptron, SVM, anything. And we and it could be  any classification task from basic tasks  trying to identify if it's a spam or not spam or this is a positive review or a negative review and  many tasks. today we're going to continue again on that and then we'll talk a bit more about neural networks. I'm going to do a quick recap of logistic regression because  there was a question on the notation which I want to clarify. I've seen this in the last class and  there was a question on the normalization of constant Z. The way is this coming from. one thing you have to note about logistic regression just  any other linear model because we are trying to we covered linear models last time and today we are going to cover nonlinear models. The major difficulty for linear models is that they cannot learn very complex imputes or any example that is nonlinear.",
    "because the logistic regression formulation is very simple and also  about the naive basis, very simple assumption that some of you were able to give examples why this would not hold, why independence assumption would not hold every time. And that's the major issue with linear models. But nonlinear models are more robust to model different inputs. just to clarify, in the case of logistic regression, you are  more familiar with this notation by 1 / 1 + e raised to the power minus U. Are you more familiar with that for logistic regression? And this is called the sigmoid function. everything you calculate in the case of logistic regression, you want to look for a certain weight that allows you to model your input X to give you Y. And that weight in logistic regression can be referred to as A1A2 to an and your bias term. this is your weight or what is called the Theta parameter for logistic regression. if you let's assume this is U, this is the original formulation of logistic regression in terms of sigmoid. that you want this probability for every variable for every Y you want to calculate to sum up to 1. The best way you can turn this into a probability distribution is to use a function that  ranges from zero to 1. An example of that function that is used for logistic regression is a sigmoid function. most of the time  materials online, this is the  formulation you will see. let's substitute everything we calculate and the Theta values we want to compute A1A to AN&B. Let's substitute everything we want to calculate as U and this will be 1 / 1 + e raised to the power minus U.  if you multiply the numerator and the denominator by E raised to power U,  you can transform the first formulation to this and then you can replace because everything EU raised to power U + 1. If you replace it by Z you can get the exact formulation we have in class. I hope that clarifies it. , if you're able to compute,   that if you're able to compute all these parameters, you can get the Z, ? And then this Z will be used to normalize it  that you can get the probability distribution. it's a constant that needs to be calculated. , you cannot assume any constant, ? what is the constant value? This constant can be calculated  that you can normalize it to form a distribution. And you have , , , once you have the values A1A2 to AN, you can calculate Z, , yes, it's the better of guessing those parameters, . E to the power of U because U is all these terms and U is dependent on the values of A1A2 to AN, ? You need to find these parameters to be able to calculate it. How exactly do you calculate Z? Train the model or run this calculation. That just to ensure the distribution or calculating it.",
    "I'm trying to tell you that this Z you can reformulate what we add in the format class to another notation, which is 1 / 1 plus EU raised to the power minus U. that means you don't need to calculate Z again, do you understand? Because it's equivalent to this. Do you get the point? Z is not equal to,  Z is equal to east raise to power U + 1. But if you rewrite all this formulation, it will be equivalent to 1 / 1 plus E raise to power minus U. Then you don't need to think too much about what is caused at Z. How do you need to calculate it? , is to turn it to a probability distribution. All ,  let's go back to artificial neural networks. artificial neural networks can be said we  biologically inspired. It's a  learning model which automatically learns nonlinear functions from input to output. each neuron takes a scalar input and produces a scalar output. this is a very basic idea. X1 is just  a scalar, and a neural of X would be a scalar. But of course you can generalize this  that it produces vectors or even matrices. And then you could even have this weight to be a tensor. It can be generalized to that. theoretically,   there's a paper that proves this, that any function you can model with a neural network and that's why the more data we have, neural network has not disappeared because it is able to model any function provided you have large enough data. while neural network was not very successful in the early 90s was because the models were not trained on large amounts of data. simpler models  SVM, excel where you have less data. And that's why if you have less data for your problem, you should  just try these basic approaches  perceptual  logistic regression and naive base and  on. , but if you have infinite amount of data, I shouldn't say infinite large enough data, then  neural network will be able to model it very . And one of the tasks that  demonstrated this is language model. when you're trying to predict what is the probability of the  token given the previous or the context or the probability of the old distribution of what's in your corpus, the more data you have, the better you are going to be able to model this. And it just gets better. And that's why the more people throw more data. I've been following the GPT story. GPT one came the train on, I don't remember a few gig of data. And GPT 2 was trained on  40 gig of data.",
    "And  the models are trained on trillion soft tokens. And the more data, the better is the performance in general. And this has been responsible for many applications  language modeling. I told you and one of the success stories and it's good that our community is also part of the success story. all the professors that have been working Asia Bengio has been working on for many years and when it became successful, they just applied the same principle for language modeling through the same architecture to speech recognition. It seems to work through the same architecture to machine translation and for every task it just excels. because the more data you have is just the better the performance. And also the same thing for objects detection. You  didn't learn about Imagenets, which is a very large data set that was trained on,  convolutional neural network. And also it's  transformed the field. the idea of the feedforward neural networks. there are different kinds of feedforward neural networks, , there are different kinds of neural networks. We have the feedforward neural network which is and then we have recurrent neural networks which we are going to study in the subsequent classes. But for the feedforward neural network,  that everything just moves forward. The only thing that is going back is when you back propagate the loss and you do back propagation, then you need to propagate back propagate the loss just because you need to update your parameters and make them get better  that you can get better output. You get an input and then you compute. you randomly initialize your weights and then you use that weight to compute what will be the output at the  layer, which is the hidden layer. And then because you have randomly initialized weights, those weights are sometimes small values from  -1 to one. And initially they don't mean anything because you just randomly initialize. But of course, nowadays there are better ways to initialize the network  that you can get better performance. And there's a lot of research on how to do initialization. But for a simple just to clarify things, imagine you randomly initialize all the weights and all you have to do is that you multiply the input by the weight matrix. You get the output and then the output will serve as input to the  layer. You multiply the weights and then gives you another output and then the output will serve as input to the final layer which is the output layer and then you get the output which is your Y. If you want to do a classification task, then you still need to use a function to convert it to  a probability distribution  that you can say this is the  class. for the feedforward neural network, I've been talking about weight matrices or your weights. You can mathematically formulate it as this. You have a vector of impute, an example of a vector of impute. Just assume you want to classify if a sentence is spam or not. Every word can serve as your X.",
    "Let's assume you have a five word 5A sentence with five words and then you have X1X2X3X4X5. Then you multiply it by your weight matrix. what is in the middle here is  a weight matrix and then plus the bias term and then you  pass it through an activation function G and then you get H1. Then H1 will serve as the input for the  activation function. H1 will  be the input for the  layer and then you multiply it by the weight of the  layer which is W2 and then you pass it through another activation function. Usually it could be the same activation function. You mentioned randomizing the initialization of the weights, that we do that for devices as . Or you can typically you can also set this to 1A constant, , or. you're just keep on changing the weights and then adding a bias occasionally in order to  go. that's for the initial phase you randomize, you randomly just put some random weights. there are many functions and you can just give me random numbers, flows numbers from -1 to one, and then to generate and fill up your matrix with just random numbers. it's just weight is just  a random number random matrix. And but you don't do this every time. You just do this once for the initial stage. And then you use a technique called backpropagation to improve the weights  that it gets better to be able to map the input to the output. Remember, we're trying to learn a function that correctly maps the input X to Y. And your learning phase is  what will modify this width matches to get better in predicting the  output. All , I hope that answered OK. , it's  an intercept. , it's  a bias term. All ,   you can do the math. if you multiply X.  let's assume. OK, give me one minute. I can just repeat what I said. it's  you have a vector X1X2X3X4X5 and then you have a weight matrix. One of the dimension of the weight matrix will be equivalent to that of your input and then the other dimension you have to. It's  a hyperparameter you have to define ? here it's  multiplying 1 by 5 matrix or vector by another matrix that is 5 by 8.  your output will be 1 by 8 and this one by 8 also will be the dimension of your bias term. And then you can add them together. You pass it through an activation function of G Yes. , that's a good question.",
    "the dimension also will be the it's a dimension of your layer, the 1st. , if it's the first layer of your network, that's a dimension. you can have for the first one you can have a dimension of 5 by 8 and the  one you have a dimension of  8 by 22, and the last one we have a dimension depending on your final output can  be 22 by two because you need to project it back to your output. and then you do the same thing. each one will  be the input and then you multiply it by width matrix 2 plus the bias term of two. And then you pass it through another activation function and then the last one is  Y equals to H2 W 3 and the dimension of Y will determine if it's a multi class classification. If you have four classes, then it should be the dimension of your last layer by the dimension of the number of outputs that you need, ? All ,  the activation function, there are many choices of activation functions. for many, many, many years, people use very simple activation function  sigmoid. And another one that was popular was tan H And the reason is because  we showed you  very nice plots of sigmoid because it can model everything from zero to 1. And many years after when we add what is called the revolution of neural network called deep learning, and then people just make this much, much simpler. And then you can have a maximum of 0 and X.  that means you can have a simple activation function called the rectifier, which also seems to work better than Sigma functional time H  there's a lot of research. , I don't think not more this time, but there were a lot of research then on what is the best activation function, What is the best way to initialize the network Because everyone was lost. We don't know why neural network worked the first time. And then people are trying to go slowly to understand the different process. And especially for computer vision, this rectifier seems to work very, very . OK, why do we need nonlinearity? Why do we need nonlinearity? This is why we, this is what we wanted to achieve because we want it to be able to learn inputs that are nonlinear. yes, linear and non linear data, that's a good question. in linear data, ,  it's very clear with the SVM formulation, something that you can just draw a line to separate the data and then you're gonna have  a hard margin and a soft margin that  separate them in a high dimensional space. if you cannot linearly separate your data in a high dimensional space, then it's nonlinear. , SVM can also work for nonlinear data if you use kernel functions. Kernel functions  takes it to a high dimensional space that it's separable, but in a nonlinear way. I can show you an example on the board. In those scenarios you cannot really separate it  with a line. we should see that you need  a non linear function that can  separate your data. Yes,  I didn't get your question. that if you pass it through this function, it makes it nonlinear. This is  you can simplify this to be something  a logistic regression.",
    "If you don't have this big weight matrix, it's just a linear function and it is at the last layer that you try to add the probability. But this one, you try to concatenate different layers together. as you are passing from one layer to the other, you need a function to make it nonlinear. , OK.  at the last layer, I told you that at the last layer, depending on the number of outputs, we have treated a very simple case where you have binary output, you have y = 1 or y = 0. But there are some cases where you have multiple outputs. An example of an NLP task that you have multiple outputs is if you want to do part of speech tagging. How many people are familiar, are not familiar. You are not familiar with part of speech is just very basic,  part of speech in English language or I don't know French language or any other language that you can distinguish between a verb and a noun and a proper noun and a punctuation and  on. that means every single word in your vocabulary you want to classify the tag. in that case you don't have to refute the size of your output. , you don't have two outputs. The size of your output depends on the size of your entire vocabulary, the number of types that you have. And in that case you need a function that can turn this to a probability distribution. in the case of parts of speech, the output will be the number of the types you have in your vocabulary, . We tokenize all our characters in English language by characters and every element in our vocabulary. You can do character level tokenization. A would be  a token, B would be a token, C would be a token. And in English language you can have 26 characters to be your size of your vocabulary. And then you can also add some punctuations, some other common symbols. Let's assume your vocabulary  is 35 when you Add all the symbols. your output,  you want to do categorized characters, your output will be the size of your vocabulary. But for some other task  where you have  word level tokenization, the number of types will be more. You can have the number of types to be 50,000 which is your vocabulary size. at the output layer you are predicting values for all the 50,000 and this is a problem in NLP because  you have a huge output layer that you  need to and  you want to determine what would be the tag out of all these 50,000 types. Work just for the work it will work. in the case of language model, your input vocabulary size is equal to your output vocabulary size. if you have 50,000 at the input, you also have 50,000 at the output. , you want to predict what is the  word given the previous. It could be any word in your vocabulary. you need to have the same size of your input as also at the output.",
    "In the case of path of speed tagging,  the number of types is just how many path of speed tags do we have. If you use UDB, we have  universal dependency. You have  18 types or path of speed types. your input will still be different because it depends on how many words you have in your vocabulary. But your output will be 18 because you have  18 path of speak tags and out of this 18 you need to determine what is the path of speak tag for this word ? And for this you can use a software function to convert the output into a probability distribution. , K is the number of class in your output layer. , I and OK, I  OK.  we use a random variable X. Let's assume we want to do output. Our output would be Y.  for every,  we have in part of speech, you have 18 types. for every type in the part of speech, will be your XI, will be your Yi. But you need to compute this probability over every Y.  you're going to have a probability for every item in your output layer. , it's for every class. And then you then you have  something similar to what we did the other time. You have  something called a normalization constant, which is the sum of all this probability, ? It's the sum of all these values, ? It's very similar to what we did for the logistic regression. OK. And this can work for binary. For binary, you don't need to  do this because it's very simple formulation. But for multi class, we typically use  the softmax and it's very important in the development of many. , it should be softmax over Yi if Yi is your output. This might be a silly question. It's just E raised to power XI. imagine you have a vector of output. OK, I need to use it. 18 types at your last layer and  you do this neural network, you have another one to 22 and at the last layer you have 22 to 18.  you have a probability. you have the output because these are scalar values. you have  0.1 here you have 8, you have 22, you have 19. Or  2, you have 0.001, you have zero. This doesn't sum to one, ?",
    "to make it sum to 1, you have. This will be your Yi. you take this over a soft Max function of Yi and then you convert everything to a probability. eventually you have soft  parameter of this and then you have this will be  minimized and then this will  be  0.002 and   this big value of 22 will  be  0.3 and you have other values  0.02. if you want to select what would be the  value, then it's very clear for the proposal, ? That this should be the  . the problem is that when you have your Yi, you don't have a probability and then. Converge into your probability  that you can select processing and that's why you need something All   , if you want to do the optimization you need a loss function  one of the most popular loss function that we use is the cross entropy loss. that when you have a neural network, you have a predicted outcome. This is what you get when you do the feedforward neural network and your output is called this Y dash and then you have Y which is your gold standard correct answer. you can  you need the loss function to know how far away are you from the words from the correct answer. An example, this is one of the most popular loss functions used for classification. Classification is cross entropy loss which is coming from information theory. But it's a very simple idea that for every Y you have, you try to compute what is the cross entropy between the Y, the original Y and what you have predicted. And you sum it all over every instance of Yi that you need. you can have cross entropy for a binary case. You can also have cross entropy for what do you call it for a multi class case. A very simple example is if you have an output of one or zero, you can do the calculation. You can have one times log of what was predicted,  0.5 + 0 times the others, ? this would be your first loss from the output of the neural network and then you back propagate it and then you continue. All ,  for training neural network, one of the popular approach is the gradient descent. and  that you find the gradients of loss functions with respect to the parameters of the network and gradients it's from derivatives, you have to do the derivatives and then  which direction, what's the direction of the gradient to modify the parameters of the network. And one of the algorithms that's used till today is back propagation and one of different artists talk they have tried to replace it but they have not found a good alternative to  since 1986. You can still use it  in 2024.  that you have this is your parameter Theta. All your width matrices can be  categorized as Theta. And then you perform what is the derivative over Theta, and then you have the learning rate. And then you subtract the Theta from the product of the learning rate, multiply by the derivative, and then you improve Theta. If you do this over many iterations, you improve the values of Theta and then you can get a true Theta that can correctly map your input X to Y. OK, Do you have a question? Do that when you're back propagating when. It's as simple as that if I remember correctly.",
    ",  it's  for stochastic gradient descent. the difference between the original gradient descent or the stochastic gradient descent is that for the gradient descent, , . The weight of every layer is what you randomly initialize. you randomly initialize it, and then you modify it to get better. The only hyper parameter here is your learning rate. for the gradient descent you do the sum over the entire couples. you compute the loss over the entire training data. But for stochastic gradient descents is that you can randomly sample one and then you compute the loss over this one. It's as simple as that. instead of doing sum over all the samples in your training couples, the alternative would be just to take a small mini batch of the training corpus and update the weight. And for SGD, it's just the mini batch is just the size of one. And if you do this over and over again, it can lead to faster convergence because you don't have to. You can  parallelize things over the network and that's why it's been done. for the SGD overview,  very simple. You have a function of the input and the weight matrix you want to learn, and you have the training samples SK YK. You have a loss function of L and  that sample a training case XKYK and then you compute the loss over this training case and then you compute the gradient and then you update the weights of Theta. And then you do this over and over again. And of course, while doing this, the only important thing is that you also save the values of the inning weights. You save it somewhere  that you don't lose the values because you also need them to perform the derivatives. OK, this is the most technical part of the lecture, but this is also very simple if you understand the chain rule of derivatives in calculus. that you if you want to do back propagation from the output Y to go back to the input, you have to do you have to compute the derivative also from the output back to the input. if you want to compute what is the derivative of West three, you compute the derivative with respect to the weight because you want to modify the weights matrix, ? you compute the derivative with respect to the weight. if you compute the loss, the derivative of the loss over W3. this is  this means you are computing the derivative first over the activation function before you can compute what is the derivative of the activation function with respect to West. Do you have a question? it's  you want to compute what is the derivative of sine 3X and then you have to 1st compute what is the derivative of 2X. And if you substitute 2X to EU and then you compute the derivative of sine U and then you multiply the derivative. this is the idea of the chain rule. there's a function blocking U to assess what you want to compute the derivative for.",
    "you compute the derivative over this and then you compute the derivative over what is inside and then you multiply the derivative. And then you do the same thing for if you want to compute the loss for if you want to compute the derivative of the loss with respect to W2. Again, you start with the loss over G3 and then the loss of G3 over G2 and then the loss of G2 over W2, because this is what you want to get. And you can repeat the same thing for W one once you have access to the last activation function which is W one with respect to data of W1. it's important to treat a word representation. I've been hinting about this a couple of times. here when you are talking about words as the first question is how do you convert words to numbers? Because here we are talking about derivatives. Everything is a number, ? But in NLP we are operating on discrete values. It's either a cat or dog. there's a need for you to convert every single word into words, a representation. the simplest way you can convert it is what is the position of this word in the vocabulary. And if  the position of this word in the vocabulary, you can say whenever you see word dog puts it as #33 in my vocabulary of 50,000 tokens, whenever you see word X-ray puts it as number,  49,000. . But this can also be converted to A1 auth encoding. You want to ask a question? You have 50,000 words in your vocabulary or types in your vocabulary. You can rank them alphabetically from A. A is a word in English to some, I don't know, zebra. Let's assume this is the last word in your vocabulary. You sort it and you number them. The first word is the first. The first index is A, the last index is 50,000. Every other thing would be in between here. for one not encoding, , you set it to one position 33.  that means you can have a matrix definition of your vocabulary. does not mean something useful because there's no notion of similarity. there's no way to know if 100000 and 0001 are similar. If you multiply them, you compute cosine similarities. We give you 0 and that means we need to come up with a better representation for every word in our vocabulary that have the notion of similarity. And that's why  in the early 2000s we started working on word embeddings.",
    "How do you have a good representation of the word  that you can have notion of similarity, You can know what is the difference between,  you will know the similarity of different words or different types. , no, you don't need limitization because if you are able to find a good word representation, the similarity between use and using and used will be very similar. They will have high similarity. If you are able to find a good representation for every word in your vocabulary then you don't need to levertize and then you can learn everything using a neural network. Similarly for sentence representation, once you have a vector for a word, if you want to represent a sentence you can just Add all this together ? But this  this is not a good way to fully represent a sentence. There are other ways of what is called component wise vector multiplication and there are also more sophisticated options  concatenation. You can just concatenate the word representations. , that's a great question. It can be used, but there was word embeddings before attention is all you need paper. we, we need word embeddings just because we want to know, we want a better representation for words  that there is a notion of similarity between words, ? No, we are  different word representations. what Vec, I don't know if you've heard about that Glove embeddings. And  we have  sentence embeddings because people find there's very obvious limitation of word embeddings. , you cannot, you cannot do word sense disambiguation very  with what embedded? If someone says a bank, it could mean a financial institution, it could mean Bank of a river. If you have a single representation for bank, that representation is  biased to one of these two definitions, ? you need something that captures the context in which the word appears. And that's why nowadays we focus more on sentence representation. Can we have  a better representation of the entire sentence rather than the word? , for the word tokenize. And pass through a model, yes. , OK. Modern trends in neural networks for NLP Nowadays what we do the field has changed a lot. We don't really spend a lot of effort in learning better word representation. What we do  is we do what is called large scale pre training on a big corpus and then we do fine tuning. Just give me one minute. Large scale pre training is an example is this. If someone says this is a dash bank, you want to train a language model to be able to predict the blank space or the Max token. if you train this, this is an example of what we call maxed language model because you are trying to fill in the gap. And if the language model is able to fill in the gap properly, that means it has learned a very good representation of many words in the large text composure provided.",
    "And this is how techniques  birds model, Roberta model has been developed. And after you are training language model this way, you can fine tune this language model by removing the Max language model head because you don't need this again and replace it with another head for the classification. And this is called pre training fine tuning paradigm. for any NLP task you want to do nowadays based on neural networks, you're going to start with a pre trained model. You're not going to train everything from scratch again. You pick an existing model and then you fine tune it on a downstream task. And this concept is very related to transfer learning, which I will cover in 2 minutes. apart from live pre training, we have also made a lot of advancement in hardware because  we have better hardware that can train on a large amount of corpus and we can also highly parallelize these operations on GPUs which have better functions than playing games. They're building better deep learning models. the first one is you are able to learn better relationship within input and output. And one key thing is that it reduces the need for future engineering. you don't need to do lemmatization and stemming and all this feature engineering again. There's also more efficient use of input data via with sharing. And one of the good thing with neural network is these two things I want to mention is multitask learning. you don't need to just train a model for a single task. You can train a model for multiple tasks at the same time. And that's why you have a model  CHAR. GPT can do many things. It's not only able to do generated text, it can also prompt you to do classification. It can prompt you to do even regression tasks. It can do many things. that means you can train a neural network to solve multiple tasks at the same time. Another nice thing is you can do what is called transfer learning. That means you train a neural network for a particular task and then you can initialize weights of your new model for the new task you want to solve and then do transfer learning. for the case of computer vision, the Imagenet data set that was trained on CNN can also be used for other tasks  object detection. even something a bit far away from what it has been trained for. Another example in NLP is that if you train a language model or a mass language model on English Wikipedia, you can also use it for other tasks  sentiment classification, topic classification, question answering and  on. And this is the idea of transferring. That means you don't have to train for every task from the scratch all over and all over again. You can benefit from large scale pre training and then just fine tune this for your downstream task.",
    "And this is very important. there are other challenges on neural networks. It needs a lot, a lot of data and by  you should be aware of that. Also, there are  many hyper parameters to tune. I've only showed you one, which is the learning weights. But there are   many hyper parameters because better algorithms have been developed that and  you don't only have one learning rate to do hyperparameter tuning for, you have momentum, you have other weights, you have decay parameter and  many parameters. And then you have things  number of eating neurons. What is the function to use? Another difficult thing is that neural networks tends to work very , but it's difficult to interpret. Nowadays things are better because you can ask to explain the prediction. Previously, you cannot even explain what happened in the network. It just gives you the answer. You don't know what happened. There's no notion of interpretability. neural networks for NLP, there are many open questions on how to use linguistic structure. Some people say let's just forget about linguistics and there's still debates at top AI conference. Why should we be investing  much time on linguistics? And when is linguistic feature engineering good? There's sometimes when if you incorporate domain knowledge, you can  boost your performance than a lot of engineering with neural networks, how to do better, multitask and transfer learning to new domains, to new languages, to new setups, to new tasks. And of course, which will be, it will be cool to be able to develop new tasks and challenging tasks for NLP. the last part of the lecture is the evaluation metrics. The C plus evaluation metric you can use to evaluate your model if it's good is accuracy. How many correct predictions over the entire test set? if you have 10 examples and your classifier is able to predict 7 correctly out of 10.  what's the accuracy? Or 0.7 if it's one. this is always not good. Let's assume you want to do spam classification. And the problem for spam classification is that spam appears  rarely,  it's very rare. you can have   5 spam out of 500 examples. if you use the notion of classification, the notion of accuracy, your classifier can  just ignore all the spam output and still have a good accuracy, ?",
    "Because it's just about the correct prediction. there are better ways of thinking about this. Some of these have been motivated from information retriever,  computing. What is a precision you can say out of what has been predicted? And you can talk about the notion of recall that out of how many are correct of the entire class. this is very important to distinguish between precision and recall. there's a way to easily distinguish it by building a confusion matrix where you can have true positive, true negative, false positive, false negative. I don't think I OK,  let me try to do that. Then it's easy for you to compute. if you have a simple confusion matrix, that is very clear. your precision will just be true positive divided by true positive plus false positive and your recall will be true positive divided by true positive plus false negative. apart from that, you can also compute what is called the harmonic mean F1 score because this error is better than accuracy for many tasks just because there's a notion of different  errors that your classifier can make by computing this precision and recall. And while doing this every you can do what is called macro average, which is a very important concept. That means you take, you want the classifier, you want to treat all classes equally. That's the idea of macro average. weights of each class are important. if you have a spam, even though you have 5 out of 500, if your classifier misses all the five that are spam input, you're going to penalize the classifier heavily because you have what is called a macro average. If you have the micro average, that means you want to sum over what's correct, and then you have weights. You have weights for each sample equally,  everything that is correct, you combine them together before you do the average Y for macro average. For each class you compute the F1 score and then you have equal weights for both of them. you compute what is if you have two classes, spam or no spam, You compute what if the F1 score for spam, what is the F1 score for no spam? And then you say F1 score of spam plus F1 score of no spam divided by two. This is the idea for micro average, the sample equally, that's the micro average. , I have an exercise for you. This is the confusion matrix which I already drew on the board. But you can have it more complicated than this, where the  answer will always be on the diagonal. if your classifier is doing a good job for Class 1, the  one will be on the diagonal. You have more values there, and for Class 2 you have more values there and  on. All , OK,  this is a task for you. This is very simple and I already gave you insurance on the board.",
    "what is the,  what is called? in this example, what is your true positive? And what is your true -20  you can compute the precision. what's the precision for this? What's the precision for spam 0.2? What is the recall one third? And what is the F1 score for spam 217 2 / 7 and that's what's 2 / 7? OK. What is the F1? What is the precision for non spam to third? What is the record for non spam 0.57? what's the F1 score of non spam 0.615? what is the macro average for this? What is the macro average for F1? that means you have to do the precision. You have to combine the F1 of spam and F1 of non spam macro average is equal distribution, ? that will be what 0.85 zero .285 + 0.616 / 2 And OK, what's the accuracy? How did you get half everything? accuracy, I don't have the notation on the ball. accuracy would be true positive plus true negative divided by the entire thing. OK.  micro average, micro average will take you some time to calculate. you can do that at all. Thank you for joining the class. But  you already solved it. OK, I did some solution. On the bot, but it's in the bot. I had a quick question about softmax being the output layer for RNN. in an architecture  this where?"
]