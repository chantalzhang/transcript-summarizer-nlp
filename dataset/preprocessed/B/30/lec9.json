[
    "today's class lecture 9, where we'll continue on hmms, and we'll  move to conditional random fields. today, we'll consider sequence modeling with features with a focus on linear chain conditional random fields conditional rental fields. especially for a token classification. Tasks  battles, please tagging, chunking. and name them to recognition, and they were  some of the most important models that dominated the field for  more than 5 years. , when deep learning came around it was still very important to have conditional random fields to  improve performance. And today we're going to see how we can go from Hmms to crf. And the interesting thing here is that you can  combine conditional random fields with some other deep learning framework  Cnn Lstm. That is based on transformers architecture. You can combine it with Crm by just appending the Crf layer to the  the last layer of your network. And then you can  boost the performance by one or 2 points. , they are not they're still very useful to today. , people don't use it that much as before with the Llms. That can do almost everything  but for token classification task. We  find Llms to still struggle a bit with talking classification tasks. And  some of these methods can still be useful in practice. If you want to improve performance? the last time we talked about,  we finished off with also provides learning of Hmms. We today we'll talk about some of the shortcomings of Ed Markov models, and then we're going to move from generative tasks to more discriminative tasks and of course, we examine the linear chain conditional fields. ,  for the Ed Markov model, this is a summary of what we have been talking about in the last 2 lectures. we have a Markov model, where we have different states  q. And then we also have observed variables . For the case of Patos Beach. it will be the States will be the pos tax. and if we also talk about the ner models. The States will be the different ner tax,  we examined last time. and the observed variables will be the words,  this is a generative model. And I want to emphasize in generative model. If you want to predict what would be the output of y. Given an input X for generative model.",
    "to    to compute the joint probability. Can you help shortly, Doc? Thank you bye  for the case of   again. if you want to solve the transportation task , you want to solve for y given X, there are 2 ways of going about it. It's you can   go through it with generating approach or discriminative approach. A very simple generative approach would be of naive base. This competitive approach would be  your normal Asvm. you can also do that . Another example of the generator model. Why, as you will see in a couple of slides, crf is an example of a discriminative model. in the generative model to reemphasize. If you are supposed to find an output y. Given X, your approach is  to. I'll try to learn the joint probability between X and Y, and the assumption is that if you can learn successfully the joint probability between X and Y, then you learn everything about the model. And if you have been able to compute all the statistics needed. you can  compute Y.  that is the idea of generating model was, as , in practice, this is  not possible, because for you to compute that you have to make some assumption which is  the Markov chain, assumption or independence assumption to  be able to compute these probabilities. They do not make this assumption because some of these assumptions may not be realistic, ? you are not able to compute the jump probability every time. And in that case a discriminative model is more realistic. Just give me more native model is just trying to optimize , can I get the a local minimum or the global minimum? If I optimize this loss function. And a very good example is the artificial neural networks. We're back to the . Which is a generative model. Can we learn this general probability? It's difficult to learn this joint probability over all the States, and therefore we make the Markov. we make a Markov assumption where we  explore conditional independence assumption. Instead of saying that the  State depends on all previous states, we only say the  state depends on the last one. And also we add, that is what we call the state probabilities, and for the emission probabilities, we say, if you are able to successfully learn the generative model, we will be able to also have the initial probability, which is the probability of absolute given the State. And this is a very simple example of a generative model for a diagram distribution where we see Q.",
    "And you can also emit all one based on every single State. You can also emit every, all eyes based on every Q state. last time we discussed the following. Because our focus is to learn this joint probability, which is the probability of Q comma O given the parameter theta. we consider a couple of our guardians. first, st for you to compute this probability of Oq. you need to know what is the likelihood. and the likelihood is what is the probability of all given Theta. And we said last time that you can easily compute using dynamic programming, you can compute the forward. You can compute this likelihood. P of all, given theta using either forward algorithm and backward algorithm. And if you're able to compute that, you could also integrate that into the computation of What do you call it? Into the computation of the entire State probability, which is probability of Q. Comma. And for that you can use the vertebia algorithm and vertebia algorithm is very similar to the forward algorithm. When the 4 are going in the sum over all the previous States. Why, in the Viter we are guarding, you  just take the maximum. if you remember, in the last slides. And the last time we also said, . It's also possible that you can do this when you don't have any labor, they do no label data, no problem. Then you can use em algorithm to  estimate the labels. Even if there's no label data. And then you do this attractively. 1st you randomly initialize the States. and then you compute your needed parameters. and then you maximize in the maximization stage you try to get a better initialization. and then  if you're improving. just to revise what we did is that it's possible to compute this iteratively, even if there is no labor data. and the idea would be to predict. You repeat for a while, and then you predict the current state sequences, using the current model. And that current model initially, just initializes randomly.",
    "And then you update the parameter of your current model  that you can have a  based on the current predictions. that again, you repeat for a while very similar to what in the last slide you predict the current state sequences, using the current model with the vitabi algorithm. And after that you update the current parameters using the current predictions as in the supervised learning case. again, for the emigrating  that we believe that we randomly initialize Theta K and your Theta K is all your  all your A's and B's in the, in the last formulation, you randomly initialize them, and once you have randomly initialize them. You can use this to  have the expected count based on the it is structures. And after the at the maximization stage, then  that, you see, can I get a better Theta K plus one. I computed the expert accounts based on initial values or randomly initialized data K, and at the maximization stage, can I get a better one. that I can have a better initialization. And you do this attractively over a couple of  you. You do this over a couple of times. on to go more into the details. what we discovered is that . for the expectation maximization, it boils down to 1st  computing the probability of every State given the observation. and after that you can  compute the transition probability from going from State I. Which is going from the previous state qt. And then, if you apply the E step. that normally you initialize what is your Sita K. Because Theta K. You don't know, because there's no data to learn it from in the unsupervised case. you randomly share rise your A's and your B's, your A's are coming from the forward. Are coming from forward algorithm and your betas are coming from the backward algorithm while trying to compute the forward algorithm and the backward algorithm. You need these parameters A and B and the parameters A and B are computed from this grid that I showed you the last time. I still have it in just a minute. we go to the last presentation on Hms. this one where you have you can compute. and for the unsupervised setting you are not able to compute these alphas and the betas  but 1st you assume some initial values for the B's which is your emission probabilities, and then you assume some initial values for the ace with which are your transition probabilities. You'll be able to compute your alphas for the forward algorithm. And then you also be able to compute your betas for the backward algorithm. And once you are able to compute this, you'll be able to run your Em algorithm even for the 1st state, because you randomly initialize your A's and B's because they are known, unknown. , you don't know them. A question is probability of O. Given Theta K, you computed using the entire forward algorithm at that state.",
    "Yes, because you can  use forward algorithm. it means you initialize the entire table as random values. And then it's updated at every iteration. If I go to the example. let's assume you are a zoom. Random values for all this. your A's are coming from the transition probabilities and your B's are your initial probabilities. To be honest, you can generate this for me. Goshen is  not the best. better probability distribution you can use. and if you generate this from any probability distribution just to fill all these values randomly, initially. then the  thing is that if you have all these values, you can run your forward identity ? And you can run your background as you're ready. because there is a formula to use. But the initial value you don't know. I just want to be sure that I was showing the same thing on zoom on the class. at the E steps you already assume values for Theta K, ? You do with some random initialized values, and then you are able to compute your Alpha highs and your better eyes. And then you can  compute your gamma apps at the E step, and then for the second parameter, you can do the same thing . your B's your, , are your A's, because these are randomly initialized values. that is Tita K, . And once you have computed all this. you already have the values of Eij gathered eyes. And based on this, you can  run the maximization stage. And the maximization stage is that you  want to get a better values or better initialization for A's and B's. A's and B's here will  form your new Tita. the hard em, this is more. This is over a single tag. why the soft em, it's more. This is the bow vash a guardian.",
    "This is distribution of loan books. and  for the em, this is what we are doing. A small soft em soft where we belong into soft game. And and this is the stock version of the em, and we can  relate this to what we have done previously. The same way we competed. once you re-estimate your A's and your B's, you have a new value of Theta K plus one. and then you can go back to your E step and perform the algorithm again. And then you go back to your end. how do  you are making some progress? You  you're making progress when the training set likelihood , you're not making progress. , if the likelihood doesn't improve . that if you can estimate the likelihood given Theta K plus one, and the probability is bigger than probability of theta. And this is the way  you're making progress. And if you do this a couple of times. And this likelihood keeps improving. Then  that you are learning a better estimation for your T. 10 k plus one. And of course, there are some additional proofs that you can check here. ,  this is where we stopped the last time. And we said, , there's some issues with this, because it's very sensitive with to your initial in your initial random initialization. If the values you have generated in the metrics are very bad. and they are not very realistic. It's been very difficult for you to improve for the year majority. Better ways of initialization would be  using external knowledge, , from an external purpose where you already have more realistic counts that you can use. And of course,  this is just a revision of what we did last time. And today we can move to the  topic off. I forgot the last the last thing we discussed is that apart from part of speed tagging, there are also other sequence bodily tasks  chunking where you are not only classifying every token. but you cannot try to  categorize things  the  phrase, rather than only is a group. And also we describe the name density, recognition task, which  doesn't require you to annotate every single word but import. Accept the important entities in the sentence. And, , in this sentence, Maggie University is located in Montreal, Canada, and my gear University is an example of an organization.",
    "Montreal is an example of location, name, density, recognition can be different, depending on the domain. I have to emphasize that. Our task also in the biomedical domain where you will not be identifying locations because that data, there's no location. , technically, you can have location. I can give you an example of 2 applications in the biomedical domain you can be. all you need to identify will be things  disease of patients. , let's assume you want to analyze. I can make a note that has been written by a medical practitioner. the entities you will be recognizing there will be more than just personal name, and even personal name needs to be distinguished. You can have something  patient name. You can have something  doctor's name. You can have something  age. You can have something  disease type. You can have different kinds of symptoms. , depending on the domain, you can also define your set of named entities. and even for general domain you can expand these named entities, , you can distinguish between different kinds of location and GPS, which is geographical political entities,  you can distinguish between entities  city names, country names. continent names, and then you can distinguish it from entities  mountains. and reverse and give them different kinds of entities. there are different classification for any hour that we can  use depending on the application. And here the idea for any error is that you  need to detect spams of multiple words that are relevant to the entity. last time we also talked about the streams. if there's no scheme here, it's very difficult to know the entity. You have the entities  organization. But there's a question if comma is  part of the organization, or if everything is a single organization here rather than multiple organizations. And here it's clearly that all of them are multiple organizations  Magu is different from you. Come and you them and you need a scheme , that you can annotate this properly. And one of the most popular schemes is something  I will be tagging. But  we have many schemes. There's iob one there's iob 2, that is, I/O BA scheme, and there are many schemes. Let me try to give you  a 3 letter word, and I can show you the different schemes.",
    "on the board you have,  3 kinds of scheme. the 1st one that was proposed is, I will be one. But  we just call it Iob or BIO, and if you see bio, usually we are referring to. I will be 2, because this is the one that is mostly used in standardized data sets. the difference is In the first, st  we're trying to address the problem. And then we just say, , every entity start with iod. But if there is an entity that's an organization that  starts after you received in Montreal, then you will use the B, the B tag. you would have if you have You  have Mcgill after Montreal as another organization without space you would all use the organization for this. This is the 1st one. Here you have what is called the beginning of the entity. and then you have the inside of the entity, which is the eye. And everyone asking is the same. the 3rd example, here you have what is called  the start of the entity. if you have,  a multi word expression, you  don't use B, you just use the start of the entity, the inside of the entity and the end of the entity. and for every other one where they are not much a word you use. these are   the different popular schemes that are available for any annotation. ,  what are some of the shortcomings of the standard? The 1st question is, how do we have more features to? And this might be useful for Pos tagging, , we can have important features of our words that , we can integrate to this. to the algorithm that makes it very easy to predict, , in English language. if a word is capitalized, this is most likely a proper no . if you can add this feature even just by writing rules, without all this fancy, according, you may be able to detect a lot of proper notes. how can we include this feature into the standard agent? And also what prefixes suffixes? How can we add this feature? these are some of the shortcoming of standard Hmms, because  it's difficult to add new features. And the answer, today we want to talk about crf, which gives us an opportunity to have new features  to our model. But the difference between this approach is that  we are going to consider something more of a discriminative model , I said in a generative model. You want to lend the judge probability  if you want to classify y given x. This is what you want to classify, but for generation model.",
    "You 1st have to build or try to learn the joint probability, and once you learn the general probability, everything becomes obvious, of course, with some assumption  independence assumption to make it easier. , the only way a reason why we have independence assumption is  that it's tractable  to compute this joint probability. because, if not, if you don't make this assumption, it's not possible to  copy this ? those are the generating model. But for discriminative model, you can  learn the probability of y given x directly without learning to join partners. And this is what we typically do nowadays. ,  this is more  a having a tax specific model for secrets labeling. But you cannot use this to generate new samples of water pos sequences. if you learn a giant probability and it's a good one, you can  generate new examples. or you can use it for data augmentation to generate new examples. But for this community talk, it just learns everything in your daily. the question is ,  the idea of crf, because , these are  research driven work. , there's an  have been used for a few years. And in 2021, someone proposed. I know that which is  a modification of what you already know. how do we modify the original formulation for our dhmm. for the linear chain crfs, conditional random field. we can learn the probability of Y given X using this function. here you find out that we can learn a function of Fk. that depends on the previous one. the present one and the input  and we also have a normalization constant, which is all of our order in states. And I'm going to define what will be F of K. This is the new function you want to learn. remember, what we want to learn is the probability of y given X. But what we are saying is that even while learning this function of xt going to yt. We are also saying it depends on the previous value of y and Z, and we want to learn that function. FF key, and then we normalize it. and for just to go back to the idea of the . in , we are concerned with 2 things are these transition probabilities from one participant to another one. and also the emission probability for a meeting award given attack. And here we replace the product of numbers by just a linear combination of weight and feature values.",
    "which we completely simplify the what we are doing. we  replace what we have in the , we have a function from one part of speech which is in the YT. and  we replace it by this indicator function of yt minus 1, 2 dt. Multiplied by another indicator function where yt equals. are you familiar with education function? This is the very simple definition. it's just going to give you either one or 0 indication function. indicator it will give you either one or 0.  if the value, the relationship inside is true. you just have inequality inside, and if it's true, it gives you one. If it's false, it gives you 0.  that means this function of Dt to n, and we either give you one or 0. ? For the that's a modification. Oh, that's the 1st modification. that with this we can  add new features. That we cannot add in agent hubs. one example of the feature is . We want to know if a word is capitalized. Then we have a feature  that. , what is the probability? What is the indicator function that yt equals stash? When xt is capitalized, we can add a new feature  that. Once ending with Ed, we can add another feature  that, and  that, we can add more features to our modem. That is the idea of adding new features to Cf. Which we are unable to do directly with each of us. And there are many, many features you can add. you can say, , if the lens is less than 5, you want to add a new feature. If the 1st word is a determiner, you want to add a new feature,  there are new features that you can add. and the interesting thing is that when you're working on Crs. part of speed tagging, using Crm, you have to come up with a lot of features. this requires a lot of feature engineering. To be honest, you have to find features yourself  you can  just a minute.",
    "the more features you get. You can improve your performance just by coming up with new features. , which makes  very easy to extend. based on what you define? , you can say yt equals a part of speech  we showed in the last slide. And then you'll say at the same time when xt is capitalized. in terms of the inference. that we still use things  your forwarding. But there's a slight difference. Is more of a generative model. And Rss Crf is more of a discriminative model. for the forward algorithm and , we need to compute the likelihood. And also we need to find the States that maximize the joint probability that's for the  and for the crf, what we are doing is to  compute the Z of X, which I'm going to show you how to compute based on forward algorithm again and for the Vitambi authority. We are only interested in the value of Y give an X.  this is the forward I got in for Hmms. Which I believe you are quite familiar with after seeing this for the last 2 lectures. the modification we do here is  hardened this function. In the last one we have. Times, which is the initial state probabilities. We have the initial pro activities. B, and then we have, the transition for a brief space. For the Crf, we are just interested letting a function. which can be user defined. based on the features we want to add. And after this modification, apart from this modification. every other thing is the same for the forward agriculum. Why is the exponential function? I believe that's that's the way the author has proposed it. you have the exponential of this and these values here you find out that these are  just these are just indicator functions which I don't want or 0. It's coming directly from the formalization which they're very sure of. But I don't think it's it's going to ruin anything.",
    "you'll be either multiplying by one or by E depending on the result of the multiplication. ,  because F of K would be one, ? Is that a 1 or 0? each of the 3 can do. But even if you have the log of exponential, the big number you can use this log sum of exponential trick that is not gonna ruin your estimation. But the way we learn it is that, unlike you can. There's no closed formulation that you can derive that this is the mle for the crf,  you can. There's no analytical emery solution. But we have to learn. It's using gradient descent or one example of gradient deter method is Newton methods to find where the gradient is 0. But of course there are also other good Edison methods that can be used in terms of convexity, which many of you may be familiar with , when you have a convex function. If you take the negative, or  when you have a concave function, if you take the negative of it. It's going to become a convex function which I believe that. We normalize in a the definition of. why aren't we normalizing here? it's just to make a privacy distribution. you  don't need to normalize. We're just taking the maths. , aren't we using the forward algorithm to compute Z of X. Yes, if you do it over everything. You'll find that you're gonna have this set of things. because that's the probability, . we keep moving from one state to the other. And at the last states, we just solve over everything. the forward algorithm isn't computing probability of y given X, it's computing Z of X,  it's computing Z of X, yes. that's what we said, . And for a gradient asset. you walk in the direction of the gradient to maximize l. Of theta. But of course you can also do gradient descents where you have a negative of that. You work to minimize the loss function.",
    "Given the sense you minimize but of course there are other metals  the conjugate gradient, or what is called the Lbfgs. which approximates using the second derivative . instead of maximizing the log likelihood. You minimize the negative log, likely. And that's how you can convert a concave function back to a convex function. And then you do this for a while. first, st you compute every other thing you need to compute using the forward algorithm  a forward algorithm. And then you can estimate new values of your data after taking the derivatives. And here we can  compute what is the gradient of the log likelihood based on the Crm formulation. what if you take a derivative of this? I have the proof also here. But  we can go through very quickly. This is what we're going to find after taking the derivatives. Can you see it's too tiny? we'll walk this on the board. we want to take the log or a full drop of all this. and if you apply the log we are going to have some extra this time. Can we start over live? And then the availability of why. and if you substitute what you have, then   computes. then you are good to have summation. and then we have a logo. Here you have the love and the exponential which we can cancel out. Bring up the submission all . Why did you take the Derek and see of a loss function with respect to a particular? If you take the derivatives of the loss function with respect to a particular etc. K. ,  this is very easy to estimate, but this will. This could take more time. But the 1st part 5.  this summation will go away because we're just concerned with one sister cake. what would the left is summation of I. Submation of T, and Theta K is done, and then what you'll have will be the meanings F of K. One C y, 2 minus one.",
    "And then we can also apply the derivative for this one. this population will stay because of derivative. because this is the submission over every PPYI give them a time. and if we take the revenue team of this one no. This will be equal to one over 0 of x. the entire thing, one over Z of x. And here we have to sum over all the States wide information of solution of team 5, 1, oh. And if we take derivatives for this one. we still have this one because it doesn't depend. Please go to the thing on Tikka here. and then we have the exponential of this. You're going to use the chain rule. This call is the explanation. I'm  if you want to speak with the regulative one. If in 2 of x,  this is  indirecting which I spare to evening. You can edit this here with respect to 2 x.  if you apply the channel. you have the same expression information over here, and then we are 14  slide. we need to compute the derivatives of what I'm inside, and then we have. We have the solution over T, because we are only interested in one particular case. we really have the submission of vaccine here. one t, 1 t management. You will find out that everything here is very similar to the formulation we have here above. commission in my old device. the only difference is this solution can  go out. This does not depend on why,  it can ring. And then you have probability of 1, 5, 3, 1 x, ? When you have the solution. Why don't see 14 number one x, 2. The final expression on this , salvation solution over, apply. David Ifeoluwa Adelani: Do you have questions? Yes, I didn't understand the last step where the 2 sums become 3 sums, and it says, can rearrange in terms of local State transition.",
    "here you have , instead of having yt. we just changed the expression. here we have y  music in the slides. There's a 3rd sum over. the last that one is  enterprise. But let me double check my notes, but  from the very 1st step the sum over, I we just forgot to write it  to the . No, no, because in the formulation here. this is all about, why? it's just the last time. but I will confirm my notes, and  I'll post on it here. in terms of the interpretation of gradient. if you want to interpret it, , we have 2 things. We have the expression here. If you go back to the last slide. We have this expression on the left side. minus the expression on the  hand side. And what is interpretation of that? the F of K is  a when you talk about the overall ingredient is the difference between this one. which is on the left hand side and the  one. the one on the left hand side can be seen as the empirical distribution of the future K. In the training purpose which you can  estimate. because these are uncrafted features based on what you have in your compass. if you want to know if something is capitalized, you can estimate this from your journey compass. and the last one which will be the expected distribution of Fk. As predicted by the current model. because this one is taken estimation over every, over the over, the entire Z of x, which is the orthogonal   that when the compost likelihood is maximized the gradient is 0.  the difference is 0.  intuitively. This means that finding parameter estimate by gradient descent is equivalent to telling our model to predict the features in such a way that they are found in the same distribution as in the gold standard. If you're able to achieve this. That's how crf is able to work based on the user defined features. Another thing is that you can have regularize a regularizer to this. I know some of you ask about regularizer in your homework assignments depending on the model you used to.",
    "You can have L. 2 regularizer. This is an example of L 2 regularizer. and if you have the ultra regularizer over this you, if you take derivatives of Theta K squared, divided by 2 to I forgot this one Sigma Square. If you take the derivatives with a specific Theta K, you're going to have this expression, which is Theta K divided by sigma squared. I believe this is correct. Based on the law of derivatives where the 2 above is going to cancel the 2 below, and then you have minus Theta K divided by Sigma squared. , but apart from using graded designs,  we said in the previous slides or previous classes that you can also use stochastic gradient descent. and the idea of stochastic gradient descent is when the mini winning batch size is one that's where you have stochastically resent. And  that if you do this stochastic gradient descent over many iteration you can  approximate the gradient decent category and  that  you initialize what is your values of details randomly, and then you randomize or order over your samples in your training compost. and for each Mini batch for Sgd. Then you compute your derivative over this Mini batch. And then you find new values of Theta, and you do this over and over again over many iterations to find your ideal setup and ,  that's it. board line, minus log Z of X. That should be sum also for all the guys that should be some of our work. The  step, if you get out of that. what is Z of X doesn't is not over the I's . But this is, there are webs, ? it's it's in the sound for all the eyes, and then in the  line you take it out, and then we take the derivative over just one log set of exposition. It's the derivative of the sum of all of them. I will go by this again, and  send you an updated version on it. But I'm working on that. Think that's also a demo. , where each class is. And it's just a perceptual or no. Or features we had in the slides. They didn't take exit into account. And what is Theta K. Is it still initialization, transition and emission? It's just  a number, ? It's just  a weight, ? And it  it accounts for whatever features we include.",
    "But here there'll be many more  . And we are calling the which will be one the hard em, ? Because is it because it,  you get this exact state specifically that maximize is there? And we talked about both for unsupervised  we have no labels. And we're we're trying to  iteratively find,  the best labels, is there? Which cases would you prefer? , which algorithm, , , , when would you use the software  if you have,  an unsupervised problem?"
]