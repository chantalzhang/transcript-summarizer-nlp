Nlg is a set of rules to translate the meaning representation to the language outputs, then it should work regardless of what generation task you are pursuing. There's no separate modeling of each of those steps, at least in the most extreme form of that. Once you have a model that can generate some outputs, you can just rewrite it. The downside is, you have no guarantee the constraints would be respected. But in practice it might work better. There are things you can try and experiment with here. In fact, here they're binary. In practice, you can still solve reasonable sized problems with, especially with industrial strength.

Natural language generation has been approached with different methodologies and different approaches in the literature in the past. One natural language generation which is always available and always there and requires very minimal amounts of Nlp is just canned text. But then we can start to get into use cases that are a little bit more flexible and more adaptive. And we'll look at each of those. You want the output sentence to be formed and grammatically correct. And and this is why we would approach this with a type of solution such as linear programming. And then you have a tree structure, and then you can just do some simple rescoring with a language model, even with a very bad language model to linearize it, and you'll get something reasonable. There's 1 line of work which is related to that, I would say, at a high level which is this, chain of thoughts, work where you have a model, generate outputs, and then you condition on the model generator output to generate something else. And it also allows it to inspect and choose, to correct something and fix something. And the other major trend is you try to fix these issues with human feedback where you sample a collection of model outputs.

We're going to talk about natural language generation. It's good to learn about them to expand your toolkit. Natural language generation and abstraction has many use cases. We can divide up Nlg into data to text versus text to text, depending on the source, the input. Almost all of the text is scripted ? That's that's natural languagegeneration. Each of these steps involves rules. the advantage of this type of system is that you have a lot of control over what happens. This is a lot more familiar to us in, the recent literature. , you have your source text , if you're doing summarization or translation. and then you have all of the tokens you've generated far. training, it will learn to respect the constraints. That's definitely a good approach to try. and then you feed that in as coefficients to some formulation where you then solve this constraint optimization problem. Another trend which is still ongoing, because it's not a solved problem by any means is to do controllable text generation.

We can also compare and contrast Nlu and Nlg. We can talk about that which is a more declarative approach to optimization. And we contrasted extractive summarization, which is where you take snippets of the source text and concatenate them together. 2 ways you can reconcile it. what are the positives and negatives of things just thinking about things this way. In this second approach of a neural nlg. You just assume that the pre trained language model, , can figure all of that out. What are some pros and cons of the second approach? If there's 1 poorly generated token, will it affect future? This model is very accessible. That's a positive which is with the neural approach. It is not accessible, because, . it may be different levels of accessibility. Microsoft's Chatbot was released into the wild on the Internet. It was just a matter of one day before Microsoft had to take down this Chatbot. It's funny, and it is funny, but it's still a concern, ? , it's just that this these days is happening at a larger scale.

Jackie Cheung: We're going to look at shared machine learning systems. And we focus on extraction because it was easier to think about. We can focus on these interesting issues to do with modeling, content, and modeling the relationship of words with respect to each other. In the course we've defined a generative model as a model that gives you a joint probability distribution over everything of interest, over your inputs and your labels. The 2 approaches we've seen far is one is highly structured, detailed, rule-based. And the other is a neural model, and if you have access to the internals of the neural model you can derive scores. a corpus of text labeled with a property you care about polite versus impolite , could you please do something versus? rather than single tokens, it started off being single tokens with special meanings. These are things that the model should actively not generate. It turns out this unlikelihood training can be a little bit tricky to get it to work.

--- lec22.txt ---


