Natural language generation has been approached with different methodologies and different approaches in the literature in the past. One natural language generation which is always available and always there and requires very minimal amounts of Nlp is just canned text. But then we can start to get into use cases that are a little bit more flexible and more adaptive. And we'll look at each of those. You want the output sentence to be formed and grammatically correct. And and this is why we would approach this with a type of solution such as linear programming. And then you have a tree structure, and then you can just do some simple rescoring with a language model, even with a very bad language model to linearize it, and you'll get something reasonable. There's 1 line of work which is related to that, I would say, at a high level which is this, chain of thoughts, work where you have a model, generate outputs, and then you condition on the model generator output to generate something else. And it also allows it to inspect and choose, to correct something and fix something. And the other major trend is you try to fix these issues with human feedback where you sample a collection of model outputs.