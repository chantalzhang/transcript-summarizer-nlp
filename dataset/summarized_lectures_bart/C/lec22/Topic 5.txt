Jackie Cheung: We're going to look at shared machine learning systems. And we focus on extraction because it was easier to think about. We can focus on these interesting issues to do with modeling, content, and modeling the relationship of words with respect to each other. In the course we've defined a generative model as a model that gives you a joint probability distribution over everything of interest, over your inputs and your labels. The 2 approaches we've seen far is one is highly structured, detailed, rule-based. And the other is a neural model, and if you have access to the internals of the neural model you can derive scores. a corpus of text labeled with a property you care about polite versus impolite , could you please do something versus? rather than single tokens, it started off being single tokens with special meanings. These are things that the model should actively not generate. It turns out this unlikelihood training can be a little bit tricky to get it to work.