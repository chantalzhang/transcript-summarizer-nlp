The idea of hidden Markov models, which used to be very popular until deep learning overtook it. We have prepositions very, very common stop words in of up above. And then you have a verb which is worse here. And here you have the hamburger is dash than that one. This is the way we model it. And we are modeling both parts of speech and words together. In the very simplified 1, you can estimate all these probabilities very similar to how we estimate all the probabilities for naive base. You can estimate the initial probability distribution and you can also estimate the emission probability. And then we have all these algorithms, forward algorithm, backward algorithm, Viterbi algorithm that we're going to discuss in the class.