Last week we described how to come up with features for your classifier. Today we'll talk about how to train a classifier on a training set. The idea there are two assumptions, but the most important assumption is this independence assumption. And once you have done that you can optimize this likelihood using gradient descent.  support vector machine is a very, very important algorithm. Given enough training data, they tend to perform very well. For current language models, people are trading for over six months, over a year. It's really expensive and often requires a lot of data.

We'll start with linear classifiers. Output Y can be a discrete outcome or categorical variable. We already differentiated what's the difference between classification and regression. We're going to try to do a discriminative task instead of a generative task for the task of regression. regression, SVM, boosting and many techniques. Some people believe that there's some interaction about you have the neuron and the dendrites and the ASEAN in the brain. in the class I can ask you about what about K nearest neighbor decision trees, random forests, and on.

, , I didn't post it this morning, but I will post it after the class. We have a principle called cross validation which is also used for model selection. Theta is all your set of weights or your parameters you want to learn in the model. Some parameters cannot be learned automatically, you have to fix them. The Bayes rule is that all variables are independent, probability of AB equals to probability of A. This is what it means if you say A&B are independent given C and you assume conditional independence. And for the training, that for every after you have trained the model, you compute the likelihood over the entire data. marginal, but you can ignore it. probability of X is the marginal distribution. in that case you have to compute the joint probability of every X. typically for naive base, it's not that you cannot estimate it, but typically you don't need to. Because you need to compute this over every single features and the calculation for every feature is different. you how you will compute what is the likelihood in logistic regression in a minute. in the example I showed you here, we use a very simple example where the features are review notes, dose assignment, and ask question. If the word yo that we say yes signifies is more correlated with spam, if it appears many times in your document, it should be a good feature.

When you are trying to learn a function this, typically you have X, Theta. Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. likelihood is you're learning a probability, what is the best probability that can fit this data? that's the idea of likelihood and the probability, the higher the better, the closer you are to one. We typically just default to what we have, which is the words we have and then use them as features because that's all we have. And then we can do counting. Here we generalize the A1A2A3 that I told you into what is called the weight matrix. And here you can also stack different perceptions together.

Test classification is a form of machine learning. It's a way of learning how to cluster data into different categories. The idea of training is that you want to select the hyperparameter that minimize the error on your training data. You maximize the likelihood, but you minimize theerror.

Last week we described how to come up with features for your classifier. Today we'll talk about how to train a classifier on a training set. The idea there are two assumptions, but the most important assumption is this independence assumption. And once you have done that you can optimize this likelihood using gradient descent.  support vector machine is a very, very important algorithm. Given enough training data, they tend to perform very well. For current language models, people are trading for over six months, over a year. It's really expensive and often requires a lot of data.

We'll start with linear classifiers. Output Y can be a discrete outcome or categorical variable. We already differentiated what's the difference between classification and regression. We're going to try to do a discriminative task instead of a generative task for the task of regression. regression, SVM, boosting and many techniques. Some people believe that there's some interaction about you have the neuron and the dendrites and the ASEAN in the brain. in the class I can ask you about what about K nearest neighbor decision trees, random forests, and on.

, , I didn't post it this morning, but I will post it after the class. We have a principle called cross validation which is also used for model selection. Theta is all your set of weights or your parameters you want to learn in the model. Some parameters cannot be learned automatically, you have to fix them. The Bayes rule is that all variables are independent, probability of AB equals to probability of A. This is what it means if you say A&B are independent given C and you assume conditional independence. And for the training, that for every after you have trained the model, you compute the likelihood over the entire data. marginal, but you can ignore it. probability of X is the marginal distribution. in that case you have to compute the joint probability of every X. typically for naive base, it's not that you cannot estimate it, but typically you don't need to. Because you need to compute this over every single features and the calculation for every feature is different. you how you will compute what is the likelihood in logistic regression in a minute. in the example I showed you here, we use a very simple example where the features are review notes, dose assignment, and ask question. If the word yo that we say yes signifies is more correlated with spam, if it appears many times in your document, it should be a good feature.

When you are trying to learn a function this, typically you have X, Theta. Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. likelihood is you're learning a probability, what is the best probability that can fit this data? that's the idea of likelihood and the probability, the higher the better, the closer you are to one. We typically just default to what we have, which is the words we have and then use them as features because that's all we have. And then we can do counting. Here we generalize the A1A2A3 that I told you into what is called the weight matrix. And here you can also stack different perceptions together.

Test classification is a form of machine learning. It's a way of learning how to cluster data into different categories. The idea of training is that you want to select the hyperparameter that minimize the error on your training data. You maximize the likelihood, but you minimize theerror.



