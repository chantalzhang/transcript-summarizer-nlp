Language models that they try to scale, to 500 languages, but in terms of performance we I don't think we have a good luck with that. It's more on multilingual nlp and crosslingual transfer. We have this collaborative projects where we have native speakers and Mls. And then we have 2 versions of the Masaka Project. language is covered in multilingual birth. the lesson is, I try to cover as many scripts during pre-training, that you will be it will be easier to adapt to new languages in the future. How the sound are they related to each other. We have the futura distance, which is a combination of all these distance measures. for you to do transfer learning.