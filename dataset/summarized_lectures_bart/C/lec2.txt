The main topic of today's lecture is to assign a label or category to a piece of text. You can also ask further questions after class there and . OK, first warm up question. Give definitions for natural language understanding and natural language generation. If people don't answer it, then I'll just make sure to make a note and put it on the midterm or something. Every single word should and punctuation if you include that should end up being its own unigram. There might be bigrams, . bigrams would be something good day has a count of one and then every adjacent. , that's a decision you can make. should bigrams span punctuation or sentence boundaries?

NLP has to do with understanding natural language as a phenomenon using computational methods. Most systems in AI is going to be some combination of the two. Find an interesting NLP problem that involves language data or involves some need. And then you can solve this problem by using machine learning techniques. using a system in order to predict whether the new e-mail is spam or not, and hopefully it does a good job. And in fact, usually we can record or often we can try other strategies rather than just recording the word itself. And then you could choose to record this more abstract form. , it's just to we explain what I just said.

 computational linguistics has more to do with the scientific aspects of the field. that's in that sense, it's closer to engineering. What about natural language generation? You missed all of the intro, Yes. , inferring patterns in order to produce more natural language. machine learning system that will solve some optimization step. Each e-mail can either be a non spam 0 or a spam one. A feature could be a word or it could be something else as we'll talk about. In practice they'll always be some abstraction of the full contents of the inputs. Each of these different schemes that we can choose will end up resulting in a different model after training. This is why there are whole courses on machine learning. If you have extra time this weekend, I encourage you to play around with that. And then I haven't forgotten, we should come back to the question of should you start create a startup based on tax classification for making trade predictions?

Today we're going to focus on the part of mapping between this textual data and some output label, OK? It could just be sentiment, is this positive or negative? If we were to take the previous example and talk about predicting positive versus negative sentiment, then that counts as the task of sentiment analysis. Formulate the NLP problem as a machine learning problem that's a very common paradigm. In machine learning, we have document classifier, document label. We specify what the possible YS mean and then we're going to train a classifier on top of the document. You have your input document and then feature extraction means converting. And then we have to also talk about the inputs. The main key behind both limitization and stemming is to rewrite. in these examples, here, , relational and relate, if you have the word relate, it might also end up being relate. then you're not distinguishing between relate and relational, . Amount of data that you have. Words verbs and nouns and adjectives and even punctuation in text. that you've in which you formulate it, then it should be, is it just positive versus negative or is it positive, neutral, negative, or very positive, very positive. here, if I formulated a sentiment analysis, what we're solving is a sentimentAnalysis problem of , what is that particular piece of text described? this is something that you can think of as a validity problem.

NLU is about analyzing the internal word structure and how words are composed. NLP is not machine learning and machine learning is not NLP. In supervised learning, a model has access to your input data as as its corresponding output label for the purposes of training. In unsupervised learning, the words B&A, they appear in similar contexts and they appear near other classes of words. Y is the easiest in the sense that you just need to have some output. OK, then in terms of the feature extraction step, feature extraction refers to taking your input textual document and then extracting features from there that you think might be relevant and useful to pass on to the classifier. Machine learning is based on something that is wrong. I'm just saying that it'll be very, very difficult to do a better job than them fast enough such that you can make money. We can still fix this by expanding the set of features. We condition on where it's where the piece of text comes from.

The main topic of today's lecture is to assign a label or category to a piece of text. You can also ask further questions after class there and . OK, first warm up question. Give definitions for natural language understanding and natural language generation. If people don't answer it, then I'll just make sure to make a note and put it on the midterm or something. Every single word should and punctuation if you include that should end up being its own unigram. There might be bigrams, . bigrams would be something good day has a count of one and then every adjacent. , that's a decision you can make. should bigrams span punctuation or sentence boundaries?

NLP has to do with understanding natural language as a phenomenon using computational methods. Most systems in AI is going to be some combination of the two. Find an interesting NLP problem that involves language data or involves some need. And then you can solve this problem by using machine learning techniques. using a system in order to predict whether the new e-mail is spam or not, and hopefully it does a good job. And in fact, usually we can record or often we can try other strategies rather than just recording the word itself. And then you could choose to record this more abstract form. , it's just to we explain what I just said.

 computational linguistics has more to do with the scientific aspects of the field. that's in that sense, it's closer to engineering. What about natural language generation? You missed all of the intro, Yes. , inferring patterns in order to produce more natural language. machine learning system that will solve some optimization step. Each e-mail can either be a non spam 0 or a spam one. A feature could be a word or it could be something else as we'll talk about. In practice they'll always be some abstraction of the full contents of the inputs. Each of these different schemes that we can choose will end up resulting in a different model after training. This is why there are whole courses on machine learning. If you have extra time this weekend, I encourage you to play around with that. And then I haven't forgotten, we should come back to the question of should you start create a startup based on tax classification for making trade predictions?

Today we're going to focus on the part of mapping between this textual data and some output label, OK? It could just be sentiment, is this positive or negative? If we were to take the previous example and talk about predicting positive versus negative sentiment, then that counts as the task of sentiment analysis. Formulate the NLP problem as a machine learning problem that's a very common paradigm. In machine learning, we have document classifier, document label. We specify what the possible YS mean and then we're going to train a classifier on top of the document. You have your input document and then feature extraction means converting. And then we have to also talk about the inputs. The main key behind both limitization and stemming is to rewrite. in these examples, here, , relational and relate, if you have the word relate, it might also end up being relate. then you're not distinguishing between relate and relational, . Amount of data that you have. Words verbs and nouns and adjectives and even punctuation in text. that you've in which you formulate it, then it should be, is it just positive versus negative or is it positive, neutral, negative, or very positive, very positive. here, if I formulated a sentiment analysis, what we're solving is a sentimentAnalysis problem of , what is that particular piece of text described? this is something that you can think of as a validity problem.

NLU is about analyzing the internal word structure and how words are composed. NLP is not machine learning and machine learning is not NLP. In supervised learning, a model has access to your input data as as its corresponding output label for the purposes of training. In unsupervised learning, the words B&A, they appear in similar contexts and they appear near other classes of words. Y is the easiest in the sense that you just need to have some output. OK, then in terms of the feature extraction step, feature extraction refers to taking your input textual document and then extracting features from there that you think might be relevant and useful to pass on to the classifier. Machine learning is based on something that is wrong. I'm just saying that it'll be very, very difficult to do a better job than them fast enough such that you can make money. We can still fix this by expanding the set of features. We condition on where it's where the piece of text comes from.



