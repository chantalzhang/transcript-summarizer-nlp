All , today we'll move to Lecture 4, which will be on nonlinear classifiers. Very briefly, who can remind me what we do? OK, there's another one . the data set is very similar to the test sets, but you still have to do the calculation. And there are a lot of smotting techniques that we typically use in NLP. These two definitions, ? Just give me one minute. GPT can do many things. It needs a lot, a lot of data and by you should be aware of that. Also, there are many hyper parameters to tune. There's no notion of interpretability. And when is linguistic feature engineering good?