Last class we talked about syntax and structure and hierarchical structure in natural language. It's a, it's a model of formal grammar that we can apply to model the syntax of natural languages. And for us they correspond to words in a sentence. And this is one key difference between natural language and programming languages. different levels, I presented some examples of that in the 1st lecture. The natural languages are ambiguous in many, many different ways across many different levels. There are usually multiple parses that can correspond to any sentence of a language. And the sentence is, I shot the elephant in my pajamas. Spans a certain subspan within your sentence of the words within the sentence. Each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. I shot the elephant in my pajamas, and here we have the updated Cnf grammar from before. We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect. For shot it would be V. Or N, N or in it would been P, for my it would For my. And I'm gonna go And I's going to go bottom up and column bottom up. By column, left to . The basic idea is, if a cell is empty, then that's not possible to build something there. The rule is just you're just checking that break point. If I'm going too fast, just raise your hand to get me to explain some more. The probability of a parse tree is going to be the product of the probabilities of all of the rules in that parse tree. That forms our probability distribution. symbol A on the left hand side. And then we can define a new parsing problem which is to recover the most probable parse tree which is. to find the Argmax among all the trees. In the most famous Treebank.st, for the longest time from 19 Exciting stuff. In order to estimate these we discussed to do with estimating values of parameters with HMMS and Ngram models and forth. To estimate these parameters of these new categorical distributions, we're using counts. And this is the mle estimate OK. And the MLE estimate for that rule is for the probability of that rule.

We are going to continue our discussion about syntax. We talked about these things called constituents, which are groups of words that acts together. And we also talked about tests for constituency. And the other thing we talked about is we talk about context-free grammars. you see in the sentence you're trying to. the possible rewrite rules and you try to find a way to rewrite the non terminal symbols to get finer and finer grained until you get to rules that let you generate the actual words. using the rules. rules by matching the words 1st to non-terminals and then the non- Terminals to bigger non-Terminals. rewrites to something else, you want to remove that intermediate level that you have AV rights to the thing directly. These are called unary rules, because it involves exactly one hand side. And it has some rules that are unary. st , can you guys help me? The Np to N, yes. One important thing to note is that you keep the original rules, too. you have 2 copies of the rules of generating. You can do one diagonal at a time, do the diagonal, and then to do the things to the diagonals and forth. You can do any combination of them that you that you would. And then every time you see a starting symbol, you can just use the back pointers to trace through all of the possible paths. If you tilt your head 45Â° this way you can even see directly in the parts in the chart. say that each non terminal symbol that rewrites to something is to say that. Each non-terminal symbol that. rewrited to something else that forms are probability distribution. This is called a tree bank. A tree bank is a collection of trees in a bank, with , a with sentences.

There are other kinds of parsing. given a particular CFG and given a sentence made-up. The goal that we have is to recover all possible parses of the sentence. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. CYK algorithm allows us to deal with ambiguity because you can store multiple things. It's unlikely you'll find a constituent if your grammar is reasonable. If you want to run the CYK algorithm exactly, there are no heuristics. If you create an Np constituent here. We already know that we won't use it, we're throwing that out. But if there are 2 possible tokens we can create. It's the exact same ideas as you've already looked at. It won't grow. Then you can use what we already know.

I hope you all had a good week reading week and you're enjoying the unseasonably nice weather. We're gonna talk about Zky parsing or cyk parsing. we're going to talk about CKY parsing or CYK parsing. And then we're Going to motivate why we need that. And we're gonna talks about an algorithm to do that. Parsing is no longer as popular, but you still should know about it because it's very good pedagogically. There are three possible cases of CFG rules that do not conform to CNF form. And for us, we're going to create a table to store all of these possible constituents. And finally, we'll read the table to recover all of the possible parses of the sentence. In our grammar it's going to be Np. you can build a rule with the constituents that you've already found there. And then you can use all of them to build bigger chunks, bigger constituents. And we just need to check all possible breakpoints to make sure that we cover all of the possible ways to build that bigger chunk. One to 5, 5 to 6 is all worthy. And one to five, five to six is all OK. And then here, 0 to 6. it's also not possible because of all of the empty cells there. Any anything that ends with my . you should just do every possible pair. And this is how you can recover all the parses. We are no longer computing all possible parses because of because of this. There are other parsing algorithms. if you're interested, you can look them up. The the one that I would look at is to do top down parsing with the early algorithm.

There's one topic from last class we didn't quite manage to cover, which would be interesting to talk about. Here's just a about CF, GS and constituent trees. There's gonna be an S node which is the starting symbol. And with certain kinds of grammars, 1 tends to be more efficient than the other and forth. convert this grammar fragment into Cnf. And I didn't formally define what counts as a , a question. I never formally defined what it means to be an acceptable change to the grammar. Once you see the example, this will make sense. In the other notation, just keep a list. and then you just add another entry to the list. You can decide to pick the tree with the most probable parse. But we need to change things a little bit because CF GS did not have a concept of probability built in. We're going to make CF GS become PCF Cfgs become Pcfgs. way to do this if you only care about the most probable tree, which is that we modify the CYK algorithm directly to incorporate probabilities as we go along. And that'll be more efficient because then you don't have to explicitly create all of the ambiguous trees and evaluate the probabilities for all. This is just another way of rewriting that. I don't know what's you prefer in terms of being more human interpretable, but it's just the same thing.

Last class we talked about syntax and structure and hierarchical structure in natural language. It's a, it's a model of formal grammar that we can apply to model the syntax of natural languages. And for us they correspond to words in a sentence. And this is one key difference between natural language and programming languages. different levels, I presented some examples of that in the 1st lecture. The natural languages are ambiguous in many, many different ways across many different levels. There are usually multiple parses that can correspond to any sentence of a language. And the sentence is, I shot the elephant in my pajamas. Spans a certain subspan within your sentence of the words within the sentence. Each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. I shot the elephant in my pajamas, and here we have the updated Cnf grammar from before. We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect. For shot it would be V. Or N, N or in it would been P, for my it would For my. And I'm gonna go And I's going to go bottom up and column bottom up. By column, left to . The basic idea is, if a cell is empty, then that's not possible to build something there. The rule is just you're just checking that break point. If I'm going too fast, just raise your hand to get me to explain some more. The probability of a parse tree is going to be the product of the probabilities of all of the rules in that parse tree. That forms our probability distribution. symbol A on the left hand side. And then we can define a new parsing problem which is to recover the most probable parse tree which is. to find the Argmax among all the trees. In the most famous Treebank.st, for the longest time from 19 Exciting stuff. In order to estimate these we discussed to do with estimating values of parameters with HMMS and Ngram models and forth. To estimate these parameters of these new categorical distributions, we're using counts. And this is the mle estimate OK. And the MLE estimate for that rule is for the probability of that rule.

We are going to continue our discussion about syntax. We talked about these things called constituents, which are groups of words that acts together. And we also talked about tests for constituency. And the other thing we talked about is we talk about context-free grammars. you see in the sentence you're trying to. the possible rewrite rules and you try to find a way to rewrite the non terminal symbols to get finer and finer grained until you get to rules that let you generate the actual words. using the rules. rules by matching the words 1st to non-terminals and then the non- Terminals to bigger non-Terminals. rewrites to something else, you want to remove that intermediate level that you have AV rights to the thing directly. These are called unary rules, because it involves exactly one hand side. And it has some rules that are unary. st , can you guys help me? The Np to N, yes. One important thing to note is that you keep the original rules, too. you have 2 copies of the rules of generating. You can do one diagonal at a time, do the diagonal, and then to do the things to the diagonals and forth. You can do any combination of them that you that you would. And then every time you see a starting symbol, you can just use the back pointers to trace through all of the possible paths. If you tilt your head 45Â° this way you can even see directly in the parts in the chart. say that each non terminal symbol that rewrites to something is to say that. Each non-terminal symbol that. rewrited to something else that forms are probability distribution. This is called a tree bank. A tree bank is a collection of trees in a bank, with , a with sentences.

There are other kinds of parsing. given a particular CFG and given a sentence made-up. The goal that we have is to recover all possible parses of the sentence. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. CYK algorithm allows us to deal with ambiguity because you can store multiple things. It's unlikely you'll find a constituent if your grammar is reasonable. If you want to run the CYK algorithm exactly, there are no heuristics. If you create an Np constituent here. We already know that we won't use it, we're throwing that out. But if there are 2 possible tokens we can create. It's the exact same ideas as you've already looked at. It won't grow. Then you can use what we already know.

I hope you all had a good week reading week and you're enjoying the unseasonably nice weather. We're gonna talk about Zky parsing or cyk parsing. we're going to talk about CKY parsing or CYK parsing. And then we're Going to motivate why we need that. And we're gonna talks about an algorithm to do that. Parsing is no longer as popular, but you still should know about it because it's very good pedagogically. There are three possible cases of CFG rules that do not conform to CNF form. And for us, we're going to create a table to store all of these possible constituents. And finally, we'll read the table to recover all of the possible parses of the sentence. In our grammar it's going to be Np. you can build a rule with the constituents that you've already found there. And then you can use all of them to build bigger chunks, bigger constituents. And we just need to check all possible breakpoints to make sure that we cover all of the possible ways to build that bigger chunk. One to 5, 5 to 6 is all worthy. And one to five, five to six is all OK. And then here, 0 to 6. it's also not possible because of all of the empty cells there. Any anything that ends with my . you should just do every possible pair. And this is how you can recover all the parses. We are no longer computing all possible parses because of because of this. There are other parsing algorithms. if you're interested, you can look them up. The the one that I would look at is to do top down parsing with the early algorithm.

There's one topic from last class we didn't quite manage to cover, which would be interesting to talk about. Here's just a about CF, GS and constituent trees. There's gonna be an S node which is the starting symbol. And with certain kinds of grammars, 1 tends to be more efficient than the other and forth. convert this grammar fragment into Cnf. And I didn't formally define what counts as a , a question. I never formally defined what it means to be an acceptable change to the grammar. Once you see the example, this will make sense. In the other notation, just keep a list. and then you just add another entry to the list. You can decide to pick the tree with the most probable parse. But we need to change things a little bit because CF GS did not have a concept of probability built in. We're going to make CF GS become PCF Cfgs become Pcfgs. way to do this if you only care about the most probable tree, which is that we modify the CYK algorithm directly to incorporate probabilities as we go along. And that'll be more efficient because then you don't have to explicitly create all of the ambiguous trees and evaluate the probabilities for all. This is just another way of rewriting that. I don't know what's you prefer in terms of being more human interpretable, but it's just the same thing.



