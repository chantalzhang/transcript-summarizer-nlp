Lecture 6. today we'll be talking about smoothing. someone asked a question last time about this, and then I can explain in details about what is called smoothed. I , I will just do a review of how we evaluate language models. Lambda. if you sum up all K equations from step 3, which is the step 3. You're gonna have what you have on line 5. And here you can also say that Line 4, which. is submission of are all theta I's equals to one. You divide accounts by what's the total size of the corpus. then you average a trade off. Between what corpus am I going to use to build my language model for this language. Is it a very big one or a very small one? The more data you have oftentimes the better it is. But for statistical language model.

David Ifeoluwa Adelani: If you always make your probability to be equal 0, then it's going to ruin your estimate every time. If you roll a die it can be 1, 2, 3, 4, 5, 6. If the probability is monotonically increasing, then you have to multiply it by the derivative. And by this we can also derive a realized mle or the optimal mle for the data we are looking for. To modify our goal is to modify our accounts in such a way that we reserve probability. The way we compute accounts is that we say c plus one multiplied by the frequency of the count plus one divided by F of C, and an example is the following, the probability of unknown is f 1 divided by N, which is.

If you're able to estimate probabilities for your language model for every combination you have in your training set. How do you compute the unigram distribution for a particular world? How would you compute it? That would be count of every time cat appears divided by count of all the words in your couples. And we also went to an example in the last class to clarify this. for Tridram, how are you going to calculate this? The idea of smoothing is the probability distribution to shift some probability mass to cases that we haven't seen before, or we are unsure of. And then you understand, the idea of you have reserved some probability for the unknown token. The easiest way you can do is just to do what's called interpolation.

The passport language model is to predict the world. We make an assumption that which is a very simple assumption. You can also maximize the log rhythm, you will still be able to find your optimal product. We have 3 worlds that appear once that's soccer, model and tops. It's very easy tofit.

where you try to redistribute the some probabilities to unknown tokens. and you have not estimated that probability. to talkings all and grams that you don't know their probability when they count is 0. You can make it depend on just the last token or the last previous 2 tokens. Given the context C, and that will be probability of N. WTWT. And then you can compute the infinite probability. Terms of more that complexity trade off. You have to think carefully about this. WTWT, thank you for joining today's class. We hope to see you again next week.

Lecture 6. today we'll be talking about smoothing. someone asked a question last time about this, and then I can explain in details about what is called smoothed. I , I will just do a review of how we evaluate language models. Lambda. if you sum up all K equations from step 3, which is the step 3. You're gonna have what you have on line 5. And here you can also say that Line 4, which. is submission of are all theta I's equals to one. You divide accounts by what's the total size of the corpus. then you average a trade off. Between what corpus am I going to use to build my language model for this language. Is it a very big one or a very small one? The more data you have oftentimes the better it is. But for statistical language model.

David Ifeoluwa Adelani: If you always make your probability to be equal 0, then it's going to ruin your estimate every time. If you roll a die it can be 1, 2, 3, 4, 5, 6. If the probability is monotonically increasing, then you have to multiply it by the derivative. And by this we can also derive a realized mle or the optimal mle for the data we are looking for. To modify our goal is to modify our accounts in such a way that we reserve probability. The way we compute accounts is that we say c plus one multiplied by the frequency of the count plus one divided by F of C, and an example is the following, the probability of unknown is f 1 divided by N, which is.

If you're able to estimate probabilities for your language model for every combination you have in your training set. How do you compute the unigram distribution for a particular world? How would you compute it? That would be count of every time cat appears divided by count of all the words in your couples. And we also went to an example in the last class to clarify this. for Tridram, how are you going to calculate this? The idea of smoothing is the probability distribution to shift some probability mass to cases that we haven't seen before, or we are unsure of. And then you understand, the idea of you have reserved some probability for the unknown token. The easiest way you can do is just to do what's called interpolation.

The passport language model is to predict the world. We make an assumption that which is a very simple assumption. You can also maximize the log rhythm, you will still be able to find your optimal product. We have 3 worlds that appear once that's soccer, model and tops. It's very easy tofit.

where you try to redistribute the some probabilities to unknown tokens. and you have not estimated that probability. to talkings all and grams that you don't know their probability when they count is 0. You can make it depend on just the last token or the last previous 2 tokens. Given the context C, and that will be probability of N. WTWT. And then you can compute the infinite probability. Terms of more that complexity trade off. You have to think carefully about this. WTWT, thank you for joining today's class. We hope to see you again next week.



