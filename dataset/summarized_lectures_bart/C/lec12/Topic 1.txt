Last class we talked about syntax and structure and hierarchical structure in natural language. It's a, it's a model of formal grammar that we can apply to model the syntax of natural languages. And for us they correspond to words in a sentence. And this is one key difference between natural language and programming languages. different levels, I presented some examples of that in the 1st lecture. The natural languages are ambiguous in many, many different ways across many different levels. There are usually multiple parses that can correspond to any sentence of a language. And the sentence is, I shot the elephant in my pajamas. Spans a certain subspan within your sentence of the words within the sentence. Each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. I shot the elephant in my pajamas, and here we have the updated Cnf grammar from before. We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect. For shot it would be V. Or N, N or in it would been P, for my it would For my. And I'm gonna go And I's going to go bottom up and column bottom up. By column, left to . The basic idea is, if a cell is empty, then that's not possible to build something there. The rule is just you're just checking that break point. If I'm going too fast, just raise your hand to get me to explain some more. The probability of a parse tree is going to be the product of the probabilities of all of the rules in that parse tree. That forms our probability distribution. symbol A on the left hand side. And then we can define a new parsing problem which is to recover the most probable parse tree which is. to find the Argmax among all the trees. In the most famous Treebank.st, for the longest time from 19 Exciting stuff. In order to estimate these we discussed to do with estimating values of parameters with HMMS and Ngram models and forth. To estimate these parameters of these new categorical distributions, we're using counts. And this is the mle estimate OK. And the MLE estimate for that rule is for the probability of that rule.