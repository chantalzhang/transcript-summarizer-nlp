David Ifeoluwa Adelani: North campus. I will skip the review of last time. Because we use bags of engrams. We don't really care about the order. we can use and grams instead of considering one word, we can consider 2 words that follow each other. We can compute what is called word frequency, or we, the popular word that we use is what iscalled time frequency. to estimate how much information in bits. information of X in slide 36 equals the log of 2 of one over B of X. and entropy is just what is the expectation? What is the expected amount of information we can get while observing a random variable? Then you can have a very high expected value of the information.