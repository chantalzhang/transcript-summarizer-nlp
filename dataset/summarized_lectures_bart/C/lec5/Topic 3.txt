be discussing Engram language model. We do not fully care about the context of the of the words, we only treat them as bag of words or bag of engrams. Then we're going to examine more statistical language modeling based on n grams, and then we'll do or maximum likelihood by relative frequency. Can you compute a unigram and background language model, using the following sentence? Yes, you have to tokenize it , and the choice of tokenization can be. That is probability of is that you are going to take them 2 by twos. And that is how you have 2 over 5.