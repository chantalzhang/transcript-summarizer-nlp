Last class, remember, we were still talking about lexical semantics. In compositional semantics, what we to do is we to talk about, say the sentence level where sentences have meanings that you can derive by looking at the parts of the sentences. language is compositional, but it's also not perfectly compositional in particular idioms or expressions whose meanings cannot be predicted from their parts. We're going to be building up a 1st logic sentence as the as the meaning representation of the sentence by using fragments of logic in the subparts. We need to have some algorithm for constructing these logical formulas at the sentence level from the its parts. In order to do that, we need to use another tool from computer science and whatever. We'll use Lambda calculus to define a precise algorithm to do . is, we have to go back to Cfgs. and augment them with lambda expressions with the idea that every time you're doing a syntactic composition. You're also correspondingly doing some function application in Lambda calculus. The idea here is every terminal rule. you take those semantic attachments, and then afterwards is a mechanical process. of looking at your augmented Cfg with those semantic. attachments and then running those procedures. And then we needed lambda calculus to store these partial computations.

Compositionality is the idea that the meanings of sentences is not just some. lexical semantics might give you the meanings and behaviors of each of the individual words. This idea of compositionality is really important, because you could argue that this is what lets language be really, really flexible and useful. The approach behind Montegovian semantics is that natural language can be made as precise as logic, logical languages of our logic. We can never get away from something symbolic and logical, at least not completely. If you say all wugs are blorks and all blorks are cute, then you can conclude that all wug are cute.

Inference is to make something explicit that was implicit before in language. If you say something , I want to visit the capital of Italy. and theCapital of Italy is in Rome. Then you can make an inference, which is, I want. to visit Rome because chances are, it'll be confusing for most of the. rest of the class. a predicate. and then you can have a function to get the capital, from Italy, from the from Italy to some other entity. Or here's another logical sentence that all wugs are blorks. And there was another question, . , that's a great question. The question is where does this function of capital out come from? It's just, I'm just defining that this could take the form of a function.

, the midterm is, , Wednesday, if I remember . It's not about the meaning of 2 words in a sentence. One important concept from the second half of last class is this idea of a term context matrix. You're using the distribution of words in order to infer something about the meanings of those words. And it's still based on these fundamental ideas. The tradition that we're going to discuss today and class is to use logic to model sentence meaning. And that means at some point. using 1st order logic using those predicates and functions then an interpretation of it or a model of it is possible. In rolex students, we have 3 predicates, and we have grade of X is equal to a why can't we just have 8 here. But just a is a constant, it gives you an element in your domain of discourse. Yet we can't tell if it's true or false yet. wouldn't that contradict the 1st statement that students who do both get an a . It's not defined by the set of logical symbols that I put earlier, logical connectives that had earlier. You just have to define what it means. how do we interpret this logic within a particular context? why associated population, it always applies another qualified into the inside. that's part of designing this augmented grammar. If you have a different rule that works in a different way, you have to follow that. That's what we're trying to do here.

Using cosine similarity, you can see if two words co-occur with each other more commonly than you would expect by chance. You can also use singular value decomposition, which lets you factorize the original term context matrix into the product of 3 matrices. And in practice doing this truncated Svd often improves performance. Other properties of the meaning representations of a sentence. We talked about 2 different views of it. that's 2, or 3 different view of meaning. you can still talk about the meanings of the sentences with respect to each other. Pandas are purple and yellow. what is the meaning of pandas are Purple and Yellow. in our current world. We need a domain of discourse which involves you and friend one and friend 2. And we also need the abstract concepts of the grades. And then the semantics of it all is that you need to associate. every rule that results in a leaf node, , involving a terminal.

Last class, remember, we were still talking about lexical semantics. In compositional semantics, what we to do is we to talk about, say the sentence level where sentences have meanings that you can derive by looking at the parts of the sentences. language is compositional, but it's also not perfectly compositional in particular idioms or expressions whose meanings cannot be predicted from their parts. We're going to be building up a 1st logic sentence as the as the meaning representation of the sentence by using fragments of logic in the subparts. We need to have some algorithm for constructing these logical formulas at the sentence level from the its parts. In order to do that, we need to use another tool from computer science and whatever. We'll use Lambda calculus to define a precise algorithm to do . is, we have to go back to Cfgs. and augment them with lambda expressions with the idea that every time you're doing a syntactic composition. You're also correspondingly doing some function application in Lambda calculus. The idea here is every terminal rule. you take those semantic attachments, and then afterwards is a mechanical process. of looking at your augmented Cfg with those semantic. attachments and then running those procedures. And then we needed lambda calculus to store these partial computations.

Compositionality is the idea that the meanings of sentences is not just some. lexical semantics might give you the meanings and behaviors of each of the individual words. This idea of compositionality is really important, because you could argue that this is what lets language be really, really flexible and useful. The approach behind Montegovian semantics is that natural language can be made as precise as logic, logical languages of our logic. We can never get away from something symbolic and logical, at least not completely. If you say all wugs are blorks and all blorks are cute, then you can conclude that all wug are cute.

Inference is to make something explicit that was implicit before in language. If you say something , I want to visit the capital of Italy. and theCapital of Italy is in Rome. Then you can make an inference, which is, I want. to visit Rome because chances are, it'll be confusing for most of the. rest of the class. a predicate. and then you can have a function to get the capital, from Italy, from the from Italy to some other entity. Or here's another logical sentence that all wugs are blorks. And there was another question, . , that's a great question. The question is where does this function of capital out come from? It's just, I'm just defining that this could take the form of a function.

, the midterm is, , Wednesday, if I remember . It's not about the meaning of 2 words in a sentence. One important concept from the second half of last class is this idea of a term context matrix. You're using the distribution of words in order to infer something about the meanings of those words. And it's still based on these fundamental ideas. The tradition that we're going to discuss today and class is to use logic to model sentence meaning. And that means at some point. using 1st order logic using those predicates and functions then an interpretation of it or a model of it is possible. In rolex students, we have 3 predicates, and we have grade of X is equal to a why can't we just have 8 here. But just a is a constant, it gives you an element in your domain of discourse. Yet we can't tell if it's true or false yet. wouldn't that contradict the 1st statement that students who do both get an a . It's not defined by the set of logical symbols that I put earlier, logical connectives that had earlier. You just have to define what it means. how do we interpret this logic within a particular context? why associated population, it always applies another qualified into the inside. that's part of designing this augmented grammar. If you have a different rule that works in a different way, you have to follow that. That's what we're trying to do here.

Using cosine similarity, you can see if two words co-occur with each other more commonly than you would expect by chance. You can also use singular value decomposition, which lets you factorize the original term context matrix into the product of 3 matrices. And in practice doing this truncated Svd often improves performance. Other properties of the meaning representations of a sentence. We talked about 2 different views of it. that's 2, or 3 different view of meaning. you can still talk about the meanings of the sentences with respect to each other. Pandas are purple and yellow. what is the meaning of pandas are Purple and Yellow. in our current world. We need a domain of discourse which involves you and friend one and friend 2. And we also need the abstract concepts of the grades. And then the semantics of it all is that you need to associate. every rule that results in a leaf node, , involving a terminal.



