Jackie Cheung, Professor: Hi, everybody welcome back. people can online should be able to hear me. We're gonna talk about discourse and co-reference resolution. last week you had an enjoyable week talking about machine translation and multilinguality with David ? And today, we're going to move on to a new topic which is to think about processing language beyond the sentence level. Coherence is the fact that there are relations between different sentences or utterances. In a discourse , that type, the relation that logical relation between them. That's what makes a discourse coherence. cohesion is the use of linguistic devices to tie together text units. Coherence is defined at the level of logical relationships between sentences or smaller chunks, whereas cohesion is the observable mechanisms that you can identify that link together these chunks of text. If you have coherent passages, will you always find cohesive devices between them? I'd say most of the time. But it might be that in naturally occurring tests, almost certainly. , you'll find cohesive links between.  lexical chains is just about relatedness. And you can compute that using, their word vectors, or something co-reference chains. There are also discourse markers that we use to help structure a passage. And these discourse markers help us explicitly mark what are the discourse relations between different parts of that discourse. Co-reference is a dichotomy or opposition between language things that you can . speak or say, or write down and hear, and see and read, and forth. versus things in the world, or in some hypothetical world that they point to which is at in some more, at some more abstract level. It's usually denoted with, , with form. Co-reference is when you have multiple mentions or referring expressions that all points to the same thing in the world. This is a practical tasks that you might want to solve in its own . But also it's convenient, because we don't have to create an explicit representation of the content. Anephra is the fact that linguistic expressions their interpretation depends on other linguistic expressions. Maru and his are co-reference, because they point to the same entity in the real world. There's also this term of Cataphore, and these are anaphores that point to cats. You can also have possessives. But these are also referring expressions. There are other phenomena that appear at different levels of frequencies across languages. One of these is one of this is called 0 Anafera, and this is interesting. Many languages allow you to omit pronouns in certain contexts. I don't speak Spanish, but oh, that's also the example sentence, it says, not speak Spanish. here that the pronoun that was dropped, was I? Because it's indicated by morphology on the verb. because aglo here is 1st person singular for a speak. Italian also works this way, and Russian and others. which is that you can refer to entities that are not directly introduced, but you can figure out based on background knowledge. Here you're also using context and world knowledge to figure out what's happening. Here the bridging reference is between windows and table versus office. Not all pronouns are referential. There's something called pleognastic pronouns which are common in English and French. In French, too, there are constructions where you can use to place focus or emphasis on certain parts of the sentence. There are lots of unclear cases, of whether 2 events are the same event. Hobbes Algorithm, the very early work from the 19 seventies. And then we can talk about machine learning approaches that try to implement the same intuition idea. But in a more statistical context, with state of the art approaches to this task currently be foundation wells or large banks. Or why doesn't his point to Japan? , one import, one relevant queue has to do with number and gender. A second cue that can be useful is recency. We tend to refer to things multiple times, we tend to do in bursts, and they tend to all happen close to each other. Hobbes algorithm from the seventies is a heuristic algorithm that puts together all of these cues and comes up with a method for co-reference resolution based on that. This is a traversal algorithm where you do a tree traversal over multiple trees. And what it requires is that it requires constituent parse trees of the sentences in the discourse. This is from, the second edition of the Drafsky textbook. It sets up this way where you draw a syntax tree in a very particular style, with a particular style. and then it sets up that it respects all the constraints from binding theory. that if it's reflexive or not reflexive, certain things happen. But I'm not gonna go through each of the steps in detail, because it doesn't really add much to our discussion. can you guys identify noun phrases by. , Alice is a noun phrase, a beautiful cupcake is a. noun phrase. a beautiful Cupcake in the Patchesserie windows. A noun phrase the Patricia windows a noun phrases she and it and Bob, those are noun phrases. They point to the it's just from the perspective of, , a machine. Since a beautiful cupcake is a noun phrase, but it's true. And then they'll propose this, she in the second sentence, and it doesn't match as . And then the noun phrase it encounters. , what you're suggesting is that there could be some cases where this heuristic fails. These days, people tend to solve co-reference resolution using some statistical approach with a machine learning method. And one common way to decompose the task is to break it down into 2 subproblems. and then solve each of those problems separately before recombining the outputs of each of the modules. Once a task is set up as machine learning tasks that you could train a system to solve. I'm not sure exactly how to cast mention detection, as those hearst patterns are usually about detecting relations between 2 noun phrases. But here we are trying to figure out which spans of text are mentions in the 1st place. Is it possible to define a loss function specific to those things immunization for reference. If you ignore the hierarchical structure of mentions. If you just cast it as a chunking problem. Then it becomes a lot ner in that you're chunking the words in your text into chunks that correspond to mentions. Yes, you could do Bio tagging scheme and then run any sequence model train on some labeled data to do mention detection. I'm , really work to use a generative model, a decoder where the input would be sequenced. In 2013, Durett and Klein trained a initial neural model or , a log linear model, but they have, 3 million features, and they use word level features. They also attempt to use the structure of Wordnet, which we discussed, we discussed. And, the they used, they encoded recency as . The way that is set up in this model. It's there's a shared set of parameters that encode some sequence. And then they have different output heads. there's 1 output head which is for mention, scoring. for here there are different possible mentions. and then each of those you can score that, and then you can try. It gives you a score, for whether it's a mention or not, and you can do supervised learning on this. In the recent literature there are 2 general kinds of models. The 1st one follows in the footsteps of ete. , if you replace, the Lstm part with a transfer pre trained transformer. The second class of new models that have been proposed is what was mentioned earlier. These are decoder models that frames co-reference resolution as a sequence to sequence task. 1st you put in a sentence and then the model predicts that here there's nothing. It predicts shift, which means, let's process the sentence. and then here you already start to create a cluster. And then here with the 3rd sentence. then you pass in this updated context with 3 sentences. you ask the model to extract all the clusters, the new clusters that it finds. and you incrementally process this until you reach the end of the discourse. , that's a great point. how is this related to some of the previous work. We've seen the class about predicting some actions for parsing. It's inspired by the same idea. You can think of this as something a transition based parser where you're predicting actions which create new substructures in the inputs.