Lecture 7. today we are going to be talking about part of speech tagging. Early in the 90s, this used to be very important task. If you want to know about trendy topics in AI, in NLP, I want to encourage you to attend this workshop. giving a sequence a sequence of words. How can you predict what is the tag for each of the words? OK, this is the sequence labeling. There are other tasks that are sequence labeling, name density recognition is also sequence labeling where you try to identify what is a personal name, what is an organization. What is part of speech? it's a way of defining a syntactic category that tells you about the grammatical properties of the world. Part of speech tag covering 37 categories including punctuation. Order schemes that reduce this a bit to 18 called universal dependencies. Some are open classes of path of speech and some are closed tags. Many of them are intuitive, if you remember the very common ones. You also have, you can have verb in past tense or in past participle. Punctuation should be another one which is OK, it's obvious but punctuation is the last category. OK, we have other part of speech you can distinguish between modal verbs and auxiliary verbs the car and wheel. And you can also have different conjunctions. Stanford has A tag set, which is a Stanford dependency tag set. And then they have a more simplified tag set - we have the open classes, the closed classes. And the interesting thing with this universal dependency tagSet is that it's fairly easy to extend to many different languages. , PTB, which is the pantry bank doesn't distinguish between intransitive verbs and transitive verbs. And then you have something listened versus heard. Brown corpus has 87 tags for this, why PTB only has 45? also another thing is about language differences. In Japanese, which I don't speak, there's no great decision between nouns and pronouns. Pronouns are also open class rather than closed class in English. Pentry Bank and universal dependency are two different task sets that can be used. You can do this for different languages, for different data sets or for different domains. The interesting thing is for part of speech, there's also dependency between a Canadian. and a geography nerd. In the Markov process, we have States and we have transition probabilities from one state to the other. And , your state will be the part of speech. And then you can say what would be the probability of predicting the word? What would be a probability of predicted the stack? And we have already seen some Markov processes. The idea of hidden Markov model is coming from. something is hidden, you model it as an Ed variable, but you still have to model the hidden variable because you don't know it at this time. given a part of speech, you want to generate a word. And there are different words that can be generated given different probabilities. And then you have transition probabilities from 1 tag to the other. When you unroll this out, this means that the verb depends on another part of speech, and the word that will be generated depends on the part ofspeech. The way it's being modeled is that there are many possibilities for this part of Speech to generate a word, because that is the class of that word. In a Markov representation, we have the observed which is the observed random variables O1, O2. And then you have the tag set Q1Q2 to Q5. And how do we estimate the joint probability? All we have to do is to multiply all the transition probabilities times all the emission probabilities. , you have to also estimate that from your corpus. The two things you said I cannot, what's the difference? the word depends on that POS, Yes. But we are not using the context of the words in this modeling, in this simplified one. In the very simplified 1, you can estimate all these probabilities very similar to how we estimate all the probabilities for naive base. But in actual fact, you have to estimate this over the entire training corpus. How many times do you have the word D given determiner in the entire corpus, whether it's sentence one or sentence two? You have to calculate this. There's a dependency between this. , but of course you can do that, but what will it mean? , yes, we're trying to say it depends on it. , this is a simplified 1 here. We're just saying bigram assumption, it can also depends on the last two part of speech. But say that it doesn't depend, it's false, ? this is the way we estimate the probability for the initial probabilities. What's the probability of having a noun given a determiner? What's the probabilities of a noun being given a verb? You estimate all these probabilities, all the different combinations, and then you estimate also the admission probabilities. You have to do it differently for DT and differently for VBD. , for we can exclude the ones that are obviously 0 because you're going to smooth anyways. From class we're going to talk about different algorithms on what is the best way to tag a sequence. We have the following questions which we need to estimate. And then we can also use some algorithm to determine how to estimate the tags given all what we have observed. But is just to say, OK, for the stuff we use this data set and I just added script and I specify which language which set of languages for we use. , if a language containing many languages, but we just use for two or three languages. But if you think the table is better, , but they can edit that section.