David Ifeoluwa Adelani: , I also have people on zoom. I believe you can all see my screen. , , today we'll be talking about neural machine translations. That is more relevant to our models have been trained than Ibm models , nobody uses that. And also we'll touch on some important concepts , how do you get your Machine translation output. In practice nowadays what we usually do before the neural models is what is called phrase-based, statistical, empty. And after that we have the newer-based statistical engine for the phrase based status. that you can leverage the knowledge of your status to put Lm and use it to do machine translation directly. Free space smt. instead of splitting the sentence into words. you just split it into phrases. and then you will be able to compute the probability of the target phrase given the previous source. This distortion is important, because. depending on the language you're trying to translate to the word order might be different. When you translate word for what the question is, can you get what is the best candidate translation for this word out of the different, many possibilities. The greedy step is to take the change that produced the maximum probability. for the neural machine translation this often leads to the current state of the art model. But also this requires a large amount of data to be able to train immersion translation previously for any single pair of language. You need at least a million parallel sentences between. the initial language you want to translate from to the target one. One very successful approach is to use recurring neural networks. You can reason about some issues with this approach. And what can get wrong? And we'll talk about a modification of this, we where an attention is introduced. But for , all you have to assume is that you are only interested in a single vector, that summarizes everything in the source text. The model learns to focus on the most important one, which is apple. And the short answer is that from this encoder decoder instead of just throwing away the outputs at every encoder memory cell. And then you use this to compute what is called the attention. This attention will be used to multiply the final output you got.  attention can be seen as a soft version of a retriever system from a memory which has a key. These are just random mattresses that we give different rules. One random matches we serve as the query matrix and another mattress will serve as a key mattress. Another one will serving as a value mattress. You assign different rules to the different mattresses to model what is called the attention. Attention gives you an idea of what the model is trying to do. This visualization of attention gives an idea that the motor is paying attention to the most important part of the sentence. Sometimes it helps to give you some additional information. But in the case of Martian translation, you may not be able to see a very plain trick. The attention, architecture, or the transform architecture is that it has been applied to different modality, to vision. It seems to work to. Although we have been trying to replace it. You can try to. replace it in that . to replace Rnas because Rnas has a lot of problems one of the bigger issues of Rnas are things vanishing gradients. I'm imagine that you have a representation of w. 1 w. 2 CW. Head, and then you want to know which of these is more important to w.1. We want to use this vector to decide how important the world is to another world as part of the attention computation. And the way we learn this is using the query, key and value. We want to know that if you have thinking machines. what is the most important thing? Is it thinking or machines? This is what we want to compute. and it's safe attention, because you are doing attention to the same sentence. And here, when you compute, you multiply the query with the key and then you have this value. , although you are trying to attend to me, but I have another word that was referring to me. And you are able to get this just doing this dot product computation. You multiply the softmax with the value. and then you take the sum you sum. You have to learn the embedding, you can just ignore the encoder path if it's not necessary. Model tries to use both. It wants to have a good encoding and also do a good prediction, they use words. for machine translation, you need a good encoder to encode the entire sentence. Rnn is very cheap. You just need to predict one world at a time. We're just feeding the entire context in order far to predict the token. And this brings us to another architecture that's really famous. It's we had this transformer era we have. and then we have the bet era that's rain for 4 years. And Chatgpt came, and then everybody forgot about that. and it will also train on what is called sentence prediction. Does this sentence follow this sentence, and the way it was trained is very simple. and then, if 2 sentence follow each other, you'll see a continuation. If someone is talking about a sentence in economics, and there's another sentence in out. it's not very likely that you following each other. And then with this task we are able to know. to predict if the sentence followEach other, and combining this Max language model with sentence prediction, you can create a very powerful encoder. More than a 75 billion models that have been released. It's just very difficult to run them. 3 came up with another idea which is really cool, which is, you can do what is called 0 shot learning. The interesting thing is that almost all Nlp tasks, I would say, I don't see any tasks that cannot be converted. , however, for the T. 5, they are not reconstructing the entire sentence. What they are doing is just providing what is missing in the input. And they scale it up from a very small model to even 11 billion parameter model. And you can cast any task as a text generation task. And it will give you it will generate what will be the outputs. We also have a multilingual version of this that was trained on, , one on one language and a lot of data, 6.3 trillion tokens. And, , you can use this for the machine translation task. it can do what is called crosslingual transfer. , you have a language in English and then it will be able to translate it to German. Even if this pre trained model has not been trained on a language. there's a way you can just quickly adapt it to that new language. If you have, 5,000 parallel sentences, and then you can fine tune this pre-trained model. Another thing that can boost the performance is what is called chain of thought prompting. You give it an example. And this is the answer. You can also prompt it where you provide an example, and then you. provide an explanation on how it arrived at the answer, and the explanation doesn't have to be much. this is teaching the language model to reason before it produces.