Lecture 4 will be on nonlinear classifiers. We'll examine different text classification techniques and then we examine at least three. We spent a lot of time on naive base and I want to assume by you have gone through the lecture notes and redo the exercise.  assignment one. will be on test classification. you can try out different techniques we studied naive base, SVM and many techniques. My guess is that it will also you also require tools NLTK or psychic line. If you have not installed psychic line, you should install it and start playing around with it. because the logistic regression formulation is very simple and also about the naive basis, very simple assumption that some of you were able to give examples why this would not hold. And that's the major issue with linear models. But nonlinear models are more robust to model different inputs. And then this Z will be used to normalize it that you can get the probability distribution. I'm trying to tell you that this Z you can reformulate what we add in the format class to another notation, which is 1 / 1 plus EU raised to the power minus U. That means you don't need to calculate Z again, do you understand? Because it's equivalent to this. The models are trained on trillion soft tokens. And the more data, the better is the performance in general. And this has been responsible for many applications language modeling. It seems to work through the same architecture to machine translation and for every task it just excels. In the initial phase you randomize, you randomly just put some random weights. There are many functions and you can just give me random numbers, flows numbers from -1 to one, and then to generate and fill up your matrix with just random numbers. And then you use a technique called backpropagation to improve the weights that it gets better to be able to map the input to the output. the dimension also will be the it's a dimension of your layer, the 1st. , if it's the first layer of your network, that's adimension. You can have for the first one you can have a. dimension of 5 by 8 and the one you have a dimension. of 8 by 22, and the last one we have adimension depending on your final output can be 22 by two because you need to project it back to your output. and then you do the same thing. If you don't have this big weight matrix, it's just a linear function. But this one, you try to concatenate different layers together. as you are passing from one layer to the other, you need a function to make it nonlinear. at the last layer, depending on the number of outputs, we have treated a very simple case where you have binary output. In the case of path of speed tagging, the number of types is just how many path ofSpeed tags do we have. And for this you can use a software function to convert the output into a probability distribution. For binary, you don't need to do this because it's very simple formulation. But for multi class, we typically use the softmax. If you want to do the optimization you need a loss function. One of the most popular loss function that we use is the cross entropy loss. For training neural network, one of the popular approach is the gradient descent. Back propagation is an algorithm that's used till today is back propagation. , it's for stochastic gradient descent. The only hyper parameter here is your learning rate. For the gradient descent you do the sum over the entire couples. You can randomly sample one and then you compute the loss over this one. And if you do this over and over again, it can lead to faster convergence. In NLP we are operating on discrete values. It's either a cat or dog. There's a need for you to convert every single word into words. The simplest way you can convert it is what is the position of this word in the vocabulary. But this can also be converted to A1 auth encoding. If you are able to find a good representation for every word in your vocabulary then you don't need to levertize and then you can learn everything using a neural network. We don't really spend a lot of effort in learning better word representation. What we do is we do what is called large scale pre training on a big corpus and then we do fine tuning. For any NLP task you want to do nowadays based on neural networks, you're going to start with a pre trained model. You're not going to train everything from scratch again. You pick an existing model and then you fine tune it on a downstream task. This concept is very related to transfer learning, which I will cover in 2 minutes.  neural networks for NLP, there are many open questions on how to use linguistic structure. Another difficult thing is that neural networks tends to work very , but it's difficult to interpret. The C plus evaluation metric you can use to evaluate your model if it's good is accuracy. Because it's just about the correct prediction. There are better ways of thinking about this. Some of these have been motivated from information retriever, computing. What is a precision you can say out of what has been predicted? And you can talk about the notion of recall that out of how many are correct of the entire class. what is the, what is called? in this example, what's your true positive? And what is your true -20 you can compute the precision. what's the precision for spam 0.2? What is the recall one third? AndWhat is the F1 score for spam 217 2 / 7?