Jackie Cheung, Professor: , Hi! we're people online can hear us. We're going to talk about natural language generation. We are going to look at, techniques that are not currently, , the trendiest or the most popular. But it's good to learn about them to expand your toolkit. If you want to describe the overall distribution of opinions, , of that product, and synthesize and the thoughts that people have on specific aspects, then you're going to have to do some abstraction and abstractive summarization. That's why we need to talk about natural language generation overall. We can also compare and contrast Nlu and Nlg. For something a GPS system or some other very routine formulaic things, you do not want creativity. You want things that are highly structured and expected of a certain form. And your GPS system is telling you. Oh, here's a joke, orHere's a poem for you to figure out the directions that you need to take that does. not make any sense. When I say , take street name to street, name the street name that's called a slot. usually in a template, you have these slots where you need to put stuff in the slot. and then, appropriately, the thing you put in is called the slot filler. in this example, if the template, if. the overall structure of the template is a take the Slot Street name to the Slot. Street name. Using the discourse structure of a document to find out where the important things are. The 3rd step is to select the specific lexical items that you might want to use to generate the final output. And then , as another part of micro planning is to decide how all of these words fit together. Part of micro planning is to generate referring expressions. This term comes from. We talked about this last week when we talked about co-reference resolution. The last step in this abstract description of an Nlg system would be surface realization. This is to fully convert all of those the specified discourse plans, and the outputs and decisions of the micro planner. , you have various options, you can just call it. you can call it the blackboard. But then there are multiple blackboards in the room, that's not a good choice. And this goes to show the state of the field back, then. which is they assume a highly detailed semantic structure. and then they apply a cascade of deterministic rules to convert that structure into a string. This type of structure you see here on the is called an attribute value matrix. This is this should be expressed as the subject in the sentence. and then that handles syntactic alternations , are you using the active form or the passive form, or some dative alternation? They're usually under specified . for all of the missing information, you fill in some default features with agreement, features, and forth. And then you order the components with respect to each other, fill in inflections. Blah, blah, linearize the tree into the final string. hopefully, you get the idea. , you have your source text , if you're doing summarization or translation. and then you have all of the tokens you've generated far. And then you use that to predict the token and around 10 years ago this was implemented, as, say, an Lstm model. But then, these days it's a pre-trained language model with, based on a transformer architecture. And how do you reconcile these 2, there, that's an excellent question. There's no separate modeling of each of those steps, at least in the most extreme form of that. You just assume that the pre trained language model, , can figure all of that out. If there's 1 poorly generated token, will it affect future? Yes, , that's a -known problem as . You can have some neural data to text generation as . You decide which steps of the analogy pipeline should be in their own modules with a separate planning step. And then come up with a way to embed the input data structures into a format that can be ingested by the neural model. and then add mechanisms in the decoder that are appropriate for that specific generation task. NLP uses a neural language model to generate text. It can be used to translate, simplify, or transform text. In this talk, I'll talk about a particular approach to a particular problem. And I'll also introduce another technical approach that you can add to your toolkit. Major: There are many constraints about the form of the output sentence that you should generate. Major: The idea is that you represent each of the individual sentences by some structured representation their dependency parse tree. And then you can create a sentence graph sentence graph by merging the input sentences, dependency trees at the nodes. The wave of AI back in the late eighties and early nineties was based on prologue. Prologue is the idea that we can have general purpose algorithms that solve any problem as long as you can write down your problem in a form that the solver recognizes. This represents a very different approach to thinking about AI problems. You sum over all the edges, and then you have a grammaticality score which tells you how often this head word is generated generates a dependent with this label. and then an important score, which is how important is the word W in this context. And then everything else you can write out as constraints. All of the optimization problem looks this. you're either maximizing or minimizing something. You can write out a constraint to we. indirectly, by counting the number of edges, just taking the sum of all. because this tree, each selected edge, corresponds to a word, because it's a dependency tree. You need to sum up over all of your x's and make sure the sum is less than your budget, which is a constant in the optimization problem. There exists polynomial time algorithms for that although the most efficient ones are not. Once you get to the integer case, it turns out that it makes the math a lot harder. But still the solvers are really good these days, you can have very good ilp solvers and also stat solvers. , I'll end with a few more trends in the energy literature in the past few years. and these also introduce interesting additional techniques which are a bit more familiar. They're closer to the neural settings. One trend is to think about correctness again. this is a problem that I've worked on in my lab. Microsoft's Chatbot had to be taken down after it said racist and misogynistic content. The company had hoped that people would interact with it and it would get better and better through its interactions from users on the Internet. But it was just a matter of one day before Microsoft had to take down this Chatbot. which is to tell them decoder what not to generate. this is interesting because it's the opposite of this standard approach to neural training. the standard approach only has the second term here. The last term here, which is, predict the word given all of the previous contexts. That's the that's the standard log likelihood objective. There's a stronger pressure here with this approach to push these things down. is there a possibility additional numbers to get the calls? , I'll try to see if I understand the question. There's this work in adversarial learning, and then you're interested in figuring out if there are ways for the model to itself, automatically identify things it should not generate. The other major trend is you try to fix these issues with human feedback where you sample a collection of model outputs, and you get people to decide which one they prefer, and which one is better. And that reward signal can be used within this other machine, learning paradigm that we haven't talked about called reinforcement, learning to try to affect the model's behaviors.