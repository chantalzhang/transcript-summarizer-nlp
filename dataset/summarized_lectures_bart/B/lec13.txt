Today we're going to start talking about lexical semantics. But 1st we'll wrap up a few things from the last lecture on parsing and syntax. And, , I don't know if you saw, but I posted an ed for the readings for this lecture. The latest version of the draft of Durafsky and Martin seems to have taken out this material. At the end of the last lecture we discussed probabilistic, context-free grammars in their most basic form, which sometimes people call vanilla Pcfgs. The idea here is that you have a particular way of estimating the probabilities of rules through a maximum likelihood estimation. But what I claimed at the end, and I didn't elaborate on because we ran out of time is that this approach doesn't work very . In English, certain classes of nouns are more likely to appear in the subject position versus the object position, and vice versa. This is not just in English, but it might be a cross lingual thing as , just because of, how the world works and what people tend to describe what happens in the world. The question is for is there a way to create subject, noun, phrase versus object noun phrases, and account for those that way. In vanilla Pcfgs, there's no relation between any of these rules in terms of their probabilities. They all have to be estimated separately, just by counting the number of times you see each rule in your tree bank. This work that is really elegant and really nicely solve this problem within the this setup of probabilistic context, regrammars. On the other hand, this other problem, with this sparsity issue with too many rules is because we're making independence assumptions that are too weak horizontally. horizontally across the hand side of a production. We're modeling every single combination, and in every single ordering of hand, side symbols, non-terminals, and terminals, as independent of each other in their generation. which is too weak of an independence assumption. instead, we, we can again change our assumptions and make a stronger independence assumption, that we can more reliably estimate those parameters. This is the 1st sentence of the Penn Tree Bank. This is just a tree represented in a different way using bracket notation. We're gonna pretend that every hand side is a Mini Markov chain that we need to learn. Then for the horizontal problem, we'll pretend that all the children are expressed within the parentheses. for every non terminal symbol on the left hand side. It there's a on for the it's handside. It's a mini little language model there, involving those symbols that would appear in the hand side of a rule. But then, the technically, we've already covered all of these, all of this technique, because this is just an end ground model. In this example, it's saying the line , Mp, Dta, the 3rd bottom . Are we saying that it's a proper noun, and then you're choosing one of those 3 brackets, or it's I got lost? That's that's just what we're doing. This process is called markovization. because we're making Markov independence assumptions. The standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter. The scheme we just described with the background model is called 1st order, and again, you can do any other order and can interpolate. This has a real effect on Parson. The question is about, what does the horizontal markup order mean? , the one we just described is H equals. One is 1st order, which corresponds to a diagram model. If you do H equals 2, it means you look at every triple. , I just want to know if from this data, if this set oh, God. Although in a slightly different way, and the probabilities are all messed up, I would keep it separate from this. This method and this procedure is about training of the model and learning the parameters. And then the Cnf thing is for the parser. It's for the cky parser and what it requires. you can do this 1st and come up with a model. and then once you get to the point of parsing. Then you can convert all the rules to CnF. and that might require a bit of thought.  semantics is super important because it's the representation of something in the world that you can interact with the world with your technology. And if you're a language, technology, enthusiast, and you you are. and things mean things words mean things in the World. We want to be able to have some formulation and representation of meaning in the. world, that we can represent and understand things. The meaning of telephone is that it's a function that takes in an object from the world. And then it gives you true or false. And that's the meaning of the word telephone. or if you can also, think about themeaning of words in different ways. The intentional definition is talking about the conditions, the necessary and sufficient conditions for something to mean something. The reference is about the objects in the world that it points to. This is not a new idea. one of the 1st people to propose it. The 1st was frege in 1892. This is the second a main general area that people work in. And it's much more popular in the computational side to work in this as , which is to think about how the meanings of words relate to each other. You can express these in some logic as . I assume that most of you have heard of synonyms. synonym means that 2 words roughly mean the same thing. There are linguists and people who say that true synonyms don't really exist. There's a whole comic based on this. It's very difficult to separate synonyms from antonyms using many computational techniques. Polysemy is the phenomenon of a word having many meanings. In particular, polysame involves multiple related meanings. Sometimes these are ingrained and natural to us that we don't even realize there are slight differences in the meanings of in these different situations. The word newspaper has many, many different senses. It can be very difficult to distinguish between harmonymy versus Polysemy. The 1st and 3rd sense are very close to me. But we just we don't even think about this when we process language. What the intended meaning is in which sense of the word is intended. There's a metaphorical extension of the physical idea of location to some abstract space. Position as a job in an organization at 1st glance, it would seem to me that it's just entirely distinct from the physical location. I would argue that all of this is these are instances of felicity. The loony is at an 11 year low here. This requires a lot of background knowledge and real world knowledge to fully understand here. If metony becomes popular that it becomes a dictionary definition, does it stop being metronomy at that point? Some of these are ingrained that they appear in dictionaries. Don't be a censored body part. The relation is Hallonomy and meronomy. Something can be meton a metonym, and it can be a polysemis or . And , with Haronomy and Morontomy, is that similar to we saw on the 1st slide about this. It's just a different style of hierarchy. , if you already have remembered all the terms yet or not. What relation does that exhibit? , yes, these are synonyms there and there. Yes, there are, which is a some homonyms. yes, and which one is which head of the forecast? In Wordnet, there are 6 different synsets associated with the word table. Each of these think of it as a concept with a certain way that you can realize it in language using some words. This is a this is a metonym which has become conventionalized that it's listed as a separate sense. She sets a fine table room and board. , this is how it's organized. If you click on one of these if I click on the first, st since that it gives you its connections to other synsets it has a direct hyponym or a full hyponym. You can look at Member maronyms. And then each of these is its own synset. The general idea here is that you can use the words in those contexts to help you disambiguate. In the 1st context, here. the fact that you see the word tired is informative and typing. , whereas in the second context, something flowing and graceful would help inform that this hand is referring to the handwriting style.