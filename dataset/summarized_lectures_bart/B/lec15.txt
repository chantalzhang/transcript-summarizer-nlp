today, we're going to talk about compositional semantics. But first, st it'd be good, , to just quickly summarize and recap what we did last class. If you have a midterm conflict, please send me email, by the end of today. We need to figure out if we need to book a room for midterm for the makeup midterm. A lot of search engines are still based on word of co-occurrences. This has been a very important, influential technique, which is called point wise mutual information waiting. You instead record your term context, matrix some notion of how much more likely or less likely than chance. If 2 words happen to occur with each other more commonly than you would expect by chance. that means the numerator here is greater than the denominator. And , then, that you should record a score of 0, a Pmi value of 0 for that pair of words. not really because if you choose a different base, that's just multiplying all the numbers in your entire term context matrix by a constant. Another thing that people often do is that they often discard negative values in the Pmi. If you get a negative Pmi, often they discard it, and they just record a 0 instead. this is called positive point wise mutual information. Sigma K is a dimensionality reduction technique. You're taking the dimensions of the original term context matrix which explain the least amount of variance. And you're just getting rid of those dimensions and projecting your matrix to the remaining dimensions. In practice doing this truncated Svd often improves performance, because you're removing some noise. I don't have time to do this topic justice. But there's a lot of math and linear algebra and algorithms and people have come up ways to. do this very quickly. you should look into that if you're interested in this and this idea of Svd. And truncation and principal analysis, you find that all over the place. For in many applications this block. Sometimes people still use word embeddings of this time. Does the skipgram have a lot fewer parameters? Or is the just the embedding of the word? I would have to think about that. I don't think , because, , it's because the parameters are shared and the sibo. it's not you have a parameter. Compositionality is the idea that the meanings of sentences is not just some. It's not arbitrary, and you can derive the. meanings of a sentence by looking at the meanings. of the subparts of the sentence. We should also talk a little bit about some other properties of the meaning representations of a. sentence. and within this module what counts as a good meaning representation. The relationship of words to each other with all these somatic relations. And we also talked about relating the meanings of each word to the things in the world, to the references, and also to the sense of the word. that's 2, or 3 different views of meaning.  language is compositional, but it's also not perfectly compositional. in particular idioms or expressions whose meanings cannot be predicted from their parts. These are clear violations of compositionality, because there's no regular function. There's no function that you can apply here to derive the overall meaning from the meanings of each of the parts. The meaning of red is influenced by what you're composing it with. Montegovian cement is the idea of using a logical formalism to represent the meaning of a sentence with a tight connection to syntax. The tradition that we're going to discuss today and class is to use logic to model sentence meaning. Inference is to make something explicit that was implicit before in language. If you say all wugs are blorks and all blorks are cute, then you can conclude that all wug are cute. Unlike in your previous logic classes. , it's , in terms of natural language sentences. this is natural language inference. 1st order, logic has these components, and we need to define them, that we can talk about translating natural language to 1st order logic. First, st order logic can be defined as having a domain of discourse, which is a set of entities that we care about. And, , you have an instructor of function that takes X and returns other elements which corresponds to the instructor of the course. , you have seen 1st order. , then there's an existential quantifier and the universal quantifier. the , these are the basic elements of 1 first order predicate calculus. we might want to define some procedure that converts a sentence the capital of Italy is Rome into some logical formulation. It's a it's a logical connective, ? it takes in 2 arguments, 2, 2. Truth values, the one thing before and the thing after. And then you can draw a truth table for that hopefully. if the left hand side is true. then the hand side has to be true. In order for the whole thing to evaluate to true. these all have precise meanings. Or we create this function just because we want to analyze the simplest of the capital of Italian. Logic consists of the predicates and function names and arity. as as a set of sentences. An interpretation is where you interpret it in a particular scenario. to see if that if those sentences are true or false. We're going to come up with a 1st order. Logic, characterization of the following students who study and do homework will get an a. , students who study and do homework. , they also have to study. And they'll get an A, how about we do sometimes it just. And to be pretty, to be more clear, we can add a parentheses. . Do we need the.do we need? Oh, you mean this dot? technically, yes, but it's not a big deal. You need to have that relates to the X to the student . the way that I'm doing it here is to say that grade of is a function that gives you the grade of the student, the X entity. and that's equivalent to a. And then this is a predicate in disguise. And it gives you a truth value great of X equals 8. Do we have to bracket the entire thing. , I'm gonna delete it. it's not defined by the set of logical symbols that I put earlier, logical connectives that had earlier. But if you define it with a truth table. Then, , you could use Xor. But , Xr would be correct. You just have to define what it means. And the students who do neither get a seat any questions about this. A is true if it's the grade of a rather than a lowercase a. The uppercase a is just a name. I, B returns B, and see returns, see and equals is a special predicate. Is there a way to define functions that are not just this input, gives this outputs? We're going to be building up a 1st logic sentence as the as the meaning representation of the sentence. In order to do that, we need to use another tool from computer science and whatever. We need to define a precise algorithm to do . Lambda calculus allows you to describe computation, using mathematical functions. S, and what it does express. is it replaces all instances of x within t with the expression s. , , Lambda x of x plus y apply to 2 simplifies to 2 plus y, because you're replacing everything that's X here with X with the with the 2. You can even take Lambda expressions and plug them in as as arguments within the body. If this was confusing one thing you can do is you can rename the variables.  semantic composition is the idea that words have logical representations with things to be filled in, and then you express those things using lambda expressions. According to this theory, this approach. and you can write it down formally, as your semantic attachment. where you have, some left hand side rewrites to N hand sides becomes some function where you're taking the semantic representations. Here, I'm using dot sem to refer to each of those the partial the pieces of logic representation there, and you're combining them in order to . gets the overall meaning representation. , , 1st of all. Ignore all of the logic. We just have a syntax tree. This you can remember from before , as rewrites. Rewrites to a proper noun, and then proper noun rewrited to the word comp. And then the semantics of it all is that you need to associate. It's you go bottom up. for each of these rules you look up. The semantic attachment associated with those rules. And we need the logic, because that's what we're aiming for. It's , that's why associated population, it always applies another qualified into the inside. And then, in terms of the order in which you apply things, you have to look at the semantic attachment rule. that's part of designing this augmented grammar.