I.I.T. class will continue with, we'll start with linear classifiers. What's the difference between supervised and unsupervised learning? What is the use of training set, a validation set or a test set? Why do you need this? We have a principle called cross validation which is also used for model selection. And that you can split your training data into different chunks or subsets. And by doing this you can aggregate the test accuracies and then try to pick your model parameters. SKLN is a tool that allows you to train your own neural networks. It's a very small package which you can install. And then if you have very basic tasks test classification tasks, topic classification or sentiment classification, you can easily run this package on it. Today we'll talk about how to train a classifier on a training set. But today we are focusing on linear models. Last week this will be another revision where we already have feature extraction. OK, if you think a little bit abstractly on this, your function can be any classifier. When you're learning a model, you need to add a parameter teeter. Teeter can be all the weights you want to learn. Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. If you are trying to do a spam classification, every feature can be the types. The naive base can be seen as a simple generating model. The idea there are two assumptions, but the most important assumption is this independence assumption. for each sample of Y you generate a vector X by generating each feature independently conditioned on Y. We make an assumption that we are generating this data as a generative model. If you have a discrete data, we assume that a distribution of P of Y&P XI are given Y are categorical distribution. This is a very simple assumption. For natural language processing our outcomes, we assumes that we have very, very large features, which is the size of our vocabulary. For every X&Y you pick from your data, you want to estimate what is the probability of the joint probability. Multiply the products you have on the individual which also consists of the products across all the features. And for the training, that for every after you have trained the model, you compute the likelihood over the entire data. Of XY equals to 0 plus probability of XY. You have if a student reviews notes, does assignments and asks questions and this is the grade, what is the probability that this student gets an A? If and that's the last one, it doesn't review notes. He has not performed any assignments but always ask questions. You can get a probability of X if you use the marginal, but you can ignore it. The internal answer is this. If you apply this rule, you want to compare these two probabilities, ? Probability of y = A and probability of Y equals not a given X. The one that has the highest probability will be your answer. If it's no, it will be one over the probability, the number of times you have grade. P of X of the vector. my calculation is even wrong. here you have, I don't think my calculation OK. one, I have two, which should be 1 / 3, one over three, 1 /3 and then you have 1 / 45. you have OK. the way you compute it is what's the probability of N in the first column? That means it doesn't review notes. All , the answer is not a. And I hope this gives you an idea of all the maps we wrote on the book. OK, quickly I will rush through the remaining of the slide before we go out of time. There's a simple distinction between type and token. The idea of type is that the identity of the word is the count of unique words. The logistic regression is a way to compute what is the likelihood of Y given X but given this parameter of Theta. The log of the product of probability is very important in NLP because it's what you use to compute things like publicity of a language model and many things that. SVM is generative for a discriminative model. Naive base is your good. It's always a good bet and you can do the math. What you have to do is to try different algorithms. You can try logistic regression, SVM, boosting and many techniques. The way they interact also model how artificial neural network works. This is one of the most successful algorithm that we have currently in machine learning. Given enough training data, they tend to perform very well. The disadvantage of this is that training can take a very long time. Sometimes you can train a machine translation model for a month.