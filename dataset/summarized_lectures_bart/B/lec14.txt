Jackie Cheung, Professor: But good evening. We are going to continue our discussion today about lexical semantics. lexical comes from lexicon, and the lexicon is this idea that we have some mental storage of entries related to language. something in our mind that holds words and their meanings, and also their characteristics. I'm going to go through some algorithms again, not state of the art algorithms. They don't use any neurotechnics. But the reason that I'm presenting them is to give you the flavor of how you can use knowledge about the problem and your intuitions and build them into computational algorithms. The 1st algorithm is called Lusk's algorithm. Lesk's algorithm is based on the idea that words extract some information from their context. For example, suppose we want to disambiguate the word bank. The words in the context that will help us figure this out will be words deposit and check. And your context, then, will be represented by your contexts. And then you find the overlap and find the sense that has the highest overlap. Explored that in assignment one ? then you can use that you can filter out. There are all sorts of things you can do, and they each yield a slightly different algorithm. But the general high level idea remains the same. Can you do partial overlap by characters, or is it always by whole? The idea of lemmatizing and counting lemmetized Lemma overlap is, is implementing that idea. In other cases the partial overlap might be overlap in terms of the prefix, which might not be that meaning ? And there was a question there. pictures to what exactly does the bundle here represent? if you can go further back we have. And then you see the overlap between those. In Jarowski's setting there are only 2 senses. and all of these contexts will involve the word plant in one of those two senses. The heuristic that Jarowski proposed in his algorithm is to pick one other word that will correlate and co-occur with that sense with very, very high probability. by doing this you can automatically label some of your data set involving the word plants. This is how the model pulls itself up by its own bootstraps. The words being used are live and manufacturing are those derived from life, form and factory or . How are the words life and manufacturing selected? Are they derived fromLife, form, and factory? And then, based on that, you have to be clever and select a good seed word to create the seed set.  bootstrapping lets you discover other words that co-locate that are found near each of these senses. This is great, because that means this new classifier is useful, even for new samples that don't contain the original seed words of life and manufacturing. After one round you take your highly confident labels and you add them already into the csat. In Yaroski's algorithm they only look at the current round. We don't have any correct answers while running bootstrapping . It's not a supervised method. We only have the confidence scores. Oscis algorithm is not an automatic dictionary. It uses other cues and other clues that help you label each sense. Is this considered unsupervised or semi-supervised depends who you ask? It's just that you have automatic labels at the beginning, then, in that sense, it's it's slightly supervised. There are standard methods to use. , 2 algorithms done, and then 2 or 3 more to go for today. What we covered then far is worth sense disambiguation. which is to figure out which is the intended sense of the word. But they're a lot more tasks within lexical semantics which people have worked on, and which are popular. The idea behind Hearst patterns is that if we can identify expressions this such as then we can discover hyponym, hyperym pairs automatically. And that way, if you're constructing Wordnet. you don't have to , just sit down and think really hard about what are hyponyms and hypernyms of certain senses of words. You can have an initial seed set of words. it's 2 words in a hyponym hyperym relation. Or it's you need more than that. and then you go into a corpus. and you find all of the context in which those 2 words appear. And then, once you have those contacts, you can find which contexts are very common. Then you can apply those contexts more generally and extract a whole bunch of new words that could potentially be in the same semantic relation. Learning is more the particular machine learning, optimization, paradigm or technique that you apply in order to change the model parameters. The idea of bootstrapping with reinforcement learning is just a different thing that makes sense. These Hearst patterns have also been discovered and used for other relations, such as between cause-effect relations. this is Gizhou's work in 2,002 earthquakes caused tidal waves. She found other verbs that also indicate causal relations , induce or give rise to or stem from, and forth. , we see that we've seen 2 examples of applying the strategy of bootstrapping. in general, it can be very useful if you just don't have a lot of resources or time or data. what you're talking about sampling with replacement. There's a difference between the different lexical, semantic relations which change how they affect, how words appear near those words. We've looked at the relationships between 2 words that co-occur and their intervening words. Here we have another canonical example. We have extinct birds, such as dodos, moas, and elephant birds. The idea of distributional semantics is that you understand a term by the distribution of words that appear near that term. Large language models based on predicting mass tokens in context are implicitly relying on the same notion that the context around which a word appears is useful to getting the meaning of that word. when it's the center of your attention and you're looking at all the words around it, that's that means it's a target word. or it can itself serve as the context word for other words. And what you end up with is something called a term context matrix. where each row represents the counts that are accumulated for a certain target work. In the original version of this article, the question was: Is this gonna be a square matrix that's very sparse. In practice, what people do, we'll get into some of that. But one thing they do is they don't keep all the columns. They only keep the the . 5,000 or 50,000 most common words as context words. And there are, we will discuss ways to automatically compress this large term context matrix and do something smaller. Computing cosine similarities between word vectors is a common way to find synonyms. But it turns out that cosine similarity and word co-occurrent, similarity of context in general give you a lot more than synonymy. For that reason a major problem with the approach of distributional semantics has been that it's very difficult to separate out the different reasons why words or phrases may share similar distributions of context words. It's about synonymmy and hypernomy and hyponomy. cat is not similar to scratching post, because in the taxonomic tree they're very far apart. abstractly, similarity is only about taxonomic categorizations and hierarchies. relatedness includes anything that might be associated in some more general sense. and good is related to bad. Aha is this term context, matrix analogous to word embeddings? We will return to this in 10 slides or . What are there any measures of similarity which are of similarity directly and not relatedness? Yes, but it presupposes that you already have your taxonomic tree. their data set had 353 words. And then they went and asked people how related these 2 words are to them and to, and they ask people to give a score between 0 and 5 or something that. And that's their gold standard. This is the average amount this. on average, this is people's judgments on , how close these two words are. Word embeddings are these trained vector space representations of words to predict words in context. we're up to just a decade ago word 2 Vec. Was a really famous model that did this. This was the beginning of this whole deep learning revolution and nlp, . Whereas thinking about things in terms of matrix. factorization, those tweaks were not obvious in the matrix factorization setup. , there were lots of things that you would have to do in the. during the factorization and compression process to achieve the same high level of performance as. with the Skipgram based approach.