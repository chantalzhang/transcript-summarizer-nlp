OK. All right. Good morning, everyone. All right. OK, I think we can start. So welcome to so I think lecture 7. Can you, can you all hear me? OK, Welcome to Lecture 7. So today we are going to be talking about part of speech tagging. Part of speech tagging. Early in the 90s, this used to be very important task. Yeah, it's hard to hear now. Is still better, Slightly better. OK, now, how about now? Is he OK? Path of speech tagging. Today we'll be talking about part of speech tagging. Is it better? All right, Hopefully you can manage. Like I was saying, it used to be a very important task in the 90s because it was very essential for many applications from machine translation to speech recognition and so many tasks, and also for named entity recognition. And it's also a popular feature. Basically, if you need additional features, you try to get part of speech tags of all the words and then you use this as additional features for order NLP task from word sense disambiguation to named entity recognition to order information extraction task. The first, the first announcement is that October 2nd, the lecture is cancelled. We have an NLP workshop at Miller. So how many people don't know Miller? So Miller is the Montreal AI institute. I think you should just Google Miller Quebec. So it's an institute that focuses on artificial intelligence. Edit by Professor Yoshua Bengio. And so we are having an NLP workshop, NLP in the era of generative AI. As you know from this lecture, we have not really talked about very trendy topics. So if you want to know about trendy topics in AI, in NLP, I want to encourage you to attend this. So it will be from October one to third. Unfortunately, there are so many people that are interested in this workshop, so you can no longer attend in person, but you can attend online if you register right. So, so far in the lecture we have been talking about how to do test classification, talking about different algorithms like naive base, support vector machine, logistic regression and neural networks. So today I mean also last time I think last week we discussed about language modeling. We talk about N gram language modeling. We talk about smoothing techniques so that you can address words. Why do you smooth? Why do you need to smooth? Yes, for unknown words. Also to prevent having a probability of 0. Basically ruining your computation for several tasks. So today we'll be making a series of predictions from a sequence. Basically giving a sequence a sequence of words. How can you predict what is the tag for each of the words? OK, so this is the sequence labeling. There are other tasks that are sequence labeling, like name density recognition is also sequence labeling where you try to identify what is a personal name, what is an organization, what is a location in a text. So today we'll be focusing on part of speech in English because I believe if you're attending this class, you understand this language. And then we have part of speech tagging as a sequence labeling problem. Then we're going to examine some things coming from actually motivated, like from probability theory. It's like a continuation of naive base. In the naive base, we make an assumption which is the independence assumption or to say concretely we made a conditional independence assumption. And since this technique has been very successful, it has been extended to other techniques like to other tasks like part of speech tagging where you can also make the assumption that the part of speech tag, you can generate another part of speech based on that. And also you can generate a word based on that very similar to the naive based assumption. So the only difference now is that you have a random variable in the Markov chain that can generate either A tag or a word. And this is the idea of hidden Markov models, which used to be very popular until deep learning overtook it. All right. So this would be a little bit different from what you know from maybe primary school if you are not familiar with part of speech. So we talk about the different part of speech in English language. The common ones like nouns like restaurant, dinner. We have verbs like find it is. We have adjectives like good modifier of a noun. We have prepositions like very, very common stop words like in of up above. And actually prepositions are also the most common, probably one of the most common in your corpus and determinants. And also you have adverbs like quickly, well vary and so many. So what is part of speech? So it's a way of defining a syntactic category that tells you about the grammatical properties of the world. So if you have the dash was delicious. So you would know that now typically a noun, we fit this right. I mean English language is following the structure of SVO. I mean subject, verb and object. So in this case, you need a subject which is typically like a determiner and a noun. And then you have a verb which is worse here. And then you have the objects delicious. So you know that maybe a food was delicious would be appropriate. The food was delicious. Here we were appropriate, which is a noun. And here you have the hamburger is dash than that one. So this is more like an adjective to compare to compare 1 hamburger to the other. And there are sometimes that some you can have the same structure that one may be more grammatical and the other is not grammatical. Like you have the cat's head and then you have the cat enjoyed, which is less grammatical compared to the first one. OK. So like, it's very important to know that this might be slightly different from what you know from your grad grade school or high school. So where we know that. OK, what is a noun is like a name of a person, Anima. Yeah. And things like that place or things that is like a noun. This is how it was defined for me when I was in primary school. And then you have like verbs, which is like action word. But now we have a broader category. Actually you could have part of speech tag covering like 37 categories including punctuation. And then we have order schemes that kind of reduce this a bit to like 18 called like universal dependencies. So that because you have different parts of speech for different languages. And at some point, people came together to say, let's unify this, which also has some disadvantage, which we're going to talk about that later, OK? And of course, nouns can also be action or events like wedding, like construction, there are events and then it's also noun. And then verbs may not necessarily be our actions, for example, to be developed to be OK. So one of the most popular tree banks that have been useful for many, many years until recently when universal dependency caught up with it, is the Penn Treebank tag set, which you know, by Penn, you know where it's coming from. So we have all these different symbols and at some point it looks very, very, very confusing. You have CC to be coordinating conjunction, CD to be cardinal number, determiner. But there are some that are more common than the other. And then you could also group them into different classes. Some are open classes of path of speech and some are kind of closed tags, but many of them are kind of intuitive. So if you remember the very common ones like determiner, cardinal number are coordinating conjunction. We have preposition adjectives, adjectives. That's a very interesting symbol, JJ. And then we have also we have noun and then you could have plural of nouns, which is NN versus NNS and you have proper noun NNP. And then you have Internet PS. And then you have other things like pronoun that is also very popular. We have processing pronoun, we have symbol, and then you have interjection, which is interesting. UH, and then you have verb. You also have, you can have verb in past tense or in past participle. You know, when you start adding these kind of rules, you know that this will not transfer very well If you go to some other languages, Some languages do not have the concept of past tense. Like the language I speak, Yoruba, doesn't have the concept of past tense. And then some languages also do not have the concept of plural. And then also you find that this pantry bank may not work across different languages. Yeah. Punctuation should be another one which is like OK, it's obvious but punctuation is the last category. OK, so we have other part of speech like you can distinguish between modal verbs and auxiliary verbs like the car and wheel. And you can also have different kind of conjunctions. You can have main conjunctions like hand or bolt yet and you can also have something like subordinate conjunction that happens sometimes like after that that can connect. Two clauses together for example 2 phrases together. We also have like particles that sometimes you have particles that can be parts of a verb and depending on the tag sets. Also they are standard definition of what each part of speech tag is. OK, And so first I want to distinguish between open class and closed classes. Closed classes are like part of speech that is very difficult to add new words to them while open, they're kind of open. You can always add new words to it. For example, before Google, there was nothing like Google. But now you could use Google as a verb. And then you can also say Photoshop. Instead of saying I want to edit this picture, you can say I want to Photoshop it as another verb. And then you have nouns like Twitter, Facebook, and you have like adjectives that are not very common. But there's this website where you could see, you could browse to see our new words that have been added with their part of speech. All right, so for the cluster R class we have, these are kind of ingrained in the language, so they are very difficult to expand. For example, pronounce, IE. The only thing you can have is that maybe nowadays people will probably use one more than the other. And then you have determinants, We have quantifiers, we have conjunctions that are very difficult to expand. So closed classes tend to convey grammatical information and they tend to be more functional words or stop words. Do you have questions? I believe this is very good. Yes, yes. I mean, if you don't put them in a sentence, you cannot form good sentences. They are important, but maybe for future engineering, if you want to do test classification, maybe they are not so important. But in some other good question, in some other tasks they may be very important even for test classification. So if you are trying to distinguish between spam and no spam, maybe function ones are not very important. But if you want to distinguish who is the author of this article, you can have an author that uses more function words than the other, and then maybe they will form important features to actually distinguish one author versus the other. So they are not completely useless depending on the task, right? OK, Yes, OK, Now, so I want to tell you about this universal dependency task set, because nowadays people try to work on this instead of the Penn Tree Bank. Penn Tree Bank is very, very, very old. So it's in the 90s. At some point, we have different tag sets. So Stanford has A tag set, which is like a Stanford dependency tag set. And then we have some from Czech Republic, we have some from Japan. And then there's a lot of confusion of which is the right task set to use, especially if you want to expand to new languages. It's not very clear if you should use the French style, Japanese style, or the US style or the British style, anyone. So there's a project started. They started a project by a couple of researchers from Stanford and many universities and many, many universities, and they came together to start this universal dependency project. And it's a very ongoing project. So basically it's been ongoing for more than five years. So this is driven by linguist and competition linguist. And then they kind of have a more simplified tag set. So we have like the open classes, the closed classes. And the interesting thing with this universal dependency tag set is that it's fairly easy to extend to many different languages. Of course, there are some issues. So like two years ago we tried to expand universal dependencies to a bunch of African languages, and we faced so many, so many challenges. I'm not saying that there are no problems, but this is probably a bit more scalable than you have the Penn Tree Bank. The Penn Tree Bank has like distinction between their past tense and some languages don't have this right. So we have a distinction between singular noun, plural nouns. Some languages don't have this, but this one just has like noun which is noun. And then you have proper noun, you have verb and also you have the open classes and the closed classes, very similar to what we have in the pantry bank. So in the open class we have the adjective, erverbs, interjection, noun, proper noun and verb. So you can easily extend this based and add new words to this. For closed classes they are more very similar to the grammar and to the way the language is being used. And also you have other classes that are very difficult to put them into either open or closed classes like your punctuation. You asked about symbols and XX is just like, we don't know what should be the right category for this, but anything that doesn't belong into all this, we just call it X. So most of the time when you are doing this kind of annotation, X is just maybe there is a grammatical error in the word and then it doesn't make any sense. And then there are some jargon or maybe there's some strange hashtag on Twitter that you want to update and it doesn't fit into anything. So this will be X or some links URL will be X. OK, yes, OK. For corpus differences, depending on the corpus, you could have more fine grain tags than another corpus. For example, PTB, which is the pantry bank doesn't distinguish between intransitive verbs and transitive verbs. And then you have something like listened versus heard. And then if you look at Brown corpus has like 87 tags for this, why PTB only has 45? So also another thing is about language differences, which is very, very, very important. And this is one of the motivation for the universal dependency project. For example, in Japanese, which I don't speak, there's no great decision between nouns and pronouns. Pronouns are also open class rather than closed class in English. So this is one major difference between, for example, path of speech tagging in Japanese versus in English and wallow. For example, verbs are not conjugated. So you don't have things like for person and tense. So having like different annotation for past tense and present tense and continuous perfect is very, very different from that of English. And then in this language called celicial languages in Pacific Northwest, also the distinction between nouns and verbs. I mean in most languages you'll be able to easily distinguish between nouns and verbs, but in some languages this is also very difficult. OK, I have an exercise for you. Can you give a cross POS tag labels to the following passage? I think the difficulty here is you are seeing these tags for the first time and then makes it probably difficult. So this is I don't know which one issue. Okay? Should I write a sentence on the word and then show you the tax sets? Okay, OK, now I will show you the text sets. OK, let's use the complicated one first and then we will use the universal dependency. I think my solution is very similar to this, but maybe I'm wrong. This is my solution. Maybe I'm wrong, but so if we take geography as so, are you saying JJJJ and N maybe Yeah, but I think you get the idea. So another example is like this one sensation in Iceland after evolved in social media. I think we also have very similar situation like the first example where sensation is like NN, you have you have preposition, they have like a proper noun, then you have another preposition, you have a pronoun, you have a verb, you have a determiner, you have an adjective and then a noun. And in the last example, you have influencer, which is a noun, and then you have a preposition. And then you have what? Proper noun? Sorry, you have a pronoun, adjective, noun, preposition, determiner, noun, and penetration. So let's say we want to convert this to universal dependence now. OK, now let's use this tag set. What are you going to get for that on the board for what you have on the board using a different task set? So A will still be what will still be now this will be Det, right? We have two kinds of vets. This is difficult. We don't have position, All right, OK. So we kind of show two different task sets, so using Pentry Bank and using universal dependency. So one of the second example that would be what if you want to convert this to universal dependency? Sorry, I have to show you that. So this would be also now and then ad position. You have proper now ad position. Then you have actually that's after maybe something else. It may be the subordinate conjunction in universal dependency. And then you have pronoun, verb, determiner, adjective, and noun. OK, The last one will be also very similar. We have noun acquisition. You have preposition. Sorry. You have now preposition, pronoun, adjective, noun, apposition, and then you have determiner, now apposition. OK, I believe it's clear, right? OK, All right, now let's go to the algorithms. Yeah, I think by now you already know the task we are working on. You know, we are working on part of speech, you know, different task sets that can be used. So for the rest of the lecture we are still going to continue with the pantry bank. If you are interested in the project universal dependency, you can go to the website. There is a way you can join the project if you are interested. And you can do this for different languages, for different data sets or for different domains. All right, So, yeah, for path of speech tagging, if we have some data that is labeled with path of speech tagging, what kind of problem is this? Yes, yeah, it's supervised and it's classification. OK. So from the knowledge of language model in lecture, this is also very important for path of speech because in language model. You have a context and then you want to predict the next word. The interesting thing is for part of speech, there's also dependency between a Canadian. For example, there's a lot of dependency between Canadian geography nerd. And then there's dependency there. And if you want to predict what will be the next tag, you need to understand what is the tag of the previous word. Very very similar to laggard model. Now the only difference now is that you need to predict both the tag and the next word. This is the way we model this. In longer model our concern is just how to predict the next word. But now we want to predict the next tag. But the way we will do the modeling is that we want a model that will be able to predict both the next word and the next tag correctly. OK, so I saw the dash, the team won the match and several cards. There's some dependence on what would be the next word that will come. So here we have an example here. And when we are doing part of speech, even if you want to model this, I think in the next few lectures, we're going to discuss things like conditional random field, which was very, very, very popular in the early 2000s. And if you want to model part of speech, you have to consider what is the previous context and the current word. You can even consider what is the next word to be able to determine what is the right tag. So having this different word information, you can use it to predict what will be the right tag for the word you are trying to predict. And this brings us to a framework that can help us to model this, which is called the Markov chain, the Markov chain. The idea is what is called the Markov process, and the idea is that we can change this probability of predicting the next word and use the Markov process. Basically, we have States and we have transition probabilities from one state to the other. And in this case, your state will be the part of speech. And then you can now say what would be the probability of predicting the next word? What would be the probability of predicting the next stack? And we have already seen some kind of Markov processes because we are trying, for example, in the naive base, we have a very simplistic Markov chain where you have the class, for example, with a spam or no spam, and then you want to predict each word that will be generated from spam or no spam. It's a very simple example of the Markov process. So in morphology where you are transitioning between phonemes that make up a word. In language model, when you have transition between words that make up a sentence. In other words, they are like finite state automata. So this is an example of the markup model where you can have words and then you have interaction between the words and all these interaction, all these arrows in this graph, I mean they're directed graphs because it depends on the other one. So all these arrows, you also have transition probabilities and these transition probabilities will sum up to one because it's a probability. So this is an example of a bigram model. What will be So can you give an example of how the trigram will look like? Yes, yeah, you have like 3 errors here. So if you unroll it in time steps because now it's easier to follow. If you unroll it, then you have the car of hands run. So it's very easy to see that for you to predict the word of car, it depends on the previous word, which is a determiner, and then to predict the next one depends on the previous one. And each of these arrows we are going to have what is called the transition probabilities. So and based on that we can have a concept of what is called the eating variables very similar to what we have in the naive base. So we can consider part of speech to be predicted as eating variables because you don't see them during test time. For example. We have other phenomena that actually are very similar to that of parts of speech you have like encrypted symbols and output of Ed messages. So you can see, you can look at parts of speech as something hidden that you don't know, but you have to model. And this is where the idea of hidden Markov model is coming from. Something is hidden, you model it as an Ed variable, but you still have to model the hidden variable because you don't know it at this time. And then you have an assumption for that hidden variable, You have a probability distribution assumption for that hidden variable. So another example is gains are output of functional relationships, whether it's the output of hidden climate conditions that we don't know and stock prices are output of market conditions, Yes. So like I said, if we want to model part of speech, then we have the part of speech as the part of speech will now be the states. Those states can be anything, any word. So verb can be any word, can be, be or have or do. And then you have also the different probabilities, different transition probabilities, going from one node to the other of going from one state to the other. And then we have a modal transition between parts of speech and output for a part of speech to how do I explain this for a part of speech? Because it's like an hidden variable that you're using to model words. So basically something is hidden and it's like you generate a word. See it as more like a generating model. So given a part of speech, you want to generate a word. This is the way we model it. And there are different words that can be generated given different probabilities. OK, let me connect this to language model. In language model, you want to know what is the probability of predicting the next word, right? And here given a part of speech, what is the probability of generating this world? So basically we are modeling both parts of speech and words together that A tag can generate the next tag and that same tag can generate a new world. And this is the way the modeling is being done. And then you have transition probabilities from 1 tag to the other and then you have another probability to emit the world. Is it clear? Yes. So hidden variable. The idea is that it's an assumption that because you don't know this variable at Test time. So let's say you want to say what is the path of speech for this sentence for each word. You don't know the part of speech that is an hidden variable because you don't know it at this time. However, when you want to model it, the assumption is at the modeling stage is as if you know it and is that this part of speech depends on a previous part of speech in the time step. So when you unroll this out, actually this means that the verb depends on another part of speech, and the word that will be generated depends on the part of speech. So the way it's being modeled is that there are many possibilities for this part of speech to generate a word, because that is the class of that word. So given a part of speech, you can generate different words that can fit that part of speech, but you don't know the word. You can only determine the word if you know the transition, the emission probabilities, Yes, But at the modeling stage, this is how it's being modeled. So. And yeah, you have a question. Sorry, I can, yes. So basically we want to learn the different probabilities of how possible is it that this part of speech will predict another part of speech, and how possible is it that this part of speech will predict a word given our corpus is expected to predict it because now you have a corpus. So given an annotated corpus, I have estimated all the different probabilities. Can you predict what would be the part of speech and test time? Yes, there's a question. Yeah. Mm. Yes. So it depends on the corpus how many, what's the probability of having now followed by now given the corpus, right. So in this assumption, we assume we have a training data that we can use to estimate all these probabilities, right? Yeah. So the way the trigram will work is that now you say it doesn't depend on just one previous tag, it depends on two previous tags, right. Very, very similar to how we computed probabilities for the language model. Yeah. So if we unroll it, the time steps. So basically this is the way it's modeled. And you can see now each part of speech, there's a possibility for them to generate what is the next part of speech. Given the transition probability, given the probability on that arrow, you can generate that we have to go from the terminal to now. And given an emission probability, you can generate a word. And this is the way it's being modeled. And once you have done that, you can repeat the same process. That noun can now generate another part of speech, which is preposition, and also can generate another word with some probabilities. Yeah, Yes, Yeah. It's possible if you don't have a good probability, if you are not able to estimate good probabilities, Yeah, If you could, you are not able to estimate good probabilities. It's possible to predict the wrong thing. Yeah. Yes. Sorry, Let me ask. OK. No, this is just dependent on the tag. This is like a bigram. Yeah, it's a very simple bigram. Yeah. You have a question. OK, Yeah, yes, this is a very simplistical diagram. So that means it just depends on the previous tag or the word depends on the tag and then the next tag depends on the previous tag. OK, so we have a probability. Now if you want to say what is the probability of D is the terminal, then car is a noun and of is preposition and is noun with a plural and then ran is a verb that is in the past tense. OK. So the way it's like you have to 1st have the initial probability, which is the probability of DT multiplied by. It's like having a unigram probability multiplied, right? What is the probability that it's going to emit the word D? And what's the probability that it's going to transition to the noun? And then you repeat the same thing because this is like a bigram model. So the next tag will be what's the probability that the noun is going to emit a car, which is the next word times? What's the probability that it's going to transition into the next tag? Yes, PDT, you are going to estimate it based on your corpus, what I'm going to show you in another slide here, OK. And you repeat the same thing. Basically you just multiply everything. Very similar to what we did in the language model. But now we are now saying that it does not, that the tag, the next tag, would depend on a previous tag and a word would depend on A tag. And then you multiply the probabilities together. Very, very similar, yes, They are unaware there's no relationship in this formulation, Yes. But of course there are ways you can make it depend. So if you have like you expand the context size here and then you multiply the hidden state transition and the observation emissions together. And of course here we are also still taking the independence assumption. OK, so there's a way we can generalize everything into what is called a graphical model if we represent each state as a random variable. So the Q here, which is a random variable for every tag, probability of QT equals A tag, and then the observed variable, which would be your words, can also be a random variable and the OT. So now you see that we kind of distinguish between two things, the observed variable, which is kind of shaded and then the latent variable. In graphical models, instead of saying hidden variables, we often use what is called a latent variable. It's kind of the same thing. And also the assumption is conditional independence. So there are a lot of algorithms that have been developed for graphical models that are very, very popular, not only for supervised tasks, even for unsupervised tasks like the LDA. OK, So in any Markov representation, this is what you have. So just to generalize, we have the observed which is the observed random variables O1, O2. In our case this would be the words. And then you have the tag set Q1Q2 to Q5. And how do we estimate the joint probability? Because we are making an independence assumption, all we have to do is to multiply all the transition probabilities times all the emission probabilities. So basically you're saying that the probability of Q2 given Q1. And then if you want to estimate, if you want to predict Q3, it would be the probability of Q3 given Q2 multiplied by the probability of Q4 given Q3 multiplied by probability of Q5 given Q4. This is like a bigram model and at the same time you multiply it by the emission probabilities. Probability of observing O1 given Q1 times probability of observing QO 2 given Q2 times probability of observing O3 given Q3 times probability of observing O4 given Q4. That's probability of observing O5 given Q5, yes. Yeah, you have to also estimate that from your corpus. Yes, OK. You have to 1st, you have to estimate the what is called like the initial probability. It's like the unigram. How many times does this Q1 occur in my entire corpus? Yeah. How about some words that occur? Yeah, then yeah, your probability will be 0, right? Yeah, your probability will be 0. But if you apply smoothing, your probability will be non 0 and you can consider with your calculation. Yeah, yes, yes, yeah, yes, yeah. But it's they are both equivalent, right. The two things you said like I cannot, what's the difference? Yes, yeah, yeah. So the word depends on that POS, Yes. But we are not using the context of the words. We are not using the context of the words in this modeling, in this simplified one. Yeah, this is a very, very simplified one, just so that you get the idea, right? Yeah. OK. So I know you asked about the initial probability. So the initial probability of Q1, you can estimate it. So for every part of speech that you have, you can estimate what would be the initial probability based on your corpus, right. And then you say what's the probability of a verb, What's the probability of a determiner? What's the probability of a pronoun in my corpus? Right, Yes. Relative frequencies. Yeah. In your sentence. In the sentence. So this is queue one. What is the probability that this part of speech begins a sentence? This is queue one, yes. Yeah. This is queue one because queue one is the beginning. So what is the probability that the terminal starts the sentence? What's the probability that verb starts the sentence? This is your initial probabilities for Q1. So once you have these initial probabilities, the next one is that you can now estimate your transition probability. What's the probability of Q2 given Q1? What's the probability of Q3 given Q2? And so on. And the last one is the emission probability, which depends on the word. What's the probability of a word given the path of speed tag? OK, so how do you train HMM path of speed tagging? Since you have a label corpus, this is like a supervised task. In the HMM, the very simplified 1, you can estimate all these probabilities very similar to how we estimate all the probabilities for naive base. You can estimate the initial probability distribution. You can estimate the transition probability distribution. You can also estimate the emission probability. Do you have a question before we do the exercise? Yes, yes, yeah, yeah, of the whole thing. That's the joint probability. It's like a language model. So it's like probability of all the observed variables and probability of all the observed taxes. So in this case, it's just basically, all right. Yes. Probability of the entire corpus, yeah. Yes, it will be per sequence. Yeah. Per sentence basically, yeah, this one, if you want to estimate the same thing, this one should be the entire corpus. But you can also estimate this per sentence, yeah, because there's some probabilities that you can only estimate per sentence, right? Like this probability of Q to given Q1. But no, no, sorry, the other one, probability of O1Q1. But in actual fact, you have to estimate this over the entire training corpus. You look at the entire training corpus in different sentences. How many times does O1? How many times do you have the word D given determiner? How many times you have the word A given determiner in the entire corpus, whether it's sentence one or sentence two, you have to calculate this. Yes, Yeah. Because Q1, this is your initial probabilities. So you compute like unigram probability for every part of speak tax. What's the probability of proposition in your entire corpus? What's the probability of the terminal in your entire corpus? Yeah, Yeah. Sorry, I missed the Yeah, I missed the initial one. Yeah, yes, yes, because they are also parts, it's treated as another token. Yeah, yes. So why would you want to do that? I mean, you can, but why? But there's a dependency. I mean, this is our model. Like there's a dependency between this. So that's why. Yeah, but of course you can do that. But what will it mean? Yeah, yes, we're trying to say it depends on it. Yeah, actually this is a simplified 1 here. We're just saying bigram assumption, it can also depends on the last two right part of speech. But say that it doesn't depend, it's actually like false, right? Yeah, right. So this is the way we estimate the probability for the initial probabilities. I mean you remember the way we compute probabilities. The probability of an outcome is that you count all the outcomes and then you divide by all the events. So if you want to estimate the probability of Q1, this will just be number of times you have Q1 starting, starting the sentence. How many times do you have preposition starting the sentence? How many times do you have determinants starting the sentence divided by all the possible sentences you have in your corpus? I believe that is clear, right? That is the Pi I and then you have the transition probabilities. How many times does these two parts of speech Co occur together in your corpus in different sentence, in different sentences? How many times do you have this core together divided by the number of times you have I, which is that part of speech? So it's very similar to how we completed the, for example, the naive piece. And the last one is for the admission probabilities. How many times do you have this word K and that's tag together every time. Do you have it? How often do you have them together divided by the number of times you have the tag? And then like you say, you can also still do smoothing because some probabilities will be 0 and we don't want them to be 0. So you have to also perform smoothing for the OOV events. OK, So this is the exercise and then we'll wrap up the class. So given this exercise and this is your corpus, you have how many sentences in your corpus? So you have 4 sentences in your corpus. Can you compute what would be the initial probability distribution, what would be the transition distribution and what would be your emission distribution for just two tags DT and VBD? So I'll give you like a few minutes and then we'll solve this together. So how do you start for this kind of exercise? So basically, you have to estimate all these probabilities first, the initial probability π highs. So the first thing is try to compute π highs for DT and VBD followed by what are the transition probabilities? What's the probability of having a now? Sorry, what's the probability of having a noun given a determiner? What's the probability of having a noun given a verb? So you estimate all these probabilities, all the different combinations, and then you estimate also the admission probabilities. No, no, no, not this one. So you have to do it differently for DT and differently for VBD. Yeah. So I mean, yeah, basically for them, for each of them differently, yeah, each different things given DT. Yeah, I mean for now we can exclude the ones that are obviously 0 because you're going to smooth anyways. The idea is that you have to compute the probabilities for every combination in your corpus. Yeah, you have a question, but only a few cases will be non 0, so I will only focus on that in this exercise. So they are very very easy to compute. So I believe by now you should have computed the initial probability. So what's the initial probability for DT? What? Yeah, 3 / 4. Is that what you got? Yeah, OK, right. So what is the initial probability for VBD 0? Great. OK, at least now I know some people are following. All right. So what is the transition probabilities from OK, now let's think about different combination from DT to NN. Yes, four out of six, that's correct. So what are from which other combination do we have? I don't think we have any other combination that is non 0. Yeah. DD to JJ, Yes, 2 / 6. OK, OK. And there's no more. OK, Yeah. And what about for VBD? What is the transition probabilities from VBD to preposition, for example, to iron? Yes, 2 / 4. Yeah, also from VBD to JJ adjective 1 / 4. OK what are from VBD to DT 1 / 4? OK, now the last one. Let's talk about the initial probabilities. So what's the probability to have D given DT one? Yeah, because the three the three sentences you're always have the Yeah, OK. So what's the probability of for VBD now what's the probability that from VBD you go to SAT? Yeah, 1 / 4 And what's the probability from VBD you're going to worse 3 / 4? Yeah. OK. So I have this calculation. I think I'm missing one things, one more calculation, I don't remember which one, but I think I'm missing I think DT to JJ. So I think that's the only one I'm missing which you can compute here. OK, OK. And then, All right, so in terms of the inference, how do we now do the inference now that we have a model, how do we actually tag a new sentence? I think this is what we cannot finish in this class. But from next class we're going to talk about different algorithms actually on what is the best way to actually tag a sequence and. We have the following questions which we need to estimate. Now we have computed all the probabilities so we have our Theta and now that we have our Theta we can compute what is the observation given Theta. And then we can also use some algorithm to determine how to estimate the right tags given all what we have observed and given the parameter we have estimated. And then we have all these algorithms, forward algorithm, backward algorithm, Viterbi algorithm that we're going to discuss in the next class. OK, if you have a question, what you can ask. Otherwise, thank you for attending the class. I Yeah, you're using just the one question for storage, you are mainly using your home directory or scratch. There's, I think there's a, there's a directory other than home. So basically what after we log in, we are automatically directed to home directory. But there's an actor directory's name is Scratch, and you can store all the big files there. And I think the space for Scratches around. Yeah. I'll give you a score. Thank you. And after you try this stage, we will have another score. And. I don't know. Yeah, actually, I think the paper is supposed to. Peter said he will be responsible for the experiment section. I mean, I mean the result section, but because he said he can because and Peter said he can do that and he reported some of the scores to me and it doesn't seem because for example, for the accuracy on multiple SQ was just 26%. Yeah, I think, I think the first thing will be after tonight because tonight about I plan to finish the I plan to finish the first 3 instructions and the first half of the first first. Maybe it's better for me to just use the pointed out the point at the three end. So you would finish the 1st for other parts. All other stuff it shall be mostly done. And I think you what you can do is after you can in the polishment you Polish the 1st 4 pages and there are some figures that can be added. For example to help illustration, you can first read what I wrote and write a figure. I'm sorry like how I how I ordered the databases during training phase for example to to help smoothing the training where they was interleaving databases from different languages. SO11 database from English, one database from French and then this for just many that I have many forms of data and each of them is to contain one database from each language. Yeah, yeah, that's what I did. Now is just use, but if you think that's better to use table, you can edit that part. But now is just to say, OK, for the stuff we use this data set and I just added script and I specify which language which set of languages for we use. For example, if a language containing many many languages, but we just use for two or three languages. But if you think the table is better, yeah, but they can edit that section. I have it. It's been years since I just took like a couple.
