David Ifeoluwa Adelani: Yeah. Can you also see my screen? Everything is good trying to change the charts? Can you hear me? Can you see the slides? Yeah, okay, yeah, I think we can get started. Yeah. so welcome to lecture 10. Unfortunately, we are not able to have this in person, due to the email we got from the university. So basically, today is the lecture 10, and from next class we have another professor taking it, Professor Jackie Sean. and then. we'll be taking the course for the next couple of weeks. and today I'm going to be rounding up with recurrence neural networks. Just if you remind us. I don't know if you know already, but we have reading assignments. One has been posted on head so the deadline is Friday this Friday, October 11.th If you have questions on the reading assignments, please post on it. I will try to answer. Also. We have a reading break next week, so of course, probably you know already about the Thanksgiving on Monday, and then we have the entire week. We don't have any lectures. but there will be lecture on Wednesday. also we have some tutorials on our end. that will be offered by one of the tutors. Please do attend it. All information are on Ed. Do you have questions so far on this? You can raise up your hand if you have questions. I think that's how it's gonna work today. Alright, so yeah, so going to the outline of today. today, we're going to review crf, that we discussed last time. We skipped the lecture on Wednesday because of the Nrp. Workshop. I'm very happy that some of you came in person to attend the workshop by. So a couple of students from the class. Thank you so much. I also hope you learned something that is useful for the research and for deepening your understanding on Nop, and they discuss very trendy topics that you can focus on for research. And I hope the workshop was useful for you. So today, we will review the Lccrf. And then, after we're going to review some neural networks because we actually touched on some of this concept previously. And then we discuss recurrent neural networks. and after that we'll move to long, short term memory networks, which is the most popular and the most used. and also the most performance Rnn kind of architecture. And at the end we are going to talk about Lstm Crfs, which is also the topic of your reading assignment. Right? So last time we talked about linear chain crfs, and then we talked about this formulation that here we are focusing on a discriminative task. Hmmm, focuses on a generative task. Where you try to for the generative task. Basically, you try to learn the joint probability. Given a set of parameters, theta which you can compute based on your coppers and for the linear chain crf. we are interested in a discriminative task. Basically, we are interested in the probability of Y given X. We are just interested in the output, and we can formulate it this way where we multiply the parameters by a set of features. This set of features are user, defined features. You can define your features for different applications. for part of speed tagging. The features are different. You can also, define features for other tasks like any other tasks. So this kind of gives you the flexibility to define different features. Unlike the Hmms. Where, you have to try to learn job probability for any kind of feature, you decide to add, and this makes it very difficult to extend makes it difficult to incorporate new features. And Z of X is our normalization constant. And I think we also showed how you can find the close form. formulation. Using mle. So we're talking about different kind of features. we can actually compute some crf features very similar to the Hmm probabilities where we just can't compute the indicator function. For example, if we are interested in the previous tag of P, part of speech to determine what will be the current act. We can check in our corpus using this indicator function if a determiner proceeds in now. and the indicator function is as the is defined as the following, where you have one, if the condition is true or you have 0, if it's false. and then you can actually expand these features, for you can add additional features apart from the hmm kind of features for the hmm, we have what is called the emission probability, and we have the transition probabilities that can be converted as features in Seattle. But also we can also, we can had new features for for other tasks, like part of speed tagging, you can say, Okay, capitalization can be important. For example, if you want to detect things like proper noun in English language. Of course some languages may not really keep this capitalization as an important feature before English language. Capitalization is important in determine part of proper nouns, for example. and also determine. And you can say, I also have a feature like that. If it's beginning the sentence you can ask. You can have a feature like, was the hands with Edie. You can have features in terms of lines, and so on. So it's very flexible to have new features for crf model, and just to kind of relate it, we can also use the forward in terms of computing the inference. We can also use the same forward algorithm and the a verteb algorithm for the Lccrf in the forward algorithm. We are interested in learning the likelihood or probability of X given theta. Why, in the Lccrf, we're interested in Z of X for the vertebra algorithm, we're interested in the joint probability we are looking for. What is the parameter of the model that we can take? That will help us to learn the joint probability. Why, for the Lccrf. Our major interest is in actually a discriminative task. Basically, what is the probability of what we're interested in the task? Once we have the task, we're okay. And at the end of the lecture. We try to compute the mle for this by taking derivatives, and we have this formulation in the last slide. I think some things were not clear, and I think there was a few errors in my formulation. For example, I did not put ZZ of XI. So I. We only have Z of X on the board, so I have modified it to have Z of XI. And I've posted the corrected version on Ed. Do you have questions on the last lecture? Yeah, it's a bit difficult, since I cannot see your face, and I don't know your reaction. I don't know if you're following. Do you have questions? No questions. Yeah. Okay. If you can on your camera, we can. If it's convenient for you. Then it's easier for feedback. Otherwise, of course you can. yeah. okay, that's better. Thank you. I think we have some. So it's easier for me to kind of get some feedback if I have, some people would cover us on. Alright so I believe there are no questions, and some of you already checked the corrected version. How many checked? How many people check the corrected version of the formulation on ad, because I post a corrected fashion. Okay, I can see some note. I think, yeah, it's fine. Okay, going through this lecture. We want to review artificial neural networks. And if you remember, in enough, I think. The second week of the 3rd week we're talking about different classifiers. We talk about naive base. Then we discuss a support vector machine. We discuss perceptron. And also we discussed artificial, neural, network, artificial neural network. it's very interesting. And also it can land from a large amount of data. So efficient run. It so is the kind of learning model which automatically learns nonlinear functions from impute to output. Actually, there are some proofs. That shows that if you have for any nonlinear function. attention, neural networks can learn it. There's another proof that you can learn any function theoretically. If you have enough data with neural networks. So it's really an interesting architecture or method, because number one, it's biologically inspired. But nowadays I think the models we use do not really have a lot of connection to biology or cognitive science. For example, the concept on transformers attention is all you need. There's nothing really connecting that to something more biologically inspired. But the initial formulation, at least for the fitful neural networks, is more biologically inspired. So you have network network of computational units called neurons. And each neuron takes scalar inputs. And then you have, like the product of the parameters, you want to learn. And the input very similar to the logistic regression. So in logistic regression, we have this kind of formulation. You want to learn parameters, a 1, a 2 to a N, and of course, the intercept. So now look at neural network, at least the feed forward neural network as a generalization of logistic regression across multiple layers. That means the output of all logistic regression is being passed to another logistic regression, and you're not kind of stack them together into layers. So as a whole, the network can theoretically compute any computable function given enough neurons. And I will say, Given enough data. Yeah. So for the feed forward neural network, all connections flows forward. Basically, you have the input x 1 x 2 x 2 x 4. You multiply with the weight matrix which the weight mattress actually describes all these connections. So you can have. I mean, you can always connect any directed graph into into a matrix. Right? So you know, all these connections actually shows you. you can have a matrix based on the dimension of the input and the dimension of the eating layer. And you can form a matrix based on this. And then and then you can have values for all these weights initially, when you're learning this, you randomly initializing with metrics. So all these connections have weight on top of them, and then you randomly initialize them. and then you can do computation from x 1 to X. So if you want to compute the 1st leading layer, you have connections from every impute, and then you can compute what will be the value of the 1st one, and then it goes on from one layer to the other. So if we understand that all these connections are weights, and then can be expressed in terms of metrics, so we can have it in this form. So once you learn all the weight matrix, all the weight matrix, for example, from impute to eating layer. That is, one weight matrix. From the 1st eating layer to the second eating layer. You have another matrix, and then from the eating layer to the output layer. You have another eating matrix, you have another weight matrix. So you can actually compute what would be the value. So think about impute. So where every every input here there are scalars. x, 1 is a scalar. x 2 is a scalar, x 2 is a scalar, x 4 is a scalar. If you combine all the scalars into a, vector, you have a vector, so X is a, vector, and then you multiply by a weight matrix W one, which is the 1st width mattress connecting the input to the inning layer. Then you add it with a bias term and if you look at this formulation to that of logistic regression. You can see the connection that's in a logistic regression. You only have, like a vector of parameters. Now, you have a matrix of parameters. Okay? And then, if you compute that, you pass it through an activation function g. 1, you have the output of GH. 1. So the output of h 1 we serve as inputs to the next layer, and then you can compute H 2, and then the h 2 h. 2 can also be passed to multiply the last weight matrix, and then you can have the output for different application. You can ask, actually, pass this last output. H, 2 W. 3 into another activation function it could be a soft Max function if you want to have a classifier. if you want to have, like a multi-class classification. you can have something like a softmax that the result of H. 2 W. 3 will be passed into the softmax. You can also have a simple a simple, a simple, logistic function. Also, if you only have 2 values. Do you have questions so far. Okay, I believe this one. I'm checking those people with their videos on, like, Yeah. all right. good. So let's proceed. So for the training of neural networks. We discussed gradient descent and stochastic gradient descent. This is a revision for stochastic gradient descent. We are only interested in one training, example, and then we can find gradients of the loss functions with respect to the parameters. And how do we find it? We use an algorithm called back propagation. It's a very old algorithm. But we have not been able to find a better algorithm. And the idea just boils down to an efficient way to use chain rules of derivative to propagate the error signal from the loss function backwards to the network. And if you propagate the errors, you can update the weights matrix so like your w. 1 w. 2 W. 3. You have a better parameters for them, and then you can use this to improve your model. So for the logistic for the stochastic, gradient descent. You have an impute sk, yk, you have a function you want to learn which is F, and then you have a lux function. L. An example of the lux function would be something like a cross entropy cross entropy loss. For if you have a binary classification, I think cross entropy can also be generalized to multi-class classification. That's an example of a loss function, a very trivial loss function. If you have a regression task can also be your mean, square error. So if you take the num of your output and your prediction, you can compute your mean, square error. That's an example of another loss function. So you can repeat for a while basically, you sample a training case, and then you compute the loss, and then you compute the gradients before you can now update the parameters of Theta. And if you do this over several iterations. you know, if you go through, for example, land language models. If you go through this over, for example, a million samples, of course you don't need a million samples for every application for some application, even if you have to like a thousand examples. This is already enough. Okay, and the example is here where we can actually compute the forward pass. You have an activation function. You multiply the input by the 1st weight matrix plus the intercept. And then you take the output as an impute to the second weight matrix. And then you do the multiplication. And at the last stage you have another activation function. This G 3, for example, if it's a multi-class classification can be a softbox function. and you compute the loss function between what has been predicted and your gold label, and then you can do back propagation. Another thing approach which might be useful, since we are more interested in language is what is called time, delay, neural network. and the idea would be, you're not only interested. Which is another fit forward neural network, but you want to kind of incorporate like a timing information. What happened in the past what happened. And then you can incorporate this information into a simple fit for learning tool. And then we can construct it using a context window. So instead of. So if you want to predict, like a language model, what is the probability of the next? Or let's say, for part of speed tagging, you are interested in the tag. You're interested in the tag of every word, but you want to incorporate the previous word or the next word. You can also do that using a simple, fit forward network. However, you need to have some timing information and that timing information. We kind of have a context widow to incorporate previous words, or the next words. around the center. What you want to predict you want to predict the path of speech for so one approach is this, I mean, this is what I'm trying to say. So let's assume this is a simple fit for our network. And actually, this should be actually a connection. So there should be a connection from Wt, maybe I can do it very quickly. yeah. So here I think there should be. there should be a line that kind of connects. This. So what we're interested in is computing the tag from Wt and the tag here is. Qt, that's what we're interested in. But actually, you could use timing information to incorporate the previous word Wt. Minus one, and the next word Wt. Plus one. And of course you can even increase the context window. And if you increase the context window, you can have 2 words after or 2 words before. And you can also do this using the simple what do you call it? Fit forward and we have architectures like what embeddings have been lent using this way, where you can incorporate previous information and like future information. a better way to actually do this is recurrent environmental. This gives you an opportunity to kind of incrementally incorporate previous information into the current model. And you can do this over a time period. So basically, you have something that is recurrent. So the time information can be easily incorporated into a recurrence information without using a time delay neural network. So The simple idea of recurring run network is that you have different states. You know, we talked about Hms, so you also have different states. and in each State you have an initial state, vector X 0. This is what you start with. And then you have an impute one to hand. And the idea is that you want to be able to compute all the States and all the outputs. So at the end of the model, once you have trained this model, what you will have learned is all the States information. and you will have also be able to compute all the outputs information. So this doesn't. This is better approach than you know. Learning. Via a time delay neural network, and all the States are an O are very important to compute what will be the next output. So in the recurring neural network there are 2 kind of outputs. Number one. The 1st output is given an input x 1. Once you use all these parameters, R. And O, you can compute the y 1 output. So for you to compute y 1, you need SO. And x, 1, to compute y. 1. And also you have to output from every recurring run network. Right? So the 1st output can be a y 1. But also, you need to pass order, state, important State vector, information to the next. you need to pass it to the next cell in the recurring manager. So the idea of recording analytics work is that you have all these different cells you have cell one cell, 2. The cell is all this ro. and then you have the state vector and the outputs. and at every cell you can always produce and outputs, and also an additional modified state information to the next one. and this is the idea of the recording random talk. So you have the Rnn. SO. Which is the initial state. Then you pass this information of your x 1 to hen, and what you you will get at the end of the day is that you will get all this state vector, information s. 1, to Sn, and also you get all the Y information, which is your output. y, 1 to N, this is a simple idea. Of course, you can design recurring network with different form. You can even design a recurring network where you don't need outputs at every cell. For example, if you're just interested in in classifying, if something is spam or not. Then you probably you don't need output information at every cell. Well, if you want to do part of speech tagging every tag is associated with what with every impute. And that's for that kind of task you would need to output at every cell. So, okay, there's a question. Yeah. Shidan Javaheri: Sorry I just wanted to confirm in the previous slide is the theta the parameters shared by every hidden state? Or is it different for every hidden state. David Ifeoluwa Adelani: So the parameters? Yes, that's a good question. So the data involves all the States parameters that you need to save. Shidan Javaheri: So it's different. Parameters for every State are all incorporated in detail. David Ifeoluwa Adelani: Yes. So all the parameters units will land our data. We just kind of say, theta for everything. Yeah. alright. So long. Term dependency in language. If you look at the 1st example, I will look the world that you have described that doesn't make sense to her hub. So you see that there's a you need to have a connection between look and hop right. And you also need a connection between this word. You want to look up. So that means you need a way in language so that you can connect dependencies that occur within a very long context and imagine it. Now, if you want to do a summarization of a document, you need to be able to connect the information that is at the beginning of the sentence to the one at the middle of the sentence, even if 20 words separate. So how do we do this? We cannot easily model this with Hmms. Or even crf, but because, Rnas, you know, in this architecture can pass the information from the 1st impute using this state vector to the second, impute. So some of this information in the 1st impute are being transferred to the second impute. Sorry are being transferred to the second cell right, and some of the information in the second cell are being transferred to the 3rd cell. So you'll be able to connect the information that is far behind to the one that is appearing in the future. So, but so just to compare the Lc Crf and Rnns. So if you last class, Lccrf. our linear chain crf, a more linear model. So if you are building a classifier, as you will see in the assignments, the reading assignment is that you can actually just replace your linear layer at the last layer and replace it with that of the Crf. because this is more of a linear model. Why recurring neural network focuses on nonlinear architecture of neural networks and is more expressive, is more expressive. Of course, you need more data to train a recurring neural network in terms of feature engineering for the Crf, you can always you need to do feature engineering. as you will see which is also the topic of the assignment is that some some Crs requires. You need to actually do a lot of feature engineering. Why, for Rnns, you do need to do feature engineering. You can learn everything directly for from the data. And actually, this kind of reduces the time you will spend in doing feature engineering so and the last one is in terms of polynomial matter, if you remember. the forward agreeing is polynomial time. So you have to say, n squared T. That was the time complexity. So it's polynomial time. And then for Rns, you can have an approximate inference. So yeah, we have different kind of Rna architecture, like. I told you that every cell doesn't need to have an output right? You don't need every cell to always have an output. so we can have one to one if you have one to one. This is very similar to I mean, like, if you forward a talk yes, there's a question. Shidan Javaheri: I'm so sorry on the previous slide. I didn't quite understand what it means. Approximate inference, only greedy or beam search for Rnn. David Ifeoluwa Adelani: So for our end. So the the final outputs. So let's assume you want to output a word here. There are many based on probability you could have 2 words that are equally probable. Right? So if you use the softmax function, then you need, like an algorithm to actually decide which one, so that you can improve diversity. If not, you will just be predicting the water is most likely, and that's why you need this kind of approximate inference. So these are are well-defined ways to actually select the next one to be predicted. Okay, so looking at these different architectures, you have one to one, you have one to many. you have many to one, you have many, too many, you have many, too many. So if you want to do document classification. What would be the best approach it so by now I will know who is following. Is it one to many, many, to one, many to many, many to many document classification. Given a document, you need a single output, which is the class of the document. Yes, I think someone is trying to type something many to one. And why is many to one? Can you justify that if you want to say it. Vivek Verma: Is it because the document text has like, the input, is like many words. And the final is like, we just need a classification. So it's like one output. David Ifeoluwa Adelani: Yes, it's 1 output. So you but you need for for the documents. You still need to analyze them, because you have words following each other. So I think many to one is correct for language modeling. Which one would you use? Okay? And let's see. okay, someone is typing. Yeah for language modeling. You will use many to many that's correct. For part of speed tagging, but which menu to menu would you use for longer? The 1st one or the second one? Why is the 1st one? Yes. Shidan Javaheri: I feel like you would capture the context of the entire thing that's being said, and then give the output in a response. David Ifeoluwa Adelani: Why can't the second one work for also for that? Because you. Shidan Javaheri: I. David Ifeoluwa Adelani: And then you predict the next one right? And you have the state information to predict the next one. Shidan Javaheri: I think it could. Yeah, I feel like the 1st one would be more performant. But I don't know. David Ifeoluwa Adelani: Who has a contrary opinion. Okay? What of for machine translation? Yes. Shidan Javaheri: I actually think the the 1st one would be best for machine translation. Yes, because. David Ifeoluwa Adelani: But but yeah, this one would be better for machine translation. Yeah, that's correct. Okay, for part of speak tagging. This should be trivia. Right? I mean, from all the diagrams of the Hmms you have seen. So the last one here on the list. Yeah, okay, cool. Alright. I think you have an idea. Okay for the Lstm which is probably the most popular Rnn architecture or Nlp, so the model includes what is called a memory cell. So then, this cell. So we can actually unveil what is in this cell. It's for, for we have different kinds of Rnn, and this cell. We can try to store a lot of information there to actually be able to capture long dependencies between words. So this cell that we have here for Lsta, we have different cells. So we have different Lstm cells. And inside each memory cell. we have a vector of weight in the evening layer. And then we want to learn all this information. We want to forget some old information as you move from one to the other as you move from one state to the other of moving from one cell to the other. You want to forget some information that would not be necessary. For example, in the part of speed tagging. You want to forget some information and you want to keep some information. Maybe, for example, the terminal information is important in predicting the next word. But if you are like you, you want to forget. maybe, what's the attacks that have occurred? within. for we want to forget terms like tags, or whatever code, more than 5 times away in the in the context. Because you feel this is not important. So you can forget new information. You can extract new information using all the cells. And all these are just kind of either element, wise multiplication or veto multiplication. And you can also have an activation function to determine what information to keep, what information to to to to kind of forget. So you can have like an activation for her that just gives you 0 1. And then, if it's 0, you forget that information, if it's 1, you keep that information and you cannot integrate this relevant information into memory and then pass it to the desktop. And then you can predict this at every time. Step. Okay, so we have different basic operation. We have masking where you can multiply by a vector between 0 and one and depending on the value between 0 and one. This will determine if you would forget that information, or you're returning that information. And we have some activation functions that are very good at this. like the sigmoid function, it always gives you a value between 0 and one. So if the value is near one, you should keep that information. If it's near 0, you will likely forget that information. We also have operations where you do a component wise operation basically just multiply the components for one vector, to the other. You can add information from one vector to the other. You can multiply that information. And you can also concatenate this vector, information. So and the last one that is very important is the nonlinearity. We always need nonlinearity because we are moving. You know, we have a lot of computation, and then we we want to keep them within a certain range. And so we always need that. And I'm gonna tell you one of the reason why you need to. You need to be careful to have all this information within a certain range. If not. you can have problems in your recurring neural networks like what is called exploding gradients and then vanishing gradients. And then you need to really take care of this. Yeah. So there's a very nice tutorial that I would recommend. It's been like close to 10 years old, but it's still kind of relevant. So because when all this architecture came, it's like they have developed all these architectures in the nineties, 1993. And then, when deep learning revolution came. I think around 2014, 2015, everybody was kind of just lost. And then we have some very interesting tutorials. And I think you can look up. And I think this one is really good. Yeah. So one of the problem I told you of that you need to take care of. When you have this kind of recurring network is what you need to take care of what is called vanishing gradient and exploding gradient. Think about it when you have to do a lot of multiplication in time and across different layers. What happened? You have a lot of multiplication to do. and then you can also have what is called the problem of either vanishing gradients or excluding gradient if the wait So if the values are big and you multiply them over a period of time, you're gonna have a very big number, and that's what is called export ingredient. and if the number is near 0 and you multiply them too much, then the gradient is 0, and then you cannot really make a lot of improvements over your network. So these are very critical problems that our ends are not able to handle properly. But Lstms has a way to fix that. So in Lstm we can propagate a cell state directly to fix the what is called the vanishing gradient program. So I haven't shown you this diagram. So it's really, you see what is inside. These are kind of all the different information in a standard recurring way network. So I have. I hope I have. Okay. I don't have a very good. I should have given you a whole diagram on this, but you can find the entire diagram in this tutorial. But a diagram like this, the the 1st part of the information. when you are doing the multiplication in the Lstm is, you have to say, what information do I need to forget? And once you have forgotten that information you have this ft. Here, which is representing which information that needs to be forgotten. And then the other information which other information needs to be propagated to the next stage will flow like this. But if you have the problem of vanishing gradient, you can just pass the information directly from one cell to the other. So there is no repeated wait application between the Internet State across time. That is one way to actually fix the problem of vanishing gradient. Apart from from that, you can also not only have a single Lstm. But you can have bi-directional. Lstm. so the idea of bi-directional Lstm is very, very simple in your standard neural network. You want to predict the next word given the previous. What if you reverse it right? What if you decide to predict the previous word given the current world. Right? That is a way to do bi-directional. So you have information progressing forward. And then you have another information progressing backward. Is that clear? So that is one way to to actually capture different context of information. So you can call, you can capture the information before the current works. So, for example, in the case of part of speed tagging by Lstm is very, very popular for this task you always need the previous information. and also the information that is in front, which is the next. Information is also important in determining what will be the tag of the world you you're trying to predict for. So you can go ahead from just a simple, by Lstm, to have what is called a is, you can move ahead from just a standard Lstm to what we call the bi directional Lstm. Alright. So this is an example of if I Lstm. Here you have the forward layer where you move forward, and then you have the background layer so as you can see. So it's like for the forward layer. You're going from xt minus one to xt to xt plus one, and for the backward layer you move from xt plus one xt xt minus one. So you go in the reverse direction. But if you combine both the forward direction and reverse direction, you can actually have a very powerful model. So one example of the model is there's an architecture called Elmo that came out just before birth. I think birds became more popular for classification tasks. But Elmo was actually using this kind of idea to build a language model that is going forward way and backward way, I mean by Lstm model. And this is very important in capturing all the context. Whether it's happening before or the ones that is happening after. So it's very important for tax like part of speed, tagging name extra recognition, chunking any or any kind of task that require you to look into the context of information around you to make an informed decision. By Lstm. It's a good model for it. and once you have have all this information from the forward layer and the backward layer, you can concatenate them together to make the final prediction. Okay, the more interesting thing is that you can, because crf is just like another linear model. So instead of just using a simple fifall neural network at the hand. So here where you have this sigma. you can have once you concatenate them, you concatenate the forward layer information and the backward layer information. You need to pass it to a last feed forward neural network to make the final prediction. The interesting thing is that you can remove that. Lastly, yeah. and then replace it with crf, and if you replace it with crf, crf, gives you additional information because you can. Crf is good for sequence tagging because you can actually learn what is the relationship, not you can learn the relationship between the tags, not only the relationship between the words. So for the crf, you learn the relationship between the words and relationship between the text. And this gives you an additional information to actually boost your performance. So from practical experience. If you train a by Lsdr model with a simple linear layer, and you might achieve something like 88%. And if you replace this linear layer with a crf, your accuracy can jump one or 2 points. and then you can move from 88 to 89, or to 90. And the reason is because Crf actually capture relationship between the output levels. For example, the tax? And the answer, of course, can we combine? Both? Yes, we can combine that, because crf actually gives you some boosted performance. Yeah. And this is coming from a popular architecture in 2015, 2016. And one of this paper, I think is one of the papers for your reading assignments, but if you have time, I will encourage you to also read the 1st one which is also cited in the Lampler paper. and both both both of these papers. They use this by Nstm Crf model. and you see they have the forward information. They add backward information, and for the crf, you have you have relationship between the different tags. And you can add this information to actually improve your performance. Okay, so in the last one. How how do you actually formulate this? So if you take the outpost cost of the Lstm you concatenate. If you're using a by Lstm, you concatenate the forward information, the backward information. You can combine this with the Crf by using what is the transition probabilities between the tasks? Because this is what you get for free from Crm. And of course you need to learn this. You can learn this joint alike. What do you call it? End to end you can land, you can back, propagates all the errors from the Crf to the Lstm model and update your weight. end to end, and you can now combine this information by combining all the Lstm scores with those of the transition probabilities. and then you can now make what is the final decision on the task based on this. And this is how you can combine the Lstm with the Crm. okay? And lastly, here. this shows you how we can do inference and training using the Lstm Crf and here, what you do is that for every epoch? Of course, you select your batch and you actually compute the bidirectional Lstm. Crf, you do a forward pass from every state, every cell to the other in the Lstm. And also you do a forward pass in the backward direction, because you have a bi-directional Lstm. And after you pass this information to the Crf, you can also do the forward and the backward at the crf layer. and the last in the 3rd step. You have the bidirectional Lsm Crf model backward pass. and after that you can do back propagation from the element, from the Crf layer back to all the Lstm layers. and then you can update the whole, the parameters of the model. And you have to do this over many iterations to actually learn this Lstm Crf model. So I'm really excited about this bi-directional Nstm, it used to be a very strong model before birth came. And of course, now we have a transformer before transformers. and I think some people are still trying to revive Lstm to see if it can be competitive with transformers. So I think we have a very good understanding of this. I think. it's really important to try to connect the old architecture we have been using for long in Nlp with a more new architecture, which is a transformer. Okay? To summarize today, we consider Lstm, which is the backbone of many modern Nlp tasks from language modeling in in the 2,015 for any Nlp task. It was like this when the planning came. End of the task. Everybody used Rnns. No, you use Lstm for all the tasks from pathosp, tagging to nameless recognition, to language, modeling to what sense disabilization. And then you can decide to have like crf to improve performance. So crf did not completely disappear then, and for vision task. Everybody uses Cnn, so every vision paper they just stick with Cnn every Nlp papers they stick with Lstms. And that was it. But when transformer case came, you know, we found that you can use the same transformer also for both language and for vision. Initially, it was for language demonstrated monitoring task and machine translation. I think the Vision Committee were curious if they can use the same architecture also for vision tasks. Maybe at some point they were reaching eating a bar, and for for the Cnn they were not able to improve performance, and then they try to integrate transformers also for the vision task. So Lstm Crs is very important for many tasks, especially if it's a sequence labeling task like a language model in predicting the next word, of course, for language modeling. You're doing this crf layer because you just want to predict the next word. But if you need a tag for every token so very good for for a very good architecture would be to use. and Lstm Crf. For name, method, recognition, participant chunking. And I would argue also that it's competitive to transformers. If you have time, you can read a hell more paper which I can type here. Hell more which actually combine. This by Lstm, to have a portrayed language model. So, Elmo, you can also fine tune Elmo, for your downstream tasks just the way you fine tune events model for a downstream task. From from the next lecture. Of course we'll be looking at the article structure of a language which will be taught by. I prefer to first.st Do we have questions? You have questions. So everything is clear. I'm trying to look at. Let's easy. Okay. Yes, there's no question. Okay? Yes. Shidan Javaheri: Sorry I have a question, and there's also a question in the chat. The one in the chat is asking, will we talk about Bert or transformers. Later, in the course. David Ifeoluwa Adelani: Yeah, I think we will talk about I I don't think we'll go into a lot of details, but there will be some aspect of Brighton transformers. Yeah. Shidan Javaheri: Awesome. Thank you. And then I also had a quick question on Slide 22. If that's okay. David Ifeoluwa Adelani: Think slightly. Too yes. Shidan Javaheri: Thank you. I just wanted to confirm for large, for for language modeling, which of the too many to many architectures is best. And why. David Ifeoluwa Adelani: So I think you can. You can use both right, but I think the second one on the left is better, because think about it. You know you have x 1 x 2 x 3. And for every x you want to predict a word that follows it. So do you understand what I mean. So if you have x 1, so there's no board here. But I basically on this red, you have x 1 x 2 x 3, and you want to predict. x 2 x 3 x 4. Do you get what I mean? So like this, it's very easy to. It's just a continuation. Shidan Javaheri: Sorry. So it's you. You have x 1 x 2 x 3. And you want to predict x 4 x 5 x 6. David Ifeoluwa Adelani: That is one way to think about it. Depend, if you have the context window. So you have x 1 x 2 x 3. But if the context is one, so this will be x 2 x 3 x 4. And then it goes on like that, right? So this is one way to to think about it. So for for the second, for the 1st one here. in the many. To many this is more appropriate for machine translation, because you need to encode all this information and then pass it into a vector here and then here, at the last stage, you need to decode right for a different language. Shidan Javaheri: So I had imagined that that would also be helpful for large from language modeling. Because then you have all the context of the whole sentence, but because it depends on the context window, you're predicting the next word given. Either the previous or some variation of that. The one on the right is better. Did I understand correctly? David Ifeoluwa Adelani: So for machine translation, you need to encode all the sentence information for language, modeling for every word you have seen. You need to try to predict the next one. So that's why the last one here is the better one. Shidan Javaheri: Thank you. That makes a lot of sense. David Ifeoluwa Adelani: Your role is in time, you are predicting the next word also alongside. Yeah. Shidan Javaheri: Thank you and thank you for all the lectures. David Ifeoluwa Adelani: Alright, thank you. I will still be back at some point like after every 4 weeks, of course. To have some more advanced talks. and maybe to talk about things like machine translation and crosslingual transfer. And so on. Alright, okay, thank you so much. And I will put a video online. So for people that are not able to join. Alright, thank you. Have a nice day.
