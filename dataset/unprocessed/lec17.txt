Hey! David Ifeoluwa Adelani: Hi, everyone airports. All right. Hey? Welcome to the I don't know. 7 cents lecture. I think we have very few number of people today. How was the midterm exam was good. Yeah. it was not enough time some people finish before time, I guess. oh, okay. yeah. From. When do we expect that we'll get the grades back? We depend on the availability of the Tas. I mean, this week. It's a conference you have an OP. So we will not be working on it this week. maybe next week. So I don't know the exact timeline. Yeah. hopefully, before before you go on break? Yeah. So you have the results. okay. yeah. Okay. Fingers crossed. I guess. Yeah, So today we'll be talking about the machine translation machine translation which is a very interesting direction. It used to be a very, very difficult problem. but now I'm not sure it's very difficult. But of course. even the current models doesn't solve the task. It's just very good at the general machine translation domain but there are some specific domains that even if you try it the current models may still fail. For example, it will still fail for low resource languages, it will still fail for some domains, for example, like medical domain finance domain, and very, very specific domains that maybe they care for. In the industry. So but machine translation has been there for so long. So initially. It was an interesting military project. In the era of the World Wars and and also it became an interesting projects. For example, there was some pioneering work that happened also in Canada, you know, Canada was bilingual is bilingual and some of the earlier couples that was used for bashing translation is actually based on Canadian parliamentary proceedings, which by law it should be in French and in English. So you have this couples that could be used to train machine translation models. Similarly, we have some European coppers like European Parliament also have. It's called Europe. So, which is very similar idea, because in the EU they have some official languages like English, German, French. Actually, I'm not sure if English is still part of it, or it's just a language that they cannot do away with but so they have this official language, and then they need to translate between all this set of languages. But since it's a very, very old project one of the earlier versions is the Ibm model. So this should not be very surprising to you, because Ibm is also a very, very old organization prior to Microsoft, Google and Facebook and open AI, for example. so I wanted to inform you that we'll have another reading assignment that will be released today. and it should be due in 10 days time. and this will be on machine translation. So hopefully it kind of bridge the whole methods with the new methods. And it can be quite interesting. And you populate new things from the renewal cycle. So the outline of today's talk will be Oh, there's another announcement. If you are supposed to take a makeup examination. This will be on Wednesday. I think, by 4 pm. So oh, 4, 30, yeah, that means you are taking it. So you know the time? Alright. So by 14. Yeah. alright. So machine translation, we'll talk about why, it is a hard problem inferability and superior wharf hypothesis. Then we talk about the vocros triangle. Then we'll start thinking about the problem by discussing the noisy training model for empty on the Ibm model. Ibm model is quite interesting because it's based on statistical model, basically just thinking about the probabilities that the word can be translated to the order by having something like a bilingual dictionary. What can you do with this? So if you have a bilingual dictionary that you can map everywhere to another. and then can you create like a statistical model for machine translation. Okay for machine translation. This is a German text, automatic text verified. So I like national processing. So this is an example of a machine translation? From German to English. And why is this a difficult problem? So in this lecture we have been taught about concepts like morphology syntax. semantics. pragmatics, discourse, and so so why is empty difficult? So because you could have languages with different syntatic structure, with different morphology, with different semantics, and so on. are pragmatics and discourse. So one good example is, considering the lexic lexical gap that could occur between different languages, so lexical gap, I mean the meaning of the word may not have a translation in a language. maybe that entity is not even translated. For example, you have commonly cited examples will be colors us in English. Maybe we have different colors, and you can even have different combinations of colors. For example, you could have a color like bonds, orange. which may not have a good translation in another language. or maybe the translation of Orange does not even exist. not to talk about mountain range, or do you just describe it somewhere, somehow. and you can have things like kingship terms and slacks. and and so on. So in this language. It does not distinguish between mother and father. so it's difficult to translate an entity or not an entity now, but a proper. Now. like moda or father, or in now rather mother or father in Chinese. So there's no meaning of broader. And also in my kind of I'm looking at my native language, and I was thinking, oh, do we even have translation of Broader? We have a translation of Elder by Yonga. but we don't have a term for brother. and I think, like Chinese. We also don't have a term for grandmother or grandfather. You have to say the mother of the mother, the mother of my mother, or the father of my father. So if you speak Chinese, maybe you can relate to this and then also some languages does not kind of They don't have plural and some languages they don't have. You cannot even say so for plural, for example, in our language you have to really specify the number to indicate Laura popularity. So you have to say 2 items or 3 items. So all these kind of things can have confusion if you are working with a very a statistical model. and this might make machine translation challenging. Also, there are more advanced concepts now, like formality. which is also important, is this is former term some languages are very strict. For example, in German formality is a very strict concept right? And if your translation model has only seen data that are very, very impolite, it could be a problem so that is an example. So we also have issues with pragmatic duration. some words such as again stop, or more. you suppose, or contain an assumption about the world. So if you have the word like mark called again. this already presupposes that my account. This is very clear in English, right? So because in English you can use this kind of words which already assume the presupposed information in the common ground between the speaker and the air. But in some languages this concept does not exist. For example, if you have phone Mac phone again again, I didn't know a phone. In the 1st place. This this word again, actually kind of mean slightly different things depending on the context. In this language our call starts me sets, which is, I think, a Canadian language such usage usages do not elicit a challenge from from the air. But in another language, this may actually have a problem. Okay, so we also have other examples, for example, in morphological examples where you have different levels of requirements of for inflection. For example, Ibansu language like Swahili. You have a lot of prefixes, and I don't know. Maybe in Zulu you can also have suffix. And then this kind of would change the meaning of the world, and then the the prefix might actually change present tense to past tense and future tense. So we also have languages with noun classes which a good example, would be the Bantu languages in Africa, where or something you can call grammatical gender. So every items like nouns can be categorized into different classes. So the way the prefix you will use for a person will be different from the prefix you will use for an animal. It'd be different for the preface we use for a car, we different for the preface you will use for a house. So it's kind of just different categorization of the nouns. So and also we have Syntac differences like what other differences? So some words, you know we have the X view. Are you aware of those things like as your subject verb objects. but you also have languages that use. Vso verb subjects objects. I don't remember it. Does Arabic use vso or sov sov vso yeah, correct. So so different languages use different structures. And sometimes, like you can write from left to right. And then Hebrew Arabic could do right to left, and then you have this. Things can can affect the new it can affect So your machine translation model. We have a semantic which actually is talking about the meaning. Our special relationships are grammatically distinguishable. And then you have pragmatics, where, depending on the context, for example. you might be interested in something like politeness. formality. And then the translation will be different. So there is a marketing translation tax that is called formality, where you could give a text in English and then give it a formality, level, formal or informal, and then it will produce the translation based on this information. It will produce different kinds of text. So and if you're interested in this, you can check this website called Walls world at philosophy. language structures which you should be aware of has a lot of information about many, many languages of the world in terms of their linguistic structures. Okay, I think. okay. see? Okay. okay. so in there are different languages require or allow different morphological, synthetic, somatic discourse properties. And they interact in different ways. We have different other linguistic aspects and non-linguistic aspects that actually refer the overall culture of the speakers of the language. So the 1st hypothesis we want to consider is a pure wolf hypothesis. I must comment that not everybody believes in this. So the pro, the the question is, is it even possible to produce a perfect translation. So some people have argued now that for you to work on any language, if you can create a perfect translation system, everything you're able to do in English, you can do it on any single language. Is this correct? So should we just improve on machine translation task, and then we can solve all the languages, all the tasks in different languages. Oh. is this not true? So it's something you can think about. So if you have a perfect passion translation system. for example, from English to Chinese or English to Hindi. Every task you can perform in English. Can you also do the same thing in English? Or there's some cultural differences that will make this impossible. Yeah, is it possible to have perfect translations within subgroups of languages? So, for example, between all 9 languages to have exact translations. And, for example, between French and Arabic, it's, is it possible? Perfect? Is the problem they'll come perfect is the problem. So you can have high accuracy, no highly accurate translation. but saying something is perfect. It's that's the complication. Because, depending on the context. So do you have a high quality translation between English and German. The answer is, yes, right so. But if I do translation of Lego documents. and if you read it, maybe a native German speaker. You might not agree with the translation right? So maybe you get a gist of it. But the formality is not there. The language. The the way the Lego language supposed to be written is different. And you know, Lego law is I mean Lego proceedings. Legal documents are different in different countries. I think there's I forgot, there's common law. There's all this kind of law that different countries adopt. And it might be a problem. But it's possible to have a highly accurate translation in a particular domain for saying, parfait translation. Yeah, I'm not sure. Yeah. So. But nowadays, if you ask many people. They will tell you, okay, maybe for some languages, machine translation is solved because maybe on 100 cases. if you can have 98% of the cases, you must agree with it. Yeah, you can. But yeah, cases that yeah. it was just something wrong is going on. So for sapir hypothesis. The language you speak affects your thoughts. The strong version is that language determines and constrains all human interaction and thoughts which is really really strong, which I will personally not agree with this. The language determines and constrains all my actions and thoughts. Which version language may influence human actions and thoughts slightly in highly specific ways. So very few linguists actually believe in a strong version of this. some language even just reject everything. So in terms of spatial organization, in a language called Quotayore use uses an absolute system. For example, they don't have things like left right ahead, and everything just is not eastward sound. So how do you do a translation like that? So if something says, there's a sentence, says the coffee southwest of the dinner plate, this is kind of that's strange. because you you would typically say, check the left side or yeah. And then you said South West. and maybe this coming from culture or historical, that everything they want to do they have to use the navigation, go left, right to the river, or get the fruit or something. and also in some languages in English, you do left to right in Hebrew you do right to left, and in and this other language good. Your you will use east to west. Okay, so for machine translation. Even now, unfortunately, we are not at a point of worrying about Sapia worth identity. Because this is a very you know, there's a theory. And there's practicality. Right? Theory is, can you even have your offer translation? But what we can do is to actually just minimize the complexity and say, can we just achieve translating about events and participants or just focusing on eye level concepts rather than worrying about having a public transportation every time. And the question is, how do we measure progress? If you're aware of text generation a bit, you might be aware of this metric called blue score. So this is one of the metrics that you can use to measure the quality of machine translation. and the idea is very simple. You have a text in English. You have a text in other language, say French. and you want to check Oh, okay. great. You have the text in English. You have the text in French, which is the reference. and then your machine translation model produce another one which we can call the hypothesis or the output. And then you want to compare this hypothesis with a reference. And how do you compare? You want to do like a matching of n-gram matching of the words right. and to see if they actually correspond to what you have in your reference. And one way to do this is using the bluescope so so the metric was introduced in 2,002, and the idea is that if you can have a metric like that just doing the counts. You can fix the engram, Count, to like 2 or 3 or 4, and then you can check how many of the engrams actually do much. And based on this culture, you can have a blue scorn. And then the the way we are going to use to measure. How good is the metric is if it aligns with human judgment. So yes. Why is it important to do endrams like, why can't you expect a number of words that are differentiated. the number of words unigamp. So you can check unigram. That is one. Your hand is one. Right? Yeah. But yeah, I think what we I mean, you can do unidram of unigram diagram trigram. So but typically, I think people use like public 2, 3, 4. And the reason is that. the way you evaluate how good the metric is is to actually give it to humans to judge how good the metric is. and the way you do that is to produce a correlation between. This call that the the metric gives you and that of the human. So if you ask human to rate the quality of a transition from 0 to 100, and then you have a metric giving you a value from 0 to 100. You cannot check. Do they correlate with each other for every sentence you have tested. or your evaluation sets. So if they correlate, that means the metric is good. if they don't correlate, that means the metric is bad. So when blue was introduced, they also do the discretion surprisingly. it has a high correlation with human judgment. But this doesn't scale to if you transfer this to other languages. So it's worked very well in English. So it's kind of the 1st size was created was created for mostly Latin based languages. So if you move to a language that use a different script like Hindi even siri exclude like Russian. Then maybe something doesn't work again. So blue is actually focused on what is called precision. It's precision oriented for each engram in the proposed translation you have to check if it if it is found in the reference translation. if it's found. Yes. and this is why it's called a precision based so and in practice, blue incorporates an additional brevity, penalty and a geometric mean over several values of N. So in practice, what we do is that we don't just focus on one end like, which is a question you mentioned? We we do for N equals 1, 2, 3. And now we do a geometric map over it. So but me blue is not the only metric that is available. We also have other metrics another one is called material score which is actually a 1 to one match very similar to what you're talking about. and between the output and the reference. Then we have another one called translation Error Rates which is the number of edits required. Are you aware? Are you familiar with edits? Distance? Something like, edit edit. Distance where? Which signifies, how many edits do you need to make to this text so that it's going to look like the other one. So this is the tier. So if I edit the the hypothesis. What's the distance to that of the reference? This is what you're trying to measure. and a more popular one these days is what is called character. f. 1. Because for some languages you need to measure things at a character level. especially for morphologically rich languages. If you use blue score, it doesn't correlate at all with human judgment. And then you need to use metrics to actually consider the character, level information. and also languages that use their critics, languages of different scripts like, if you use languages that focuses more on character that works like Chinese. They even have their own specific character and tokenizations. So character enforcement. Okay? And more recently, we moved on to an embedding, an embedding based evaluation you have probably seen a bird's model in this lecture. I don't know if you have seen this so very similar to the birds. Architecture you use like a multilingual birds model, and then you can train an estimator model. So an estimator is you share the model, giving a pre-trained model like Bert's model. you send in your hypothesis, you send in your source text, and you send in your reference text. You extract the different embedding. If you send in your hypothesis, you extract the embedding you send in your source text, you extract your embedding, you set in the reference text to extract the embedding, and then you concatenate. The embedding together are connected to the linear layer using the feedborn neural network, and then you can train based on the miss. So this is a regression task. That's why it's changes in Miss Square Arrow. And based on this, you can actually create an estimator that you can use for estimating the performance of a machine translation task. Do you have questions? Yes. By the way, maybe this is useful for some of the projects that you'll be working on. If you're interested in machine translation. you typically expect you to evaluate a statistically based metrics like blue score or Chrf and an embedding based metric like Novid scope. Yes. the number of edits one is it where it is when you're making the edits, the edits that you're making? Yes. workplace, right? Or what's the approach? So it could be word by word. Yeah. For example. yeah. okay, so our developers are triangle. So this is like a triangle to describe different ways. You can do translation at the bottom. You have the soft surface to the target surface. How can you do a direct translation from a source text to a target text and for direct translation. This is like a lower level way of the machine translation. This is based on things like using bilingual dictionary. So if you have words in French, you have the words in English, can you just do an alignment to the words and then create a machine translation. The other one is. what if you have languages that have very they're very similar or very similar synthetic structure, if it can modify the synthetic structure a bit. Would you be able to do a better translation right? Or do you need to include semantic information? And this is the higher level. So so if you can get, if you can take the source language to an Interlingua. then you'll be able to do a translation to retarget cycles. So this is the theoretical concept, and we try to see if we can have what? What would be the interling in our case if you want to use like a neural based machine translation. or it's a statistical based machine. The early efforts early Mt. Researchers developed a set of bilingual dictionary rules to map from one language to the other. and there are some. Some of these dictionaries have been created by many linguists as well, for example, even English to including very low resource languages like people. Just create this bilingual dictionaries that you can see if you go to linguists language, libraries, or something like this, you are going to find a lot of these bilingual dictionaries. And the question is that can you use this to to translation? So the Interlingua is actually a conceptual space common to all languages, that if you can take the source text to this intelliga, you'll be able to do the translation and the advantage is that you can use to develop a general empty system. For direct translation. implies a system. a system that is trained on and works for a set, a set of specific pairs of languages for direct translation to happen. You need this bilingual dictionary, because without this information you cannot do this alignment with Interlingua, adding a new language only requires translating into into the Interlingua. So, for example, if you have it costs a space that you can map it to one example in deep learning is. if you want to do translation, you have to convert every the entire sentence into a single vector and then send it to the decoder which is going to decoded to your, to the language of your interest. And if you have a multi-way machine translation model. That means all these different languages will be sent to the same space which by which you cannot be decoded. So the disadvantage is, what should an interval look like they? I mean, at that point. This is just like a theory that has not been implemented. This is prior to the planning right? So it might be difficult to work with such an expressive. but maybe now it's a little bit possible. Alright. So for the statistical machine translation, which is what we're interested in. The Ibm model is a statistical model. and the idea is less as though we want to translate from English to Russian. This was developed by assuming a noisy chatter. cool. and I don't know where this this is coming from. When I took. When I look at an article in Russian. I say this is really written in English. but it has been coded, is some strange symbols. I will now proceed to the code. So probably just think about the time of of war, or something that you want to decode something using a noisy track channel. So you can look at a machine translation like a form of encryption and decryption. So you have the original text, and then you encrypt it, and then you send it through this channel. and then you need to. Yeah, you need to decrypt it to get information. So suppose we are trying. This is like a question to you. Suppose we are translating from Russian to English. Which of the following is correct. using your knowledge of base? Yes. the 1st or the second one. Yeah, the second one. Why I was speaking about like the model of the previous page, that. yeah, he is the prior. So yeah. so you need to compute. I'd like to. Yeah, you are okay. Yeah, because we don't know. Pof, yes, yeah. okay, that's correct. So the pov is your prior, which is also your land model. and then the P of F given E will not be your translation model. So if you combine this, you can actually use like a very basic statistical model also for translation. So the key thing is that we need to do an alignment. You need to do match one word to another. and then you can now count the number of times the world in the. So so suppose you have a lot of text in English and in German or French. and then you want to count the number of times that this ward is associated with another world because they always call together. You can use this to actually create an alignment model to do the translation. So you want to train a model probability of F given he with probability of the source given the target. We call this the word alignment model. So you have a text in English And another text in French, which consume the same text in 2 2 languages. Canadian answered, which is like a parameter debate in English. And then you have a text in English. You have the text of French, and then you want to see okay, like Canada is equal to Canada here. and then maybe my French is not good, but you have some. So what's that attribute? And then you can do count? Yes. so it's like you're creating like a generative model. So you have to do all the counting. So something like you remember the way we did part of speech. All right. Yes, so so when you do the counting, that's the way you will do the training of the model. So it's kind of supervised because you need a lot of text and a translation. But it's kind of accounts based model right that you use to train the alignment model. Okay for the sentence alignment. There are a lot of tricks you can use. You can make use of various tricks to get sentence alignment. like the sentence lines, the cognate words. for example, if if language use a similar autography, so an example of a cognate word is like, for example, reference and reference. You know. you will know this is reference, right? It's just that it's it's now in French. because the autograph is kind of different. and then you have something like metric and metric. So metric and metric is just written differently. but they have the same roots. World. Because most of this language, for example, the Latin days English is not Latin based, but it borrows a lot of words from Latin. and then you have metric in French as actually has something similar in English. So these are examples of cognit words. You can try to use tabs to send sentence lines longest common subsequence of characters. and then you can define a similarity function between the sentence and some of the words. Can you can actually do one to one translation based on this similarity. And before you actually do any fancy account of mines. and of course, you can also use some advanced dynamic programmability. Like the dynamic time working, which is for any distance. So for the water alignments. even after the sentence alignment we do not have words that are aligned. and then factors to consider for the word alignment. You have to think about things like possibility of translation. Many, too many mapping is impossible to do the mapping directly. Another thing that is very important, especially if you move to different languages is the word order. So you can have a word that is beginning in the sentence in one language. another language is in the middle was on the hand. So this kind of may make word alignment a bit difficult. So let's play this game. How many of you are aware of the linguistic Olympiad? So you know the Maths Olympiad. Okay, that's good. So. So we also have linguistic Olympia. So the idea of linguistic Olympia, this is very simple. It's really fun. You can do it. So basically, they can give you 2 languages. and then you based on your understanding. They can give you a language or 2 languages. And then they give you rules of the language, and they ask you to perform a task in a language so given some rules. can you translate from English to maybe a Canadian indigenous language, that you have no information about, and they give you. They provide the rules based on your on your linguistic knowledge and the rules that have provided. And you saw the task. So this is a very simple one. So so he is language in East Africa that is morphologically rich. So that means you have prefix beginning the world. And so you have a single word. Which can I try to translate a phrase in English? So let's decode this. If you have all these words in Swahili, and this is the translation in English. What are the Swahili morphines for play? This is this should be straightforward. So you have to decode the rule here, right? And they told me what display? Yes. so she's a yeah, that's it. That's seems to be correct. Yes, all right? I understand. Yes. Yeah. And cook, I don't speak soilly. So we can have fun today. Yeah. Cook. Yes. Pika. Why, pica. why not papika or Apica. What? I don't know. I'm trying to confuse you. Maybe you're correct. Yeah, I think Peter might be correct. Yeah. So what if I you? These are more confusing. Yeah. I also don't know the precise answer for this one. So what is high, based on what you can decode what? And and I, yeah, that's looks correct should be correct. Are you? Okay? Maybe this is more complicated. Yes. like, well or so, maybe it's just you. Yeah, it's possible. And then you have we? It will only have one example, but it would maybe be included. Yeah, it may be truly okay. You get the idea. So you pass this Olympian exam. So I think I provided some for a solution, but not for everything. And then, if you look at the because morphologically, return is, they just keep attaching, so you can have a very long world, and they just keep attaching, prefix, depending on what has been added. Yeah. but the roots what will still remain constant like. Understand? Even if you have understand, understood. And then it's not very clear what changes if you move from understand to understood. So can you. Decode is what we change from, understand to understood this. Think like the past tense uses we, and the current tension is now. does everybody agree with that? I mean, your answer seems very possible, because that's the only thing we can say here. And you have to remember. I don't speak soil. I can ask my soil, speaker friends. if you can decode this. But yeah. alright. So for the Ibm model one. So Ibm developed a series of 5 differential models that make increasing powerful assumption. I think we are going to do more than one or 2, and then we'll move to more neuro-based, empty. which will also be the topic of your reading assignment. And model one is mostly basic. So you know, each word. each source word is aligned to 0 or one target world. Very basic assumption. So you don't try to model different distortion of water. You just say, Okay, every word maps another word. and then you don't try to model likelihood of fertility like some phrases like, Take a walk you don't try it, but maybe this is more complicated for a morphologically rich lover like Swahili. because everything is just attaching it. And if you do this kind of an alignment, so you have a no node allows. What's an F to align to nothing in E. That means it's possible that you don't have a word to word matching, or a word in that sense. So some we can align. So d is probably in the French. And then you have these 2 words that kind of mean the same thing, and calling and the mandate sorry I didn't. I cannot pronounce French words very well at this point, and then you have different word to word mapping, and sometimes you find out that the order is different. Right? So you see that standardized in French is coming before the containers in French. so you can do the world toward mapping. And then, after you have known the alignment, so you just produce the indices. You know you say one is aligned to 1, 2 is aligned to 2. But now, the next alignment in French goes from 3 to 4, and then you have. or Lee the L with apostrophe is null, because there's nothing is attaching to it. And then you have another word standardized. which is common the 5th world is now going to the 9th world in English, and so on. Okay. And since each word is, you're aligning 0 to one, so the length is the same. So the length of your alignment model is the same as your target model. Yeah, sorry as your target text. So if you want to formulate it. which by now you should be familiar with this kind of formulation. Probability of, let's say, a French. What given English so you can materialize over the alignment which is a and this one to one based on probability theory. And and then you can also oh. use the Bayes rule to say. this is a joint probability given E. And this will give you the probability of F. Given Ea. Multiplied by probability of a given E. Joseph is in the past room. and based on this. But the question is, how do we compute probability of a given E, so of course we are make we are. We make a very simple assumption. And one way you can do this computation is just computing what is the uniform probability of the translation lines. or you can also compute the uniform probability for each possible alignment. And this is one way you can use to compute it. So this is a very strong assumption. I think I can. Sorry I cannot give more details, because this is just the I'd be a model that I'm trying to split at the moment, which is just the formulation. They are provided so. and in this case. probability of a given E is mostly concerned about the translation lines at the moment. So because the assumption here in the other slide is that you want to align every word to 0 or one target. so the length has to be the same. So the next one is probability of F given e comma a. And here we can actually use compute this using Emily. So the probability of F given E. And the alignment model. You can just compute it if you have a bilingual corpus using mle. and the idea of using Md. Is just counts the probability that this word occurs both in English and French, and then you normalize by the count in English. using the number I made. and after that you can multiply it. Oh. with each other so! But the question is that let's assume you don't. You don't have. you cannot use Mle. What are you going to do? Right so for my lecture? Also part of speech, when we are not able to compute it? The counts, what did we do. We use em already? Right? So with which you can start with the initial counts in, run at random, and then you try to modify the accounts until you are able to find a true distribution. So for the expectation maximization, you initialize the parameters. Probability of A. F given E. Randomly, and then you do the SE. Step and the M. Step for the E step. Given the current parameters to compute the expected value of the Count of F of E over the training data. And for the M step, you try to compute a new value of probability of F given. E, okay, so what's the probability of the alignment? So here, if you look at our formulation. we have probability of FE, which would have the composed probability of F given e comma a given E. We have computed the probability of a given E. We have computed the probability of FE. Given a. What is the probability of a given ef so probability of a given ef possible can be decomposed this way, using the Bayes rule which is the pro the pro the product of probability of a EF. Given probability of EPE. PF. Given E. I can try to write it on the board. But I'm just gonna write a second right of. And after that she can marginalize over a. So if you imagine a light over here, you're gonna get the one I'm talking about. Okay, maybe I should just write it. If you want to compute this, this will. This will probably so ef and they will pass to each other. and then you can. Now this one can be expressed by marginalizing over a do you have question? Yes. when we have a sentence. And we're aligning English word or away from the line already have basically. what does it mean to marginalize over all the alignments? So there, maybe there there are different ways to do the alignments, basically. So you can follow different rules in doing the alignment. And you can marginalize over different rules by which are doing the alignment. So in order to do this, we need to predify all possibilities of ways of doing. Yes. depending on the language. You have different ways of doing this. So it would be a language based rules and not dependent on the length of the sentence. The length of the sentence is also one of the things we should consider based on what we talked about. So a and all its possibilities consider both the language and the length of the site. Yes. it possible to give an example of how? I think there might be an okay. There's 1 example. but here we are. This is probably just using em a guardian. And here you have an English sentence and a French sentence. and then you want to compute this translation model from F given E uniformly so here. What is the probability that mansion or mason is the translation of red? This is what you want to complete right. What information is the translation? Or is the translation of red? What's the probability of this? But this is a very actually, this example is probably too simplistic, because now we have exactly the same value. But if you run this multiple times using the em that you might be able to achieve to go to the to the more plausible counts that you're expecting. So for example, here, what I mean, if you just want to compute mle for this. This is very straightforward. This is the count of how many times you have Mason and Red together. both in English and French. and you'll see that they are called together in the 1st example. So the idea is that you just have a sentence French. You have another sentence in English. You don't know the translation. You don't know which word is the translation of the other, and then you just can't. How many times do you call? Now think about having a lot of sentences. let's say 2 million sentences with different translation. At some point you are going to see a different count for for what you are expecting and based on the count. How possible? this word is the transition of the other is going to have a higher probability. and then you'll be able to do the translation based on this. and then you can create different alignment rules to actually match to compare the sentence in English and sentence in French. and then you cannot apply the Ibm model, which is just based on the statistical model. So but in practice you don't initialize the translation model of F, given E. Uniformly given reasonable sizes of lexicon. Too many parameters. too much memory and computation. Rather, you tend to restrict it to what Ps based on how it is aligned in the set experience. But as we'll see in the next class, there's a way we can adapt this so that you can have something. There's more like there's slightly better. And this would be Ibm model 2. And after that we're going to examine the last model which would be neural based. I think the neuro base is a little bit more interesting than this Ibm world. because you are likely to use that now than the Admin model. and also because the Ibm model often gives very poor performance compared to genero base model. Okay. thank you.
