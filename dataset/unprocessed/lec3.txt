Yeah, yeah. None. None, None. Yeah. No, None. I. Hello I'm trying to fix this thing and it's not working. None. None, Yeah. Hi everyone, sorry for the delay in starting the lecture. So who can remind me what you have been taught last week? I think last week was Jackie, I probably introduced you to test classification. So what's the difference between supervised and unsupervised learning? Yeah, Sorry, I didn't post it this morning, but I will post it after the class. Yeah. All right. Yes. Maybe I have to speak louder. Can you hear me? Is it clearer? OK, Yeah. So what's the difference between supervised, unsupervised? Yes. Yeah. Thank you. So basically you don't have output for the unsupervised. Thank you. So what's the difference between classification and regression task? Yes, OK. It's continuous. Yeah. All right. So I think you probably also did some feature selection, how to come up with features for your classifier. So who wants to comment on why do you need things like lemmatization or stemming one beauty features? Yeah. What do you mean by non necessary features? Yeah, exactly. So you kind of want to reduce the number of types that you have so that you have like more compact vocabulary. All right, So today we'll continue with, we'll start with linear classifiers. Linear classifiers. So yeah. So last week I think we described classification where you have an input X. So an input X can be a vector, can be a matrix, can be anything. Then you want to learn a function F that will give you the output Y. Output Y can be a discrete outcome or categorical variable. For example, if you want to know you want to classify a particular sentence into spam or non spam so you can learn a function F, It could be a simple function like linear regression. I think we discussed linear regression last week. Or it can be a more complicated function like a deep learning algorithm. Like it could be like LSTM. It could be a transformer architecture. But the idea is very simple. Basically you want to learn a function. It could be as simple as linear regression or more complicated as artificial neural networks. So other examples. For example, you want to detect if a movie has a positive or a negative sentiment and for clustering you want to just give it an input. You want to cluster the data into different categories. So I think we already differentiated what's the difference between classification and regression. So let me ask a question. What is the use of training set, a validation set or a test set? Why do you need this? This is statistical. I know you may know this probably before, but yeah, that's what is that validation or test set so they can generalize, not sure. OK, you want to try validation set? Yeah, that's would be the test set. Yeah, yes. Test your model or yeah, learning function. Yeah, thank you. All right, so quickly, let me go through cross validation. So let's assume you have a very small data and you don't have you only have training data. You don't have another split for test data, another split for validation data. What can you do? We have a principle called cross validation which is also used for model selection. And the idea is that you can split your training data into different chunks or subsets. And then you can, let's assume you have like 500 samples and you have like 5 fold validation. And so you can split it into like 100 hundred hundred hundred 100. And then you can train on the 1st 400 and evaluate on another subset. Let's make it very simple. Let's assume we have like threefold cross validation. You can split it to generalize. You can split the training data into K folds and then test. Or let's say you split it into K folds. You train on K -, 1, and then you evaluate on the last one. So the idea is, OK, let's say for the first fold, you pick the first fold as your test set and then you train on fold 2 and fold three. You combine them together and train. The experiment too is that you train on fold one and fold 3 and then you evaluate on fold 2. And the last one is that you can train on fold one and fold 2 and then evaluate on fold 3. And by doing this you can now aggregate the test accuracies and then try to pick your model parameters. It could be you're trying to pick an hyperparameter and then by doing this K4 cross validation you'll be able to choose what will be the right hyperparameter even if you don't have a validation set. Is that clear? Is this simple enough or do you have questions? Yes, what? Do you mean by a? Hyperparameter. OK, great question. So when you are trying to learn a function like this, typically you have X, Theta. Theta is all your set of weights or your parameters you want to learn in the model. And some parameters cannot be learned automatically, you have to fix them. So in the case of SVM, there are some parameters that you have to just fix and you don't know this parameter, but you kind of theoretically know the set of values that this parameter can take. So it could take like between maybe 0.01 and one or not bigger than one. So since you are not sure, you can try to tune your model to pick what is the right hyperparameter. Should it be 0.01? Should it be 1? Should it be 0.5 S? And then you can have an equal split of the range of values between 0.01 and one. And then after training your model on using this different hyperparameter, you can choose what would be the best hyperparameter in that case. For example, if you are training a simple spam classifier for SVM, we have a hyperparameter and then you can say let's set this to 0.01. And then you train for the K fold. And then you have the average error for that hyperparameter. You try the second hyperparameter, you try the third, you try the fourth, you try the fifth, it could be 10. Then you select which one has the highest accuracy on this, and basically you can now pick that hyper parameter as every parameter you're going to use to train the entire model. Is that correct? Yeah, yes. Or roughly equals size because. Yeah. So I mean, it's going to be different model, but you're going to just train on the same data. Let's say you trained for the experiment one, you're going to have Model 1. Every training results into a different model. It will be different models. But the most important thing that you're evaluating here will be what is your validation accuracy across the different experiments which you are going to average for a particular hyperparameter. OK, I hope that answers your question. Any further question. OK, you have to set it as a person performing the experiment. And if you don't know the hyperparameter, you can have a set of values. So every method have some theoretical back in and they have some hyperparameter that needs to be turned. In the case of neural networks, you have things like learning rates, but you don't know what should be the right value of the learning rate. So you have to give it different set of values and then try it out to determine which would be the best hyperparameter. OK, all right. Yes, yes, you would choose the model, you would choose the hyperparameter with the best validation accuracy, yes. All right. OK, so this is a very small example which you can try. How many people are not familiar with SKLN? OK, It's used to be very, very, very popular, like let's say 10 years ago. Because the best models we have, even neural networks, sometimes they still struggle on some tasks. And you could, it's a very small package which you can install. And then if you have very basic tasks like test classification tasks, for example topic classification or sentiment classification, you can easily run this package on it. So. For. And then you can actually practice some of these things you have been taught because #1 if you have a test input, you have to determine what will be the features. So you have to have a vocabulary. You have to use principles like TFIDF, which is a way to normalize your data so that you don't have unimportant tokens. I think last week we also talked about things like stop words, and sometimes you can actually exclude these stop words because they don't influence the performance of your model. So this kind of package allows you to try out different feature selection. But nowadays we don't really do a lot of feature selection because we learn a big model like neural networks that actually tries to learn the features automatically given a text. It just tries to learn what are important features based on this. So this is a very simple way. If you install this package SQL line and then you have a very small data X here you just probably have two features 2 dimensional and then you have this output which is either 0 or one and then you can fit the model and predict. So this will give you the logic and then you can add a softmax on this to determine what will be the probability. OK. OK. Today we'll talk about how to train a classifier on a training set. But today we are focusing on linear models. I mean, last week probably this will be another revision where we already have like feature extraction. You can determine the features you want to use for a particular text, and then your inputs can be a document or a sentence, and then you have a document label. OK, so if you think a little bit abstractly on this, your function can be any classifier. So in the 90s or 80s, people have developed so, so many methods. One of the most popular methods in the 90s rather was support vector machine and became so popular that people even didn't care about neural networks because neural networks was not making progress for so many years and they were making a lot of progress with support vector machines. And one of the reason is because it's convex. That means you can actually get the true parameter of your model and then you can prove it. And if you are mathematically inclined, you like something that you have an exact proof and you know how your model will behave. So support vector was very popular, but also it has limitation that it can only work very well on very basic tasks like test classification. And that's one of the reasons why we have this kind of packages like SQL. Another popular technique is naive base. And for so many years it was very difficult to beat this simple baseline of naive base. But nowadays I think you can fairly beat them easily. But I will argue that even if you build a classifier for a very basic task, you should still have some of these baselines. Maybe you don't need to build an LSTM or a transform architecture to have a 90% accuracy, while if you just train naive base you can already achieve that score. Here's a very simple task. For example, most test classification data set like sentiment or spam detection. If you know about the Oron is it Eron data set, it's a very popular spam detection data set. You can even for using these basic methods, you can already achieve close to 98% out of 100. And then this task was not very trivial. So I would encourage you to also try this out. Although maybe you don't need it so much these days, but it's good to try out. OK, so for naive base, like I said, when you're learning a model, you need to add like a parameter teeter. Teeter can be all the weights you want to learn. So Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. So if you have a very, very simple logistic regression classifier, you just need to learn things like what is the bias of the model and things like that. So even for this kind of basic model, you still need an, you still need a set of parameters. So the idea of training is that you want to select the hyperparameter that minimize the error on your training set or maximize the likelihood on your training data. So pay attention to this. You maximize the likelihood, but you minimize the error. So likelihood is like you're learning a probability, what is the best probability that can fit this data? So that's the idea of likelihood and the probability, the higher the better, the closer you are to one. And for the error, I mean, the lowest you can get is 0. So if you continue to minimize your error, then you can get a very good model. OK, do we have questions? OK, yes, Theta is a parameter. OK. So everyone is familiar with Bayes rule. OK, so I believe the first equation here you can do the math the probability of Y given X&X. Here you have to look at it as a vector or yeah at minimum at the vector because X can be the set of features. If you are trying to do like a spam classification, every feature can be like the types. If you lemmatize then different words, like let's pick an English word, use, and then you can have different variants used using and all that will be converted to words to the root word. And then you have use and then you have different features or different words that can be categorized as your features in other tasks. Let's say I'm going to give an example that is more concrete, but I want you to look at X as not just a single variable, but like a vector of the different features you want to use to learn your probability of Y given X. And then if you do the math, so the probability of a joint of a conditional probability would be probability of the joint divided by the one that is conditioned on. So this is a very simple Bayes rule, and if you don't know it, just master it. It's as simple as that. Probability of Y given X is probability of XY, the value of probability of X. Then you can rearrange this using the same idea and then you can now say this is probability of Y multiplied by probability of yx given Y divided by probability of X. Is this equation clear? I can use the board if it's not clear, so it's just a way of rewriting the same thing. Let's start with the drive probability of a. This can also be written as this, so if you just now replace this with XY, so I believe you'll be able to see it. So here we have so see this as the. Rule you need to follow. So can you all see it? I don't know if yes. OK, Yeah. So see those two things as the rules you need to follow. Probability of AB equals probability of a, divide given B, probability of B, and then if you do the math, probability of A given B will now be probability of the joint AB divided by probability of B. And this is the exact thing we did here. So we just changed the variable. So probability of Y given X equals probability of XY, probability of X. Yes, they are exactly the same. And that's the same idea we used in kind of rearranging it here, so that you now have sort of saying probability of XY will not be written as probability of Y, probability of X given Y and then you still have probability of X below. Probability of X is estimating the entire probability over all the set of features. But for the naive base, typically we don't estimate this probability. The reason is because this is constant when you are trying to pick what is the best, what is the best Y given X. I'm going to explain a bit further. So naive base can be seen as a simple generating model and I'm going to show you the data generating process in a minute. The idea there are two assumptions, but the most important assumption is this independence assumption. So for each sample, that means for each sample of Y you generate a vector X by generating each feature independently conditioned on Y. So that means every feature of X is independent. So that means if you have X to be 5 different features, they are not dependent on each other. The way it's been generated is like this. So X is a vector, you have X1X2 to SK and then a probability of XY is now a probability of Y and the product of probability X given Y. And you are able to do this because they are independent. Just give me one. So this is the original formulation for. This is a set of variables, let's say X1 to X5. So this time we written as probability of X1X 2X3, let's say to XI given I. So the independent assumption says if two random variables are independent, probability of AB equals to probability of A. Probability. Of B. This is why you say. They are independent. A&B are independent. This is what it means if you say A&B are independent given C and you assume conditional independence. So this would be probability of a given sorry, given C multiplied by probability of B given C and then. If you do the math so. This would be probability of Y, probability of X1 given Y, probability of X2 given Y, and so on the probability of XI given Y. Is it clear? Yeah, OK. So and this is what we call you assume independence between the variables. So then basically you have a product between probability of XI given Y and it's a very strong assumption which doesn't hold everything is that you assume all the features are independent of the output. Who can give me an example of where this will not hold this independence assumption time series and you appreciate? Yeah, because it depends on the previous thing. Another example, yeah. So because they are related. So if you assume this independence assumption, it will fail for some applications and that's why people decide to work on better algorithms for other data sets. OK, all right. So here for the naive based model parameters, the parameters to the model Theta consist of what we call the prior class, which is probability of Y. We call this prior class because although we want to find the probability of Y given X, we make an assumption that we are generating this data as a generative model. That first you have less as you want to train a data, sorry, you want to construct a data. The way it's been constructed is that you have the label and then you provide an example. So it's like you want to create an example of a spam e-mail and then you say, OK, for spam give me an example of a spam e-mail or for no spam, give me an example of no spam. This is the generation process. And for the spam e-mail that has been generated, you can now say each of the words which are features are independent. So that means every word in it can actually lead you to predict that this is spam. This is the idea because you are saying that they are independent of each other. So that's the data generation process. And because you start with Y, we call it the prior class distribution. It's still prior. So parameters of each feature's distribution are conditioned on the class. And this is the probability of each feature XI given the class. So if you have like a discrete data, we assume that a distribution of P of Y&P XI are given Y are categorical distribution. This is a very simple assumption. So for categorical distribution, an example, a very good example of a categorical distribution, if you still remember your stats probability theory is Bernoulli distribution. The idea of probability is that you know that the probability must sum up to 1. So for a categorical random variable, it follows this distribution if it can take one of the K outcome each with a certain probability. So if you have, if you are flipping a coin, this is a very good example of a Bernoulli distribution. It's called Bernoulli because you have just two outcomes and then you perform this experiment once. That's a Bernoulli distribution. If you perform the experiment multiple times, then you have like a binomial distribution. But typically you can also assume that this categorical distribution is a multinomial distribution because you don't have two outcomes, you can have more than two outcomes. So for natural language processing our outcomes, we assume that we have very, very large features, which is the size of our vocabulary and the size of our vocabulary. For some applications it could be like 30,000, for some we could like 100,000. It's like all the possible combination of words that you have in a language. So it's like you want, if you pick an English Dictionary, whether the one from Oxford or Cambridge, all the words there is like this is like a dictionary. And this will kind of signify all the features that you need to estimate the probability for. OK, how do we train a naive based classifier? So the idea is that we have to compute the likelihood. Like what is the likelihood on fitting? The idea of likelihood is that you want to fit on all your training data. And that's why we are computing this sum. I mean this is a product, but if you put log it to be a sum, you are computing this product over your entire data set. For every X&Y you pick from your data, you want to estimate what is the probability of the joint probability. That is the likelihood. If you assume this independent assumption for each of your data, then you are going to have the product over all the data. Multiply the products you have on the individual which also consists of the products across all the features. Do you have a question? I think if you reuse this formulation on the board, I think you will be able to do the same. It's just read it multiple times. OK, for a categorical distribution, if you really want to train your naive base classifier, you want to say what's the probability of AY as taking one of these values? Let's say what's the probability that Y takes a value of spam or Y taking the value of no spam for this kind of binary classification task and the same thing. You are going to do the same thing for this discrete probability, which is what's the probability of X taking the value of each of the features given the value of the Y you want to predict? Yes, OK. Yeah; Is just saying that you need, you need to find this parameter of Theta to estimate this joint probability. If you can find this Theta, you can estimate the joint probability of PX, Y and that's why it's not together because if it's together it means something else. Yeah. And for the training, the idea is that for every after you have trained the model, you compute the likelihood over the entire data. You can now predict a new. For a new document you want to classify or for a new sentence you want to classify, you just reuse the base rule probability of Y given X equals the probability of the joints divided by P of X. And then you assume that every Y we pick a value whether spam or no spam, which is our running example. And then you also have different values for the features and then you marginalized over. So to calculate P of X, the normal thing you are going to do is to marginalized over all the random variables. There's another rule that I have not written on the board to marginalized is you marginalized over the joint probability. So that's the how to marginalized. That's a very simple example over the joints, OK, so and that's how you apply the base rule. Remember all the features are independent and I'm going to take you through a running example. Maybe this helps to clarify things with an example. OK. OK, this is an example, yeah, probability of PX1, Yeah. So if you have if Y can take 2 values. This could be probability. Of XY equals to 0 plus probability of XY. Equals to Y. Yeah. OK, I know it's a bit a lot of content but this is an exercise. I want to see if you can attempt it. Then I will show you the my own solution. So I will give you like 5 minutes table of whether a student will get a or not based on their habits. You have a nominal data and a Bernoulli distribution. Bernoulli distribution I told you has two values. Is that a yes or no? Is that a yes or no? So you have if a student reviews notes, does assignments and asks questions and this is the grade, So what is the probability that this student gets an A? If and that's the last one, it doesn't review notes. He has not performed any assignments but always ask questions. Using the knowledge of Bayes rule, can you compute what would be the probability of Y equals grade A or not A given these three features which is review notes, does assignment and ask questions. Do you have a question on the task? So that means you have to compute. Basically you have to compute what you have below. What's probability of Y given X, which is the product of probability of Y and the product of the conditional. What will be the great for this student? You can get a probability of X if you use the marginal, but you can ignore it. It doesn't affect your results because it's constant. So probability of X is the marginal distribution. So you just have to sum over the joints to get a probability of X is a vector of the features. So in that case you have to compute the joint probability of every X. Sorry, OK, I get what you mean. Yeah. So typically for naive base, it's not that you cannot estimate it, but typically you don't need to. Why? Because you need to compute this over every single features and the calculation for every feature is different, right? So, but I can assure you that for this task, even if you don't compute it, you will still get your answer. All right. So I think I don't know if anybody was able to solve it. Yeah. What the answer is the grade. So what's the grade? Is the a or not a? OK, OK. How did you arrive at not a? So the two probabilities, if you can give me the probability of of a given X and probability of not a given X, what are your probabilities? OK, all right, I will give you an int of the answer. So the idea is that for you have to compute what is the probability of Y, Sorry, what's the probability of grade of y = a given? Review notes, right? And then you compute what's the probability of y = A given those assignments, probability of a given as question. So the internal answer is this. And if you apply this rule, basically you want to compare these two probabilities, right? Probability of y = A and probability of Y equals not a given X, right? This is what you want to compute. Then you know that if you do the math, this probability of y = A equals probability of X, which is a vector given Y equals a. The idea is that if you do the math, for example, what is the probability of what's probability of review notes? What's the probability of review notes given grade? A. OK, let me start from the basic. What's the probability that Y is equals to A and this 3 / 5, right? Yeah. And what's the probability of not a 2 / 5, right? So what's the probability of review notes equals yes, given grade equals a what, 2 / 3? Yeah. And then what's probability of DOS assignment? What's the probability of DOS assignment given grade equals A2 over 3? And what's the probability of ask question given probability of y = a one over 3. So we're able to compute the probability. Yeah, yeah. Because once you're supposed to compute is not yes, yes, yes, all the time. What you're supposed to compute is probability of. It doesn't review notes and probability of no assignment and probability of ask question, right. So if you say, what is the probability of? Yeah. So if you say, and this is if you compute these individual probabilities, yeah, yeah, 2 / 3 should be 1 / 3, OK, I think. Yeah, this should be 1 / 3, and then the other one should be 1 / 3, and the last one would be 1 / 3. So if you say, yeah, OK, that's correct. So you're going to have 1 / 45. So what's the probability? The probability? Of. Dozen review notes, Probability of dozen review notes is if you pick the first tree, the first tree lines, Yes, appears two times, right. And if it's no, it will be one over the probability, I mean the number of times you have grade and that's 1 / 3, right? Yeah, the one that has the highest probability between probability of. Y. Equals and probability of y = a or probability and probability of Y equals not a. The one that has the highest probability will be your answer. Yes, this is my approach. Yeah. P of X of the vector. Yeah, of course it's. So here. Maybe my calculation is even wrong. I think here you have, I don't think my calculation OK. So I think one, I have two, which should be 1 / 3, one over three, 1 / 3 and then you have 1 / 45 and then you compare to the second one. So basically you have OK. So the way you compute it is what's the probability of N in the first column? That means it doesn't review notes. And if you compare all the ones that has grade of A so you have 1 / 3 and the other one, what's probability of no assignment? You only have one N out of the first 3 where you have a grade of A and that's another 1 / 3. And the last one asks question. You have Y, which is yes, ask question out of three possibilities. And that's 1 / 3. And that's how I got the first one. So in order to show, it should be 3 / 5 * 1 / 3 * 1 / 3 * 1 / 3, and then you're supposed to have 1 / 45. Is that clear? Let me just update the slide. I think. OK, I'm not able to edit on this laptop, but yeah, so 1 / 45. And then the other part, which is probability of Y equals not A, you're going to do the same. The probability of not a is 2 / 5, right? And the probability of dozens review notes will be 1 / 2 because you have two appearances of not A and that's 1 / 2, the second one 1 / 2, and the third one is 1 / 2. And then if you compare these two, which one is greater 1 / 45 and 1 / 20? 1 / 20 is greater than 1 / 45. And that's how the answer is not a. Do you have question? No, no, it's not Theta, but that's why I said ignore probability of X, because if you put it in inequality, you can just remove them, right? If you're comparing, you can remove them. Yeah. Which one are you talking about? Probability of X? The notation. The notation seems correct to me. Yeah. The only mistake here is just where I have 2 / 3. It should be 1 / 3. Yeah. So yeah, yes, yeah. So the idea is that if you already have this kind of this table of results, you can compute all the different probabilities and at the test, at the test time, you can estimate the probability for the new document and then decide what would be the category. OK. In this Case, No, it's not going to sum to one because you have not estimated the probability of X, right? Yes, yes, if you estimate the probability of X. What I'm just saying is that even without computing the probability of X, you can already decide which one is bigger. All right, so the answer is not a. And I hope this gives you an idea of all the maps we wrote on the book. OK, so quickly I will rush through the remaining of the slide before we go out of time. So there's a simple distinction between type and token. So the idea of type is that the identity of the word, which is the count of unique words and token is every single instance. So for an example is below, you have YO appearing 2 times, right? So if you have completed the probability of that token, you're also going to say what's the probability of spam, probability of YO given spam. And then you're still going to repeat it because this is computed over the probability of token, not the probability of types when we are doing this kind of calculation. So please take note of that. There's an important distinction between that. So another distinction is generative versus discriminative task. At the next class, we're going to try to do a discriminative task instead of a generative task. So for a generative model, we learn a distribution for all of the random variables involved, which is a joint distribution probability of X, Y. And for a discriminative model, the only thing you care about is the parameter. You learn the parameter Theta. If you can get the Theta, you already solved the problem. So and that is the idea for most algorithm ML algorithms that have been developed, they just focus on how do you estimate the Theta such a way that the loss is low. And once you can do this, who cares about having the estimation of the right probability, because sometimes the probability of XI just showed you. Sometimes is not very easy to compute for some application. But early days of machine learning, most researchers are split into two. Some are only working on discriminative tasks because they just want to have the best model. Some continue to focus on how do you estimate the right probability. Because if you can get what is the right prior distribution, what is the right distribution that you can use to fit your data, then you can get a very accurate result. But nowadays I think people that focus on discriminative seems to win. But of course, there are two ways of thinking about a problem. So for the task of logistic regression. So the idea again, although it has regression in the name, it is not a regression technique, it is a classification technique. So linear regression has continuous values, but for you for you to make it a probability distribution, you have to apply a simple function that will convert the output into a probability distribution so that everything can sum to 1. So the probability mass function can be written as this. And what when you want to try to solve which is also called logic? When you want to try to solve a problem using logistic regression? OK, I think this is how it is if you convert it to a probability so that now everything can go between zero and one. So I'm going to show you how you will compute what is the likelihood in logistic regression in a minute. Here I want to clarify that features can be anything. So in the example I showed you here, we use a very simple example where the features are like review notes, dose assignment, and ask question. In natural language processing. It's a big question on how do you get your features. We typically just default to what we have, which is the words we have and then use them as features because that's all we have. And then we can do counting. How many times does this word appear? How many times does that word appear? So if the word yo that we say yes signifies is more correlated with spam, if it appears so many times in your document, it should be a good feature. So we typically just default to this. So you can design your feature. For example, you can say for different tasks like nameless recognition, you can say capitalization is very important, can be an example of a feature, the length of the document can be an example of a feature, the number of stop walls can be an example of a feature, and anything can be a feature. OK, so in practice the features depend on both the documents and the proposed class. Does the document contain the word money? So if money is always associated with spam, then money will be a good feature for detecting spam. So for the parameters of the logistic regression here, the Theta will just be once you fit the model, you need to compute what is the parameters. So in this case, it's very simple, all the values A1A2A3 to an and of course and the intercept. And our idea is that we want to compute what is the likelihood, what is the condition and likelihood on our training campus, which is very similar to what we did in the naive base. You want to compute what's the probability of Y given X but given this parameter of Theta because now you want to find those parameters and those parameters in the logistic regression are A1A2A3A to an. And if you had a log to this, you can have what is called the log likelihood. And log likelihood is very important in NLP because it's what you use to compute things like publicity of a language model and so many things like that. So and the idea here is that if you say if you have log of the products of two probability, this will give you the sum of the probabilities. I hope this is clear. All. Right. And once you have done that, you can now optimize this using gradient descent. I won't go too much into this because this is more of a machine learning concept, but if you do the math, I think you'll be able to arrange this very quickly with this idea of the products. The log of the product of probability will be the sum of of the log of the probabilities. I hope I'm saying it right. OK, so another technique that is very proper. Do you have question on logistic regression? I was, I was a bit in a hurry because of the time, but do you have a question I can try to clarify very quickly? OK, the last thing I want to talk about is support vector machine, which is a very, very important algorithm. And the idea is that you want to project your data in a high dimensional space and then you want to draw a line that separates the different classes. So you have the blue dots and then you have the red dots. Can we, if we project it in a high dimensional space, are we able to draw a line that clearly separate them? And this line can be a very hard margin or a soft margin. So how can we draw this line that separates the two examples? And then here we can have a hard margin, which is the red 1 or a stuffed margin, which is like the green one, because you can have some dots that will be very, very close to that. And this is the idea of, this is the major idea of the SVM. So our SVM is generative for a discriminative model. They are discriminative. Why? Because you don't need. There's no assumption of what is the probability distribution on this. All you care about is just solving the problem. What is the best hyper parameter? What is the best parameter of the model that can help you to separate your classes in a high dimensional space? So how do you decide naive based logistic regression? SVM can work well in different tasks and settings, usually giving very little training data. Naive base is your good. It's always a good bet and you can actually do the math. It's very very simple to do this in practice. What you have to do is to try different algorithms. You can try logistic regression, SVM, boosting and so many techniques. OK, so the last one here is the perceptron that leads us to it's a very simple extension of the logistic regression. Here we generalize the A1A2A3 that I told you into what is called the weight matrix. And now we focus on what is the right weight matrix and the right intercept that will give me that can be used to fit my data. And here you can also stack different perceptions together. And from this is where we actually move to what is called neural network. Because neural network is more like a generalization of this concept for artificial neural network, there are so many people that believe that it's inspired by the human brain. So people do not believe in this. So you can completely ignore everything I say here. Some people believe that there's some interaction about you have the neuron and the dendrites and the ASEAN in the brain. The way they interact also kind of model how artificial neural network works. And this is probably one of the most successful algorithm that we have currently in machine learning. One, we have to we have some advantages of neural network. Number one, they can learn very complex functions. There is only a theoretical proof that says that they can learn any function. They can learn any function. And there are many different network structures that you can actually come up with for neural networks. So as you go on in your career, you can come up with another fancy architecture that will become famous. Better than Transformer are also leveraging this idea of neural networks. Given enough training data, they tend to perform very very well. Like even typically your large language models like ChatGPT are still based on neural networks. The disadvantage of this is that training can take a very long time. Sometimes you can train a machine translation model for a month. For current language models, people are trading for over six months, over a year, and if something is wrong, we have to start that over again. It's really expensive and often requires a lot of data. OK. And of course, we have different other kinds of classification algorithms that will not be covered, but you can read them up like K nearest neighbor decision trees. So in the next class I can ask you about what you know about K nearest neighbor decision trees, random forests, and so on. Thank you. Yeah.
