Jackie Cheung, Professor: Well. yeah, I'm moving. Well. good evening. Shit. Hold on. Alright. Hi, everybody welcome. Sorry I'm I started a bit late. Okay, so today we're going to start talking about lexical semantics. But 1st we're going to wrap up a few things from the last lecture on parsing and syntax. And, by the way, I don't know if you saw, but I posted an ed for the readings for this lecture. The latest version of the draft of Durafsky and Martin. 3rd edition seems to have taken out this material, but I think it's actually very important. So I posted a previous version that still had this material. So you can download that. Okay? So first, st here's a review. Quiz, not for marks. Don't worry. All right. What are the components of a Pcfg. And yeah. So what are the components of a Pcfg. And what do they, I guess, do? Yes. Is this a table of, I guess, like derivation of the words, and then the probabilities associated with each cell that's definitely related to Pcfgs. But that's like, once you already have the components of a Pcfg, how you use it to actually parse? Yeah. I forgot the word. Yesterday I said tokens, and I was wrong. So it's a description of transitions between tokens other than not tokens. Yes, those are definitely part of it. Okay, so so instead of tokens those we'll call them non terminals. Okay, so we have non terminals. That's definitely one of the components. Yes. And then you mentioned that there are rules which is like a left hand side, non terminal which rewrites to some combination of right hand sides, which are term non-terminals, or terminals with probability. Yes. so this implies that terminal symbols are also part of a Pcfg. Yes. anything else. I think we're still missing somebody else, maybe. Yeah, yes, a starting symbol. And I I think that this is pretty much it. I would just add that a room like a reminder that, like each non-terminal symbol forms a categorical, you you define a a categorical distribution for each non-terminal symbol over over all the rules with that non terminal symbol as the left hand side does that make sense? Raise your hand if that makes sense. Okay, so I need to explain some more. Okay, so what this means is that so? Okay, so what are your non terminal symbols? Each non terminal symbol is a syntactic category. Right? So a non terminal symbol might be noun phrase, or it could be verb. okay. So what this means is that for each of these non-terminal symbols they're non-terminal, which means that they need to rewrite it into something else. So how do they rewrite? That that's by following a probability distribution. Okay? So for so there might be, say, say, you're looking at verb phrases. you might have rules that where the verb phrase rewrites to just the verb, because it's an intransitive verb. or the verb phrase rewrites to the verb plus the direct object. Right? If it's a transitive verb. So there are a whole bunch of different options you can have for what's underneath a verb phrase, node. and what this last condition property is saying is that that process follows a categorical probability distribution. That's all I'm saying. Okay, so I hope that makes sense. Otherwise, the next bit is gonna not make any sense at all. So if you still need me to clarify, please ask. Okay. So then the and at the end of the last lecture we discussed probabilistic, context-free grammars in their most basic form, which sometimes people call vanilla Pcfgs informally. And the idea here is that you have a particular way of estimating the probabilities of rules through a maximum likelihood estimation. And that's by looking at a tree bank that has been annotated with all of these syntactic trees and looking at the structure of that and using that as a way to estimate parameters, the parameters of these categorical distributions that I just talked about. Okay, so if you have the probability of Alpha rewrites to Beta, then the way you derive the Mle estimates for that is to take the number of times you observe the rule. Alpha rewrites to beta divided by the number of times. You see, Alpha overall. But what I claimed at the end, and I didn't elaborate on because we ran out of time is that even with smoothing and regularization and all that, this approach doesn't work very well. And it's because this approach doesn't model the context very well around the rules, and it's also pretty bad in that. The rules themselves are very sparse in the in the sense that there are many, many possible rules, and it's really difficult, even with a very large corpus. to get enough statistical to get enough data to have reliable statistical estimates of the parameters of these distributions. Okay, let me illustrate. I mentioned this last time, too, but I'll go over it in more detail. So we know in language that there are different syntactic positions within a sentence. So with noun phrases, for example, in English you can have a subject noun phrases, and you can have object noun phrases. and it's very clear that the distributions of them are not the same. We mentioned the case of pronouns. so in English you have differences between I and me. So in in English all the pronouns that you have to change their form, you have to decline them according to the case, whether it's a subject or object. or generative, and all that. But and in other languages they have a lot more of these cases. Okay, so if you speak Finnish, for example, then there's like 15 or 16, or I've lost track of how many cases they have. But what this means is that if you have a subject noun phrase versus a object noun phrase. then they should not follow the same probability. Distribution in terms of what you Gen. All the internal parts that you generate with a Pcfg. And that can't really be captured by a standard vanilla. Pcfg. Because all you see is the the Np on the left hand side. So that's a problem. This next problem, this next issue is a less obvious one. But it's maybe even more important, which is, that certain classes of nouns are more likely to appear in the subject position versus the object position, and vice versa. And this is not just in English, but it might be a cross lingual thing as well, just because of, I guess how the world works and what people tend to, how people tend to describe what happens in the world. So in particular, in the subject position. the fillers of the subject position tend to be nouns that are animate. So, for example, humans or animals, or maybe other moving objects or things that are perceived culturally to have some kind of like, you know, intentionality or movement, or something like that. and it's more likely cross linguistically, but but in English and and cross linguistically, that they appear in the subject position than the object position. because there's a correlation between subject position and some semantic role of like entities that tend to do things or like have actions in the world and cause changes in the world, whereas the object position, there's a correlation. Again, it's not perfect that those tend to be entities that receive some action or suffer some consequences of something and so forth. And because of this, then again, you would expect the distribution of words that appear in these impacted positions to be different from each other. Yes. 1st case. Is there not a way that we could create like a subject now phrase, and then an object, now phase and have different rules for all these to account for them. Yeah. So the question is for is for the 1st case, is there a way to create subject, noun, phrase versus object noun phrases, and account for those that way. Yes, there is. And, in fact, that's exactly the solution that we'll explore. But we'll do it in a way that doesn't require linguistic knowledge. Because if we take that approach. then you really have to be like really good at linguistics and take many linguistics classes in order to decide which are the distinctions that matter and how to separate them out, and how to identify them in a corpus so instead, we'll take a a more automated I'll call it okay, a more algorithmic approach to do this so that we can get a lot of the same effects without needing to learn too much linguistics. although you should learn linguistics anyway. Okay? And there are also many other cases of obvious dependencies between very distant parts of the syntactic tree. Which which have effects on the word distributions, you would see. Okay, here's a second problem. So consider the subcategorization of verbs, but this time with modifiers as well. So we already talked about subcategorization of verbs, and that verbs can take different numbers of arguments. so they can be intransitive and take 0 arguments, in which case you might have a rule like Vp rewrites to Vbd, vbd, just means like the past tense form of a verb get in the country bank. but you can add adverbs. To this. You can add prepositional phrases. To this. On the other hand, you can also have, like other subcategorizations. So the the verb eats, for example, is kind of unique or not unique, but it's special, because it can be intransitive, but it can also be transitive, so you can eat a sandwich, so eat a sandwich. and you can likewise add like adverbs and prepositional phrases, and so forth, ate a sandwich, quickly, ate a sandwich with a fork. You can even change the order. The orders around sometimes like quickly ate a sandwich with a fork. So in this vanilla Pcfg, there's no relation between any of these rules in terms of their probabilities. They all have to be estimated separately, just by counting the number of times you see each rule in your tree bank. But that's really bad. Right? So first, st that means you need a lot more data to get enough samples to estimate the parameters. Well. And second of all, clearly, that's not really factorizing the problem in the right way. So it seems intuitive that maybe that we rather we should have, like some probability that a verb is modified by an adverb. and some probability that a verb is like modified by a prepositional phrase, or it takes on some object. So we would like to be able to factorize this probabilities in some way. in order to both get more samples for each to estimate the parameters, and also just intuitively, it seems to make sense for this particular problem. So the general solution for both of these problems of like, not enough context and too much sparsity is actually going to be the same. So I'm going to present this paper. This work that I think is really elegant and really nicely solve this problem within the this setup of probabilistic context, regrammars. So really, the main problem with vanilla Pcfgs is that they make independence assumptions that are both too strong and too weak in different ways. So remember what our independence assumptions is like, what you choose, the the relations between different random variables that you choose to model or ignore. Right? And so with Hmms. We already talked about how Hmms. Make this really strong independence assumption that, like, you, only model some local effects, but overall, it can still be quite powerful. Well, it's the same kind of thing here. Okay with Pcfgs in that the vanilla version. It makes independence assumptions that are too strong, going vertically in the tree. So if you talk about like subjects versus object. Nps, what you're saying is really that, like, you want to model more context up and down the tree around where the Np is, because that's how you can figure out if it's a subject Np. Or an object Np. Or something else. Right? So, for example, the subject Np. Will tend to be closer to the top of the tree where the the node above it might be an S. Node. whereas the object Np. Would be the way we've been drawing syntax trees, it'll be closer to the bottom of the tree. where its parent will be a verb phrase node. And so that means we need to change the independence assumptions vertically to model more of the context. On the other hand, this other problem, with like this sparsity issue with like too many rules is because we're making independence assumptions that are too weak horizontally. so horizontally across the right hand side of a production. Essentially, we're modeling every single combination, and in every single ordering of right hand, side symbols, non-terminals, and terminals, as independent of each other in their generation. which is too too weak of an independence assumption. So instead, we, we can again change our assumptions and and make a stronger independence assumption, so that we can more reliably estimate those parameters. So what are we gonna do? Essentially, we're gonna take that what was already proposed. But do it algorithmically, which is, we're gonna split up the categories vertically and split up the rules horizontally. Okay? So vertically, we're at, we can add more contexts by annotating parent categories. So whereas before you might only model Np. Rewrites to a personal pronoun node, or whatever. Now I'm going to use this notation where I use this carrot. and then you also indicate the parents of that node. So rather than Np. Rewrites to something. Now you have Np. With a parent of S, so this would correspond to a subject position for a noun phrase. and in the second case you can talk about an Np. With a subject sorry with a parent of Vp. Which might correspond to the object, position. and then otherwise everything else works the same. So so think of this as essentially a new symbol, a new non-terminal symbol in your vocabulary of your Pcfg. And it has its own distributions, which are different from the ones of just the Np. And again, you don't need to learn any linguistics to do this because this can be read off from the structure of the tree in the tree bank. In your supervised learning procedure in the Treebank data, you can directly see what was the parent node. And so you can algorithmically add all of these annotations and expand your set of non-terminal symbols. And you're good to go. Yes, but having so many additional like. I guess non terminals. Would that further decrease the individual probability? So would it still work out? Or yes, that's a great point. So it's always a trade off. So this is again, this like complexity versus data expressivity like this, this expressivity versus like a ability to estimates like sparsity trade-off. So the so now we're making the model more complex because we're weakening the independence assumptions. So you would expect that you would need more data to learn reliable estimates of the parameters. So the idea is that we think this trade-off might be worth it because it allows the model to be a lot more expressive. Yes. yes, right? So the Np. With the parent S would have its own categorical distribution at some point. That's right. So the Np. With the parent of S would have its own categorical distribution that sums to one. Yeah. So one thing you can try to combat this sparsity issue is to try to do some kind of interpolation or smoothing. so that you still allow everything that is a empty node to share information with each other through some interpolation scheme. But but then you still model them as a separate probability distribution. Okay, so yeah. So so this is the 1st sentence of the Penn Tree Bank. Very sadly Piravincin passed away, like, I think, last year or 2 years ago. So yeah, time moves on for everybody. But okay, so, but how this would work is essentially, we would annotate everything. So I'll copy this slide. Oh, okay. okay. So here the S node at the root. You can just annotate that its parent is the root of the sentence. and then np, here is parent would be the S. And then this Np. Its parent would be an Np, so you don't do this recursively, I mean, like you could do it for more than one level. But you won't like, but you don't do it so like Np, Np. and s, or whatever, unless you are planning to go 2 levels out. Okay. So maybe we should do this like bottom up. But anyway. And so then you would keep annotating and hopefully, you kind of get the idea. Okay, I'm going to stop here. You get the idea right? So you basically just look at. So this, oh, maybe one thing I didn't explain is this is actually just a tree represented in a different way using bracket notation. But since you're a computer scientist hopefully, you can recognize this is really just a treat right. So where all the children are expressed within the parentheses. So yeah, so basically, you just look at what the parents category main category was, and you annotate it. And then now you can run the same procedure to do supervised learning. Okay, so this is a quick fix for the vertical problem. Then for the horizontal problem. Basically, we're gonna pretend that every right hand side is a Mini Markov chain that we need to learn. Okay? So for every non terminal symbol on the left hand side. It there's a on for the it's right hand side. It's like a mini little language model there, involving those symbols that would appear in the right hand side of a rule. Okay, so we're gonna we're gonna break down the right hand side of the rule when estimating its probabilities. So, for example, whereas I'm going to add, like explicit start and end symbols to represent the beginning and the end of the right hand side. So whereas before you have, like Vp rewrites to this entire sequence as a single unit which is atomic, which is not broken down, and you have like one parameter for its probability. Now I'm going to break it down. So if I choose a Bigram model, then it would be broken down in this way it would be factorized in this way. So the probability of this whole thing is now equal to the probability of Vp going to start advert phrase, and then Vp, going to advert phrase Vbd, and then Vp, going to Vbd, Np, and so on and so forth, until you get to the end again. Here there's conceptually it's the the twist is that you're deciding to break down how you model this complex thing. But then, the technically, we've already covered all of these, all of this technique, because this is just an end ground model. And now you can again look into your tree bank and you have all of those trees that are already annotated. And then you can solve that learning problem. You have basically, you have to solve one n-gram modeling problem for every left-hand side symbol. But it's doable. Yeah, what what is the star stand for? Start up, multiply. This is just multiplies. Yes. and how would this still ensure that the sum of probabilities for the for every Vp on the left hand side is one. Great question. How do you still ensure that the sum of the probabilities for everything for one left hand side symbol still sums to one. So now it's through adding the start and end symbols. You can show that if you add those, then you can get it to all work out so that it's still all the possibilities still sum up to one. Okay, I think I'm going to skip this example, because here it's really not about like redoing the annotation. It's more about how you break down the rules. So for maybe the interesting rule here is like, Np, here rewrites to bt, jj, and n, right? So so the original rule act. Okay. So here, if you look at this notation here originally, we have Np rewrites to Dt. jj, and n. okay, so now, what we're gonna say is that this is now going to be equal to Probability of Np. Rewrites to start Dt. Times. Probability of Np. Rewrites to Dtj. Times probability of Np. Rewrites to Jj. And N. Times probability of Np. Rewrites to an end and then end. that's all. If you're doing a diagram model. you can also do a trigram model or a unigram model. You can interpolate, do everything, everything we talked about with N. Gram language models you can do. Yes. to understand, interpretive. So by interpolate. All I mean is. okay. So interpolate interpolating means that you have multiple models. But you don't trust any single one of them very much. So you can take a linear combination of them. so that you're kind of taking the in between of 2 models as your model. For example, you can do this, you can interpolate between a unigram model, a background model and a trigram model. And then you can take like I'll weight the unigram model with like a 0 point 5, and the background model with 0 point 3, and the tracking model with like 0 point 2 or something like that. Yes, in this example, like it's saying the line like, Mp, Dta, like the 3rd bottom yeah. Are we saying that it's like a proper noun, and then you're choosing one of those 3 brackets, or it's in that order that it's sorry I got lost. So can you say this again? So that line doesn't mean that we have a proper noun that is followed by one of those 3? Or is it those 3 in that order, like the Gta, like the Jj, non executive. And then, director. okay, so here. So this is not a, this is not a proper noun. So this is a common noun. So Np is so here the original tree structure is, you have an Np subtree, and then within the Np. Struck Np. Structure. Here there are 3 words, and each of them has a tag. So there's a determiner, an adjective, and a noun, a single, a singular common noun. So then it would be like Npm in like, at least it would be like a non executive director. Yeah, I can. I can also show it in. okay, let me show it in a more easy to read tree form. So you have an Np at the root. and then you have a Dt, and then a jj. and then and and n. and then below that, you have an a non, exact it is. And then, director. and you can draw. Yeah. So that's what that subtree looks like. Essentially. does that help? So all we're saying by doing the factorization is rather than modeling the entire thing all at once, we're not gonna do that. We're gonna model we're gonna so there's a start and end symbol. So we're gonna model, Np. start dte 1st and then Np. Dtj, and then Np. JJNN, and then Np. and and then the end symbol. That's that's just what we're doing. We're factorizing. But the the order matters. And they're adjacent. Okay. yeah. So this process is called markovization. because we're making Markov independence assumptions. Essentially so vertically speaking, that was called that would be called vertical markovization. Because you're adding ancestors as contexts. So the vanilla Pcfg would be called a 0 order vertical markovization model because you're not considering any context. and then the scheme we just described would be considered 1st order. and then you can go further. You can have, like second order, 3rd order and so forth. But of course, usually in practice, people don't go that far because of your model is so expressive and so complex that it's you don't have enough data to estimate that. And then the other thing we're doing is horizontal markovization, which is breaking down the right hand side into parts. So the standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter. The scheme we just described with the background model is called 1st order, and again, you can do any other order and can interpolate, and so forth. So, yeah. So this was proposed 20 years ago. Now and then it's it was these are the results on parsing on the Wall Street Journal through doing the scheme. So on one axis. You have the vertical marketization. Order of like, you know, V equals. One is no annotation. V less than equals to 2 is like they have a scheme to only annotate some parents, but not others. You can do all parents you can do selected grandparents, you can do all grandparents. And then here's the horizontal Markovizations markup order of like, here's the standard Pcfg with infinite context. And then here's the background model Unigram model. And then wait. No, this is the background model. H equals one. H equals 2 is a trigram model, and then H equals. 0 is like the unigram model, and H less than or equal to do is again, they have some heuristics to select, only some things to break down and not others. and you can see it makes a difference in terms of the pricing performance. So the the number here represents the accuracy of parse. How many constituents a parser trained in this way gets right. And so with the standard Pcfg assumption that would be the top right of the table. because that would be infinites, Markov order horizontally and no annotations vertically. So you would get like a 72.6 2 performance that way. and then underneath it. That shows the number of parameters in the model. and you can see you can do a lot better than that. If you do. For example, grandparent annotation with like a diagram model. Then you you get like a improvement up to like 79, and your your model is a little bit more expressive now, and it has more parameters to learn, but not like that, much more so. This has like a real effect on Parson. Yes. wanted to clarify about the order, because you described that, like the one we did the 1st order. Sure. So the question is about, what does the horizontal markup order mean? So yeah, the one we just described is H equals. One is 1st order, which corresponds to a diagram model. So like you look at every pair. If you do H equals 2, it means you look at every triple. So for this rule, for example. it would be Np. Rewrites to start Dtj. Times, Np. Rewrites to Dtjnn. Times, Np. Rewrites to Jjnn symbol. yeah, I just want to know if from this data, if this kind of set oh, God. right? That's a great question. So the question is about like whether this research sets some kind of gold standard methodology or protocol, or whether the results are specific to the Wall Street Journal. I think it is somewhat specific in that. Like each data set with each annotation scheme has its own characteristics and distributional patterns. So you would expect that maybe a different hyper parameter setting of like, what markup order to use would result in the best performance. So in that sense it's specific. But the general idea, I think still is, holds across different data sets, and maybe even in other cases beyond syntactic parsing. So so just for context, the current best parser, which is a neural parser, would be getting a score of around 95. So these are 20 year old results. So I'm presenting this, not because this is state of the art by any means, but I think it's really nice in terms of how they take these intuitions from what we know about language, and transfer it into algorithms that we can. And so that we can build into the the computational side, into the formalism and into how things work. So so I think that's really nice, plus I don't have to introduce too much extra in terms of new technical approaches. It's about. And Gram language models which we already know about. Even in their paper they actually go beyond substantially beyond 79, through additional linguistic insights. So there they do need domain expert knowledge. They can do further category splits and merges to get to around 87 f. 1 performance. Okay. alright. So any other questions about syntax and parsing? Yes, you have the horizontal markovization. It also still works, even if we do. In Cnf with just the 2 right hand side values. So so how does horizontal markup position interact with Cnf. that is a great question. So Cnf is essentially doing 1st order horizontal markovization. If you think about it. although maybe in a slightly different way, and the probabilities are all messed up. So, to make things distinct and clear, I would keep it separate from this. So this method and this procedure is about training of the model and learning the parameters of the model. And then the Cnf thing is for the parser. It's for the cky parser and what it requires. So you can do this 1st and come up with like a model. and then once you get to the point of actually parsing. Then you can convert all the rules to Cnf. And then, since this is a Pcfg, you have to think about like how you do the conversion. So the probabilities are maintained. and that might require a bit of thought. But what if we started with something that was already in Cnf. We have to reconvert it to. Not Cnf, so it's something if you start with something that's already Cnf, so you only ever convert things to Cnf in order to run the parser, so you wouldn't So there are some linguistic theories and some approaches to syntax, where they they demand everything to be binary branching. which is one of the key Cnf assumptions. So in that case, then, maybe you don't really need to do mark horizontal markovization, because everything is already binaries. Okay? So oh, yeah. In other words, the the process we like to convert the Cnf wouldn't apply on their probabilities, because you have to make sure the probabilities are mean. Oh, the question is, when you convert things to Cnf, then maybe that wouldn't be. That wouldn't apply here at. I would have to do some thinking about it, but I'm pretty sure there's a way to still convert things to Cnf. In with a Pcfg. And make sure all the probabilities of the entire tree are preserved. but we don't know how to maybe I'll assign it to you as an exercise. You can work it out. I think it. I don't think it should be that hard. because if you think about it. you're creating non-terminal symbols. And so each non-terminal symbol only is involved in one rule. So I'm pretty sure it's yeah. I think you can work it out if you spend a few minutes thinking about it. Okay, so where are we in the course? So now we've wrapped up structure and parsing. And so next, we're gonna talk about another really big topic which is semantics. And this is also this is arguably super interesting for both theoretical and practical reasons. because semantics is about meaning. And if you're a language, technology, enthusiast, and you presumably you are. Since you're in this course, you want to do things in the world. and things mean things like words mean things in the world. So we want to be able to have some formulation and representation of meaning in the world, so that we can represent and understand things and use Nlp systems to derive these representations. and then from there you can make draw inferences, maybe through some approach that's not necessarily only specific to Nlp in order to derive new conclusions and and that can help you make decisions or do whatever you want to do. Okay? So so how? So so, semantics is super important because it's a, it's kind of like the representation of something in the world so that you can actually interact with the world with your technology. That's what I'm trying to say informally. is that a stretch? Okay, it's just a stretch. Okay. okay, so then let's start off with a philosophy of like language. Okay? So so if semantics is the meaning, if it's if the study of meaning and language, then we have to 1st ask, what does meaning mean? And here I'm going to give some. Very, I would say, probably superficial answers. but that these superficial answers will at least give us some beginnings of like guidelines for how we might want to model things computationally. Okay, so what does meaning mean? I'm gonna give at least 2 answers. And the 1st answer is that meaning is about the relationship of linguistic expressions to the real world, or at least to some world. And the second answer I'm going to give is that we can talk about meaning in terms of the relationship of linguistic expressions with respect to each other. And we're going to start by focusing on the meanings of words. And this is called lexical semantics. So lexical means, it's related to words. And later on we're going to start talking about the meanings of phrases and sentences, and also how to construct these representations from the meanings of words. But let's like explore this. These these ideas a little bit more. Okay. So relationship of linguistic expressions to the world and to each other. So can we at least have some examples. Okay. for example, I guess I need to change my slides, because, like, telephone is not a word that's used very much anymore, I guess. But what does the word telephone mean. Okay? So one way you can think about the meaning of telephone is, you can think about it as you're picking out all of the objects in the world that are telephones. and so these are called its reference. And then this is also called the extensional definition of a word. And note the spelling here extension with an S. So think of it as like, the meaning of telephone is that it's a function. It's a function that takes in an object from the world. and then it gives you true or false. Okay, so here's a set of items that are telephones. And then there are many, many other like infinitely many other objects in the world which are not telephones. And that's the meaning of the word telephone. This is one answer. or if you can also, think about the meaning of words in different ways. So suppose you had to define the word telephone to a 3 year old, or to a friendly Martian. Well, probably you would, I mean. I guess one thing you could do is you can say, telephone, telephone, not telephone, not telephone, not telephone, but that would take a really long time. Okay, so probably another thing you might do is if you're if the 3 year old you're talking to, or the friendly Martian you're talking to already speaks some language that you share in common. Maybe you would try to give a definition with other. In other words. okay. So for example, you might look up the dictionary definition of a telephone. Okay. thought there was a way to follow the link. Okay, so here's the dictionary.com definition of telephone. Okay, an apparatus system or process for transmission of sound or speech to a distant point, especially by an electric device. Right? So this is another way. You can characterize the meaning of telephone. However. this presupposes. You already know what all of these other words mean, like apparatus, or sound or speech, so that could be a problem. But anyway, this is the general idea behind the intentional definition of a word, which is that you're kind of listing the necessary and sufficient conditions for something to be a telephone. And this intentional definition. I guess you can like, express it using some logic, in which case you don't have to rely on other language. So in the so, whereas before you had, like a a function that like takes in an object and returns. Whether it's a telephone or not. It's almost like now, you're still taking in an object. But your function. It checks a bunch of conditions of like? Are these necessary conditions satisfied? And are these sufficient conditions satisfied? And then it returns to recall? So it's not by an enumeration anymore. But it's by a computation of like properties or in practice, we're gonna if you use a dictionary definition, then you're defining words in terms of other words. And so then you do need to know what everything else means, and you already have to have some like language capability to to do to give this definition. Yes, what happens if dilutionary definition? So what happens if the dictionary definition mentions a function that needs to be fulfilled? That's fine. That's just part of the dictionary definition. It's just expressed that that function is expressed in language that's all. Okay. Yes. Sorry. Did I miss something. The intentional definition is different to the fictionality. So I think so. The intentional definition is like talking about the conditions, the necessary and sufficient conditions for something to mean something. So you can express that using some computation, some logic, or whatever you can express that with it, can take the form of a dictionary definition. So they're slightly different. But they're related to each other. Okay, so this is not a new idea. So this distinction between sense and reference. one of the 1st people to propose it. Maybe maybe he was. The 1st was frege in 1892 so he was one of the 1st to distinguish between the sense of a term, and its reference where the sense is more like the intentional stuff about the the. the, the properties associated with a word, whereas the reference is about the objects in the world that it points to. For example. astronomers. I guess at some point long in the past they saw this bright object in the sky this thing that looks like a star. and it appeared in the morning and in the evening. And so then they called something the Morning Star versus the Evening Star. And then eventually, somebody was smart enough to figure out it's actually the same thing in the world. And it was just Venus. Yeah. And actually, it's not even a star. but But here you have an example of something where there's the same referent we now know. It's the same reference in the world. However, there are different senses, because the morning star might be that bright thing in the sky that appears in the morning. The evening star is that bright thing in the sky that appears in the evening? Okay, so so they have different senses, but they have the same reference. And I guess maybe the logical thing to ask is, can you have something with different reference with the same sets. and that I have to think about? Yeah, yeah, you could. Yeah, like the over, especially over time. like the Prime Minister of Canada. Right? That has a certain sense. But then, at different points in time. You can evaluate that, and it points to a different referent in the real world. Okay, so that was our very superficial look at detour into the philosophy of language. So, yeah, so we talked about how to relate words to each other, but also how to relate words to the world. But we can say more about how words relate to each other. So this is the second kind of a main general area that people work in. And it's much more popular in in the computational side like to work in this as well, which is to think about how the meanings of words relate to each other. And actually all of these, a lot of these. You can express these in some logic as well. For example. So I assume that most of you have heard of synonyms, right? A synonym, right? Things that mean the same thing. You can express that probably in a logical way as well about how things that about like it evaluates the same set of objects to true, for example, right? And there are many others that you can do. You can express them logically as well. But okay. But now I'm going to define a whole bunch of terms. and we'll just go through them. Okay, so one way in which the meanings of words can be related to each other is through hypernomy and hyponomy. And this is simply an is a relationship. If you prefer to think of it that way. Okay, so it's something taxonomic. For example, a monkey is a mammal. and Montreal is a city, and red wine is a beverage. So these are is our relationships. So the smaller thing, like the the thing that denotes fewer things in the world is called the hyponym. and the word that denotes more things in the world is called a hypernym. Oh. okay, straightforward, right. You just have to remember these terms. The the way to remember, I think is hyper is like higher up. So it's like higher up in some taxonomy tree. And then hypo is like lower down. So it's lower down in your taxonomy. Okay, then, here I assume you've heard of these ones. These are synonymy and autonomy. So synonym means that 2 words roughly mean the same thing. such as offspring and descendant and spawn. and or happy and joyful and merry. There are linguists and people who say that true synonyms don't really exist. and that there's no 2 words or expressions that truly mean exactly the same thing. I'm gonna close the door. I'll be right back. Okay, that's better. Yeah. So there's some people who say that their true synonyms don't really or and true paraphrasing doesn't really exist. because there's always some slight differences in, if nothing else. The connotations of the words like. It would be very strange if you go up to like a friend of yours and say, Oh, Hello! Long time. No. See how are your offspring doing right like? You don't say that right? Like you say, children in typical scenarios. There's a whole comic based on this. What's it called? It's by that guy. Nathan Pyle. I forget the name of this comic, but the whole humor from that comic comes from rephrasing common, everyday expressions, using paraphrases that are supposed to mean the same thing. But they sound humorous if but if you do a synonymmy replacement exchange planet or something I forget, anyway. But anyway, we can pretend that synonymous exists. And it's actually it's it's still useful to come up with this idea of some synonyms of words that roughly mean the same thing. Then antonyms and autonomy is words that roughly mean the opposite thing. So again. it's really hard to say what does opposite mean? Often. but presumably there's some dichotomy, or there's some spectrum. And these words appear at opposite ends of the spectrum or opposite sides of the dichotomy in some in some way. So, for example, synonym and antonym are antonyms of each other. or happy and sad or descendant and ancestor. So in some way they're they're opposed to each other. So we'll come back to synonyms versus Antonyms, because it turns out that it's it's actually quite difficult. It's very difficult to separate synonyms from antonyms using many computational techniques. Like, you need a lot of data in order to be able to do this. Okay, so this, this will be a problem for us later on. Okay, another lexical semantic relation is something called homonomy. which means same form. but different and unrelated meaning. And again. the way to memorize this is to understand, like how to break down these words like homo, means same. and nim means form. So that's why it's like synonym. Antonym, like, like, yeah, it's like same. It's about. It's about same and difference. So there are different kinds of homonyms. There's a homophone which again break it down. Same sound. Okay? Phone means sound. That's why telephone means telephone, because it's tele means far away. So telephone means far away. Sound right? So homophone here means same sound. For example, sun versus sun. If you speak a widely spoken dialect of English, anyway. whereas homograph means same written form. So lead versus lead. Okay? So they they sound different. But they're written the same. So these are both subtypes of homonyms. They're both hyponyms of the hypernym homonym. Okay, here's another one. This one is again very interesting and comes with lots of problems for computational linguist to solve, which is called polysemy. Okay, here poly means many and sem means meaning. So polysame means the phenomenon of a word having many meanings. So yeah. And in particular, polysame involves multiple related meanings. and sometimes these are so ingrained and so natural to us that we don't even realize there are slight differences in the meanings of in these different situations. So the example here is the the word newspaper. It turns out that newspaper has many, many different senses. It has many different related meanings. A newspaper here can be a daily or weekly publication on folded sheets containing news and articles and and advertisements so like that physical object that nobody actually ever interacts with anymore. A newspaper can be a business firm that publishes newspapers. So Murdoch owns many newspapers. You see how it's different. Right? So this newspaper is about the the company. The 1st one is about that physical object with things printed on it. But it's somehow different from this 3rd sense. So I'm not exactly sure how so the newspaper is the physical object that is the product of a newspaper publisher. When it began to rain he covered his head with a newspaper. So I think here the idea here is that the 1st one is more about the the concept of that newspaper like New York Times is a newspaper in the 1st sense. or like a haha. What other newspapers do people read? I don't know. Montreal Gazette is a newspaper like as in like that. that that construct of like that. or firm that publishes, that produces regular articles for consumption. Okay. which is different from the business itself, but which is also different from the printed version of that Of that construct. So these are all very subtly distinct from each other. and then these are all different from the 4th sense of the word. which is the cheap paper made from wood pulp used for printing newspapers. In the 1st sense, I guess. Okay. so, or maybe the 3rd sense, the 1st and 3rd sense are very close to me. So they use bales on newspaper every day. So these are all very subtly distinct from each other. But we just we don't even think about this right when we process language. because it's clear to us in context. What the intended meaning is in which sense of the word is intended. so homonym can be. It can be very difficult to distinguish between harmonymy versus Polysemy. and I'm sure that the boundary is a little bit fuzzy. So one example is with a word such as position. and there are many different senses here of position. and I'm sure we can reconstruct some chain such that, like, it seems like they're related to each other. But then, at other points. It must seem like they're very unrelated to each other. Okay? So the 1st sense of position given here is like the particular portion of space occupied by something. and then the second sense. Here is a point occupied by troops for tactical reasons. so that clearly seems related to the 1st sense. Right? The 3rd sense is like a position in terms of a way of regarding situations or topics. and here it might not be immediately obvious whether these are related or unrelated. To me it seems like they're probably related. It's just that one is about physical location, and the other is about like a location in some more abstract sense. You're kind of. You're drawing an analogy. You're creating this abstract space, and you're like locating points of view in that abstract space. And then there's position with about the arrangement of the body and its limbs. Here's another one which is position to do with the relative standing of people in a society, again, is some kind of metaphorical extension of the physical idea of location to some abstract space. And then this last one is really interesting of like position as like a job in an organization like at 1st glance, it would seem to me that it's just entirely distinct from like the physical location, right? One is about physical locations, the other is about your job in an organization. But then, probably it's through this kind of a again, it's through this metaphorical extension of that. You can think about jobs within an overall abstract space of like a bunch of jobs in the organization. And they're they're they're related to each other, and they're like supervisors and supervisees, and so forth. And in that way, then it makes sense that this is a metaphorical extension. So so I would argue that all of this is probably these are instances of felicity. But sometimes it's really not obvious when you 1st think about and look at the words. and it's even possible that there's originally a chain of like of this reasoning. You can do to create this. the connections between senses, but then, maybe some sentences are lost, some senses are lost over time, and those sentences are no longer used in the language, and then it really appears to be unrelated words. So yeah, so I I would say that these are not always as clear cut as you might think. Okay, more lexical, semantic relations. We're not done yet. Here's another interesting one. metonymy. So metonymy is really tricky. If you're trying to write clearly because I find that like If you're within a particular subgroup or subculture where you're used to talking about things in a certain way and taking shortcuts in your expressions, and it's really clear within that subgroup when you're trying to write for people from outside of that subgroup, you don't realize that you've made those shortcuts. And then so you start to. Then you use the same shortcuts and other people misunderstand you. Okay, so. But so some of these are very conventionalized, and it's like used by everybody like you say we ordered many delicious dishes at the restaurant so clearly. You don't mean that you ordered the plates right. In fact, you don't get to keep the plates most of the time. You you ordered the the food that comes on the plates. So there's been a substitution there. So you're substituting one entity for another related one another. One is is related to the example we just saw. I worked for the local paper for 5 years. You don't mean that you work for the like the actual piece of paper, the the print, right? You mean. You work for the organization that produces that paper. Here's 1 that's very relevant to us. Quebec City is cutting our budget again. Okay, here Quebec City is is not referring to the city. The city itself is not capable of cutting budgets, because it's just a it's a city. It's a concept of like some geographical location or something. Instead, here it's referring to the governments, the provincial Government. whose capital is located in Quebec City. or the last example. The loony is at an 11 year low here. This is. This requires a lot of background knowledge and real world knowledge to fully understand right here the loony is not the the physical coin. It's standing for the Canadian currency. and what it means when you say it's at an 11 year low is that you're comparing it against another, its exchange rate with another currency, presumably the Us. Dollar. Yeah. So these are relatively well standardized examples. But if you start chatting with like your friends, or in a particular subgroup, or you'll you'll develop these very naturally you'll substitute words for other related words without even noticing it. and then the the other people will understand, and then eventually there will be like a norm. and then you'll have, like some some way of speaking, that's very convenient and short. But outsiders would not understand. Okay, so this is one of the mechanisms in which these things form. Yeah, if metony becomes so popular that it becomes like a dictionary definition, does it stop being metronomy at that point. Oh, that's a great question. So yeah, some of these are so ingrained that they they appear in dictionaries. So at that point, is it a meta? Is it a metonym, or is it like a polysimous word? So it's probably both. Right? So yeah, so these are just our interpretations. And the way we impose like try to impose some order on the phenomena we observe in language. So it's both. I would say it's both a metonym and and also that word is felicimous with multiple senses. Yeah, there's a specific kind of synec of metonymy, which is kind of funny because it usually involves things they cannot say in class. So this is called synecdokey. So it's a specific kind of autonomy involving whole part relations. Okay? So it involves substituting. So here the metonyms in general is just about substituting with related words, but if the relation involves whole part relations, then it has a special name, for whatever reason. So if you are in the situation involving sailing boats or whatever, and the captain says all hands on deck the captain doesn't mean just literally, all of your hands must be on the deck. Okay, they mean your entire person has to be on the deck. or many swear words involve sensored body parts. So that's another kind of synecdoche. or many, many offensive expressions. So don't be a censored body part. Okay? The next relation is Hallonomy and meronomy. So yeah, so we just mentioned whole part relations. So yes, it has its own name, too. So holyms are the whole. And then maronyms are the part. Okay? So the way I remember this is like, Hall sounds like hope. I don't know if that's etymologically correct. But that's how I remember it. So and there are different subtypes. So you can have groups and members like a class consists of students. You can have, like physical whole part relations like a car has a windshield. and then you can also have, like a composition, like the the substance composition of something physical, like a chair, is made of wood. So these are all whole part relations of different kinds. Yes, all of these definitions are not necessarily mutually exclusive. Right? Something could be. oh, thank you. Right. So are all of these definitions mutually exclusive. So I think we showed here that it's probably not. They're not mutually exclusive. Something can be meton like a metonym, and it can be a polysemis or Yeah. oh, yes. And in this case, with Haronomy and Morontomy, is that similar to like we saw on the 1st slide about this. Yeah, yeah, that's a great question. So is Hallonomy versus meronomy, the same as hipernomy and hyponomy. So while they the structure is similar, because it's it seems to be about like greater things and smaller things. They they are distinct from each other because hypernyms and hyponyms is about. Is our relationships so? Like like here, a student is not a class, and wood is not chair and windshield. A windshield is not a car. So it's about is our relationships like in programming, in object oriented programming. It might be something like inheritance. whereas a whole part relationships. It's more like the elements you put in a class. It's like it makes ups that thing. So they are still distinct from each other, even though I agree that structurally you can represent them both with, like some kind of tree struck like to to talk about like what's what's the whole? What's the part. What's the hypernym? What's the hypernym? There's still a hierarchy. It's just a different style of hierarchy. Exactly. It's still a hierarchy. It's a different style. Of hierarchy. Yeah. okay, so let's see if you already have remembered all the terms yet or not. So cold and freezing. What relation does that exhibit? Yeah, yes, these are synonyms there and there. Yes. homophones. Yes, so there are, which is a some kind of homonyms. Yes. hair and head. Yeah. yes, and which one is which head of the forecast? Yes, the head is the whole thing. So the head is the holyms, and the hair is the marinym enemy. Friend. Yeah. the month. And above you just said it. You just said the 1st 3 letters. Yes, that's right. cut as in hair versus cut as in bread. Someone anyone who has not. Yeah. yes, Felisame, yes. And you see how these are just distinct, right? Like cut hair is, you have something that's attached to something else, and you're shortening it right. Cut bread is like you have something, and you're slicing it. which is distinct also, if sometimes you can tell if something is polysemous, if you know another language, and you try to translate it into another language, and you have to use a different word. That's usually a clue that it's like it's like there are different senses. like I I think in French, I think I have mixed. I don't know how you some. The word I would use for cutting. I don't speak French natively, but the word I would use to cut bread is like Tranche. whereas to Kahair. I will use coupe. But I don't know if you can coupe a bread. So can can you? You can. Okay? So okay, I guess that it's also business in French. Okay. Last one. George Clooney, an actor. Oh, yeah. yes. Which one is which. Yes. and the other one is called. Yes, I put him great. Yeah. So let's just remember that. And you've got one part of the midterm down. Okay? So believe it or not. people have come up with the attempts to systemat systematically, annotate all of the senses of all words in the language. and and so forth. So they started with English, because, you know. the researchers spoke English. and so they came up with this resource, which is called wordnet, which is a lexical resource organized around this idea that words can have multiple senses. And in fact, they take it a bit further. They talk about these relationships, these lexical semantic relationships. and they organize everything in terms of these senses rather than in terms of the words themselves. So the primary organization is in terms of senses. and they gave a. They give a particular name to these senses, and they call them syn sets, which are, which is short for synonym sets. Okay? So the idea behind Wordnet is that in the language. Then you have nodes, and these nodes are called SIM sets, and they can be expressed in through many different realizations like with actual words. And then the edges in this graph correspond to lexical semantic relations between syn sets. and they have a different hierarchies for different parts of speech, but the one that's by far the most well developed is the one for nouns, and then the other ones they happen to. But it's not as extensive and not as well, not as connected. So I hope this works, there is a web interface. Maybe there still is one. Oh, yeah, it's still there. Okay, great. Someone. Give me a word that I can safely type here. Table table. Okay? Okay? So here, you can see, let's just focus on the nouns. So in Wordnet, there are 6 different synsets associated with the word table. Okay? So each of this, each of these think of it as like a concept with a certain way that you can realize it in language using some words. So, for example, this 1st inset can be expressed using the word table or using the phrase, tabular array. And here's a dictionary definition of it, like a set of data arranged in rows and columns like C table one. The second sense. Here is a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs. It was a sturdy table. The next one is a piece of furniture with tableware for a meal laid out on it. I reserved a table at my favorite restaurant. So this is funny, because, like the second and 3rd sense is like, it's the same physical object. But it's like a different sense, because the 3rd one is specifically in the context of a meal. Okay, the next one is flat table land with steep edges. The tribe was relatively safe on the mesa, but they had to descend into the valley for water. So you can see that here cable is not the most common word used to for that sense, and it's usually a mess up the next 1. 0, hey! Here's another case, right? This is like a this is a metonym which has become so conventionalized that it's now listed as a separate sense. So now it's it's like a blissimous thing. A company of people assembled at a table for meal or game. He entertained the whole table with his witty remarks or the arrangements. She sets a fine table room and board. Okay? So yeah, you can see this is how it's organized. If you click on one of these like if I click on the first, st since that it gives you its connections to other synsets like it has a direct hyponym or a full hyponym. You can look at like Member maronyms. You can click on any of these and and find, like other sunsets that this is related to in this network. So okay, so for example, table, there are subtypes of it that you can have a correlation table. You can have a table of contents and actuarial table, a calendar file, allocation, table, periodic table, and so on. And so. And then each of these is its own synset. Yeah. So this was just a backup in case the things didn't work online. Okay, so if you're using nltk, you can actually interface with this. You can through python. So and then there are some useful functions you can use, so you need to import it and download it. It's not that big, but it's still takes a bit of time. And then you can look up synsets in Wordnet. And yeah, these are all of the synsets that these lexicographers have annotated in within this project over the past 2 decades or something. Okay? So I'll just introduce the problem. And we'll continue this next class. Okay? So arguably, one of the most obvious things you can do. Once you have those resources, you can try to disambiguate a word into which word sense was meant by that word. Okay. So figuring out which word sense is expressed in context. here's an example. His hands were tired from hours of typing. This corresponds to a synset with this identity of hand. which is a noun of the 1st sense, the 1st sense of of that. and it corresponds to like your your actual hand physically. The next example sentence is due to her superior education. Her hand was flowing and graceful. This represents another sense of hand. which is, it's not listed here. Okay, wait. Yeah, it's not here. But it's like the handwriting, the handwriting style. And so one computational task that you might want to solve is called word sense disambiguation. So this received a lot of attention in Nlp, which is to figure out which sense of a word is met in a particular context. And the general idea here is that you can use the words in those contexts to help you disambiguate. For example, in the 1st context, here. the fact that you see the word tired is probably informative and typing. Maybe, whereas in the second context, maybe something like flowing and graceful would help inform that this hand is referring to the handwriting style. Another question that you can ask, or maybe you should always ask, is like, why do we want to solve words and disambiguation? So one reason might be, you think it's like in inherently interesting to figure out, how do we figure out the intended sense of all of these words that have multiple senses? But if you don't think it's inherently interesting. You can still try to come up with application oriented reasons to work on this task. So it says, so like, I said before, one obvious application is in machine translation. And that often different senses of a word should be translated differently when you translate to a different language. Okay, so I think I'll stop there and then next time we're going to look at algorithms for word sense disambiguation.
