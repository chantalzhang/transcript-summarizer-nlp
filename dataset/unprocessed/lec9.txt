David Ifeoluwa Adelani: Problem. Awesome. Actually, yeah. Okay. cool. Yeah. To confirm everything. Yeah. All right. Welcome to today's class lecture 9, where we'll continue on hmms, and we'll now move to conditional random fields. So today, we'll consider sequence modeling with features with a focus on linear chain conditional random fields conditional rental fields. We're very popular. especially for a token classification. Tasks like battles, please tagging, chunking. and name them to recognition, and they were like some of the most important models that dominated the the field for like more than 5 years. Actually, when deep learning came around it was still very important to have conditional random fields to actually improve performance. And today we're going to see how we can go from Hmms to crf. as it's popularly called. And the interesting thing here is that you can actually combine conditional random fields with some other deep learning framework like Cnn Lstm. By Lstm. Including beds-based models. That is based on transformers architecture. You can combine it with Crm by just appending the Crf layer to the so the last layer of your network. And then you can actually boost the performance by one or 2 points. So actually, they are not they're still very useful to today. Of course. Now, people don't use it that much as before with the Llms. That can do almost everything now but for token classification task. We actually find Llms to still struggle a bit with talking classification tasks. And maybe some of these methods can still be useful in practice. If you want to improve performance? Alright. So the last time we talked about, I think we finished off with also provides learning of Hmms. We today we'll talk about some of the shortcomings of Ed Markov models, and then we're going to move from generative tasks to more discriminative tasks and and of course, we examine the linear chain conditional fields. Okay, so for the Ed Markov model, this is a summary of what we have been talking about in the last 2 lectures. So we have a a Markov model, where we have different states like q. 1, q. 2 q. 3, q. 4, q. 5. And then we also have observed variables in this case. For the case of Patos Beach. it will be the States will be the pos tax. and if we also talk about the ner models. The States will be the different ner tax, like we examined last time. and the observed variables will be the words, so this is a generative model. And I want to emphasize in generative model. If you want to predict what would be the the output of y. Given an input X for generative model. The idea is to actually the idea is actually to compute the joint probability. Can you help shortly, Doc? Thank you. Thank you bye so for the case of so sorry again. if you want to solve the transportation task now, you want to solve for y given X, there are 2 ways of going about it. It's you can actually actually go through it with generating approach or discriminative approach. A very simple generative approach would be of naive base. This competitive approach would be like your normal Asvm. so you can also do that in this case. So the Hmm is an example. Another example of the generator model. Why, as you will see in a couple of slides, crf is an example of a discriminative model. So in the generative model to reemphasize. If you are supposed to find an output y. Given X, your approach is actually to. I'll try to learn the joint probability between X and Y, and the assumption is that if you can learn successfully the joint probability between X and Y, then you learn everything about the model. And if you have been able to compute all the statistics needed. you can actually compute Y. So that is the idea of generating model was, as you know, in practice, this is actually not possible, because for you to compute that you have to make some assumption which is like the Markov chain, assumption or independence assumption to actually be able to compute these probabilities. Some other models. They do not make this assumption because some of these assumptions may not be realistic, right? So you are not able to compute the jump probability every time. And in that case a discriminative model is more realistic. and it. Just give me more native model is just trying to optimize like, can I get the a local minimum or the the global minimum? If I optimize this loss function. And a very good example is the artificial neural networks. We're back to the Hmm. Which is a generative model. And the idea is that. Can we learn this general probability? It's difficult to learn this joint probability over all the States, and therefore we make the Markov. we make a Markov assumption where we actually explore conditional independence assumption. Instead of saying that the next State depends on all previous states, we only say the next state depends on the last one. And also we add, that is what we call the state probabilities, and for the emission probabilities, we say, if you are able to successfully learn the generative model, we will be able to also have the initial probability, which is the probability of absolute given the State. And this is a very simple example of a generative model for a diagram distribution where we see Q. 2 depends on q. 1. And you can also emit all one based on every single State. You can also emit every, all eyes based on every Q state. Yeah. all right. So last time we discussed the following. Because our focus is to learn this joint probability, which is the probability of Q comma O given the parameter theta. we consider a couple of our guardians. first, st for you to compute this probability of Oq. Given Theta. you need to know what is the likelihood. and the likelihood is what is the probability of all given Theta. And we said last time that you can easily compute using dynamic programming, you can compute the forward. You can compute this likelihood. P of all, given theta using either forward algorithm and backward algorithm. And if you're able to compute that, you could also integrate that into the computation of What do you call it? Into the computation of the entire State probability, which is probability of Q. Comma. O given theta. And for that you can use the vertebia algorithm and vertebia algorithm is very similar to the forward algorithm. When the 4 are going in the sum over all the previous States. Why, in the Viter we are guarding, you actually just take the maximum. if you remember, in the last slides. And the last time we also said, okay. It's also possible that you can do this when you don't have any labor, they do no label data, no problem. Then you can use em algorithm to actually estimate the labels. Even if there's no label data. And then you do this attractively. 1st you randomly initialize the States. and then you compute your needed parameters. and then you maximize in the maximization stage you try to get a better initialization. and then you can see if you're improving. Why, computing the likelihood. All right. So just to revise what we did is that it's possible to compute this iteratively, even if there is no labor data. and the idea would be to predict. You repeat for a while, and then you predict the current state sequences, using the current model. And that current model initially, just initializes randomly. And then you update the parameter of your current model so that you can have a sorry based on the current predictions. So for the Zetabi algorithm. The idea is that again, you repeat for a while very similar to what in the last slide you predict the current state sequences, using the current model with the vitabi algorithm. And after that you update the current parameters using the current predictions as in the supervised learning case. So again, for the emigrating the idea is that we believe that we randomly initialize Theta K and your Theta K is all your so all your A's and B's in the, in the the last formulation, you randomly initialize them, and once you have randomly initialize them. You can use this to actually have the expected count based on the it is structures. And after the at the maximization stage, then the idea is that, you see, can I get a better Theta K plus one. So I computed the expert accounts based on initial values or randomly initialized data K, and at the maximization stage, can I get a better one. so that I can have a better initialization. And you do this attractively over a couple of like you. You do this over a couple of times. So on to go more into the details. what we discovered is that actually. for the expectation maximization, it boils down to 1st actually computing the probability of every State given the observation. and after that you can now compute the transition probability from going from State I. To state J. Which is going from the previous state qt. To qt. Plus one. And then, if you apply the E step. the idea is that normally you initialize what is your Sita K. Because Theta K. You don't know, because there's no data to learn it from in the unsupervised case. So you randomly share rise your A's and your B's, your A's are coming from the forward. Algorithm. sorry. Wait. Sorry. Your alphas! Are coming from forward algorithm and your betas are coming from the backward algorithm while trying to compute the forward algorithm and the backward algorithm. You need these parameters A and B and the parameters A and B are computed from this grid that I showed you the last time. I think maybe I still have it in just a minute. Let me stop sharing. I think we go to the last presentation on Hms. okay. so this one where you have you can compute. This is the grid. and for the unsupervised setting you are not able to compute these alphas and the betas so but 1st you assume some initial values for the B's which is your emission probabilities, and then you assume some initial values for the ace with which are your transition probabilities. I'm using this. You'll be able to compute your alphas for the forward algorithm. And then you also be able to compute your betas for the backward algorithm. And once you are able to compute this, you'll be able to run your Em algorithm even for the 1st state, because you randomly initialize your A's and B's because they are known, unknown. Basically, you don't know them. A. Let me go back to. Yes, I have. A question is probability of O. Given Theta K, you computed using the entire forward algorithm at that state. Yes, because you can now use forward algorithm. So it means you initialize the entire table as random values. And then it's updated at every iteration. No, you don't. Initial. Okay? Great. If I go to the example. oh, yeah. So let's assume you are a zoom. Random values for all this. So your A's are coming from the transition probabilities and your B's are your initial probabilities. To be honest, you can generate this for me. Distribution. Maybe Goshen distribution. They are better. Goshen is probably not the best. Yeah. better probability distribution you can use. and if you generate this from any probability distribution just to fill all these values randomly, initially. then the next thing is that if you have all these values, you can run your forward identity right? And you can run your background as you're ready. because there is a formula to use. But the initial value you don't know. So you actually just want. okay, let me go back. Okay. I guess I think I'm sh Oh. okay. let me go back. I just want to be sure that I was showing the same thing on zoom on the class. Alright. So basically at the E steps you already assume values for Theta K, right? You fill this matrix. You do with some random initialized values, and then you are able to compute your Alpha highs and your better eyes. And then you can actually compute your gamma apps at the E step, and then for the second parameter, you can do the same thing right. You know your B's your, you know, are your A's, because these are randomly initialized values. These are. that is Tita K, right. And once you have computed all this. you already have the values of Eij gathered eyes. And based on this, you can now run the maximization stage. And the maximization stage is that you actually want to get a better values or better initialization for A's and B's. So A's and B's here will now form your new Tita. keep your smart. You have question. Okay? Yes. So so the the hard em, this is more. This is over a single tag. So why the soft em, it's more. This is the bow vash a guardian. This is distribution of loan books. So and actually for the em, this is what we are doing. A small soft em soft where we belong into soft game. Yeah. And and this is the stock version of the em, and we can actually relate this to what we have done previously. The same way we competed. So once you re-estimate your A's and your B's, you have a new value of Theta K plus one. and then you can go back to your E step and perform the algorithm again. And then you go back to your end. State on. how do you know you are making some progress? You you know you're making progress when the training set likelihood you know, you're not making progress. Basically, if the likelihood doesn't improve right. The idea is that if you can estimate the likelihood given Theta K plus one, and the probability is bigger than probability of theta. Given your initial Titaki. Then you know you're making progress. Oh, so. And this is the way you know you're making progress. And if you do this a couple of times. And this likelihood keeps improving. Then you know that you are learning a better estimation for your T. 10 k plus one. Okay? And of course, there are some additional proofs that you can check here. yeah, I think this is where we stopped the last time. And we said, Okay, there's some issues with this, because it's very sensitive with to your initial in your initial random initialization. If the values you have generated in the metrics are very bad. So and they are not very realistic. It's been very difficult for you to improve for the year majority. Better ways of initialization would be actually using external knowledge, for example, from an external purpose where you already have more realistic counts that you can use. Yeah. And of course, so this is just a revision of what we did last time. And today we can move to the next topic off. I forgot the last the last thing we discussed is that apart from part of speed tagging, there are also other sequence bodily tasks like chunking where you are not only classifying every token. but you cannot try to actually categorize things like the now phrase, rather than only is a group. And also we describe the name density, recognition task, which actually doesn't require you to annotate every single word but import. Accept the important entities in the sentence. And, for example, in this sentence, Maggie University is located in Montreal, Canada, and my gear University is an example of an organization. Montreal is an example of location, name, density, recognition can be different, depending on the domain. I have to emphasize that. So you can use any. Our task also in the biomedical domain where you will not be identifying locations because that data, there's no location. I mean, technically, you can have location. So I can give you an example of 2 applications in the biomedical domain you can be. Maybe all you need to identify will be things like disease of patients. Information like the doctor's name. So exam. For example, let's assume you want to analyze. A clinical notes. I can make a note that has been written by a medical practitioner. So the entities you will be recognizing there will be more than just personal name, and even personal name needs to be distinguished. You can have something like patient name. You can have something like doctor's name. You can have something like age. You can have something like disease type. You can have different kinds of symptoms. so, depending on the domain, you can also define your set of named entities. and even for general domain you can expand these named entities, for example, you can distinguish between different kinds of location and GPS, which is geographical political entities, so you can distinguish between entities like city names, country names. continent names, and then you can distinguish it from entities like mountains. and reverse and give them different kinds of entities. So there are different classification for any hour that we can actually use depending on the application. And here the idea for any error is that you actually need to detect spams of multiple words that are relevant to the entity. So last time we also talked about the streams. So if there's no scheme here, it's very difficult to know the entity. You have the entities like organization organization. But there's a question if comma is actually part of the organization, or if everything is a single organization here rather than multiple organizations. And here it's clearly that all of them are multiple organizations like Magu is different from you. Come and you them and you need a scheme actually, that you can annotate this properly. And one of the most popular schemes is something like I will be tagging. But actually we have many schemes. There's iob one there's iob 2, that is, I/O BA scheme, and there are many schemes. So Let me try to give you like a 3 letter word, and I can show you the different schemes. Time to. Hmm. So on the board you have, like 3 kinds of scheme. So the 1st one that was proposed is, I will be one. But now we just call it Iob or or BIO, and if you see bio, usually we are referring to. I will be 2, because this is the one that is mostly used in standardized data sets. So I think the difference is In the first, st I think we're trying to address the problem. And then we just say, Okay, every entity start with iod. But if there is an entity that's an organization that actually starts after you received in Montreal, then you will use the B, the B tag. So you would have if you have You now have Mcgill after Montreal as another organization without space you would all use the organization for this. This is the 1st one. the iob. 2. Here you have what is called the beginning of the entity. and then you have the inside of the entity, which is the eye. And everyone asking is the same. Yeah. So the 3rd example, here you have what is called like the start of the entity. So if you have, like a multi word expression, you actually don't use B, you just use the start of the entity, the inside of the entity and the end of the entity. and for every other one where they are not much a word you use. B just beginning. So these are kind of like the different popular schemes that are available for any annotation. Okay. so yeah, so what are some of the shortcomings of the standard? Hmm's? The 1st question is, how do we have more features to? Hmms. And this might be useful for Pos tagging, for example, we can have important features of our words that actually, we can integrate to this. to the algorithm that makes it very easy to predict, for example, in English language. if a word is capitalized, this is most likely a proper no right. So if you can add this feature even just by writing rules, without all this fancy, according, you may be able to detect a lot of proper notes. So how can we include this feature into into the standard agent? And also what prefixes suffixes? How can we add this feature? So these are some of the shortcoming of standard Hmms, because now it's difficult to add new features. So like I was telling you. And the answer, today we want to talk about crf, which gives us an opportunity to have new features so to our model. But the difference between this approach is that now we are going to consider something more of a discriminative model like, I said in a generative model. You want to lend the judge probability right if you want to classify y given x. This is what you want to classify, but for generation model. You 1st have to build or try to learn the joint probability, and once you learn the general probability, everything becomes obvious, of course, with some assumption like independence assumption to make it easier. I mean, the only way a reason why we have independence assumption is so that it's tractable basically to compute this joint probability. because, if not, if you don't make this assumption, it's not possible to actually copy this right? So those are the generating model. But for discriminative model, you can actually learn the probability of y given x directly without learning to join partners. And this is what we typically do nowadays. Alright, so this is more like a having a tax specific model for secrets labeling. But you cannot use this to generate new samples of water pos sequences. So if you learn a giant probability and it's a good one, you can actually generate new examples. or you can use it for data augmentation to generate new examples. But for this community talk, it just learns everything in your daily. Okay? So the question is now, basically the idea of crf, because basically, these are kind of research driven work. Basically, there's an Hmm have been used for a few years. And in 2021, someone proposed. I know that which is like a modification of what you already know. So how do we modify the original formulation for our dhmm. so for the linear chain crfs, conditional random field. we can learn the probability of Y given X using this function. So here you find out that we can learn a function of Fk. that depends on the previous one. the present one and the input so and we also have a normalization constant, which is all of our order in states. And I'm going to define what will be F of K. This is the new function you want to learn. So remember, what we want to learn is the probability of y given X. But what we are saying is that even while learning this function of xt going to yt. We are also saying it depends on the previous value of y and Z, and we want to learn that function. FF key, and then we normalize it. Okay. so and for just to go back to the idea of the hmm. So in hmm, we are concerned with 2 things are these transition probabilities from one participant to another one. and also the emission probability for a meeting award given attack. And here we replace the product of numbers by just a linear combination of weight and feature values. so which we completely simplify the what we are doing. And here. So we kind of replace what we have in the hmm, we have a function from one part of speech which is in the YT. Minus one to yt. and now we replace it by this indicator function of yt minus 1, 2 dt. Multiplied by another indicator function where yt equals. Na. are you familiar with education function? This is the very simple definition. it's just going to give you either one or 0 indication function. So indicator it will give you either one or 0. So if the value, the relationship inside is true. you just have inequality inside, and if it's true, it gives you one. If it's false, it gives you 0. So that means this function of Dt to n, and we either give you one or 0. Right? So that's the idea. For the that's a modification. Oh, that's the 1st modification. Alright. So the idea is that with this we can actually add new features. That we cannot add in agent hubs. So one example of the feature is okay. We want to know if a word is capitalized. Then we have a feature like that. Okay, what is the probability? What is the indicator function that yt equals stash? And then what is. When xt is capitalized, we can add a new feature like that. Once ending with Ed, we can add another feature like that, and like that, we can add more features to our modem. That is the idea of adding new features to Cf. Which we are unable to do directly with each of us. Okay. so. And there are many, many features you can add. you can say, Okay, if the lens is is less than 5, you want to add a new feature. If the 1st word is a determiner, you want to add a new feature, so there are new features that you can add. and the interesting thing is that when you're working on Crs. part of speed tagging, using Crm, you have to come up with a lot of features. So this requires a lot of feature engineering. To be honest, you have to find features yourself so you can so just a minute. So the more features you get. You are craft. You can improve your performance just by coming up with new features. So, which makes yeah very easy to extend. Yeah. so what? What's the question? Mark here. So it could be anything. So based on what you define? Right? This is indicator function. For example, you can say yt equals a part of speech like we showed in the last slide. So it could be anything. Yeah. And then you'll say at the same time when xt is capitalized. Alright. alright! So in terms of the inference. So the idea is that we still use things like your forwarding. But there's a slight difference. Hmm! Is more of a generative model. And Rss Crf is more of a discriminative model. So for the forward algorithm and hmm, we need to compute the likelihood. And also we need to find the States that maximize the joint probability that's for the hmm and for the crf, what we are doing is to actually compute the Z of X, which I'm going to show you how to compute based on forward algorithm again and for the Vitambi authority. We are only interested in the value of Y give an X. So this is the forward I got in for Hmms. Which I believe you are quite familiar with after seeing this for the last 2 lectures. So the modification we do here is actually hardened this function. We are learning. So let me show you the last one. In the last one we have. Times, which is the initial state probabilities. We have the initial pro activities. B, and then we have, the transition for a brief space. For the Crf, we are just interested letting a function. Fk. which can be user defined. based on the features we want to add. And after after this modification, apart from this modification. every other thing is the same for the forward agriculum. Okay? So yes. Why is the exponential function? Why is it? E to the oh. yeah. Good question. Yeah. So I believe that's that's the way the author has proposed it. So so you have the exponential of this and these values here you find out that these are actually just these are just indicator indicator functions which I don't want or 0. So I so the exponential. It's coming directly from the formalization which they're very sure of. But I don't think it's it's going to ruin anything. so you'll be either multiplying by one or by E depending on the result of the multiplication. Yeah, so because F of K would be one, right? Is that a 1 or 0? Yeah. So each of the 3 can do. Yes. yeah, it's the topic. And our reference. Yeah. Yeah. But even if you have the log of exponential, the big number you can use this log sum of exponential trick that is not gonna ruin your your estimation. Okay, so. But the way we learn it is that, unlike you can. There's no closed formulation that you can derive that this is the mle for the crf, so you can. There's no analytical emery solution. But we have to learn. It's using gradient descent or one example of gradient deter method is Newton methods to find where the gradient is 0. But of course there are also other good Edison methods that can be used in terms of convexity, which many of you may be familiar with you know, when you have a convex function. If you take the negative, or let's say when you have a concave function, if you take the negative of it. It's going to become a convex function which I believe that. Yeah. yes. yeah. we normalize it here. Right? We normalize in a the definition of. why aren't we normalizing here? There's no d here. What are we normalizing? Yeah. it's just to make a privacy distribution. Right? Yeah. you probably don't need to normalize. We're just taking the maths. Bye. yeah. okay, aren't we using the forward algorithm to compute Z of X. Yes, if you do it over everything. You'll find that you're gonna have this set of things. because that's the probability, right. If you remember. we keep moving from one state to the other. And at the last states, we just solve over everything. Right? So the forward algorithm isn't computing probability of y given X, it's computing Z of X, so it's computing Z of X, yes. that's what we said, yeah. yeah, it's complicated. 0 vex. alright. And for a gradient asset. So basically you walk in the direction of the gradient to maximize l. Of theta. But of course you can also do gradient descents where you have a negative of that. and then you work. You work to minimize the loss function. Right? So get an ascent. You maximize. Given the sense you minimize but of course there are other metals like the conjugate gradient, or what is called the Lbfgs. which approximates using the second derivative alright. So for the gradient descents. instead of maximizing the log likelihood. You minimize the negative log, likely. And that's how you can convert a concave function back to a convex function. Right? So. And then you do this for a while. So first, st you compute every other thing you need to compute using the forward algorithm like a forward algorithm. And then you can estimate new values of your data after taking the derivatives. Alright. So here. And yeah, okay. And here we can actually compute what is the gradient of the log likelihood based on the Crm formulation. So what if you take a derivative of this? So what if? First, st let's define? So I have the proof also here. But I think we can go through very quickly. This is what we're going to find after taking the derivatives. But we can. Yes, it's too small. Can you see it's too tiny? So we'll walk this on the board. Yeah. okay. here. So we want to take the log or a full drop of all this. and if you apply the log we are going to have some extra this time. right? Can we start over live? And then the availability of why. you know, Alex. and if you substitute what you have, then I think so computes. then you are good to have summation. and then we have a logo. everything. Thank you. No. Right from here. Here you have the love and the exponential which we can cancel out. Bring up the submission all right. Everything. 1, 2, yeah. Go ahead. Why did you take the Derek and see of a loss function with respect to a particular? If you take the derivatives of the loss function with respect to a particular etc. K. Yeah, I think this is very easy to to estimate, but this will. This could take more time. But the 1st part 5. So this summation will go away because we're just concerned with one sister cake. Right? So what would the left is summation of I. Submation of T, and Theta K is done, and then what you'll have will be the meanings F of K. One C y, 2 minus one. Let's see. And then we can also apply the derivative for this one. So this population will stay because of derivative. But this other one. because this is the submission over every PPYI give them a time. and if we take the revenue team of this one no. from 3rd or next. Here it is. This will be equal to one over 0 of x. the entire thing, one over Z of x. And here we have to sum over all the States wide information of solution of team 5, 1, oh. bye. And if we take derivatives for this one. we still have this one because it doesn't depend. Please go to the thing on Tikka here. and then we have the exponential of this. You're going to use the chain rule. This call is the explanation. I'm sorry if you want to speak with the regulative one. If in 2 of x, so this is like indirecting which I spare to evening. Multiply by. You can edit this here with respect to 2 x. So if you apply the channel. have one solution. please. so you have the same expression information over here, and then we are 14 next slide. And then. Now we need to compute the derivatives of what I'm inside, and then we have. We have the solution over T, because we are only interested in one particular case. So we really have the submission of vaccine here. Hmm. one t, 1 t management. And so let's see. Management. Sorry. hey? You will find out that everything here is very similar to the formulation we have here above. So this will be. oh. commission in my old device. So I think the only difference is this solution can now go out. This does not depend on why, so it can ring. And then you have probability of 1, 5, 3, 1 x, right? When you have the solution. Well, I'll see. That's okay. Why don't see 14 number one x, 2. The final expression on this well, salvation solution over, apply. Claris Gu: Hey? Okay. David Ifeoluwa Adelani: Do you have questions? Yes. Haven't seen the suck. Yes, I didn't understand the last step where the 2 sums become 3 sums, and it says, can rearrange in terms of local State transition. So the local state transition. So here you have like, instead of having yt yt. Minus one. we just changed the expression. So here we have y right music in the slides. There's a 3rd sum over. I 2. No. yeah. I think the last that one is right enterprise. But let me double check my notes, but I think from the very 1st step the sum over, I we just forgot to write it next to the yeah. No, no, because in the formulation here. this is all about, why? Right? So I think it's correct. Yeah. I think it's just the last time. Yeah. but I will confirm my notes, and maybe I'll post on it here. Okay, alright. So in terms of the interpretation of of gradient. so if you want to interpret it, basically, we have 2 things. We have the expression here. minus. If you go back to the last slide. We have this expression on the left side. minus the expression on the right hand side. And what is interpretation of that? So the F of K is like a when you talk about the overall ingredient is is the difference between this one. which is on the left hand side and the the right one. so the one on the left hand side can be seen as the empirical distribution of the future K. In the training purpose which you can actually estimate. because these are uncrafted features based on what you have in your compass. So if you want to know if something is capitalized, you can estimate this from your journey compass. and the last one which will be the expected distribution of Fk. As predicted by the current model. because this one is taken estimation over every, over the over, the entire Z of x, which is the orthogonal So the idea is that when the compost likelihood is maximized the gradient is 0. So the difference is 0. So intuitively. This means that finding parameter estimate by gradient descent is equivalent to telling our model to predict the features in such a way that they are found in the same distribution as in the gold standard. If you're able to achieve this. That's how crf is is able to work based on the user defined features. Another thing is that you can have regularize a regularizer to this. I know some of you ask about regularizer in your homework assignments depending on the model you used to. You can have L. 2 regularizer. This is an example of L 2 regularizer. and if you have the ultra regularizer over this you, if you take derivatives of Theta K squared, divided by 2 to I forgot this one Sigma Sigma Square. If you take the derivatives with a specific Theta K, you're going to have this expression, which is Theta K divided by sigma squared. I believe this is correct. Based on the law of derivatives where the 2 above is going to cancel the 2 below, and then you have minus Theta K divided by Sigma squared. Okay, but apart from using graded designs, like we said in the previous slides or previous classes that you can also use stochastic gradient descent. and the idea of stochastic gradient descent is when the mini winning batch size is one that's where you have stochastically resent. And the idea is that if you do this stochastic gradient descent over over many iteration you can actually approximate the gradient decent category and and the idea is that basically you initialize what is your values of details randomly, and then you randomize or or order over your samples in your training compost. and for each Mini batch for Sgd. The size is one. Then you compute your derivative over this Mini batch. And then you find new values of Theta, and you do this over and over again over many iterations to find your ideal setup and yeah, I guess that's it. Do you have questions? Yes. So here. board line, minus log Z of X. That should be sum also for all the guys that should be some of our work. Odds. Yeah. The next step, if you get out of that. what is Z of X doesn't is not over the I's right. This is the sum. Obviously, that's not okay. But this is, there are webs, right? So so like it's it's in the sound for all the eyes, and then in the next line you take it out, and then we take the derivative over just one log set of exposition. It's the derivative of the sum of all of them. Okay, I have to. I will go by this again, and probably send you an updated version on it. Yeah. alright. Alright, thank you. Cool the deadline today. But I'm working on that. Think that's also a demo. a neural network. Yeah, where each class is. It's like a soft Max. Another feature. And it's just a perceptual or no. Yeah, no. Or features we had in the slides. They didn't take exit into account. Okay? And what is Theta K. Is it still initialization, transition and emission? It's just like a number, right? It's just like a weight, right? And it so it accounts for whatever features we include. Yeah. But here there'll be many more basically like. And we are calling the which will be one the hard em, right? Because is it because it, like you get this exact state specifically that maximize is there? And we talked about both for unsupervised like we have no labels. And we're we're trying to like iteratively find, like the best labels, is there? Which cases would you prefer? Like, which algorithm, yeah, you know, like, when would you use the software like if you have, like an unsupervised problem? Thank you so much. Sorry. Let me just.
