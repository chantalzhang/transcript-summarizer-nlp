 There are over 7,000 languages in the world, and or over 400 of them, are spoken by 1 million speakers . But we don't have a single technology that works for 400 languages per world . Some languages are not supported by  One of the most popular ways to categorize languages is based on Jewish classification of languages based on how much unlabeled data are there on the web and labor data are available . The winners are languages with sufficient amounts of labor data and  The focus of the encoder parts is to have a very good representation of the model, to compress all the information into a single vector . In a decoder only model, the decoder itself learns its own embeddings . The  There's lack of legal data for downstream task and also for many languages . The problem is that you cannot really do have a good, successful, self-supervised training . Transfer learning is very popular in the last 3 years less popular  You can modify the Internet structures to do transfer learning . There's a way to do parameter, efficient transfer line, and on. And this is one way you can do. Transferring one example will be worth future instruction . The second  You can find some language that has training data, , English . And then just do 0 shot transfer to the language . This is very, very, really common . Just append, and then train the same. And then, if you  There are over 7,000 languages in the world, and there is not a lack of data set for many of these languages . We also have issues of parameter inefficiency . There are also issues of what would be the best source language  We don't really have a lot of data sets that were human generated for African languages . By working with different African communities, we're able to create different data sets . We created Nameless recognition for 10 African languages, some in West  There's no translation of some words in English language, but the models are smart enough to bypass that . The answer is , the issue of classes. entities here, you have something using ? You have something in Nigeria, one feature of  Masaka and then Masaka 2.0.0 created a new topic classification for different languages . This is based on BBC articles and voice of America . And then we have some local website where we called news articles, and we label  An existing data set creates a labor data set for many, many languages . Sib 200 is based on topic classification, based on topics classification . The project has been on for more than 5 to 10 years and they are slowly adding 2  The phrase Kappa score was pretty low, just because we're dealing with very short sentences, and people disagree a lot, even for these short sentences . Congo, Indo-europia, was the largest, followed by, Atlantic,  The accuracy by language, family and region of Europe depend regardless of their classification . If language is similar to a high resource language, the transfer still work very . But if a language is not covered during pre-training, they often have  Models Chatgpt just 1 min had much worse results than just training on English . The Tamashek, which is a Berber language in North Africa, and then you'll see that the performance is really poor, because the  Just for the languages that are unseen and script unseen, there's some that are still very accurate . But not really if you have a more popular language, I would say a language with more resources on the web . You have rich representation  This is due to what is called cost of multilinguality. If you want to cover more languages, you also need to scale the capacity of your money . People try to scale a lot Gpt-four, and that's why  The tokenizer is not in your network, it's just it's it's How do I explain this? It's a heuristics to determine. What is the what is the minimal amount of tokens that can serve a language .  How to address this limitation of lofts which we call left? A simple way is just to return your own new model . Another approach would be to use what is called parameter, efficient approach, where you can use things adapters, and  The number of parameters of our model went from 270 million parameters to 140 million parameters without there was some drop in performance . The model is also very big, and up to half of the model size is in the embedding part of the  The only language we did not see improvement in performance is English, because the model is already good for English, there's no need to adapt English again . There's still some drop for some languages Americ and Yoruba, but we will  We create a better model than previous strong multilingual models by Google and Md. how to adapt an English Llm. We also did the same thing for sequence to sequence model, and then you can specialize it also to region of languages  You train a model on English, and then you evaluate on the rest 41 languages. On the remaining 41 languages, we see some interesting transfer to some African languages . In the Francophone region of Africa, we find that they do transfer  Swahili has borrowed words from Arabic due to due to trade, because there's a lot of trade between East Africa and Arabia . Arabic is better than an African language as well, which is quite interesting . Shona to Zulu  If you use the top 2 and you co-train them together, you might have very better, better results than predict, using just one prediction or just using English language . Yoruba and Nigerian pigeon and Yoruba with predicted if we  If you train on the top 2 predicted languages, you'll find out all the time. This is better than using English every time. Will is able to provide you at least stop to language. To transfer from that will be better than  Language is already better ? if you can combine the sauce choosing the best source language, and then you combine it with this parameter, efficient approach. You can significantly boost your performance. and, better still, if you combine this with training