 Jackie Cheung, Professor: I hope you all had a good week reading week and you're enjoying the unseasonably nice weather . But I know it's been over a week and that's an eternity and we've all forgotten everything  I posted information about both the midterm that's happening as as as the final project description . I'll also post the reading assignments soon, and the programming assignment soon . There's lots of things coming up, but I'll try to make  This is just a very brief reminder of CFGS from last class . Instead of for each individual language, English, French, or German, is, does there exist a Cfp for it? Can all of them be modeled as some  There have been two demonstrations that there are phenomena in natural in some natural languages which cannot be modeled by context-free grammars . And it's two particular languages. it's only Swiss German and Bambara. It's not  In all of these arguments about what are natural languages, context-free grammars, and forth, they rely on some assumptions which you can disagree with . They rely on unlimited processing power. These constructions recursively as many  The main idea in parsing is that you have a sentence, an input sentence, and then a grammar, a context free grammar associated with that sentence . You want to come up with a tree structure, with all the constituents that describe that  One of the main difficulties in parsing is to find an efficient way to search through all of the plausible parse trees for some input sentence . The goal that we have is to recover all possible parses of the sentence . This is one key  There are usually multiple parses that can correspond to any sentence of a language . But for us, we're going to talk about an algorithm to recover all parses according to a Cfg. The most important ambiguity is syntactic ambiguity  What are the different kinds of parsing algorithms that exist? If you think about it? There are 2 general strategies. One, general strategy is, you start with the starting symbol. the S . The other strategy that you can have is  With certain kinds of grammars, one tends to be more efficient than the other, and forth . But we're just going to cover one, because these days you're lucky in that parsing is no longer as popular, but you  As is common with many dynamic programming algorithms, you need to create some data structure to store these partial solutions . We're because first we're going to convert the CFG into an appropriate form such that CYK will work with it .  There are 3 possible cases of Cfg rules that do not conform to Cnf form chomsky, normal form . We just need to take all of those rules and apply these procedures to turn it into rules that fit the Cn  For every rule and grammar where B is the left hand side, you copy that rule, but you replace B with A, A . After you do this, then you're allowed to delete a rewrites to B . These are  This means in our case, every time we see a rule where N is on the left hand side, we have to copy it . We have to . copy it and replace it with A . This means you have two copies of the  This becomes X2 and then X2 rewrites to this becomes x 2, and then x. 2 becomes x . Everything is CNF, because all rules have either exactly 2 hand side non terminals or exactly 1 terminal . This  Rules with the same hand side sequence of symbols, and that's totally fine . There's nothing to stop the grammar from doing that . We're going to set up a 2 dimensional table that will store all of the constituents that can be  The entry at each cell is the list of non terminals that can span those words according to the grammar . Each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words . The di  The general idea is that we're going to go from small chunks to big chunks . We're first going to fill out the constituents corresponding to single words, and then we're . going to do the ones corresponding to say 2 words and  The harder step is the recursive step because this step corresponds to multiple words . The key idea here is to take advantage again of Chomsky normal form . Not all rules that produce phrases are of the form A rewrites to B&C  There are 2 possibilities ? form Det NP or Det N? Det N there are two possibilities, ? that means we can build 2 possible constituents there . We can build either an Np there or an NP there or we can or we  If you care about recovering all possible parses, then you have to store both . The algorithm allows us to deal with ambiguity because you can store multiple things in each cell . And then you can use all of them to build bigger chunks  The words corresponding to the current cell . you just need to pick an order to fill out the table that respects that . you can do one diagonal at a time, do the diagonal, and then to do the things to the diagon  For pajamas it would be Np or N, And for pyjamas, it'd be NP or N OK, those were our base cases . And I'm gonna go bottom up and by column, left to . column  You have to make sure you check the 2 cells. It's not always going to be the cells to you that matter. You cannot take that as a heuristic because in this particular case, it's zero to 1 and then one  There's VP goes to VNP that works. It looks it's we can build a Vp. OK, it looks . it looks we can also build AVP also x, 1. also X1, also X. and  Is there any rule for Np, and then P. to check formally. We have to check is there any . rule for NP and then . PNP exists, but it's in the wrong order, because here we have AN  But the basic idea is, if a cell is empty, then that's not then thatâ€™s not possible to build something there . We have to check for rules of the. Form SP on the and there's no SP on  If you want to run the CYK algorithm exactly, there are no heuristics . The rule is just you're just checking that break point . Any, anything that ends with my . It's unlikely you'll find a constituent if  The 1st possible breakpoint is 4 to 5, 5 to 7 . The first possibility is NPPP. do we have any rules of the form PNP? There's a PP rule there . And then how about PX  Are there any rules of the form NPPP? Nope, but there is one for x 2 PP that creates an NP . I feel I might be missing something, but I hope I'm not. 2 to 5, 5 to  Do we have any rule which is BPPP? How about x, 2, or ? How about X2 or X1PP? Yes, we have a Vp from x 1 PP . We have a different vp.  When there are 2 Vps in one cell with the notation where you refer to Vp, and then the cell number, how do they distinguish? In the other notation, you keep a list of you built AVP from 1:  The number of parses you find is not just the number of s's you find in the top cell . Every time there's a decision point where you create . you create the same non terminal constituent, in somewhere in a smaller span  In practice, one of these parses is much more likely than the other . If you have too much complexity as determined by, say, depth or something, it's less likely as a parse . But we're going to try some  Probabilistic parsing recover the best possible parse for a sentence along with its probability . We are given the sentence sent and then Tau of sent is the set of possible parse trees . We want to find the Argmax of the probability of  We're going to extend the Cyk algorithm to keep track of probabilities in the table itself . That'll be more efficient because then you don't have to explicitly create all of the ambiguous trees and evaluate the probabilities for all of them .  If you're able to create that match that you can create the bigger constituents, you also have to compute the new probability of that constituents . And you do that by taking the existing probabilities from each of the subconstituents, and  If you create an Np constituent here, then you have to also figure out the new probability of the elephant, which will be 0 point 6 times 0 point 2 5 times the probability . Np rewrites to dead End,  If you have two possible ways if you have 2 possible ways or multiple ways to create the same constituent . Multiple ways are possible . Then you throw out all but the best . But then, when we're recovering, we That's .  At that point you no longer need to care about the probabilities of each of the sub constituents because the back pointers already took that into account when you were creating them . This is called a tree bank. A tree bank is a collection of  Everything we talked about in that module of the course to do with smoothing still applies here, because these are also categorical distributions . ESPY writes to . as we writes to it, suppose you have something a sentence, or even  Pcfgs does a very poor job of modeling all of these situations . If it's at the beginning, it's much more likely to be I than me, whereas with, if it's in the object position syntactically,