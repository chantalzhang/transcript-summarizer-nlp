 We are not able to have this in person, due to the email we got from the university . Today is the lecture 10, and from class we have another professor taking it, Professor Jackie Sean . We have a reading break week,  And then we discuss recurrent neural networks . And after that we'll move to long, short term memory networks, which is the most popular and the most used Rnn architecture . And at the end we are going to talk about Lst  We are interested in learning the likelihood or probability of X given theta . We try to compute the mle for this by taking derivatives, and we have this formulation in the last slide . Some things were not clear, and there was  We want to review artificial neural networks . It is the learning model which automatically learns nonlinear functions from impute to output . It's really an interesting architecture or method, because number one, it's biologically inspired . Given enough data,  All these connections are weights, and then can be expressed in terms of metrics, we can have it in this form . The weight mattress describes all these connections . You only have, a vector of parameters. , you have a matrix of  This is a revision for stochastic gradient descent . We are only interested in one training, example, and then we can find gradients of the loss functions with respect to the parameters . We use an algorithm called back propagation. It  Time, delay, neural network can incorporate timing information into a simple fit for learning tool . You can also do that using a simple, fit forward network . And of course you can even increase the context window. And if you increase the  Recycled environmental environmental is a better way to incorporate previous information into the current model . Recycling environmental gives you an opportunity to incrementally incorporate previous . information into a recurrence information without using a time delay neural network . Rec  Shidan Javaheri: I just wanted to confirm in the previous slide is theta the parameters shared by every hidden state . David Ifeoluwa Adelani: the data involves all the States parameters that you need to  Lccrf is a linear chain crf, a more linear model . Rnn is a nonlinear architecture of neural networks and is more expressive . Of course, you need more data to train a recurring neural network in terms of feature  Given a document, you need a single output, which is the class of the document . Shidan Javaheri: I think the 1st one would be best for machine translation . David Ifeoluwa Adelani:  The model includes what is called a memory cell . Inside each memory cell is a vector of weight in the evening layer . We want to forget some old information as you move from one to the other . And you can also have an activation  The sigmoid function, it always gives you a value between 0 and one . If the value is near one, you should keep that information . If it's near 0, you will likely forget that information. If it is near  In Lstms we can propagate a cell state directly to fix the what is called the vanishing gradient program . The idea of bi-directional Lstm is very, very simple in your standard neural network . You want to predict  Crf is just another linear model, but can combine both the forward direction and reverse direction . Crf capture relationship between output levels and relationship between the output levels . If you train a by Lsdr model with a simple linear layer  This is coming from a popular architecture in 2015, 2016 . And one of this paper, is one of the papers for your reading assignments, but if you have time, I will encourage you to also read the 1st one which is  Lstm Crs is very important for many tasks, especially if it's a sequence labeling task a language model in predicting the word, of course, for language modeling . David Ifeoluwa Adelani: We will  David Ifeoluwa Adelani: for machine translation, you need to encode all the sentence information for language, modeling for every word you have seen . Shidan Javaheri: I had imagined that would also be helpful  David Ifeoluwa Adelani: Your role is in time, you are predicting the word also alongside . Shidan Javaheri: I will still be back at some point after every 4 weeks, of course. I