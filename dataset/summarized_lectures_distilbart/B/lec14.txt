 Professor Jackie Cheung, Professor: But good evening. We are going to continue our discussion today about lexical semantics . We started talking about wordsense, disambiguation where that you have a repository of words and their senses associated  I'm going to go through some algorithms again, not state of the art algorithms. They don't use neurotechnics. But the reason that I'm presenting them is that they give you the flavor of how you can use knowledge about  Lesk's algorithm is a slightly different algorithm . The high level idea is, take all of the context . Words extract some information from them to come up with some beg of words, representation of it . For each sense of the word  There are all sorts of things you can do, and they each yield a slightly different algorithm . Can you do partial overlap by characters, or is it always by whole? How do you do overlap? That's a design decision for the  This is the idea of lemmatizing and counting lemmetized Lemma overlap is, is implementing that idea . But in other cases the partial overlap might be overlap in terms of the prefix, which might not be that meaning  The model pulls itself up by its own bootstraps, which is it gradually improves itself through multiple rounds of training on automatically labeled data . In Jarowski's setting there are only 2 senses, and all of these contexts will involve the  The words being used are live and manufacturing are those derived from life, form and factory or . The seed words here are life in manufacturing . Once you have automatically labeled data, the step is to train a classifier . It has to  Bootstrapping lets you discover other words that co-locate that are found near each of these senses . Life in manufacturing should have very high confidence, according to this new classifier that we trained because that was the cue we used  After one round you take your highly confident labels and you add them already into the csat . In Yaroski's algorithm they only look at the current round . But in our setting we don't have any correct answers while running boot  Can you think of your Oscis algorithm as having an automatic dictionary? We're using the dictionary definitions quite directly . This is algorithm number 2, Jarowski's algorithm . We're not using dictionary definitions, only we might be inspired by  We're no longer working on Wsd, we're working on detecting lexical semantic relationships . The goal is to figure out how do we find hypernym pairs, or, other words, in some lexical . semantic relation .  The idea behind Hearst patterns is that if we can identify expressions this such as then we can discover hyponym, hyperym pairs automatically . And that way, if you're constructing Wordnet you don't have to sit down and think  How do you find these patterns? curse was just really smart, but can we do an approach that allows us to be less smart? We can be Spartan in a different way, and use our idea of bootstrapping that we just  Learning is more the particular machine learning, optimization, paradigm or technique that you apply in order to change the model parameters . I can believe it's totally possible to combine the 2 of them together . The idea of bootstrapping with reinforcement  The strategy of bootstrapping can be very useful if you just don't have a lot of resources or time or data to hire experts . Bootstrapping assumes that you have access to unlabeled samples that are relevant to your problem  There are multiple senses of bootstrapping in statistics, and that's the other one . You have to run a wst to figure out in a context whether you mean the Statistical Bootstrap, which is for statistical testing . The other  Large language models based on predicting mass tokens in context are implicitly relying on the idea that the context around which a word appears is useful to getting the meaning of that word . The idea of distributional semantics is what powers large language models .  When it's the center of your attention and you're looking at all the words around it, that means it is the target word . And you're accumulating the counts for that target word, or it can itself serve as the context word  The question is on a large purpose. How do we get the main target and context words? The target words are just the words you see in the Corpus that you want to model every time you see a new word, you can add  They're the same dimension, ? that means you can compute cosine similarity between them, . and is defined to be the dot product of a dot B divided by the norms of the 2 vectors . This corresponds to the cosine of  Cosine similarity is a measure of relatedness, rather than similarity . Good and bad are very different from each other in any particular context . People are often very loose with their language, and in terms of how they talk about things,  Aha is this term context, matrix is this analogous to word embeddings? We will return to this in 10 slides or . What are there any measures of similarity which are of similarity directly and not relatedness? Yes, but  The average is paid for each vendor for the average before the decision from each readers to all . Different annotators might have different ideas of what a 3 or a 5 means in the scale . This is not a great evaluation for many of  Word embeddings are these trained vector space representations of words to predict words in context . A decade ago word 2 Vec was a really famous model that did this . This was the beginning of this whole deep learning revolution and nlp,  There were lots of things that you would have to do in the during the . factorization and compression process to achieve the same high level of performance as with the Skipgram based approach . The first half of the lecture was about word sense