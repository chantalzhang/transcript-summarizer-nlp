 Hmms and Crm models were some of the most important models that dominated the field for more than 5 years . When deep learning came around it was still very important to have conditional random fields to improve performance . And today we're  There are 2 ways of going about it with generating approach or discriminative approach . Generative approach is to learn the joint probability between X and Y, and the assumption is that if you can learn successfully, then you learn everything about  You can easily compute using dynamic programming, you can compute the forward. P of all, given theta using either forward algorithm and backward algorithm . And if you're able to compute that, you could also integrate that into the computation of  For the expectation maximization, it boils down to 1st computing the probability of every State given the observation . And after that you can compute the transition probability from going from State I to State I . After that you update the current parameters  A's are coming from the transition probabilities and your B's are your initial probabilities . A's and B's here will form your new Tita. the hard em, this is more. This is over a single tag. Why the  This is a small soft em soft where we belong into soft game. and for the em, this is what we are doing. Once you re-estimate your A's and your B's, you have a new value of The  Location, name, density, recognition can be different, depending on the domain . In the biomedical domain where you will not be identifying locations because that data, there's no location. There are different classification for any hour that we can use  On the board you have, 3 kinds of scheme. The 1st one that was proposed is, Iob or BIO, and if you see bio, usually we are referring to. I will be 2, because this is the  You can learn the probability of y given X given X without learning to join partners . But for discriminative model, you can learn a function of Fk. that depends on the previous one. the present one and the input and  We replace what we have in the . we have a function from one part of speech which is in the YT . and we replace it by this indicator function of yt minus 1, 2 dt . Multiplied by another  Rss Crf is more of a discriminative model, RSS Crf . The more features you get, you can improve your performance just by coming up with new features . The modification we do here is hardened this function .  There's no closed formulation that you can derive that this is the mle for the crf, you can . But we have to learn. It's using gradient descent or Newton methods to find where the gradient is 0 . Log sum  Given the sense you minimize but of course there are other metals the conjugate gradient, or what is called the LBFgs which approximates using the second derivative . instead of maximizing the log likelihood . You minimize the negative log,  David Ifeoluwa Adelani: Do you have questions? Yes, I didn't understand the last step where the 2 sums become 3 sums, and it says, can rearrange in terms of local State transition . David  There's a 3rd sum over. the last that one is enterprise. But let me double check my notes, but from the very 1st step the sum over, I we just forgot to write it to the . No, no  You can have L. 2 regularizer. and if you have the ultra regularizer over this you, if you take derivatives of Theta K squared, divided by 2 to Sigma Square. If you take the derivatives with a specific The  But here there'll be many more . And we are calling the which will be one the hard em, ? Because is it because it, you get this exact state specifically that maximize is there? And we're we're trying to iter