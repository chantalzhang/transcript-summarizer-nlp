 We'll continue our discussion about part of speech tagging . And today we are going to be going through some algorithms that are popularly used for this task . And last time we are trying to compute, Emily. I have to remember Emily  In Markov chains we talk about how we decompose the joint probability which by has been removed from the board . We also talk about the model parameters which we can estimate using an mle . And today we are going to try to  High equals the part of speech divided by all the possible sentences that are in your training couples . This Emily, is often calculated over your corpus. and then you have the a high J, which is computed that given the previous State,  The simple answer is that you just have to marginalize over all the state sequences . The problem is that if you try to do this, you have exponentially many paths and risk to priority . The solution is that we can try to solve  Theta can be referred to as what is the probability of observing from one to T, because for every State take a random one there . We are saying that it depends on the previous state at time . If you want to compute for  The probability of Dt beginning this sentence, . the probability of a meeting, the what you observe? If you were , let's go to last lecture . This is what we did the last lecture, ? We have the initial probability  In the last column, once we have calculated everything for distrellis, you can just marginalize marginalized means you sum over all the probabilities to get your Po given data because everything previously contributes . And then you continue to sum up all  Alphas are the are the one we computed at the forward algorithm in the backward algorithm, you compute the betters at T . The last stage you just need to marginalize over what you have at the end . After that, then  There's a simple trick that we use. if you have to multiply a lot of probabilities. And you need to take the Agmas most of the time. It's better to work in Logan, because, instead of multiplying all you  The forward backward algorithm and combine this with Em algorithm, which I will show you a couple of slides . The log sum trick is the log of the summation of exponential of probabilities . And what they say is that this is stuffed off  If you take the Ag mass over all the probability of Qo, given theta this is vitabi algorithm, which is very, very popular . Instead of as summation, you just take the maximum instead of summing, you  We are supposed to multiply. and the better it is together. The only difference is that we're going to take the maximum instead of the submission . We are still at the what do you call this? because we want to go for  Every state in the 1st column contribute to what would go to the what? that means we, if you want to compute. which is the Alpha Gt. Minus one would be 0 point 0 2 0 point 2 5 and  We may not be able to go through the calculation step by step in a class, but I will encourage you when you get home you should try to work it out yourself . For the vitabi algorithm, the difference is you take the  In the forward backward algorithm, where? we want to connect to the Em algorithm . In the algorithm, you predict the current state sequence using the current model. And then you update the current parameter of your model. you, you learn  We have a gamma I of T, which you can decompose to be this probability of Q equals? I , what's the probability? This is the transition probability from time step T to type, step t plus one . Once we  At the maximization stage you can get a better value for all the all the teta values you are looking for, and then you will go back to the expectation, to the E step . And the question is that when do you  You can estimate your offers and your betters, which are your offers . But in the unsupervised setting, you cannot estimate this . You can, that you calculate is what we call the is our theater . If you get a  Emi guardian can be used for different tasks, such as identifying syntatic chunks in the sentence . And also, you have a very popular task named Entity Recognition, where you want to identify popular categories personal name, organization, and  iob 2 tagging scheme, where you clearly define the beginning of an entity and the end of the entity, and then you have the old tax, signifying that there is no entity . East is not an entity located, located is