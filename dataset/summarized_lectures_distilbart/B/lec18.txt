 David Ifeoluwa Adelani: Today we'll be talking about neural machine translations . We'll talk about attention mechanism, which will bridge the discussion from Rnn and Lstms to the transformer . And we'll touch  When the sentence length are high, there's a need to come up with an algorithm to be able to compute these probabilities more efficiently . Ibm version 2, Ibm model version 2 allows you to model this. version 3 and then  Instead of splitting the sentence into words, you just split it into phrases . Then you will be able to compute the probability of the target phrase given the previous source first, st and then you could add some distortion . This distortion is important  When you translate word for what the question is, can you get what is the best candidate translation for this word? And that's where you have the E climbing . The greedy step is to take the change that produced the maximum probability .  This requires a large amount of data to be able to train immersion translation previously for any single language . You need at least a million parallel sentences between . The initial language you want to translate from to the target one . But also this requires  We'll talk about a modification of this, we where an attention is introduced . You encode every single sentence . And then you have this context, vector, Z, and then you pass it to the decoder side, where you decode  We tried it for a language West Africa, and then it gave banana the translation was correct . But the only thing that was missing is I changed the word apple to banana . The model already forgot about the word you are trying to refer  Attention can be seen as a soft version of a retriever system from a memory which has a key . Attention is seen as something as a salt retriever . The key is a representation of an entry in memory, and then you have  This visualization of attention gives an idea that the motor is paying attention to the most important part of the sentence . But it doesn't give you a complete picture of what's going on in the neural network . Sometimes it helps to give you  The attention, architecture, or the transform architecture is that it has been applied to different modality, to vision . It seems to work to. Although we have been trying to replace it. far no one has been very successful. you  We want to use this vector to decide how important the world is to another world as part of the attention computation . This is how you compute a value associated with the key . The queries and the keys are those words from the same sentence  We want to know that if you have thinking machines. What is the most important thing? Is it thinking or machines? This is what we want to compute . It's safe attention, because you are doing attention to the same sentence .  How important is the quality of the embedding of the word to the results of it? We need to be able to encode the word very , and it's either from a separate model, word to back, or the model starts with  You have to learn the embedding, you can just ignore the encoder path if it's not necessary . Google translate, they did away to just do away with the they added an encoder, and then they have a better enc  Rnn will feed one token and predict the token untransformable . We are making a lot of improvements in terms of developing more powerful gpus. This was really a big problem, because it's very expensive at decoding. But  Max language model can be used for many natural language, understanding tasks from sentiment to topic, classification to any classification task based color . And it will also train on what is called sentence prediction. Does this sentence follow this sentence, and then  More than a 75 billion models that have been released, it's just very difficult to run them . But nowadays we also have, more than a . 75 billion . models that were released, they put it behind. in terms of the  T. 5 was created on the C 4 corpus and it has, , over 34 billion tokens . They scale it up from a very small model to even 11 billion parameter model . And you can cast any task as a text generation task  We also have a multilingual version of this that was trained on, , one on one language and a lot of data, 6.3 trillion tokens . And, , you can use this for the machine translation task because it's a  Even if this pre trained model has not been trained on a language. there's a way you can just quickly adapt it to that new language. Even though it's not trained during self-supervised pre-training or large machine translation  You give it an example. And this is the answer . You can also prompt it with an explanation on how it arrived at the answer, and the explanation doesn't have to be much. This is teaching the language model to reason before