 There are some times that you are. different combinations in your test sets. and you have not estimated that probability. that means the id scenario will be damaged, you must be able to estimate the probability that the probability is not 0 .  We're going to see how we can derive also a general MLE. For a. for a particular purpose. If we know the probability distribution. we can have an exact calculation of what will be the Mlb. of impacts over  We often do not know the true distribution of the test set, we assume our model is good enough, and then we try to estimate, just based on our model, which is our cube? Entropy gives us a number in bits  The final formula for complexity is here ? 2 to the power of minus one over 3, because the size of the corpus is 3 ABC . The size of a couple is 5, the number of the coppus is 3, 5  We assume that, given a training data. and you have already estimated unigron probabilities. as P is probability of a equals. 0 point 3 probability of B equals . 0 point 4 probability of C equals 0 point . probability of  If you flip a coin, you can either be the head or what, or the tail . If you multiply what is the probability of producing ad times, what's theta raised by N minus one, and then one minus Theta  Theta 0, theta, one to Theta K minus one, is the probability of one to theta . Theta is a probability of every possible outcome for the Bernoulli distribution, we have 2 possible outcomes . We  Theta, I, because you want to find one of the teachers at Theta I, and then you also perform derivative with respect to the Lagrange multiplier . With respect to one theta I you're going to have this formulation  The optimal parameter of computing Theta is just. You divide accounts by what's the total size of the corpus . The problem is that if you are going to test it for an unseen data you are very likely to have a lower performance  The idea of smoothing is the probability distribution to shift some probability mass to cases that we haven't seen before, or we are unsure of . The art theta smoothing technique is what is called the art thea smoothing .  If you want to resolve some probability mass to an unknown world, you have to formulate off what is called data . Data can be, it's it can be a very small parameter . Some people can use one divided by the size of  Data is defined to be equal to one that's Laplace moving . Laplace discounting is when the letter is one . Loss data divided by size of your training purpose plus size of vocabulary, was supplied by data . That's why  Back off language model is a simple way to combine trigon probability with the background at a test time . An example of a more advanced smoothing technique is what is called good sewing. and is a little bit more sophisticated for handling the  F, 1 will be the No. 2, the number of events with a frequency of one that's f 1 f. 2 . The probability of unknown is f 1 divided by N divided by the size of your campus . F of  Every word should appear one once, if it's in your training data ? With unknown words, often unknown words don't have here, but you assume that your no wants to be . When you want to evaluate the model, ? what  There's some problem with the original formulation of goods road wish we are good to adjust events in the slide . Yes, the probability of passing 0. But it's 2 0, because a good sign. Wasn't that smart enough  Probability of the camp would be C. 2. Star, divided by N. Star . And C 2 will be star will be computed as 3, which is because c, 2 is 2 C is 2 at that stage, plus  There is a trade off between how the model expresses expressibility . Simple, good turning often fail, and simple, good turn often fail . The more data you have oftentimes the better it is, but for statistical language model, the