 Lecture 4 will be on nonlinear classifiers . We examine different text classification techniques and then we examine at least three different techniques . After perception then we'll examine neural networks . We'll also treat smolting in this class .  The good news is that assignment one will be on test classification . You can try out different techniques we studied naive base, SVM and many techniques . The major difficulty for linear models is that they cannot learn very complex imputes or any  In logistic regression, you want to look for a certain weight that allows you to model your input X to give you Y . The best way you can turn this into a probability distribution is to use a function that ranges from zero to  artificial neural networks can be said we biologically inspired. It's a learning model which automatically learns nonlinear functions from input to output . Each neuron takes a scalar input and produces a scalor output . The more data you have, the  The more data you have is just the better the performance . Asia Bengio has been working on for many years and when it became successful, they just applied the same principle for language modeling through the same architecture to speech recognition . It seems  You use a technique called backpropagation to improve the weights that it gets better to map the input to the output . Remember, we're trying to learn a function that correctly maps the input X to Y. And your learning phase  The first layer of a neural network has a dimension of 5 by 8 and the one you have a dimension 8 by 22, and the last one depending on your final output can be 22 by two because you need to project it back to  NLP task that you have multiple outputs is if you want to do part of speech tagging . Size of your output depends on the size of your entire vocabulary, the number of types . In that case you need a function that can turn  The number of types is just how many path of speed tags do we have . If you use UDB, we have universal dependency. You have 18 types . Your input will still be different because it depends on how many words you have  All , if you want to do the optimization you need a loss function . One of the most popular loss functions used for classification is cross entropy loss . For training neural network, one of the popular approach is the gradient descent . Back propagation  The weight of every layer is what you randomly initialize, and then you modify it to get better . The only hyper parameter here is your learning rate . The difference between the original gradient descent or the stochastic gradient descent is that for  In NLP we are operating on discrete values, it's either a cat or dog . There's a need for you to convert every single word into words, a representation . The simplest way you can convert it is what is the position  If you are able to find a good representation for every word in your vocabulary then you don't need to levertize and then you can learn everything using a neural network . Nowadays what we do the field has changed a lot.  For any NLP task you want to do nowadays based on neural networks, you're going to start with a pre trained model . You pick an existing model and then you fine tune it on a downstream task . This is called pre training  The C plus evaluation metric you can use to evaluate your model if it's good is accuracy . If you have 10 examples and your classifier is able to predict 7 correctly out of 10 what's the accuracy? Or 0.7 if  Because it's just about the correct prediction, there are better ways of thinking about this . Some of these have been motivated from information retriever, computing. What is a precision you can say out of what has been predicted? And you  What's the precision for spam 0.2? What is the recall one third? And what is the F1 score for spam 217 2 / 7 and that's what's 2/ 7? OK, I did some solution on the