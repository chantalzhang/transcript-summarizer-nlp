 We're going to spend a few lectures on text classification before talking about other things . This is the main announcement to the Ed discussion platform because that's where we'll be releasing all the materials . You can also ask further questions after class  What is natural language generation? What's the difference between phonetics and phonology? If people don't answer it, then I'll just make sure to make a note and put it on the midterm or something . What about morphology and  The main topic of today's lecture is to assign a label or category to a piece of text . Text classification tasks are a big part of our lives whether we know it or not, one big 1 is a spam detection. that relies  We'll focus today on feature extraction and some common processing that people in working with textual data do when trying to process text and pass them into a text classification system . And then we can talk a little bit more about the experimental methodology within  A common research paradigm these days in NLP is first, find an interesting NLP problem that involves language data or involves some need that has something to do with language, such as, which emails are spam . Machine learning is about giving  Part of speech tagging problem is that it doesn't make the most sense to think of it as a standard text classification problem . In supervised learning, we're mapping from the input to the output, it can either be a regression problem or  Most NLP work involves text . Involving text ends up being classification problems because things are often discrete . Unsupervised learning is very special in that you can solve for the best value of A&B analytically by solving the  Problem definition is a big part of NLP, and it's receiving more and more focus these days because we have powerful large language models and transformer models, ? that means we might be interested in figuring out what kinds of problems of practical  In supervised learning setting, we require having training data and having labeled set of annotations . We need to have some reliable source of that in order to be able to train our system in the 1st place . If you can't get an  In the current era where there are lots of data sets online that you can download and play around with, it's both great for the purpose of just running some algorithms and optimizing, but also scary in that . Just don't forget that  We specify what the possible YS mean and then we're going to train a classifier on top of the document . You can have a million features if you want, and then you have some values that are filled in at each dimension  For example, we're just saying that everything you think might be useful, you can call it a feature, and it's up to the job of the learning algorithm to and the labeled training examples to help you decide the effects of each  There's a process called lemmatization, which involves removing certain affixes and recovering the lemma, which is the form that you would use to look up a word in the dictionary . Or similarly, a word flies,  The motivation of stemming is similar to limitization, but it's very practical . It's because you're making fewer distinctions, and sometimes you want to make fewer distinctions . Another possibility is to do part of speech tagging and record the part  A popular basic approach for training a text classification system then is to just represent your input document as a bag of words . If N is 1, then these N grams are called unigrams . But you can choose higher orders of N  You have to apply lemmatization. It's what is the form you would use to look up that. Then, I'll give you 5 minutes to do this with your neighbors, and then we can come back and look at  Every single type of word should and punctuation if you include that should end up being its own unigram . Also there might be bigrams, . bigrams would be something good day has a count of one and then every adjacent  Are a key indicator of the relationships between words in a sentence, and their distribution might tell you something that's useful for text classification . We need a systematic strategy to try out these different decisions and figure out which one works best in our  As soon as you start working with real data, things are messy and there are lots of additional decisions you have to make . This is where a lot of machine learning happens, and this is where there are whole courses on machine learning .  You can think of them as specifying a class of possible functions F that could be learned, and the actual learning process is to select one particular F from among that class . OK, there's something called a Naive Bayes classifier  If I formulated a sentiment analysis, what we're solving is a . sentiment analysis problem of . what is that particular piece of text described? But what we care about might be getting rich or not getting rich . This is something that is  In many settings, it's it's not wrong enough for us to not do it . But in this setting, it might be the case that assumption is bad enough to not be meaningful . I'm not saying that any algorithmic thing