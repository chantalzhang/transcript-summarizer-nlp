 Yesterday I said tokens, and I was wrong. It's a description of transitions between tokens other than not tokens . Instead of tokens those we'll call them non terminals. That's definitely one of the components of a Pcfg .  In the last lecture we discussed probabilistic, context-free grammars in their most basic form, which sometimes people call vanilla Pcfgs . The idea here is that you have a particular way of estimating the probabilities of rules  Certain classes of nouns are more likely to appear in the subject position versus the object position . This is not just in English, but it might be a cross lingual thing as , just because of, how the world works and what  The main problem with vanilla Pcfgs is that they make independence assumptions that are both too strong and too weak in different ways . In this vanilla PCFgs, there's no relation between any of these rules in terms of their probabilities  We're modeling every single combination, and in every single ordering of hand, side symbols, non-terminals, and terminals, as independent of each other in their generation . horizontally across the hand side of a production, we're modeling  The Penn Tree Bank is just a tree represented in a different way using bracket notation . We would annotate everything, including the root of the sentence . We're gonna pretend that every hand side is a Mini Markov chain that we need  Gram language models involve those symbols that would appear in the hand side of a rule . When estimating its probabilities, you have one parameter for its probability . You have to solve one n-gram modeling problem for every left-hand side symbol  The process is called markovization. because we're making Markov independence assumptions. vertically speaking vertically speaking, that was called that would be called vertical Markovization . But you don't trust any single one of them very much.  The standard assumption of Pcfgs is infinite order, because you're taking the entire sequence as atomic and modeling it with one parameter . The scheme we just described with the background model is called 1st order, and again, you can  Parson. wanted to clarify about the order, because you described that, the one we did the 1st order . The question is about, what does the horizontal markup order mean? The one we just described is H equals. One  Although in a slightly different way, and the probabilities are all messed up, I would keep it separate from this method and this procedure is about training of the model and learning the parameters . And then the Cnf thing is for the  We want to be able to have some formulation and representation of meaning in the world, that we can represent and understand things and use Nlp systems to derive these representations . From there you can make draw inferences, through some approach that  The meaning of telephone is that it's a function that takes in an object from the world and then it gives you true or false . And then there are many, many other infinitely many other objects in the world which are not telephones  The intentional definition is talking about the conditions, the necessary and sufficient conditions for something to mean something, can take the form of a dictionary definition . The reference is about the objects in the world that it points to . This is not a  This is the second a main general area that people work in to think about how the meanings of words relate to each other . Hypernomy and hyponomy are terms that can be used to define relationships between words . The word that denotes  There are linguists and people who say that true synonyms don't really exist . There's no 2 words or expressions that truly mean exactly the same thing . But it's still useful to come up with this idea of some synonyms  Polysemy means the phenomenon of a word having many meanings . Polysame involves multiple related meanings of words . The word newspaper has many, many different senses . The idea here is that the 1st one is more about the concept of  It can be very difficult to distinguish between harmonymy versus Polysemy . The 1st and 3rd sense of the word are very close to me. But we just we don't even think about this when we process language. What  One is about physical locations, the other is about your job in an organization . But then, it's through this metaphorical extension of that . You can think about jobs within an overall abstract space of a bunch of jobs in the organization  If metony becomes popular that it becomes a dictionary definition, does it stop being metronomy at that point? There's a synecnec of metonymy, which is funny because it usually involves things they cannot say in class  The relation is Hallonomy and meronomy . Something can be meton a metonym, and it can be a polysemis or . Something is not necessarily mutually exclusive . It's about is our relationships in programming, in object oriented  It's just a different style of hierarchy, if you already have remembered all the terms yet or not . Cut hair is, you have something that's attached to something else, and you're shortening it . Cut bread is you have  In Wordnet, there are 6 different synsets associated with the word table . Each of this, each of these think of it as a concept with a certain way that you can realize it in language using some words . In Word  She sets a fine table room and board. This is how it's organized. If you click on one of these if I click on the first, st since that it gives you its connections to other synsets it has a direct  The general idea here is that you can use the words in those contexts to help you disambiguate . In the 1st context, the fact that you see the word tired is informative and typing. In the second context, something