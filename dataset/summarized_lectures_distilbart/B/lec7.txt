 Part of speech tagging used to be a very important task in the 90s because it was very essential for many applications from machine translation to speech recognition and many tasks, and also for named entity recognition . Today we'll be talking about making  Part of speech is a way of defining a syntactic category that tells you about the grammatical properties of the world . English language is following the structure of SVO, you need a subject which is typically a determiner and a noun  Part of speech tag covers 37 categories including punctuation . Order schemes that reduce this a bit to 18 called universal dependencies . Some are open classes of path of speech and some are closed tags, but many are intuitive, but some are intuitive  Some languages do not have the concept of past tense . Yoruba, Yoruba and other African languages don't have a concept of plural . Punctuation should be another one which is OK, it's obvious but punctuation is the  The universal dependency task set is based on the Penn Tree Bank . It has different languages, Czech Republic, Japan and Czech Republic . It's not very clear if you should use the French style, Japanese style, or the US style or  In Japanese, in Japanese, there's no great decision between nouns and pronouns . Pronouns are open class rather than closed class in English . In celicial languages in Pacific Northwest, distinction between noun and verbs is difficult .  We show two different task sets, using Pentry Bank and using universal dependency . The interesting thing is for part of speech, there's also dependency between a Canadian. , there's a lot of dependency between Canadian geography nerd. And then  Markov chain is called the Markov process . It's a framework that can help us to model this . We want a model that will be able to predict both the word and the tag correctly . We have States and we have transition  We can consider part of speech to be predicted as eating variables because you don't see them during test time . This is where the idea of hidden Markov model is coming from. Something is hidden, you model it as an Ed variable  When you unroll this out, this means that the verb depends on another part of speech, and the word that will be generated depends on the part of . the way it's being modeled is that there are many possibilities for this part  In a Markov representation, we have the observed which is the observed random variables O1, O2 . In graphical models, instead of saying hidden variables, we often use what is called a latent variable . And then you have the  The two things you said I cannot, what's the difference? the word depends on that POS, Yes. But we are not using the context of the words in this modeling, in this simplified one, just that you get the idea  In the very simplified 1, you can estimate all these probabilities very similar to how we estimate all the probabilities for naive base . You can estimate the initial probability distribution . You look at the entire training corpus in different sentences . How many times  There's a dependency between this. , but of course you can do that. But what will it mean? We're trying to say it depends on it. We're just saying bigram assumption, it can also depends on the last  What's the initial probability for DT? Is that what you got? What is the transition probabilities from VBD to preposition, to iron? And what about for VBD? Yes, 2 / 4. And what's the transition  We're going to talk about different algorithms on what is the best way to tag a sequence . We have the following questions which we need to estimate . And then we can also use some algorithm to determine how to estimate the tags given all  But is just to say, OK, for the stuff we use this data set and I just added script and I specify which language which set of languages for we use. , if a language containing many languages, but we just use for