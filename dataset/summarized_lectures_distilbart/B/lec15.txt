 If you have a midterm conflict, please send me email, by the end of today . We need to figure out if we need to book a room for midterm for the makeup midterm and forth . The midterm is, , Wednesday, if  This is the 1st iteration of this distributional approach to modeling meaning, and which eventually led to large language models . But a lot of search engines are still based on word of co-occurrences . Instead of raw counts,  If 2 words happen to occur with each other more commonly than you would expect by chance, this ratio ratio would be greater than one . On the other hand, if 2 words co-occur less likely, less likely than expected by  Not really because if you choose a different base, that's just multiplying all the numbers in your entire term context matrix by a constant . Another thing that people often do is that they often discard negative values in the Pmi. This is  Sigma K is a special, because there it's a diagonal matrix that contains the singular values of this original matrix which you can think of as some characteristic way of looking at the properties of that matrix . And in practice doing this truncated  Word 2vec is a famous model that does this from 2013 . People also train neural models of word embeddings for lexical semantic for Lexical semantics . These are relatively simple architectures and ideas . But sometimes they still use word 2  This has been how distributional semantics has evolved . The Skipgram model and the Svd based model are equivalent to each other . The topic is the, , the final major topic for the midterm. And it's about compositional semantics  Compositionality is the idea that the meanings of sentences is not just some . It's not arbitrary, and you can derive the . meanings of a sentence by looking at the meanings . of the subparts of the sentence . in particular  The relationship of words to each other with all these somatic relations, synony and autonomy . And we also talked about relating the meanings of each word to the things in the world, to the references, and also to the sense of  Language is compositional, but it's also not perfectly compositional . In particular idioms or expressions whose meanings cannot be predicted from their parts . These are clear violations of compositionality, because there's no regular function . There's no  Montegovian cement is the idea of using a logical formalism to represent the meaning of a sentence with a tight connection to syntax . Montague says there's no important theoretical difference between natural languages and artificial languages of magicians . Monte  Inference is to make something explicit that was implicit before in language . We can never get away from something symbolic and logical, at least not completely . We still have lots of observations that are non linguistic again, say, weather data,  1st order logic can be defined as having a domain of discourse, a set of entities that we care about . It has predicates which map elements of the discourse to truth values . Predicates give you a truth, value true or  The way to think about this is usually you're modeling some situation in some situation 10 critters or whatever . You're asking, is this a Wug? And that's what Wugex is doing, that if you ask,  There are conventions and procedures, that every word in English you can associate it with some piece of logic . And then we'll also define a procedure to combine those pieces of logic together in order to form the overall meaning of the sentence .  Logic consists of the predicates and function names and arity as as a set of sentences . An interpretation involves specifying the domain of this course. and it also means specifying the functions of predicates. How do the functions map entities to  Students who study and do homework are great as 8. . Do we need the.do we need? Oh, you mean this dot? technically, yes, but it's not a big deal. No wedge is closer to intersection.  You need to have that relates to the X to the student . The way that I'm doing it here is to say that grade of is a function that gives you the grade of the student, the X entity . and that's equivalent  We don't know if these expressions evaluate to true or false these sentences because we haven't applied it to any particular situation or scenario . That's when you need to talk about interpretations how do we interpret this logic within a particular context?  Grena: Are there ways to define functions that are not just this input, gives this outputs? Grena asks: Is there a way to define function that are defined beyond this input gives this output? I, B returns B,  We're going to be building up a 1st logic sentence as the as the meaning representation of the sentence . In order to do that, we need to use another tool from computer science . We need to be able to store partial comput  The general idea that we're going to pursue is that we can look at each of the subparts of the syntax tree and build up the meaning representations bit by bit, until we get to the sentence level . When we have the entire  The high level idea is that you have these words. They have pieces of logical representations with things to be filled in, and then you express those things . Using the syntax tree as your guide a placeholder until we reassign . It's  Where you have, some left hand side rewrites to N hand sides becomes some function where you're taking the semantic representations . Here, I'm using dot sem to refer to each of those the partial the pieces of logic representation there  The idea here is every terminal rule that results in a leaf node, involving a terminal, involves a terminal . And then afterwards is a mechanical process of looking at your augmented Cfg with those semantic attachments and then running those procedures . And  The questions are the grammar does not have to be in Cnf . And then, in terms of the order in which you apply things. You have to look at the semantic attachment rule. that's part of designing this augmented grammar