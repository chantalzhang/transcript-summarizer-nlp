 Jackie Cheung, Professor: We're gonna talk about discourse and co-reference resolution . We're going to focus on one particular phenomenon related to discourse . And then I will discuss and present and present some basic algorithms related to co-  Coherence is What makes a discourse a discourse . The fact that there are relations between different sentences or utterances . In a discourse, that type, the relation that logical relation between them is what makes it a discourse coherence . Co  Coherence is defined at the level of logical relationships between sentences or smaller chunks . cohesion is the observable mechanisms that you can identify that link together these chunks of text . In naturally occurring tests, almost certainly, you'll find cohesive links between  Words that are related to each other with, , say, high cosine similarity . Government and government are arguably part of the same chain . There are also discourse markers that help us explicitly mark what are the discourse relations between different parts of  There is some dichotomy or opposition between language things that you can . speak or say, or write down and hear, and see and read, and forth. versus things in the world, or in some hypothetical world that they point to  Co-reference is when you have multiple mentions or referring expressions that all points to the same thing in the world . This is a practical task that you might want to solve in its own . But also it's convenient, because we don  This phenomenon is called Anephra, where linguistic expressions their interpretation depends on other linguistic expressions . Maru and his are co-reference, because they point to the same entity in the real world, and his is an anaphore  There are other phenomena that appear at different levels of frequencies across languages . Many languages allow you to omit pronouns in certain contexts . In these languages you have an additional step before that which is to figure out when a pronoun has been dropped .  Japanese is this, Korean is this to a lesser extent . But there could be sentences . and can't do that sometimes you don't use the eye or can't . people would be able to interpret that as . , , , or  The phenomenon is that you can refer to entities that are not directly introduced, but that you figure out based on background knowledge . It's a similar issue to the previous phenomenon. here it's , you figure . based on context, what  Not all pronouns are referential in English and French . In French, too, there are constructions where you can use to place focus or emphasis on certain parts of the sentence . Co-reference, resolution is also an interesting task  Anafra resolution means that you're determining antecedents of pronouns . The task is given a pronoun figure out what it points to and previously in the text . Here are some basic algorithms for this using heuristics that we can  In English, in English, we have at least 2 to 3, 3 grammatical genders, there's masculine, feminine and neutral . You don't typically refer to countries or to Youtube with as a math with a masculine pronoun . But  Hobbes algorithm from the seventies is a heuristic algorithm that puts together all of these cues and comes up with a method for co-reference resolution based on that . This is one example of how syntactic heuristics is useful  This is from, the second edition of the Drafsky textbook . It sets up this way where you draw a syntax tree in a very particular style, with a particular style . and then it respects all the constraints from binding theory .  A beautiful cupcake in the Patchesserie windows is a noun phrase . A noun phrase the Patricia windows are noun phrases . Hobbes algorithm uses a tree traversal to find the best match in the current sentence to find a noun  This is just the to illustrate the some of how some of the queues are integrated into a simple algorithm . And then, later on, we'll look at machine learning algorithms that might be able to do a better job and resolution . And  These days, people tend to solve co-reference resolution using some statistical approach with a machine learning method . And one common way to decompose the task is to break it down into 2 subproblems . and then solve each of those  We learn with hearse patterns. That would mean that it would have to occur in the same sentence . And then the other one would be a co-reference matrix. And then you can go from there and Pmi scoring and all  If you just cast it as a chunking problem, it becomes a lot ner in that you're chunking the words in your text into chunks that correspond to mentions . You could do Bio tagging scheme and then run any sequence model train  In 2013, Durett and Klein trained a initial neural model or , a log linear model, but they have, 3 million features, and they use word level features . They also attempt to use the structure of Wordnet, which  The saving grace is that most mentions are short, ? you can make some assumptions and only compute that function for, say, spans of up to a certain size . And similarly, you there, if there are quadratic number of  In the recent literature there are 2 general kinds of models. The 1st one follows in the footsteps of ete. , if you replace, the Lstm part with a transfer pre-trained transformer . The second class of new  1st you put in a sentence and then the model predicts that here there's nothing. I don't see anything that Co refers to anything else previously. It predicts that you here of Speaker B. Refers with the cluster with index  The class is inspired by the idea of a transition based parser where you're predicting actions which create new substructures in the inputs . This would be done with a decoder. here the input is, the encoder part of the