 Jackie Cheung, Professor: We're going to look at how natural language generation has been approached with different methodologies and different approaches in the literature in the past . We'll look at, techniques that are not currently, the trend  If you want to describe the overall distribution of opinions, , of that product, and synthesize and the thoughts that people have on specific aspects, then you're going to have to do some abstraction and abstractive summarization . We can  For something a GPS system or some other very routine formulaic things, you do not want creativity . You want things that are highly structured and expected of a certain form . And arguably, that's a good thing. And arguably that's  When I say , take street name to street, name the street name that's called a slot. usually in a template, you have these slots where you need to put stuff in the slot. and then, appropriately, the thing you  Using the discourse structure of a document to find out where the important things are. This is going in the reverse in the opposite direction in order to decide how to structure the contents of the output. Then to consider some of these factors the  Part of micro planning is to generate referring expressions . The last step in this abstract description of an Nlg system would be surface realization, which is to convert all of those the specified discourse plans, and the outputs and decisions of the  The early literature called FUF assumes a highly detailed semantic structure . and then they apply a cascade of deterministic rules to convert that structure into a string . This type of structure you see here on the is called an attribute value matrix .  A neural Nlg model could be a sequence to sequence model . It's not easy to get these structures. It doesn't make sense it's a lot of work even to convert whatever input you happen to have into this structure .  There are 2 different senses of the word generative . There's one sense which is purely about the type of machine learning model . One is highly structured, detailed, rule-based and then not content-based . And one is highly  In this second approach of a neural nlg. everything just gets stuck into this conditional language model ? There's no separate modeling of each of those steps, at least in the most extreme form of that. You just assume that the  You can have some neural data to text generation as . You decide which steps of the analogy pipeline should be in their own modules with a separate planning step, . and then come up with a way to embed the input data structures into a  This technique is called integer linear programming . And it's really one instance of a broader class of methods and a more general approach, which is to think about things declaratively . We can also talk about text to text generation, such  There are many constraints about the form of the output sentence that you should generate . The idea is that you represent each of the individual sentences by some structured representation their dependency parse tree . And then you can create a sentence graph sentence graph by  The idea here is that you have this optimization problem with all of these constraints . And then this becomes an optimization problem that has a certain form that you can then pass on to a solver that figures out the solution for you . This  The 1st constraint, it turns out. ensures that each word has at most one head which you need because you want this, the overall selection to correspond to a tree . The second to ensure that the selected nodes form a connected tree  There are ways to bypass the optimization problem and then feed it into a neural model, and then ask the neural model to do its thing and predict some outputs . The downside is, you have no guarantee the constraints would be respected, but  There exists polynomial time algorithms for that although the most efficient ones are not . But once you get to the integer case, it turns out that it makes the math a lot harder because you cannot do . If it's not continuous  One trend is to think about correctness again. This is a problem that I've worked on in my lab. It's called the hallucination problem, although, to be clear, this terminology implies that language. correctness and factuality sometimes  Microsoft released a Chatbot on Twitter, but it was taken down one day before Microsoft had to take it down . Microsoft had this vision that people would interact with it, and the model would get gradually get better and better through its interactions  Unlikelihood training is the opposite of this standard approach to neural training . The opposite sign is one minus something which is the contrast words . It's the set of words that you don't want the model to generate . In contrastive learning  There's a stronger pressure here with this approach to push these things down, is there a possibility additional numbers to get the calls? The question is, there's this work in adversarial learning, and then you're interested in figuring out  The other major trend is you try to fix these issues with human feedback where you sample a collection of model outputs, and you get people to decide which one they prefer, and which one is better, and that becomes a reward signal .