 David Ifeoluwa Adelani: Today we'll be talking about the machine translation machine translation . He says machine translation has been there for long. It used to be a very, very difficult problem but I'm not sure  We'll talk about why, it is a hard problem inferability and superior wharf hypothesis . Then we talk about the vocros triangle . We'll start thinking about the problem by discussing the noisy training model for empty on the Ibm  Some languages does not have plural and some languages they don't have plural . Some words such as again stop, or more. you suppose, or contain an assumption about the world . This might make machine translation challenging . Also, there are  In Zulu you can also have suffix, and then this would change the meaning of the world . We also have languages with noun classes which a good example, would be the Bantu languages in Africa, where or something you can  Pro, the question is, is it even possible to produce a perfect translation? Some people have argued that for you to work on any language, if you can create a . perfect translation system, everything you're able to do in English  Language may influence human actions and thoughts slightly in highly specific ways . Language you speak affects your thoughts . Some languages even reject everything, in a language called Quotayore use uses an absolute system of spatial organization . We are not at  You can fix the engram, Count, to 2 or 3 or 4, and then you can check how many of the engrams do much . And based on this culture, you can have a blue scorn . How good is the  blue is focused on what is called precision . It's precision oriented for each engram in the proposed translation you have to check if it if it is found in the reference translation . In practice, blue incorporates an additional brevity, penalty  This is based on things using things using bilingual dictionary . If you can get, if you can take the source language to an Interlingua then you'll be able to do a translation to retarget cycles . This is the theoretical concept  The Interlingua is a conceptual space common to all languages, that if you can take the source text to this intelliga, you'll be able to do the translation . The advantage is that you can use to develop a general empty  The key thing is that we need to do an alignment . You need to match one word to another . You can use this to create an alignment model to do the translation . You want to train a model probability of F given he with  If language use a similar autography, an example of a cognate word is reference and reference . metric and metric is just written differently, but they have the same roots . You can try to use tabs to send sentence lines longest common  Can you translate from English to a Canadian indigenous language, that you have no information about? They provide the rules based on your on your linguistic knowledge and the rules that have provided . And then you have we? It will only have one  Ibm developed a series of 5 differential models that make increasing powerful assumption . Decode is what we change from, understand to understood this . Think the past tense uses we, and the current tension is . does everybody agree with that?  Probability of, , a French. What given English you can materialize over the alignment which is a and this one to one based on probability theory . And and then you can also use the Bayes rule to say. this is  Is just counts the probability that this word occurs both in English and French, and then you normalize by the count in English using the number I made. and after that you can multiply it. But the question is that let's assume  The length of the sentence is also one of the things we should consider based on what we talked about . It would be a language based rules and not dependent on the length of a sentence . It is possible to give an example of how  The Ibm model often gives very poor performance compared to genero base model . But as in the class, there's a way we can adapt this that you can have something. There's more there's slightly better. And after that