Jackie Cheung, Professor: Sorry the captions button disappeared. I'm trying to look for it. I guess it's happening. I don't know. Okay. and alright welcome back. Everybody. Okay. So today, we're going to talk about automatic summarization. So note that the. So I posted the slides. And then the link to the readings by Nikova and Mccune. There's a different link that I posted on Ed. So please use that because I think this link no longer works. Okay. So this week we're gonna look a little bit at automatic summarization and text generation. And we're gonna look at some of the ideas that have been important these fields. And we're going to look at how we got to the current state of generative AI from this line of work. So for today, we're going to focus on summarization. So we're going to look at what is the definition of automatic summarization tasks? What are some important sources of signals to determine whether a piece of text is important and should be included in an automatic summary. And then we're going to look at some simple, basic methods from the past for document summarization. focusing on both single document, summarization and multi document summarization. But focusing on the extractive case. So I'll define what it means, what extractive summarization means. And then, finally, we're going to end by talking a bit about summarization evaluation. Okay? So what is summarization? Well, automatic summarization is the task of turning some source text into a summary. This, like appears all over the place. Okay? So in news articles, for example, it can appear because, you can think of both the title of a news article, and this is kind of this is called the Byline. So in traditional newspapers, when people still printed out newspapers, usually articles they have like this short one or 2 sentence kind of it's called a byline in between the title and the body of the article, and that sometimes appears, and it's kind of a very, very short summary of the rest of the article. and it exists as well now that everything is online, that it also exists in that you have a short paraphrase of the rest of the whole article, and then you can decide, based on that, whether you want to click on to that article and read the rest of it. And even more than that, there are many other applications of summarization that are now in deployments. This is one that I find quite interesting from the recent literature. So it turns out that doctors actually spend most of the time not necessarily directly interfacing with patients or doing anything clinical or medical. They spend a tremendous amount of time doing paperwork. Okay, because they have to document every single patient interaction they have to write down like the summary of like what happened and like what they, what course of action they prescribe, and then they fill out billing codes and stuff. I gather it's even worse in the Us. Because of all of the insurance stuff they have to handle there. And so that's a big overhead. And that's a big cost to doctors. And it turns out that now there are technologies out there for a summarization that claim to do a a good job in summarizing these clinical notes. And so this is by nuance. This is a system by nuance called dax copilot. And then they've released a product in that space. and then you can check out whether their demo and so forth. So here in this demo, there's a video and all that's very flashy, and it says episodes of hypertension. And so it highlights specific phrases, and then it also generates the the overall paragraph, like the patient reports something or other. So that's pretty interesting. It's a deployed contacts for a summarization system. I guess. Technically, I should say that. I consult for Microsoft and Microsoft owns nuance. So technically, there's a link. But I'm not at all involved with nuance or with this product. But yeah. okay, but this goes to show summarization is in deployment, and it'd be nice to talk about like, what is summarization. How does it work? What are is it just one possible task? Or is it like a range of possible tasks? And because, like, my claim is that summarization is just. It's not just one thing. Okay, summarization is actually a family of tasks with many different flavors. I'm going to call them okay. And that's because summarization that that very general notion of figuring out what's important and distilling it down to a short passage, a short piece of text that's very under specified. There are many different possible contexts in which you might want to do that. And so we can discuss this and analyze this. And it's possible that these different contexts will lead to different possibilities in terms of the modeling techniques that you should pursue. And so we can break it down. So summarization can be a particular summarization task might involve specifying the purpose of the summarization system. assumptions about the the source text assumptions about the format of the outputs. and also assumptions about the users who will be interacting with the system. For example, if we think about the purpose. there are actually multiple purposes you can have for a summarization system. One is to be informative. So an informative summary tries to be a substitute for the original source material. and it tries to express as much of the important points as possible that were in the original source. Material. So what would be an example of that? So suppose you have. Okay, so do people use like close notes or anything like that when like reading novels or trying to redo things for English class, is it allowed? Actually, hopefully, it's allowed you can read it right as long as you don't like copy from it and plagiarize it, I think it's allowed. or just yeah, you know, you, you want to read things. A short version of things to understand like. what are the important points in the original? Then that's that's considered informative. Okay? Okay? Indicative, that's the second possible function. It's an indicative summary provides a link to the source text to help users decide whether or not to read it. So in the example I showed earlier with the news article with the byline. That's arguably more of an indicative summary, because that short piece of text is there to help you decide whether you want to click and read the full article. or, if you have like, search engine results. And then the search engine results comes with both the the link, and, like, you know, a very short extract, that of the some relevant part of the article. With respect to your query, that's also arguably indicative. And then, thirdly, a 3rd purpose might be a critical summary which provides an opinion of the source text. So none of these is really they're not disjoint like a summary can have some mixture of all of these different purposes. For example, the reading assignments that you're doing for this class. That's arguably a kind of summary of the original article. We're we're asking you to read. And it probably contains a mix of these purposes. I I would say, probably mostly informative and maybe critical. Right? So yeah, so a summary doesn't have to serve only just one purpose. But it can be some mix of them. And you might have different techniques that you want to employ, depending on what you're trying to do. In fact, arguably, you could say that like these different purposes, like some of them, you might not want to automate. So to me, it's a little bit It might raise concerns either ethically or otherwise. about automated systems that provide some kind of critical summary. potentially right, depending on the specific context like, do you trust an automated system to do a fair and unbiased job? In providing some critical summary of some material. So I don't know. Okay, another way in which summarization systems can differ from each other is simply the nature of the source. Text. So there's there are all the obvious things like what is like the domain of the text you're working with, what is the genre of text you're working with and so forth. And I'm going to restrict myself to texts for the purposes of today's discussion. You could also summarize other kinds of material. You can summarize like structured data like weather data or something you could structure, you could summarize like figures. And I don't know other kinds of content. But let's restrict ourselves to text summarization. But like another basic way in which systems can differ is whether you're doing single document or multi document summarization. And this, this factor has a large impact on the types of techniques that you should consider and the kinds of things you want to do. The reason for that is that in multi-document summarization there are additional issues you have to handle. If the multiple documents are by different authors, then chances are that there may be conflicting information or contradictory information between them. Sometimes it's just like you have a news event, and then more things happen, and then you get more information. And there's an update. Other times. It might be that if you're doing, say product review summarization, then different users might just have had different experiences and different judgments of a product. And so you have to handle that somehow. But have you seen, like I think, on Amazon? They now have, like short AI generated. I think it's AI generated reviews of like the the overall user opinions. So yeah, summarization is like everywhere. Now. also, there may be redundancies between different documents. And this is actually both useful and a problem to handle. So it's useful in that. If you have multiple people saying the same thing that may increase your confidence and trust that the thing that multiple people said is actually true or is important. On the other hand, when you're in the process of actually generating the outputs. If you have redundancy on on the input side, then you don't want to say redundant things on the output side. So you have to watch out for that and be careful about that. And finally, you just might have to do extra work to combine information from multiple sources, from from multiple documents. Okay, a 3rd way in which summarization systems can differ can be in terms of the format of the expected output. One major way. This is categorized is whether you have extractive summaries or abstractive summaries in extractive summaries. The idea is that you copy and extract parts of the source text. whereas in abstractive summaries the idea is that you need to synthesize the contents and then produce some potentially novel text for the outputs which requires more advanced semantic analysis and natural language generation. So extraction. Was the the 1st thing that the field worked on so determining which sentences should be important. And it's actually still an important task to work on for its own sake. So some domains require the exact wording to be preserved. Can you think of examples of this like, why might you want extractive summaries, even if you have the capabilities to do a good abstractive summary? Yeah, yeah, for quoting research. Yeah, what about other very high stakes. Domains. Yeah, yeah, like a purchase. Exactly. So. In that case, in those situations you don't want to accidentally put words into other people's mouths, so to speak. and so you you might want to directly copy and have it exact wording as it is in the extractive summaries outputs, and even then you can get into trouble, because then, if you lose the context, you know there are all these things now about. Oh, you're quoting me out of context, if you look at the context is not what I meant, that kind of thing, but at least the problem is slightly better in the extractive case, whereas in the abstractive case, then then then, people could really just say that claim that you're miss misattributing what they said. okay? And then, yeah. Yet another way in which summarization systems can differ is in terms of the their assumptions about the users. So in the field there's this assumption that there exists something called generic summarization. where there's no particular point of view taken, and that the what, the the goal here is to take the source texts. author's views and preserve them, and just like, restate them in the output summary, and try to preserve that as much as possible. I think more and more I would question, and maybe others would question whether such a notion of a generic summarization system actually exists. Because what counts as generic and what counts as like neutral, like neutral, without adding your own point of view. could be very tricky to define. So possibly this is more of an artifact that there's some kind of like homogeneity in the population of researchers and the tasks they work on. and and that you can make an assumption that maybe for the general public audience. they might have similar views on what, in terms of like, and also similar states of background knowledge that so that you can generate a so-called generic summary for them, and then it. It makes sense to them that you're not injecting too much of your own opinion. so so that this term exists generic, but then I think it's worth taking it with a bit of a grain of salt, even though lots of people use it in summarization and maybe elsewhere. Also. you can make other assumptions as well that are clearly not generic. So you can have user tailored or query focused summaries where the summary reflects a particular specific specified goal or priority of a user. So here is where summarization starts to get like the boundaries between something like a summarization system or something like a question answering system starts to get blurry. or even like an information retrieval system like, if you type in a search query to on your favorite search engine, and it gives you some summary outputs of some, some of the the of, say the 1st link that it returns. Then that seems like a summarization output. But it is query focus. And then it's starting to feel a bit like an information retrieval type system as well. And again, within the past few months I've noticed that this this has been integrated to in like the popular search engines. Or you might just decide to generate different summaries based on the background of the user in the in the medical space there is a corpus called up to date, not a corpus, a resource called up to date. Where? And so, if you're a doctor, you would know all about this, or if you're in med school, or whatever. So the idea behind up to date is that it's a resource where they have reference. It's kind of like Wikipedia for doctors. Well, except it's not a wiki. So it's kind of like a encyclopedia for doctors, and that they have, like a a documentation of like all of the common things that can happen like syndromes and diseases, and like tools and diagnosis and things like that, and then they can look it up. And those articles are written specifically for doctors. And so they contain a lot of jargon. They assume a lot of basic background knowledge in medicine. And it's it would not be that useful, for like a layperson. and that that up to date also contains another version of those same articles that are written for lay people like a lay audience, like patients, basically to read and try to understand what's going on. but without as much technical terminology and driving. And so you can imagine that, based on the assumed background information. You might want to give very different presentations, and and you might have to define terms in one case and not in the other, and so forth. And so that is interesting as well. So that's a type of another type of user tailored summarization system. or like, if you have a situation where you have there like, I said, there's an evolving current event, and then you might have one set of assumed assumed knowledge in your readers, and then you you can. You might want to just generate an update summary that provides what has happened since then. Okay, so then, those are all of the different ways in which summarization systems can differ from each other. So that's what I. That's why I claim that summarization is not a single task. And and there there's a lot going on and depending on the specific settings. You might need a very specific kind of model or technique. So next, I'd like to talk a bit about how do we actually determine what content is important? So like at a high level? So even before we look at any particular systems. Let's think about this. And let's also think about like which of these are easy to quantify and measure with the techniques that we've discussed in class, and maybe which ones are slightly more difficult. And maybe how that informs the choices of how the field has developed. Okay, so 1st of all, importance is a very tricky thing, right? All of us probably finds different things interesting. And and we might have different judgments about maybe what kinds of information, we trust, and what we find important, based on what we trust, and so forth. And so I guess the direct way to estimate importance in like a person like, if it does exist in like a a person as opposed to, if it existing in text, would be like, you have some kind of like scanner that like you directly aim at someone, and you can read their brain signals and see like how much it activates or something I don't know. And then you can use that to say, Okay. this piece of Texas would be important to this person, and this piece of text would not be important. However, of course, that technology doesn't currently exist, as far as I know. Maybe there's some way to approximate it, using, like some kind of fmri scan or something like that I don't know. But even if that's the case, it's not practical. so we cannot directly measure it. So instead, we have to try to model importance in a different way in text. using maybe other heuristics or other cues. That we think would correlates with this. So here are the major classes of signals that have been explored in the literature. And and again, I think it's a lot of the reasons that people choose. These is that they're maybe easy to compute or easy to yeah, and not. And maybe some of these, this is not super costly to compute. Okay? So the 1st type of signal is the distribution patterns of X within the source text material. And then there's some like something to do with discourse structure. And then also the relation of the text with the background domain inquiry. So let's look at each of these. Okay, so the 1st is distribution patterns and texts. Here. The basic idea is that we can approximate importance by looking at a notion that I'm going to call centrality. So what is centrality? It's it's it's the idea that the most important theme or the most important information. You probably say many things about that topic, and you might say many related things about it. And so that means that you can pick up on the cues of this distributionally, because it means that in the simple case you can just look at word identities, and you can see that, like the word appears again and again and again in like in a passage. And that gives you a clue that like, that's an important topic. A more sophisticated version of this might be that, okay, you have a lot of a bunch of text, and then you can use your favorite embedder. You can use word embeddings you can use. some kind of transformer based model or Lstm, whatever. Convert every span of whatever granularity you're interested in into a representation. And then, once you have that just visualize in your mind that this all exists in some high dimensional space. But I guess, for visualization purposes like, imagine like a 3D. Space or something. And then so all of these fans are these points in the space. and then you pick the points that are centrally located, and those would be the spans that if this assumption holds would express the important information. Yeah, oh, we're essentially looking right, what does centrally located mean so that you can come up with computational definitions of it? For example, you can define it as what is the average distance between this point and all of the other points. If this has low average distance to all of the other points in the source, then that's actually located. If it's very far, then it's not yeah. we want to remove like, stop words. Do we want to remove stop words? Quite possibly, I think, assignment one. that's all I'm going to say. So, theoretically speaking. You can probably make arguments either way that you can, if it's some notion some ways of modeling content would be. It would be such that it matters, but maybe depending on your stop list and depending on the specific nature of the data set. Maybe you should not remove it. So so let's take a look at like a news website. oh, I probably have to reshare the screen for the people online. Okay. okay, I'm gonna pick a news site. I don't know. I'll say, BBC, let's see, like, is is our assumption actually true. Is there any happier news? Okay, this is a happy news article. After a hundred years salmon have returned to the Klamath River, following a historic dam removal project in California. Okay, so. salmon returning, that's great. So what is the main topic here? I guess we assume that it has something to do with salmon and river. and maybe California. So let's look at this website. So if we look at like the occurrences of salmon. I thought it would highlight all the cases. You can see that salmon appears a lot right in this article for obvious reasons. So clearly, this article is about salmon, and so salmon appears a lot, but also related words will also appear right like fish appears, and like trout appears, and so forth. What would be something that would be like. Less important is, is there any sentence? Can you actually read it? Yeah, you can read it. Are there any sentences here that maybe you consider to be less important. Maybe lower down. I guess. Here it's arguably less important. It's art is talking, explaining about why dams are not good for fish, because it blocks fish migration, and how dams were demolished, and so forth. But here, this is a subtopic, too. So maybe, if you have, like a slightly longer, a bigger budget for your summary. then maybe you can say a little bit about this. But here there's a subtopic to do with dams. It looks like I guess. Here's another piece of information that's a bit less important about like negotiations and failed negotiations. I'm sure it's important to the people who are involved. But again, if we assume this notion of a generic summary exists. then maybe for outsiders. For the purposes of this article the process of how this came to be about the dam removal with negotiations, and so forth, that might be to them it might not be as important. And this is. And you can look at this because maybe if you embed this in some information space, perhaps because this is about negotiations and more like political issues. It might be a bit farther away from like, what's more centrally located, which is about like salmon and fish and ecological preservation. And so forth. Okay, yeah. So it works for this article. Let me reshare. Okay, so hopefully, the right screen is being shared. Okay? So then the next queue would be discourse structure. So 1st we talked about centrality. So the next queue that I claim is often used in summarization systems is discourse structure. What I mean by that. And what I mean by discourse structure. Is that the way that we write passages? It's not arbitrary. It's not random. And there are expectations that we have. About how a piece of writing is written. Yeah, how a piece of writing is written, because we're we've seen many instances of that type of writing. And and sometimes we're explicitly taught to write in a certain way. and so maybe we can use those patterns themselves as well to help us inform, find important information. So we just saw this example of the salmon article on BBC, so there. just okay, so it could be an article about sound, and it could be article about anything. Where do you think the important information would be contained in a news article? Yeah. Headline, the headline, yep. Where else? Like, maybe they're in the back. Yeah. Doesn't also depend on what exactly the office are trying to get our attention on so like to put emphasis on something that no might trigger the the interest of the audience. Yeah, it's not syntactical or semantically relevant to the article. Okay? So sometimes authors might try to peak interest in a certain way and write something that's not. Maybe maybe it's just to grab attention. You're saying right? Yes. okay, it could just be a specific word. Yeah. So that's yes. So that's also part of the score structure. And that's a strategy. And that's typically called a hook. They're trying to hook you into reading it. Yeah, but so that I think that fits here as well within the score structure. But what about for like event based news articles. Just more simple. Yeah. At the start of paragraphs. Is that enough? Yeah, at the start of paragraphs, because that announces the main idea. Yeah, I like that. Yeah. The end of paragraph at the end of paragraph yep, could be could be. Yeah. 1st and the last paragraph, the 1st and the last paragraph. Yep. And yeah, the more important stuff tends to be at the beginning. Yeah, these are all cues good clues for news. It turns out the most important clue. Besides, like the headline, is the the beginning. Okay, it turns out that I think all all the clues, you said are right. and they're all they have. All have an effect for news articles for event based news articles. It turns out that the beginning is the picking that the opening sentences is the the best predictor of important information. The reasons, I mean, if you think about it, I guess it's pretty intuitive. Why, that would be the case. So 1st of all. again going back to the days when things were printed on actual physical newspapers. There's a newspaper, maybe every day, right? So people are working on very, very tight deadlines. and that means editors sometimes have to. just chop things down to size so that the the printing deadline can be met and the newspapers can be printed. and so back. Then news article writers were explicitly instructed to write in such a way that the article could be cut off at any point, and it would still be a coherent article just to fit the length budgets. And so that's the way that the news was written. And so then they will always start with the most important, and go less and less and less and less important in order to support that. And even now, even today, that still makes sense as a strategy, even if things can be updated online and so forth. Just because we all have very limited attention spans. Right? So you might only have, like the patience, or at the time, or whatever to read, like one or 2 paragraphs. And then you either think it's this is interesting. You keep reading, or you abandon and say, Okay, that's it. That's good enough. So for news that turns out to be the case. For reddit. How about? For like a reddit post. I guess, like within the thread, like all the posts, are already ordered in some way. either by time or and also by like the number of Upvotes. But like, what about inside of a post itself. It belong. Post yeah, a lot of times. The last sentence, because that's where the post original poster asks the question after giving a lot of context. So right? Yeah, last last sentence, that's a good question. Yeah, that's right. So just in case some people didn't hear a lot of the times in a long post sometimes, people would ask a question, and then the last sentence, it might summarize, give some context, yeah, yeah, that's right? So sometimes I put a tldr right at the end, which stands for too long. Didn't read right? So yeah. So there's that. What about in a scientific article or in academic writing. Yeah. the abstract? Yeah, the abstract. That's right. And it might contain a lot of the summary of the important information yeah, in the back. The conclusion, yeah, that's right. In fact, if you take courses on like academic reading and writing your advice that for most papers. You don't read it from beginning to end right in in scientific articles. Usually it makes a lot more sense to read the title and then decide. Do you keep reading and then read the abstract and then decide. Do you keep reading? And then maybe then it's like abstract intro conclusion. and it's only usually when you're in, when you're doing research. you usually to get into an area you might have to read like hundreds of papers to like, understand the lay of the the landscape of the work in that in that space. And so you just don't have time to read every single paper in the same level of detail from the beginning to the end. and in that case you would really like skip around right? You would read the abstracts of everything, and then read the intros of everything and conclusions, and only for the small subset of papers of like, maybe 10 or 20, or or whatever papers that are directly relevant to your current project. You read them in detail, and then you go through everything. So those are all examples of like using discourse structure, and you use the general way in which a piece of text is expected to be written in order to help you find it and locate the important information. So a summarization system can also do that. So, in fact, in new summarization. it took, like 10 or 15 years for the field to beat this baseline, which is just to take the 1st 3 sentences of an article. It turns out that that is a really hard baseline to beat, and it took us many, many years in order to beat that. Yeah. So don't discount the score structure. And a 3rd major source. A 3rd major signal for determining importance is a little bit different. because then this is about the relationship between our general expectations and understandings of the world and what is written in texts. So the previous 2 types of signals are just, you can directly get it by analyzing the article itself. This one is more about looking at how this piece of text is related to other ways in which we structure knowledge in our minds. and so as to think about how this piece of text is related to background knowledge and queries. So, for example, okay, let me just ask you. And then you can maybe see the point. What do you expect to see in an article about a natural disaster. Yeah. Yep. Casualties. What else? Yeah. Patient? Or the doctor kilometer range? Yep, yeah. Timeline. Yeah. Yeah. Cool the costs. Yeah. Oh, cause? Oh, cause? Yes. Yeah. I'm also gonna put costs. Yep. yeah. Where to donate for relief. Yeah. Yeah. Number that's covered in casual. maybe what to do if you're affected like suggestions and advice for those affected. But, like, you see that we have very strong intuitions about what kind of information should be expressed in an article about a natural disaster. Right? So you can expect that the summary should also contain this information. and in fact, people have developed templates for this of like here. Here are some of the basic slots and basic thing types of information that are expected. And you can use even use those to check for like comprehensiveness, like, maybe it's missing something like advice for people who are affected. So maybe that's really important critical to include. Here's another one. What do you expect to see in an article about an election? Yeah, yeah. Information about candidates. Yeah. Contrasting policies. Yeah, I like this. This is very idealistic. Hopefully, people are making their decisions in a rational way, based on this information. Yes, yeah, depending on what it is yep. Votes polls, projections? Yeah. Done properly for you guys. Really. the baseline quote, yep, demographic breakdowns maybe just let you know time and how to vote. Yep. So, country, I guess. How do you? I guess, like, I guess basic information about about the the process surrounding the vote. Yeah. yeah, again, we have very strong intuitions here. Right? So, yeah, so then, a summarization system could try to learn this. So I guess there are 2 ways. Either you just directly specify it like, here is a domain. Here is the import important types of information, and try to capture as much of it as you can. Or maybe if you have multiple articles of the same type, maybe you can try to induce this and come up with like a representation of like the kinds of important information that that exists. Yeah. all right. So those are the important kinds of information. So then let's talk about how they're actually implemented in specific systems. And again, these days, like one, strategy, is to just throw everything into a pre trained, generative model. and maybe do some prompting or whatever, and try to get outputs. But let's look at like older systems, because then they're easier to like, think about and analyze and and see what's going on. So at a high level. All summarization systems need to perform these steps in some form or another. So one is to do some kind of analysis or content selection which is about determining what to say, and and like using our signals just now to determine what is important and novel and interesting and relevance. There might be more than that, though, because then, after you find the important information, you might have to do some things to do with like aggregating common or contradictory points in order to draw some new conclusions or inferences from text. And finally, a 3rd step, which can sometimes be non-trivial, is to do synthesis of this information, also called surface realization, which is to determine the specific final form of the summary. So the specific words you're going to use and how you're going to put the sentences together and all that. So let's start with the these 3 steps, looking at these 3 steps for the simplest. What I would say, is like one of the simpler settings which is single document, extractive summarization. So this is simpler because So by extraction. Again, remember what this means is, you take existing spans like, say, existing sentences. and then you concatenate these sentences together to form your summary. or you highlight them, or something. So this is the simplest, because most of the work has to do with content selection. Okay, so essentially, all of the work is in content selection. So just determining which sentences to select, whereas you don't really need to do very much transformation or synthesis or extra work for surface realization. Maybe you need to work on actually the ui like, how do you present the summary to indicate that this is an extractive summary? So maybe there are some incoherencies created by the extraction process. But then you try to indicate that to the user. Okay, so one approach is just to frame this as a supervised machine learning problem. But maybe not every, not not. Maybe this might not work for all of the different signals of importance that we discussed. Okay, so so, okay, so okay, let's think about this. So we want to design a supervised system for summarization. Maybe we throw in a bunch of words like, here is like a bunch of content, words and function, words or whatever. And then here are some discourse, features like, is this sentence the 1st sentence, or the second sentence, or maybe you also throw in features related to like discourse cues like, if are there words like? Because or therefore that might indicate Kate some conclusion. and other features of discourse structure. So between these 2 classes of features, which do you think would be more successful if we take a supervised learning approach? Okay, think about this for 5 seconds, and then I'll ask for a vote. Okay? So if you had to pick one and just one. which class of features would be more successful. Who votes for lexical features? Who votes for discourse? Features? Okay, so more people voted for discourse features, and I think I would agree with you. So the reason for this is that this is really tricky, but if you think about it. it depends on the nature of the supervised learning system. But for like simpler kinds of machine learning and simpler kinds of supervised machine learning systems. you have a very direct relationship between the the input features. So here that would be the words with the output decision of like is this sentence important or not? And the problem here is that in summarization a summarization system might have to handle articles that are about very different topics, even if they're all news articles. One could be about salmon revitalization. Another could be about like global warming. A 3rd one might be about some election, and a 4.th One might be about the economy, and they're all different from each other, and they all use different words. Right? So what is considered what would be an indicator of an important sentence in one type of article might not be an an indicator in another type of article. So that's why I would expect and and work has found, that using lexical features with is less successful. On the other hand, these discourse features about like how an article is written. is more stable within a particular genre of text, and a within a particular summarization setup. And so that's why the discourse features are the ones that are better suited to this type of supervised learning approach. Yeah. it's like right? So then the question is, would I have the same opinion for unsupervised system? So in unsupervised systems, then things are different. So yeah, then it would be. Then then we we can look at difference. yeah. Then there'll be different trends. So I guess to spoil it, because I think I'll talk about this a bit later, but unsupervised systems. Then you try to directly implement heuristics that measure centrality. In which case, then, you can use these lexical features. Yeah, question. It's all my good. Would it change based on language? In this case? I don't think so. because I think the arguments I made about how words indicate the topic. But the topic can change. That's stable across languages. So I don't think so. But yeah. but I think the where, where there would be interactions would be like different languages might involve different genres might be correlated with different genres of text that speakers of those language tend to use, and different expectations. And so then the discourse features might be different across languages. So but then that just means you need to do some adaptation or do different training steps for the different languages. Okay, so yeah, people tried this. actually, very, very early in the fifties and sixties. And they they essentially, okay. So the very early work is not really machine learning. It's more like, here are some heuristics for like trying to. I think even back then they had some statistics. They did have statistical models. Right? People back then were very smart as well. So it was based on what we can now consider a supervised learning method, using statistical methods. And then in the nineties Lynn and Hobie did something similar. They trained us to a supervised method as well. Where the input was the source text plus some human written abstracts. And then for each sentence in the human abstract they find the position in the source article that has the highest similarity to it. So this is an automatic way to generate a label. So they didn't have to ask summarizers to like or or human experts to select the the correct. the the gold standard summary. They had a heuristic to automatically generate that as well. And then what they found was unsurprisingly that, like in different kinds of corpora, usually the title is the most important sentence. with the highest overlap with the the human abstract. Oh, okay. So they had ref. They did have sorry they did have human written reference abstracts. Right? Okay? So yeah, so unsurprisingly, the title is like, usually considered the most important according to this notion. And then after that, it's it's it's dependent on the specific type of newspaper corpus. So on computer products. It turns out that you want the 1st sentence of the second paragraph, and the 1st sentence of the 3rd paragraph, because of the particular way that those products were introduced and described. whereas on Wall Street Journal, which is like economic text, it's like the 1st sentence of the 1st paragraph, and the second sentence of the 1st paragraph, and so on, and so forth. So both of these cues that I think you guys had proposed both of these right? So some of you propose, like the 1st sentences of all the paragraphs. So that's that was learned here. Whereas for economic news, it was like, just what's in the 1st paragraph. Yeah. And I already discussed this. But it turns out that in news text the opening of the article kind of acts like a summary in and of itself. So the baseline method in this style of in this genre of text should be just to select the 1st sentences of the article up to the word length limit. and we already did. We already checked BBC, so we don't have to go back. Another approach that people have taken is to try to implement this idea of centrality. And so this is like starting to relate to the question about unsupervised methods, and that this is an approach for trying to reweight words in order to better understand their centrality, their their centrality scores, I guess, or how central they are computational. And this notion is super important, because this also formed the basis of all information retrieval systems. especially the early ones. Even now I'm sure it's used in current search engines. Ex, possibly explicitly, or even if not, then implicitly. It's used and it's learned through the training of current models. And this is the idea of term waiting. You probably already experiment. You might have already experimented with this a bit in like the 1st programming assignment, or maybe in the second one as well. which is the idea that, like not all words are equally important. and you can capture this distributionally at least. By important I mean, like important to determining how central it is. In the article. For example, if you see the word be in an article that tells you almost nothing about the the topic of that article or the the contents of it. whereas, if you see the word penguin, then that gives you a pretty relatively strong idea that maybe the article is about Antarctica, or it's about like ecology and conservation. or I don't know. Maybe it's about Linux. I don't know something to do with penguins. So the way that this has been implemented is through this idea of tf, idf. which stands for term frequency times, inverse document frequency. The heuristic here is that a term is important or indicative of a document, if it appears many times within that document, but is relatively rare overall. And so tf, for example, is usually just the count of the words in the document, like, how many times does this word appear? An Idf is a little bit more complicated? There's usually some kind of rescaling by passing it through a log function. For example, one basic version is to look at how many documents does there exist within some reference corpus. and then you divide that by the number of documents with some term T, you add one to avoid dividing by 0, and you take the log for it, or something like that. And so this separate reference corpus, you need some other source of like data for this. And again, this was a really really important and basic tool in information retrieval for the longest time. Yeah. So for it, here's a worked out example. So suppose the appears in almost all the articles. Then the computation will be like this. It would be like a 35 times in the current article Times, the log of whatever. And this might, you would expect this to turn out to be a relatively small number, because what's in the log when you divide it out. it's close to one. So when you take the log of it, it's close to 0. So you get a very small number. whereas penguin, if it appears very rarely overall, and your whole corpus. but it appears just twice in the current article. The fact that, like the you have this like much larger number inside the log is usually enough to like increase the weight of the penguin sufficiently of these rare words sufficiently that it it scores highly in terms of tfidf so the idea here is that you can take this approach or other approaches. and reweight and and score terms, and then, after that, you can just directly use those scores, and like score each of your sentences by say, it's average, or its total tf, idf weighting. or or I I guess idf score right? Or you can use this to, you can use these tf, idf, weightings within each sentence or each span and come come up with some vector, representation. And then you can find, like the centroid vector. like, the is is that called centroid, like medoid. Yeah, you can find the most central vector within the collection of vectors that you have and select that to be the most important sentence. Something like that. Right? Here's another method again by Lynnon Hovey, which is very successful. The idea here is that they use tools from statistics in order to determine the important words and phrases within an article. What they did is they set up a statistical test. So where 1st they determined 2 sets of articles related articles and unrelated articles. So that's R, and not R. So, for example, if you were summarizing a document about vaccinations, then in your related set, you would have articles in the health domain overall. and then in your unrelated set you would have articles in the finance and Education domains, or something else. and then for each term, Ti. you would tabulate. It's the occurrences of that term across the related and unrelated articles. And so each term is either the term of interest. Ti, or it's not the term of interest some other term. and then you would record all of these. So these are all counts. So, for example, in the summarizing articles about vaccination tier. One might be like needle or something, and then you would see how many times needle appears in health articles versus non-health articles. And then how many times words which are not the word needle appears in, like the related in the health domain versus, not in the health domain. Once you've set this up, then you can do a hypothesis test. and you can do a binomial test. So what you can do is from the 1st row. You can ask, what is the probability that occurrences of Ti are distributed between R and not R in this way? So this is a binomial distribution, because it's like a you can think about the some theta which is like you model. Okay, it will either appear or it won't appear in each article, and then you have to use combinatorics to account for all of the ways in which the appearance or non-appearance might occur across these sets of articles. Hence the binomial distribution. and once you have that, then you can set up a statistical hypothesis and run a test. So the 1st test is that the term Ti is not characteristic of the domain. So that means the distribution of occurrences of Ti between the related and unrelated articles is the same as for all of the other terms. Not Ti. So then, the likelihood of your data set given, this hypothesis is given by this expression. where you have a single parameter. P. That you use to model each of the 2 rows, the row one and row 2 in your table of term distributions. So this is like the null hypothesis, which is that this particular term Ti is nothing special. You can treat it the same way as you treat all of the other terms. whereas hypothesis 2. Is that the term Ti is important to the domain, and the distribution of occurrences of Ti between the related and unrelated sets is different from the distribution for all of the other terms. Not Ti. So then the difference here is in your second hypothesis. You compute this likelihood, using p. 1 and p. 2. Using separate parameters. And then for each of these you can just fit what's the best parameter that maximizes that likelihood. And then, finally, once you have 2 likelihoods. There's a statistical test you can perform, using a log likelihood, the log likelihood ratio. And then this is the statistic associated with the test. And then you can rank terms and sentences by oh, sorry it should be ranked terms by this statistic, and then select sentences with words that score highly on this. So when they applied it to their data, it works very well. They have some. They show some examples in their paper which you can look at. So they did this for both unigrams and bigrams. So the 1st the the 1st set of articles, is clearly about like something to do with jails, and so forth, and the the justice system, maybe, and jail overcrowding. and then the second one is clearly about like a cigarette, cigarettes and tobacco, and like smoking and so forth. And so this topic signature method works actually extremely well. And again, this is based on just a very simple notion. This is really an instantiation of centrality. because it's saying that if this word appears more than you would expect according to some background distribution, which is what the binomial test captures. then it would score highly. And then that gives you some notion of like what this article is about in the multi document case. Let's look at that as well. So okay, so so far, we've looked at single documents. But in the multi document case, then there's additional things to consider. because then you have to consider that there might be conflicting or contradictory information. and there might be redundancy in the documents, and how to combine information between them. But actually, redundancy is both good and bad. like, I said earlier, right? So dependency is good because it helps. It's a it's a, it's a computable signal that like. If everyone's talking about it, it should be important. But it's bad in the sense that you have to be careful when you're selecting sentences to include in your summary. You don't accidentally select, like the same sentence many, many times. So you have to avoid redundancy in that way. And so one very basic system is appropriately named, some basic by Neiko von Vander Wendy. What they do is that they use unigram frequencies with a simple update term to account for non-redundancy. and this is extremely simple, but it works quite well. So it's a good baseline for multi document summarization. So first, st what they do is they compute the relative frequencies for all words, so they just take the number of times the word appears, and divide it by the length of the article, and then of all the articles. and then you repeat until the summary length is length limit is reached. So 1st you rank sentences by their average word probabilities. and then you select the best scoring sentence to add to the summary. So by default, it's just like the best scoring one sentence. So what this means is you select the sentence that contains them, the words that are on average the most frequent. Okay. And then, after this, to account for non redundancy, what they do is they do this thing, which is not very well theoretically motivated, but it works, which is that they just update the probability of all of the words in the selected sentence by squaring it, and because these are probabilities that lowers it. Right? So this means you're less likely to select a very similar sentence in the future, because the new average probability of the words in similar sentences has been decreased. And so you just run this iteratively until you reach your length limit. Yeah, like, what is it? There is a word that's pretty important in all the documents they like solving. If you talk to them about solving. And now we can reduce the weight of that a very important for it, Solomon. So that there's potentially another second very important idea about something that it's on the 1st one. But then this week's gonna go down. Because you yeah, that's a good question. So the question is like, what if this down weights like the really important words like, if it was a salmon article, then you've down weighted the word salmon. But salmon was really important. So I think maybe that's why the squaring works. This is the genius of the squaring. which is that if it was a very common word in the set of articles like about salmon. that salmon should have a relatively high probability. So when you square it, you're decreasing it by less. then you would decrease the probability of a relatively rare word and similarly, for function words. Then, even if you don't remove them, then the function words they won't like selecting them shouldn't be like that bad right selecting function words again. And so then, by squaring it, you're decreasing it by less. So. Yeah, that's probably why the score increase. Was there? Another question. No. Okay. Okay. Okay. So of course, this was a very basic system. And they're much more sophisticated methods. Now. for example, some of the later work that people have worked on in this literature include, like updating this, the inference procedure. So rather than always selecting a greedy like greedily, what you think is the most important next sentence, maybe you can turn that into an optimization problem and there. So I think people have current summarization into like a knapsack problem or something from which you might have encountered. If you've taken a theoretical Cs class where you're trying to select sentences with high informative informativeness score, informativeness score, while with while respecting your overall length budget, and maybe you also add some constraints. So then this breaks the knapsack assumption, but then you add additional constraints about, like the relations between sentences, so that you don't want them to be redundant with each other. So you can do all sorts of stuff like that and turn it into some interesting optimization problem and solve for that. It turns out there are other things that people have tried. For example, if you're doing extractive summarization, you should avoid sentences with words whose interpretation depends on like the overall context. for example, with pronouns, you should avoid them, because usually the pronouns refer to something in like the previous context, like, if you have an it and you extract that sentence, and you don't extract the context, then then you have a dangling pronoun now, because you don't know what it originally points to. Similarly, with things like, therefore, or whatever things that these words don't make sense outside of context. Right? So if you're doing an extractive system, then you should avoid selecting those sentences. or alternatively, you can select those sentences, but then you might want to cut out these discourse cues like, therefore, and so forth. and other work on modeling the coherence or flow of like the summary sentences. Yeah. And in fact, even back in 2,006, I know now these days, there's all this talk about. Oh, AI systems are better than people and taking over the world. But even back in 2,006, we were able to achieve this as a community using these simpler methods on some tasks in summarization. So in 2,006, Conroy et Al. Used the topic signature idea introduced earlier with a sophisticated non redundancy, module, and like some rules just to like eliminate some some parts of sentences deterministically. So there's something called Gerund clauses that this approach removes also restricted relative clause positives. I don't expect you to know what these are, but like. There are certain kind of syntactic constructions that they remove. or like interest, intentional attribution. She said that they would never do that, he said. She said whatever without consulting us, so just remove that, and also lead adverbs. So by doing this they got a very high score. So there's there's a metric called rouge, which I'll introduce in a bit, and then their particular system. Yeah, I think their particular system is like this system. And yeah, and I think they got like. very close to human level scores even back then. yeah, I think it. I forget which one it was. I think there were. They had 2 versions of the systems, one with the O system, one without. I think, 1 1 of the systems. This was like a a query focus summarization task. I think one of them directly had. Yeah, I think this was this was an fully automated system. Yeah. So this was the system where they combined their method with just looking at word overlap with the the topic of the summarization that that is given, and by doing that their scores were like well within the range of, like the humans which are all of these letters. and then some basic and all the other methods are like, lower a bit lower down there. Yeah. And then these days, of course. people do things neurally. So then you can do extractive summarization by training a neural method on some large scale data sets. You can do this in any number of ways you can do supervised learning. For example, if you're able to derive a label for each sentence as like a label 0 or label one. then then it's just a supervised learning problem. It's it could even be like a sequence labeling problem where you feed in a bunch of sentences one after the other, and then you have to label them like, is this important, or is this not important? And if you have a budget, then you can like rank the sentences by like the probability of being in class one of being important. You can also do this via reinforcement learning. We've done this in my group as well. So you think of summarization as a reinforcement learning problem, where your action, if you've seen reinforcement learning, the action, is to select a summary sentence or not selected. and then the reward signal is like, how good your estimation of how good it would be to select. Okay, sorry. So the reward signal would just be like how good it was to select the sentence. and then you have to define it a certain way. and if you have not seen reinforcement. Don't worry about it, but it's not the contents of this class, but it's like an alternative to supervised learning. You can think of it that way or unsupervised learning, it's like a different learning paradigm. Okay, so for the last 10 min, I'll just quickly talk a bit about evaluation. Actually, this is one of usually the 1st questions I get when, like someone asks me, I say, Oh, I work on summarization, they're like. But how do you tell if a summary is good, isn't it all subjective? Well. maybe partially. But and and also if it's subjective, is not necessarily a bad thing. But anyway, to evaluate a summary. You have to think a little bit about like evaluating for the contents of the summary, and also for, like the overall quality, like the linguistic quality of the summary. hey? Just like the reading assignments, rubrics, and so forth. But anyway, so so we have the summary content which is like? Does it accurately reflect the original content, like the source content? Does it contain the most important content? Does it include non-redundant contents? And then for linguistic quality. It's like the grammaticality of the individual sentences and the coherence of the output overall. So one thing you could do is you could just ask people to rate a summary like from a scale of one to 5. How would you rate the quality of the summary and this, like like all the evaluation approaches. This has advantages and disadvantages. So the advantage is that you can You can change the question and tailor your question towards what you care about. and you can have different sub questions on like different aspects of the summary also. This means that you do not require gold standard summaries which could, which could be tricky to produce. But the disadvantage is that this is expensive, because every time you change your system. and every time you want to do a new evaluation and compare systems. You would have to collect new judgments from people also. People are like, I don't know if you've met people before, but they're really annoying and difficult to work with for science. Okay. so, for example, different people have different interpretations of the scale. right? Like what counts as a 5 out of 5 versus a 4 out of 5, or like a 1 out of 5. So just think about like, you know, you use your phone to check the ratings of a restaurant. Even between cities, or maybe even between neighborhoods. You cannot trust. You cannot interpret the scale the same. I've noticed people in Montreal tend to be very nice. So if you see a 4 out of 5 restaurant in Montreal, it's maybe it's a sign you should avoid the restaurants. Okay. But if you go to other places where the scale might be different, a 4 out of 5 could be a very good restaurant. That's in my experience. So that's a problem. And it also means that results do not generalize across different evaluation runs. Okay. So if one research group runs this human judgment. Study on one set of summary summarizers. You cannot like take the same numbers and scores and and used and compare with them in like a different setting. Like, if you have a different research group that tries to replicate that study and like reruns this human judgment study, you cannot take like a 4 here and compare it to like a 3.5. There, it just doesn't work. It's not a good comparison. So an alternative to this is something I mentioned a bit earlier called rouge scores. So the overall process for this approach is that you ask human experts ideally, or you somehow automatically construct a reference summary. So what is like a good reference? Summary. You ask someone to write that. And then, when a system generates a summary of the same source. Text. You compute some notion of overlap between the reference summary versus the the system generated. Summary. Yeah. So in this. And that's expressed in this rouge. N. Equation. So rouge, N means you're computing it over. N. Grams. Not necessarily over individual words. So rouge one would be over unigrams. So words. But say, rouge, 2 would be over bygrams, and so forth. And then here in the numerator, you would count the number of matches and N. Grams. and between the the system summary and the reference summaries. and then in the denominator, you would count the total number of N graphs in the reference summaries. and here you assume that you have multiple reference summaries. So you might ask multiple people to writes this this reference summary, because maybe you believe there's a diversity in the range of possible answers or good summaries that people can write. Yeah. So there's blue. And there's rouge. Yeah. So there's a theme. So yeah, in machine translation. Maybe you've seen blue scores. Maybe David introduced that right? So and so, because people are too clever for summarization. This came after and they called it rouge. The difference between blue and rouge is that blue is more precision oriented in that you, the denominator, is mostly defined in terms of the number of words or n-grams in the system generated translation. whereas for summarization, because the idea is, you want to make sure you're comprehensive and capture everything in the source text. Here the denominator is defined in terms of the the length of the the yeah, that's defined in terms of the length of the source article. I think I'm gonna skip this because I don't think it's that interesting to compute where it overlaps. But okay, so there are yet other evaluation methods. So there's 1 called the pyramid method, which is a kind of structured content evaluation. Here. The idea is that you get this is really expensive. It's a human annotation that's really expensive. You ask people to highlight and find chunks of information in the in, in the reference summaries, and then you also ask people to find chunks of information in the system generated summaries. and then you ask yet more people to link the 2 of them. So it's kind of like a structured analysis of for each important piece of information which is called a summary content unit. Here do you find it in both the reference summary and in the system generated summary. Okay, and from there you can derive a score, and you can like, you can check whether the system generated summary contained the chunks of information which are considered to be important because they appear in the human written summaries. Yet another very different approach is an extrinsic evaluation. So the idea behind these extrinsic evaluations is you don't directly look at the summary in and of itself. But you see whether the summary enables users to do something else better or faster. For example. you can, if summaries are like, meant to be useful in the education setting for learning. For example, then you can directly check. You can ask students and to use a summary maybe you either. You have 2 conditions. You either just give them the original material they're supposed to learn, or you give them the original material plus the summaries. and then afterwards you can quiz them and see whether they do better on the quiz if they've if they have access to the summary. or you might be interested in testing whether summaries improve the speed of doing something else. So here in the second study, what they did was they had a maybe artificial task of here's like a hundred documents. Find all of the documents that are relevant to this particular topic. and then they either they had one condition where the people had to just look at the documents themselves alone, or they can have access to both the document and a summary of the document, and that work showed that, having summaries can improve the speed of this process. You can do things like that. All right. So I'll stop there. So so far in this lecture, in terms of the methods I focused on the extractive approaches. And then next class, we can talk a little bit about approaches to neural abstractive summarization, which is interesting, because then we need to bring in this idea of natural language generation. And how do you actually do a generation test? Okay? Thanks.
