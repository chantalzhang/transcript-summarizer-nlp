David Ifeoluwa Adelani: Hmm. but yeah, I would go there. Yeah. okay. Alright, can you hear, me, yeah, okay, I, guess, we can start so welcome to Lecture 8 where we'll be talking about. We'll continue our discussion about part of speech tagging. And today we are going to be going through some algorithms that are popularly used for this task. Forward, backward, forward, backward. and vitabi agony. And So last time we are trying to compute, like Emily. I have to remember Emily for any Markov model where we assumed just a background model and today we're we're going to try to give you more general framework that has been used because people have developed a couple of algorithms for Hmms, and this is what we will be describing today. Okay? So all right. So last time, we defined path of speech as a synthetic. that's a glory for every word in our text. For example, we have things like nouns and examples like restaurants, dinner. We have verbs, we have adjectives, we have preposition adverbs and determinar. And also we talked about different ways of being part of speech different schemes, right? So who can remind me of the different schemes? We actually examine 2 popular schemes? Yes. yes, there was a pantry. Bangkok! Scheme, and then there's a universal one. Yes, thank you. Here we I mean, this is an example of the pantry bank kind of scheme. where we use like nlp for proper nouns and for universal dependency. We use, like, I think, pr ovn so they have various different symbols. And why, what was the motivation for having a more universal dependency scheme? Yes. so kind of try to generalize across other languages. Yes, thank you. And here we also talked about stats for when you're trying to build a model for this kind of task. you need to consider the current world if you want to compute the mle. So you also need to consider the previous context. And similarly, for the part of speech. The next part of speech depends on the previous part of speech. So we established that, using a very simple diagram example. where the next part of speech we always depend on the next on the previous part of speech. And also, yeah, we made that kind of assumption which we call the Markov assumption. So. It's a general term that is used. Markov assumption, where you have to reduce the context instead of saying, you have all the context, saying, when you make this conditional independence assumption. We also call it the Markov assumption. So here we kind of relate this to Markov chains. And in Markov chains we talk about how we can decompose the joint probability which by now has been removed from the board. So we have the probability, the judge probability of all the observations and all the States. And in this case. what is the Gedian variable? Yeah. In this diagram? Yes. yeah. They are the Union variables and the o's are the observations what you observe. So in this case the States would be the part of speech. Tax the different ones that occur in your sentence, and then the observed variables which is your all, would be the words. And here we make an assumption here that, given the State one. which is q. 1. With some initial probability you can then generate what will be the observed word which would be, o 1, and also, if you go to Q. 2, you can generate with initial probability, O. 2, and then with state transition probability, you can go from q. 1 to Q. 2. So in the diagram face that we described. So if you want to say probability of OQ, where all and q are random variables, and all are all your observed. Observe observations, or observe variables. and then, Q. Are all the States which represent your part of speech. You can say, this will be the product of the q. 1, which is your 1st part of speech which you can compute, based on uniform probability, that how many times does this part of speech occur, or how many times does this part of speech starts my sentence across the entire compost? This is the way we competed. q. 1. And then you multiply that with the State transition probability for it to move from one part of speech, from the previous part of speech to the next one, and then you multiply this with the emission probability to actually generate a word given the tag because there are many possibilities. And here we also talk about the model parameters which we can estimate using an mle. And today we are going to try to have something like a like a generalized, how do we actually compute parameter theta that is more general. So this is what we want to. Consider today, I must warn you. Today's lecture is highly technical. So probably the the most one. And in this lecture. But yeah. so we have the initial probabilities. Key one. And then we have the transition probabilities from Qt. To Qt. Plus one which we can also compute using mle. And we also have the emission probabilities. which is the probability of generating award given. It's not. And to compute it, I think it's very important to do this revision. Just because we'll leverage on this in this class and here to compute your pi high height. And here you're just concerned about which part of speech can begin. The sentence is is as simple as that. So and then we have the count of this. Q. High equals the part of speech divided by all the possible sentences that are in your training couples. This Emily, is often calculated over your corpus. and then you have the a high J, which is computed that given. If you know the previous State, can you predict the previous? Sorry, the next States? And we can also compute this with very similarly, assuming this is like a diagram. distribution where we can count how many times you have in our couples, how many times do we have a tag? I, followed by a tag. J, and then we normalize by the number of counts of act. and the last one would be the emission probability which we can also compute as number of times you have one K. Contact. I. Oh. but I'm dividing it by how many times we have qt equals I in our compass. Alright. So this is where we ended the lecture the last time where we talk about actually, after we compute the likelihood. So in the last time we have. I think we just completed it. the mle. And now we want to consider how to actually compute the maximum a posterior posterior. So which is like the map, we want to actually estimate what will be the theta? Sorry we want to estimate in this case. What would be the likelihood of a sequence of observation which is the probability of all given Theta and this Theta. We don't know. But this is what we want to calculate. and this data can be calculated by examine examining all the possible states that can happen when you're trying to go from one part of speech to the other. On. Also, we want to compute what will be the best States. On the observation which is like we are trying to compute the judge probability given the parameter of theta. Okay. so and it's also possible to compute this if you don't even have any level. This is in a very unsupervised way. And here what will happen is that you 1st initialize the state with random probabilities. and then you can now try to improve on on this state values iteratively. So we are going to examine things like the em algorithm that is very popular in mushroom. Do you have questions from the last class. Okay. yes, the queue. These are the States. Because this is actually what we are looking for. Eventually, the part of switch. So why didn't we include the new one. So the one no one doesn't consider the but of course, actually, it's not that it doesn't consider it. But you can marginalize to to give you one. Do you remember the auto marginalize? Yeah. Alright. So if you marginalize over all the queues, you are going to get payable? Yeah. yes, and that's what we have here. Actually. so how do you compute the likelihood? And the simple answer is that you just have to marginalize over all the state sequences. And this is a very simple law of liability like you have the joint probability, and if you know all the joint probabilities. You can just marginalize to get the other one. Okay. the problem is that if you try to do this. you have exponentially many paths and risk to priority. So just think about it. Imagine you have 2 states, and then you have 2 observations towards to be generated. That's already like 4 possible bots that you can have. So imagine you have 2 states. and then you have 3 observations. So this is like, it's possible paths that you can take. So, and the solution is that we can try to solve this using forward algorithm which is coming from dynamic programming to avoid unnecessary recalculations. So we're trying to create like a table of all possible state sequences. And then we calculate the probability. So we now say, the next state depends on all the values you have computed in the previous States. And then we go step by step. So we go from one state to the other. And then so we kind of try to create a lot a table of values so which can be referred can be compared to like having a trellis. You know, Chinese, right? Yeah. Okay. so having a trellis of possible state sequences. And here, this is like every possible combination that can happen. So here you have all the States, and here your States are the part of speed tax. and then you have all your observations that happen via time. So you have the 1st observation, and you move to the second observation, and you move to the 3, rd to the 4, th to the 5, th and so on. and once you get to the hand of it. once you have calculated all the state values, you can marginalize the entire values to calculate what is the probability of all given theta. Okay. So the values inside the States can be referred to as what is the probability of observing from one to T, because for every State take a random one there for every State we are saying that just think about every value you select from a colon. We are saying that it depends on the previous state at time. T. So if you want to compute for time, t plus one, it depends on everything you have on time. T. So if you want to compute for let's say Alpha, determiner of 2, so it depends on all the values you have from Alpha Vb. Alpha, Nn. Alpha, Dt. Alpha, Jj. And Alpha. CD. In the time step of one. Okay, so and here we want to copy what is the probability of the observation from one to the to that time where you are to that time step. And then what is the state that you have? I mean the queue which refers to the state values the part of speech, and then given the theater. So this is like computing. What's the probability of the current tax. And what's up to now? Because this is everything that has been observed from one to 2. Okay, so here we can compute what will be the initial state probabilities. So what is the initial state probabilities? We I gave you an inch of that last week. So this is how you can compute the initial state probabilities which depends on. If what is by J. In this case. that's the initial state probabilities, right initial state probabilities. Then you multiply that you actually observe this? Right? So this is how you compute the Alpha G's. So for every Alpha they can be computed as this. do you have question. Yes, I don't understand why we can multiply by zoom. So look at what you're trying to compute probability of 4 1 wants to see. and then key of T, right? So you need what is the initial state vulnerability. which is the 5G, and then you need to multiply this by actually observing forward with this conditional independence as a machine. Have we already marginalized for queue? No. we have not marginalized for you. You're good. That's okay. Yeah. Yes. The probability of Dt beginning this sentence, yeah. the probability of a meeting, the what you observe? Yeah. So if you were okay, let's go to last. This is what we did the last lecture, right? We have the initial probability. if you remember the calculation, I can open the slides, but it's very, very similar to what we did in the last calculation. Right? So where you have an initial state, you don't have transition probabilities, so there is no way you multiply. Do you remember we factorized stuff? So we have this this big equation here. You have probability of q 1. Because you might. You are. You have probability of q 1. Which is the pi eyes, or, yeah, that would be your pi, j's. and then you don't have state probability that there's nothing to multiply right? But you do have a mission probability. Actually, what probability of a meeting that world? So and that's why you you got this. Yeah, okay? So based on on what we have done. Yeah, then, the complicated thing is that if you want to actually compute for the States of, let's say, Alpha, Dt. We are saying that this depends on everything from time one to time. C. So everything, all the States before that actually contributes to the value of the next state, which is Alpha, Dt. okay? And then if you do this over and over again. if you do this. So this is kind of like the formula, so where you are. So alpha t, minus one. These are the previous states. So all the values you sum over them. and then you multiply, which is your aids, your aids will be your your transition probabilities, and then you multiply by the emission probabilities. So actually you should be familiar with aig multiplication with Bjot, because this is what we have been doing early on. But what we are saying now is that in the middle you don't have the you don't have the initial state probabilities. Right? So you only have the values at the previous state that you need to multiply. and then you sum up all the values across all the different States. So so that means all possible ways so that means you consider all possible ways in your part of speech. Actually contributes to the next one. So in the last column, once we have calculated everything for distrellis, so you can. Now. just kind of marginalize marginalized means you sum over all the probabilities to get your Po given data because everything previously contributes. And then you continue to kind of sum up all the probabilities until you get to the last one. And at the last stage you can just kind of sum up or everything. You have calculated to have the probability of O given Theta. So we can actually transform this into an algorithm. And maybe this is easier. I don't know which one works better for you the algorithm or the training. So here. first, st we create a chalice for high equals, one to hand. I, I equals one to hand. These are your different states, which are the part of speech tax, and T from one to T. These are your observations. So you create the States across every part of speech. And then every word in your sentence which are your observed values. And then, for the 1st one, which is the Alpha. j. 1. This is the way we calculate it, which is the Pi, J. Bj. Of o. 1, and and then you have an algorithm that goes from one state to the other. And if you want to calculate, what is the time complexity of this? I think it's very clear that this will be all, and to n squared T. Because you have n twice. 1st you sum over N, and then you have another for loop over N, and then you have T. And once you have completed all the Alpha Tj's you can marginalize to have what is the probability of all given Theta. do you have question. think, okay? So we can also have a backward algorithm which is the reverse of that. So in the backward algorithm, instead of starting from the initial state probabilities that you have. Yeah, we we assume we are starting from the last. you know, from the last cell. And then you go backwards. And that's why it's called the backward algorithm. And here, and actually based on because we are just because this is the probability of a joint joint probability you can do forward or backwards, like I showed you the last time in the language model. You can say, what's the probability of the previous? Given the current, you can also do probability of the current. Sorry of the yeah. Sorry probability you can do both probability of the current given the previous, and also you can do the reverse, which is probability of the previous. Given the current right? So you can do both. Because the law of probabilities actually does allow us to do this. Okay, so here you want to call instead of completing the alphas! The alphas are the are the one we computed at the forward algorithm in the backward algorithm, you compute the betters. Here you have. What's the probability of T plus one to T, because now you're starting from T, which is the last cell and then given Qt equals. R, okay, here the backward algorithm is actually very similar to the forward algorithm. One thing we assume here is that we don't have initial step probability. But all our betters at T, the last stage are given once. So you so in the initial probabilities for the alphas, you have to multiply the step probability with the probability of actually admitting a word, and here the last one you just give them once. So the the values are known, and after that, then you go backwards by actually completing this. And after that you can now marginalize again. And the marginalization is very similar to what you have in the because when you're when you get to the 1st So you're going from the back to the 1st state. You have initial state probabilities which you need to multiply would be betters. So I will show you the offer, the forward algorithm again, in forward algorithm, everything is already integrated. So the last stage you just need to marginalize over what you have at the end. And in the backward algorithm you still have your initial state probabilities, and then you have your probability of emitting the 1st word. and then you multiply that with the benches. So so that's the back order going there. So with enough. Yes. at one. That's the 1st token one. Yeah, that's the so we have an assumption that you just give them one. 1, 1, 1, 1 1 So for the backwater guardian. I think you need for you to actually pro progress backwards. You need to assume, because you need to have something to start your going right to. You need to have values to start. You need to have some initial values to do your calculations right for the afford algorithm, I think this is very clear because you can assume the initial state probabilities. But for the backward algorithm we don't know. So we can just assume the maximum value you can get for probability is one. And then you just assume that value. And then you go backwards. Yeah. right? So for the forward backward algorithm here you just out to multiply what it offers and the betters together. So this is what is called forward backward algorithm. And before, after you multiply the alphas and the betters together. There you marginalize our A from for every timestamp, from one to 3. Alright, so as you can see here, there's a lot of probability multiplication which actually can cause problems. So there's a simple trick that we use. So if you have, if you have to multiply a lot of probabilities. And you need to take the Agmas most of the time. It's better to work in Logan, because, instead of multiplying all you have to do is to just sum. and then we can have what is called this log some trick. I can give you an annotation with like a very simple example. And here, for example. if you are multiplying all the probabilities together, like you see here, we keep multiplying the alphas and the betas together. If the values are very, very small, you can have what is called like on the flow. and and to prevent that, it's better to work in logarithm. So you just sum the properties together. And, for example, if you take the log of all these products of pis, so you will have a summitation over the log pis, and if you assume that log pi is plus AI. So now, if you want to log some trick, is what is the log of sum of probabilities? Right? So, what is the log of sum of exponential exponential of probabilities? So the formulation you have below is very used, especially if you're implementing. If you're implementing probabilities and then you multiply a lot at some point, you will get some value very close to 0. Yes. yes, alright. A Hi, Jane. The state. I understand. J, yeah. Is that different from the same A's in the forward propagation? You need to apply Bayes theorem to inverse the yes, in this case, because you are going backwards. Yeah. So they're different from the one the same age in the? Oh, yeah, because, yeah, I think to be honest, I think they are the same. Here. because the AI is the way we calculate them quits. Is this one? What's the probability? So if you compute this, whether you go from Ji to high. Okay, I see what you mean. So you're talking about the normalization. So if you are going to the backwards, yes, yes, it will change a bit. Yeah. the other side. And then. yeah. So so they are distant days. Yes, there will be distant. A's. Yeah. Yes. Sorry. I didn't understand the forward and backward multiplication this one. So this is an assumption of the algorithm. So you have 3 different algorithms. Now, you have forward algorithm backward algorithm. And then there's 1 to combine both. which I'm still good. I've tried to connect, and there's a reason why they connected both together. Because you can use this, both for the supervised setting and unsupervised setting for the all supervised setting. You don't have the labels. but you can use the forward backward algorithm and combine this with Em algorithm, which I will show you a couple of slides. Yeah. all right. So so the log sum trick is basically the log of the summation of exponential of probabilities. which I can show with a very simple illustration on the board. Oh, okay, cool. So this is our assumption. right for the notebook. Some of this. So let's let's give like very, very, very simple example. Let's assume everything here is kind of base 2. So where we have log of truth. and then you have the summing. Let's see 2. So 4, 2, 6. But something very similar to this. They're serving across all the exponentia. And what they say is that this is stuffed off. That's right. Oops. yeah. So video here. So we here. it's just the maximum of all the pis. So that's a p 1, 2. So hey? And so what is the maximum? Yeah. So let's take a very, very, very simple example. Here, we're saying, this should be involved of the maximum here in this very simple example. and then 2 rest of our heads. And then if you solve across this, so you're saying, pi. so this would be. You multiply this by. I think, 2 cool and even trying to do very, very simple example. You'll see that they actually books. Is it clear from this illustration. So you see that they're actually equivalent. Well, actually, if you're implementing it, they are not equivalent. Because if you multiply a lot of probabilities together at some point, you're just gonna get 0 because of the floating point limitation of your what do you call it? Of your computer at some point? This is just be very close to 0. But if you take the maximum all probabilities and you do the do it this way, you can actually get a signal. So this is a very, very common trick that is used when you have to multiply a lot of probabilities. So that's the locksome trick. And also we can use it here at implementation stage. Alright. So we have talked about how to compute the probability of of all we have competed. Actually, we have describe the probability of all given theta, which is the likelihood. And now, how do we actually label the samples? Which is what is the the right queue. If you take the Ag mass over all the probability of Qo, given theta so so this is kind of vitabi algorithm, which is very, very popular. which I think you can read up more is very similar to the forward. A guardian. Well, instead of as summation, you actually just take the maximum instead of summing. So you have the States here, and then you sum instead of summing, you just take the maximum. So that's the vitabi algorithm. So it's very, very similar. So you start with something very similar to this with a forward algorithm. And then you take the maximum over all the datas also you have the same runtime. Right? So so. But when you actually do this calculation, you have to also keep track of where the maximum entry to each cell came from. You have to keep track because you need it for the next to calculate the next you need you to calculate the next column right? You need to keep track of it. Alright. So we have a short exercise which I hope will help to clarify things if you do it yourself. So here, I think because of it's a lot of material. We probably would do this together. So let's assume we have 3 States XYZ. Which also corresponds to your path of speech tax. And then we have 2 possible emissions which are your what do you call it? You have your what's this symbol? Exclamation Mark. And at and we have the probabilities. So we have the 2 parameters? A and B, so what is a. yes, please, what is a. yes. yeah, it's a state transition probability. And the is the emission probability. Okay? So you have the emission probability. You have the You have your State transition probability for the forward case. even without looking at a solution. How would you calculate it. So I think I may have to write the formulas on the board, because now it's more difficult. Whoa. yeah. So I think for the the time we are guarding. I think the only difference is that we're going to take the maximum instead of the submission. Okay, I believe this is all we need for the exercise and for the forward backward. We are supposed to multiply. The alpha, j's. and the better it is together. Okay, so this is the exercise. so I just need to have the values. Think it's probably easier to take a picture just all right. Yes. and what we want to compute is actually want to compute the exclamation mark at at alright? Yes. So if we want to compute that for every stage for the forward algorithm. So what are we going to do based on all the information you have on the board? What is the formula for the 1st state you have to compute? What is the initial state probabilities multiply by the admission probabilities. So what is the initial step of abilities which is your pile here. This will be 0 point 1 times your initial probability, which is B times 0 point 1. Is that clear? So you multiply the 1st pi high with the force emission probability, because B is your emission probability. So? Bj, of o, 1. Oh, I'm good is 0 point 1. So this will be 0 0 time, 0 point 2 times 0 point 1. So for the second one, what should be the value? So this would be the second state probability that was 0 point 5 times 0 point 5. Is that clear? Yes. I mean the last one. You did 0 point 3 times 0 point 7. Yeah. Seat is 0 point 3, I agree. But then to get the app symbol, it looks like it should be 0 point 3 wide scale. No, no, we're not doing the arts. We are still at the what do you call this? Yeah? So because we want to go for exclamation mark at at. So all the probabilities for exclamation mark is what you multiply with right. So you are going to multiply each scale probability by the observation. What you what you observe. So so we have 3 things we are observing. And then we also have a 3 part of speech. In this case. 3 states and 3 things we're observing. So at the 1st State we are observing just the exclamation back. which is the this and the probabilities for each of the States. For all this establishment, 0 point 1 0 point 5 0 point 7, and the initial set probabilities are also given as 0 point 2 0 point 5 0 point 3 is the 1st color play. Okay? Yeah. So the second column. So what are you going to do after that? You? We said, every state in the 1st column contribute to what would go to the what? To the second column. Right? So that means we, if you want to compute. Oh, alpha. which is the Alpha Gt. Minus one. So your Alpha Gt. Minus one would be 0 0 point 0 2 0 point 2 5 and 0 point 2 1. So you're going to take each of these and multiply by the Aij and Bj of ot. So for the 1st one we take 1st X, we we go for 0 point 0 2. Multiply by the Aij. In this case, what is the Aij 0 point 5? That's the 1st aid. Hi, Ben. you multiply this by dj, of old thing video of Ot, we are concerned about apps. Now we are. So we must buy with the apps. And so we have 0 point 0 2 times 0 point 5, which is the 1st States there times 0 point 9, and then you add that with 0 point 2 5 again times 0 point 2, which is the second value times 0 0 point 9. Right? And then you continue. do you have question? I mean, it's just you just have to pay attention to the It's really simple, if otherwise, I think it's straightforward. And once we have calculated that you know, we are interested in the 1st value. So you only multiply by 0 point 9, because here, this is Bj of Otp. right? Video voting. And then if you go to video voting ot. we we go to So this is B of one at timestamp of 2 right? And then if you go to Dj of of 2, a time step of 2, so the value will change. So you you can compute the second Val the second cell, which would be 0 times 0 2 times 0 point 4, which is for the Y column times 0 point 5. Yeah. And then you you continue. and after you have computed everything on that column. the every values here also contributes to the finer outputs. which is, you multiply 0 point 0 7 2 9 times 0 point 5 times 0 point 9, and then you had it. So it's just the same computation. If you observe, you find out that what you are multiplying with are actually the same for full of column 2 and column 3. And why is that? Why was prime by December is because we are supposed to produce art twice right? That's whatever reason. So. And the second reason is because you are at the same States. So you are multiplying with the same state value. And also you are trying to emit the same word. And that's why we are multiplying by the same value. But in the case of the 1st one, you'll find out that we are actually multiplying by a different value. We don't have 0 point 1. So instead of 0 point 1 in the 1st column we have 0 point 9, because the symbols are different. Okay? And at the end of the day you can sum if you sum everything you have your period of all given to them. Okay, so for the backwater gardening. what you're going to have is that we said in the last column you're going to have once you have once throughout. and in this case you are trying to do the same calculation. But the formula is slightly different. So here, here you have the A high J. So what is your aid? Here? You still have one, because your contribution from the last column is one, then this would be one times 0 point 5 times 0 point 9, because 0 point 9 is what you are emitting there. plus one times 0 point 4. So if you observe in the last column. I think for the phone, I've already yeah, we are going this week for the calculation for the back, or I don't think just you're doing this in terms of the values you pick. So go to the folder garden. So you find out that you multiply 0 point 5, and then the second, so you multiply 0 point 2, then you multiply 0 point 1, and for background it's very different. And then you propagate everything to the very 1st column. and then you do the calculation. And after that you sum up everything together. But while summing the difference for the backward algorithm is that while summing all the values, you also multiply by the initial step probabilities. and also the better one. hey? So we may not able to. We may not be able to go through the calculation step by step in a class. So, but I will encourage you when you get home you should try to work it out yourself. They have a clear understanding. just using the formula and then doing the calculation. Okay, so for the vitabi algorithm, I think the difference is now you take the maximum. And what is the maximum of the 1st column? That's 0 point 2, 5, and is the maximum that you use in the calculation. and you continue the game. So you always, instead of doing the submission you you take the maximum alright. So that is in the supervised setting, where you have all the observed values. And also you have the labels which is your sequences your your States. But you could also have this in a in a unsupervised setting where we don't have the state values. So we need to guess them. And for this, what we can do is that we can initialize the States with some values. And then we try to learn this iteratively. So and what we are going to do is basically doing this, you predict the current state sequence using the current model. And then you update the current parameter. And this is actually the idea of the em algorithm. So for the em algorithm is that you 1st initialize the parameters randomly. And then you repeat the 1st time you predict the current state sequences, using the current model. and then you update the parameter of your model. Basically you, you learn a better parameter to be able to predict a better state sequence. So in the forward backward algorithm, where? And this is what we want to connect to the Em algorithm em is expectation, maximization. And the idea is that you 1st assume a set of parameter values and based on that you try to estimate what will be the value of theta. and you go through your data to estimate a better parameter of the model. So it's like, the idea is that everything is unsupervised. So you don't have what's what you should start your calculation with. But you randomly initialize them, and then you just estimate at that state. or at that time step with that value. So once you move to time, step one. What you need to do is that you get a better estimation of the parameters, and then you redo your calculation. So at the expectation, you get a split account for the eating structures and using the current Teta K and the eating structures will be your your part of speech or your State's values. And for the maximization, then you find what will be the a better, Theta k plus one at the next time. Step that actually maximize the likelihood over your training data. So so the parameters you want to estimate in this case, we we call them probabilities responsibilities. Here we have to parameters. We want to estimate. We have some epsilon, and then we have some. Our gammas we want to estimate. So it's very similar to what we are estimating previously. where we compete, what is the probability of Qt. Equals a tag given what we observe? And after that we can now compute what we? What is our state, what's the probability of Q equals? I basically, what's the probability? This is like the transition probability from time step T to type, step t plus one. So once we estimate these values, so these values will be corrected at the maximization stage. where you can estimate it's based on your training data. But you start? So, but initially you have some random values for them. and then you estimate the parameter of the model that you are looking for. and after that you recalculate what should be the exact value for Gamma, T. And epsilon of C. So so the idea for the east step we have 2 parameters that we want to calculate. We have a a gamma I of T, which you can decompose to be this probability of Qt. Equals. I given what you observe and the parameter you are looking for. Remember, we we don't know this parameter of T of Theta K. But we can estimate it with some random initial random values. and we can then use that to estimate what is our Gamma T. Of T. Gamma, high of T, and after that, and the probability of Qt equals. I, because in this algorithm, we are using the forward backward algorithm. So we know how to estimate what is the probability of Qt equals. I comma O, because we can estimate it's using the alpha I of T from the forward algorithm and the beta I of T from the backwater guardian, which we multiply together. So after we have calculated this one and then we have the normalization constant, which you can obtain by marginalizing over all the possible values of I. So at the Easter we can compute something very similar to the transition probability from one State to the other. Given our observed value of value and the initial value of Theta K, and we do very similar thing. Which is also coming from the from the forward backward algorithm, the forward algorithm you have, you are computing your Alpha J of T, and then we multiply by all the values in the backward algorithm. and then you can divide by what you marginalize. So if you have gotten the initial value of of the Gammas and the Epsilon. Given a randomly initialized Theta K, then the next stage is at the maximization step would be actually to compute a new value of Theta K plus one. and the new value of Theta K plus one we be. We want to compute new values for, alphas! New values for a better for are betters, and then you continue so at the maximization stage. It's very, very related to having a sub version of the mle. So the Alpha ij it's very. You can compare it to what we did previously. where we have the joint counts divided by the count over the initial states before we move to the next State. and at the maximization stage you can actually get a better value. Better values for all the all the teta values you are looking for, and then you will go back to the expectation, to the E step. and then you do this iteratively, and if you do this iteratively over many iterations. you will have better values for your teachers which you don't know. And the question is that when do you stop your Em algorithm you are likely going to stop. When your likelihood doesn't improve at some point it will stop improving, and then you can stop So this is the idea of the em algorithm or another way to stop is you can perform the prediction over your development set to see if things are improving. Do you have questions? Yeah. yes. yes. Where like, how are these equations derived? I see the similarity with the in the past, but I don't see why it took So I mean, you see the similarities? Because, yeah, we okay, let's look at what we're trying to Co calculate. So here you're trying to say, what is the probability of the States? You are looking for the States. Given what you observe right? And here you are trying to compute what is the transition probabilities. So it's very similar to what we did in the in the earlier stage, where we want to go from one State to the other. So that is the alpha values. It's very. It's kind of the same idea with what you're trying to achieve. Okay, for for the other one, which is the d of IK plus one. So it's also very similar to your emission, your emission probabilities. But in this case of the expectation maximization. We don't know them. But we are trying to just learn everything unsupervisedly from the data. Right? So this is what we're trying to do so. There. There are no labels to train on. So we cannot just apply vitabi algorithm directly. And now we want to do everything like our supervisor. Yep. all right. So for the Em algorithm, we are trying to find the local minimum. And the old idea is that you find out you don't know the value of Theta so the value of teta in this case will be everything in your trellis, the the States all the values of Alpha or the values of Beta. You don't know them. So. And what we are trying to do is that if I can have a likelihood given. My new parameter of theta improves the previous one. Then that means I'm making progress. So the difference between the supervised and the unsupervised is the supervised. You can actually estimate your offers and your betters, which are your offers. That you calculate are your betters. You can, that you calculate is what we call the is our theater. Basically. right? Is our theater. So. But in the unsupervised setting, you cannot estimate this. So you have 1st a random initialization of everything that is there. So you have random initialization of the entire state. and after you have a random initialization of everything, then you want, and I want to. You compute the likelihood. I want to see if you are making progress. If you get a new parameter of theta. is it better than the previous one you got, and then you do this iteratively until you, until you find out that there's improvements in the estimation of the likelihood. So as long as you are making progress, that the likelihood over your observe or your training data, which is all your observation. Given the new title you have. Computer is better than the old one. Your offers are your betters as as long as it keeps improving. Then you know that em is working, but at some point, if it doesn't improve again, of course. That means you have to stop. That's the best em can give you. There's no guarantee that you're going to get very good results. If you're interested in more proofs, there are proofs of correctness above bound Welsh correctness which you can actually check with additional materials. So yes. So there, if there are a few things you need to consider when you are working on things like the emigrating you have what is called random restarts. First, st you have to train multiple models because this is an unsupervised setting, and if you have very bad initialization. You will not go. You will not go very far. So you have to have random restarts. And actually you can pick your best parameter by always computing. What's what is the result on the development set? Another thing that might be helpful is to have a more better initialization by using an initialization based on some external source of knowledge. And actually, this will improve your performance because you have an initial estimation of the parameters of alpha and betas. From an external ledge. This will actually lead you to better convergence than having a random initialization. Okay? So the only supervised setting, which is about vash a guardian with no labor data often gives very poor results, because there are many issues like, if you don't have a very good initialization. You're going to likely have very poor performance. And it's often nice to have small values of label data that you can actually estimate better beta parameters for your alphas and betas before you start the emigrating alright. So this! So in in practice, Emi guardian can be used for different tasks. And then we have some very popular baselines on on the Wsj couples, which used to be a very popular benchmark. But of course most people are not using this against that. You can actually estimate how good is the performance. And you can see the Hmms. Actually does give you a very good performance. Alright. So we also have some other sequence modeling task like chunking where you actually find syntatic chunks in the sentence. So you're not only looking for a word and giving it a tag. But you can have something like noun phrase that you want to tap instead of just a word. And also, you have a very popular task named Entity Recognition, where you want to identify popular categories like personal name, organization, and location. And this is very, very important for information extraction you want to. When you're searching for an important thing on the web, you want to get the entity right so that you can get the right information. and if you can get the entity right, maybe this will already have an entry on Wikipedia, and then you can extract information about the entity. So this is very important for information instruction. But of course, here you need to detect the spans of multiple words. Here you're not only annotating a single word. Again, you need to identify the span. So this is an example. You have this plot, Mcgill. You can view them and concordion allocated in Montreal. And then you have organization organization. What is the problem with this scheme? If you just labeled everything as organization organization organization? So when so we have different schemes for for named entity, recognition. But this is a very busy scheme where you just label it as You just label each occurrence of the entity with the Tag organization. Organization. So it's often better to use a better tag where you can actually map. Where is this the beginning of the span of the entity versus? Where is the entity ending rather than just having a a single span, because an entity can be a single word. It can be multiple words. So in the I/O or B tagging here, we try to know what is the beginning of an entity. What is the ending of an entity. So here we have my view university. my guild, together. It's not a full organization, you have to say, my girl, because I think my gear is also a name of a person. Right? So if you see my gear university, this makes it an organization, so you can have what is the beginning of the entity, and also what is the next word that follows the entity. So this makes it clearer. and then you also have the old tax, signifying that there is no entity so like East is not an entity located is not an entity. So. But apart from this. We also have other skills like, there's another one BIOE. S. Where you can have the start of the entity. You have the end of the entity. You have the intermediate of the entity, and then you have the old tax, so everything that is not an entity will be an old tax so and but I think this is probably the most popular tagging technique which we call the iob 2 tagging scheme, where you clearly define the beginning of an entity and the ending of an entity. And you can also solve these tasks with Hmms, and as we'll see in the future lectures using crf. Okay, I think we'll stop the lecture here today next week. There's no lecture. October second, we have the end of workshop at Mila. I encourage you to attend. Thank you.
