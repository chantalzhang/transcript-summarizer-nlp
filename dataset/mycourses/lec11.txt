Jackie Cheung, Professor: It sounds like nothing. Yeah, we're talking about universities that you're you're pumped and that you're open. It's awesome. Oh. wasn't expecting so team one. The block people here. Oh. probably want to switch about this one second. let's see. outlook is phone. It's not helpful. Yeah. yeah, we'll get started in a few minutes we should leave time for people to get here, because the main entrance was blocked. And in the meantime, if you have lighting preferences, let me know. and I'll try to play around that question. Okay, I'm gonna stick with this setting unless someone tells me they prefer something else. Bye. alright. So Hi, everybody! So unfortunately, you'll be spending the next few weeks with me rather than David, and we will talk a little bit about structure. So we'll talk about syntax and semantics. So before we get there. it's just some announcements and reminders. So reminder that next week is Thanksgiving, plus the fall reading break. So that means there are no classes or office hours or lectures. Well, no classes but that the reading assignment is due later this week. So please be sure to get on top of that. The reading assignments are not meant to take a long time there, but they they are there, those to to help complement the material that we discuss in the course. Also, we're gonna be releasing the final project description hopefully by the end of the week. So please watch out for that. And also and if you need any help in finding final project members, if you haven't started thinking about that yet we we will provide help for that as well. And you can also post on the Ed of the course for that. Yeah. Question. is it doing the 11.th Okay, when I checked and my courses was the top. But you should go with whatever is officially released. Yeah, my slides could be wrong. Yeah. no. So groups of 3, please. Alright. So I'm gonna summarize where I think we're at in the course, and then you can let me know if I'm if I'm off base. So from my understanding of what we've done so far in the course we talked about text classification where we treated passages as samples, right? So that was kind of the 1st content topic. And I remember giving a lecture on that a long time ago in September or something. And then with David, you should have talked about text as sequences and looked at sequence, labeling problems. Right? So, for example, he should have taught you about n-gram models, right? And smoothing hidden Markov models. Yes, good nods. Okay? And then, most recently, Lstms, right? And crfs, maybe crfs and Lcms, so that's great. So now for the next part of the course, we're going to look at hierarchical structures. And first, st we're going to look at syntax. So we're going to ask, what is syntax? We're going to look at some particular characteristics or properties of English syntax themselves. And then we're also gonna look at a formal system for describing structure called context-free grammars. Alright. So if, like many students in in the course you were, you got interested in Nlp because of all the recent excitement with e-learning. That's great. And if, like many of them, you're reading and following a lot of machine learning blogs and so forth, you might be under the assumption that language is a sequence of tokens. Okay? And my goal in this lecture is to give you evidence to disabuse you of that notion. So while language could be described as a sequence of tokens. it's actually a lot more than that. Okay, so it's easy to forget this, especially now that we're in modern times. Often we interact with each other through textual means. So, and Texas seems to naturally come as a sequence of discrete tokens right? And in English. We even have spaces between the each word. So then. and that's how you might define a token. So we might think that language is is a sequence of tokens. But it's important to remember that text is just an approximation of language. It's 1 view of language. but it's a discrete approximation of language. and it's a discrete approximation also of speech, like the speech modality of language. And that means that there actually are a whole bunch of other phenomena where the most natural way to think about them and characterize them and describe them and model them, is not to just think about them as a sequence of tokens. Let me give you some examples. So 1st of all, language is not just a linear sequence of tokens. because the parts interact with each other quite closely and quite tightly. The pronunciation of words that are next to each other will often affect each other some of the the most of the time. It's in these very small ways that it's really hard to notice, especially if you're used to speaking a language. You never notice how the the pronunciations of words bleed into each other themselves. But I can give you some examples of this happening in in more obvious ways. So one example, one obvious example from English might be. So how many people here speak a dialect of English? That is non rhodic. So, in other words, you don't pronounce r's at the end of syllables like, instead of saying car, you might say car or like there, you would say that. So if you're from the Uk. For example, chances are quite high. You speak that a version of English like that. So one way in which the pronunciations of words affect each other adjacently, and those dialects of English, for example, would be that in most situations they don't pronounce the r at the ends of words and syllables, but they would pronounce it if the following word starts with a vowel. Okay, so and sometimes there's even a phenomenon called intrusive R, where there originally wasn't an r in the word, but then you add the extra r in to smooth that out. So to add a consonant between 2 words. So a. A a common example of this is, if you have like, if you want to say, I have an idea of what is happening in those dialects, you might say an idea of what's happening. Okay? So you might notice this if you if you so next time you, you chat with someone from the Uk, you can try to pay attention to the speech and see if they have okay. So not everyone from there has it. But some people do. Or another really obvious example is in French. So how many people here speak French? Okay, more people. So French pronunciation is kind of famous for having these effects. And it's a really difficult thing for learners of French to grasp. For example, just the the the articles that like the the. So if you have, like a in French, right. How do you say? A like a masculine, singular version of a How do you say it? And yes, that's right. At least that's my approximation of how you say it. And if you think about it, there's actually no consonant there. But you think there is, because it's spelled UN. But it's really a nasalized valve. Okay, like air comes out of your nose. However, again, if the following syllable starts with a vowel. suddenly the end reappears. and the vowel is no longer nasalized. Okay, so if it's a I know it's that you. Suddenly the end appears. Okay. Whereas if it's something else that's masculine, singular. that doesn't start with a vowel. Okay, someone give me a noun gato. Yes, there you go. So then you print. Then you don't pronounce the n. but the vowels neither like. So that's another example of pronunciations bleeding across words also intonation patterns. They work over entire sentences, or when when things are spoken. You tend to talk about utterances rather than sentences. So, and that seems like it. It cuts across multiple tokens in the sequence. If it's a question, for example, there are particular intonation patterns for questions in English, and it depends on your dialect. It depends on whether it's a yes, no question or whether it's a question with a question word, and so on and so forth. So those are some examples of why it might make sense to work at a level. That's not at the level of individual sequences of tokens. But for today's lecture, the most salient phenomenon here is that we can think about and analyze the internal parts of sentences and find patterns that we can better explain through positing hierarchical structures. So the goal of today's lecture, then, is for me to provide you with evidence that this structure exists. and also to come up with a formal computational model of this structure. So then, you can describe it in those terms in terms of hierarchical structures. And so you don't always have to go through linear sequences of tokens, as you might with the Lstms. Okay. alright. And so then, what we're gonna talk about today is syntax. In the linguistic sense, we're going to talk about how structures in a sentence can be formed and assembled to create larger units, larger structures. And you can do this recursively so you can start off with like the smallest possible units. Maybe their words, you can combine them with other elements to form bigger structures. And you can combine those structures with other bigger structures to form ever bigger structures until you get to the level of a sentence, and that's where we'll stop for now with syntax and in syntax there are some key concerns. One key concern is what makes up a valid sentence of a language. and this is a notion called grammaticality. So one simplification and formalization here is, you can assume that for every single string of tokens. There are some strings that are accepted by the language, say English, and they're part of the set of valid English sentences. And then there are other strings which are not accepted, and so they don't form valid English sentences. So this is a valid sentence is a valid sentence, a sentence, this valid is, is not a valid sentence of English, and so you can say that the 1st sentence is grammatical, and the second one is ungrammatical. and by convention, you mark, and grammaticality with an asterisk. This means this arrangement of words don't form a sentence of that language so one goal that we could have with syntax, and by looking at internal structure. is to specify an all and and exactly those sentences of a language which are grammatical. The second key concern that we might have, and why we might want to work with internal structures and syntax is, we might want to use the syntactic structure to help us infer the semantic structure to help us come up with a meaning representation of of that sentence. and that comes to like very tricky and philosophical questions about what is meaning and what is a meaning structure have to do and what counts as a meaning structure. And we'll get to that after the art and syntax. Then we'll get to semantics. So this idea of syntax and that languages have structure is not a new idea. It's not even an idea from last millennium. Okay, this is an idea from multiple millennia. Ago some of the earliest people to have thought about this that we are still aware of are grammarians from, say, from South Asia. So Panini, which I'm not pronouncing right sorry. Panini. From the 4th century BC. Actually developed a grammar for Sanskrit, which is a classical language of South Asia. And he he wrote this whole grammar of it, and you can translate it. and it's very interesting, because even back then. People are just as smart as today. Some people say, even smarter than than today, because they don't have like technology or whatever. And and they come up with, and they produce descriptions of the languages that they speak right. And you can see that it's it's going to be very similar to some grammars that you can read about today. Right? There's some option where you can add certain things, but other things are not allowed, and there's a prohibition, and they talk about affixes, say, and then they always receive some augment. So they there are multiple categories of things, and they combine in certain ways. And that's also a description of the morphology and the syntax of a language. So it's really interesting. You can read more about it. But yeah, so grammar is not a new idea. I will say, though, that there's another usage of grammar which is, which is not the one that I'm gonna be concerned with in this class. So there's another notion of grammar, which is that the it's this like a onerous thing that's really scary. And if you don't write in some particular way you get yelled at. Okay? And it's it can be even captured systematically by books. Okay? So for example, there's this famous book on on grammar called the Elements of Style by Strunk and white. Has anybody heard of this book? No, which is good because this is not the type of grammar we're going to talk about. And also this book is just bad because it contains lots of linguistically inaccurate descriptions. So these are style guides essentially, and they're prescriptive. They tell you how you should write. Okay, it doesn't tell you how a language is as and how it occurs. It tells you that it's some normative standard about how they think you should write in order to have something be correct. So in this course, we're only going to be concerned with descriptive grammars. So I that here's another common misconception, which is that somehow things that are non standard or informal or casual somehow have no grammar. And that's simply not true. All varieties of language have some grammar. It's quite possible that the informal or casual version of a language has a different grammar compared to the formal version, the written version and the prestigious version. But they they can all be described using rule, based systems. The informal version is not even necessarily simpler or or worse, or lazier, or anything like that. Sometimes it's even more complex. Okay? So there's no relation at all between the level of formality versus the complexity of a system. and sure we need to have some standards so that people can communicate with each other efficiently, especially in a written form. Perhaps. however, it's good to try to separate the 2. Okay, so for our purposes here, we want to build computational models of language as it is. And so we're more interested in descriptive graphics. Alright. So I said earlier this lecture, that the main goal I have for this lecture is to convince you that there exists hierarchical structures in language. So what does this mean? Well, if it's a hierarchy that means there are these intermediate nodes and some intermediate level representations. And in in the domain of syntax. These are called constituents. So what are constituents? A constituent is a group of words or other constituents really that behave as a unit. Somehow they form some unit, and then there are properties that you associate with a unit rather than with each of the words that make up that unit? So you might have have heard of some of these these terms, like a noun phrase and an adjective phrase. These are examples of types of constituents. Okay, a noun phrase, something like computational linguistics is a noun phrase, the word the single word. It is a noun phrase. Justin Trudeau is a noun phrase. 3 people on the bus is a noun phrase. These are all noun phrases. and you don't just have noun phrases. You can have adjective phrases like very good, is an adjective phrase. ridiculously annoying and tame, is an adjective phrase. You can have adverb phrases. There are many different kinds of constituents. So then the next question is like, How is it that what does it mean for these words to form a unit? And so the answer is, they share some similar distributional properties that we can test for. And so now I'll go through a bunch of tests to check whether something a group of words is a constituent or not. and the more tests that a group of word pass, the more likely it is that it's actually a synaptic constituent. Sometimes there are many factors that go on. And and it can obscure things. But so you should try multiple tests. But I'm gonna give you some tests. And we can try. Okay. So one test for constituency is that groups of words can appear in a similar syntactic environments. And this also gives you evidence that it's the same type of constituents. So let me give you an example for examples. Suppose you have a context. So this is your syntactic environment. I saw and then blank. Okay, after this. you can put you can put anything that is a noun phrase, basically and syntactically, it would be correct even if semantically, it might not be so. For example, you can say I saw it. That's that's a grammatical sentence of English. That's fine. You can say I saw Jean-claude Van Dam. The muscles from Brussels. That's fine. So this is also a noun phrase. You can say I saw 3 people on the bus. That's also a noun phrase. You can say things that are syntactically correct, but semantically meaningless, like you say, I saw computational linguistics. So that's technically grammatical, even though we're not sure exactly what it means, and you might struggle to come up with a metaphor where that makes sense. Right? However, here are some examples of things that are not noun phrase constituents. And so then they fail. This test. You cannot say I saw a van from Jean. Okay, you can't. You can't just say I saw a van. You can't say I saw Ondi. So that's not a constituent. So that's the 1st test yeah. Question. Sorry. So is this a test for a noun phrase constituent. Yes. So technically, this is a test for a noun phrase constituents. That's right. So that yeah, tests for different constituents. Yeah, yeah, so you have, you have to come up with some syntactic environments where the type of constituents you're interested in could fit there. Yeah. okay, here's the second test. Constituents can be placed in different positions or re replaced in the sentence as a unit. Suppose you have Jean-claude Van Dam. The muscles from Brussels meet me up. You can also say it was Jean-claude Van Dam, the muscles from Brussels who beat me up so it's the same group of words, and you can put it in different environments. You can passivize it. You can say I was beaten up by Jean-claude, madame, the muscles from Brussels. so you can move these around. And this is a slightly different test. Another slightly different test is you replace it with a pronoun. in fact, that's kind of what pro words pro forms. That's what they do. You can use them to refer to something else, and often you can use them to replace an entire constituent. But but the type of constituent stays the same. Okay, so Jean-claude, madame, the muscle from Brussels would be replaced by the pronoun he. So because it's singular 3rd person masculine. So then it becomes he beat me up. That is translated by Namda solution process. Okay, so these are all additional evidence that it's these are all noun phrases, the same syntactic type A 3rd test is, it can be used to answer a question on its own, especially in in formal language, who beat you up. Okay? So okay, it's just so as an example of something that fails. If someone asks who beat you up, you cannot answer the muscles from that's not a constituent. That's not a now face constituent. And I already said this orally. But so the type of constituent that you have is going to be called its syntactic category. So noun phrase. So all of these examples are with working with noun phrases. But you can have verb phrases, actually phrases, prepositional phrases, clauses, sentences, and so on and so forth. There's not really a universal list of syntactic categories, because. as you might expect, linguists don't agree, and there are different formalisms and different languages might like have different syntac categories that seem to make sense for them. But here are some of the most common ones that are in most grammar formalisms, and and there are likely others. Yes. Question when you define a constituent like noun phrases, is there a definition, or is it defined by its tests? Right? That's a good question. When you define a constituent such as a noun phrase? Is there actually a definition, or is it defined through the tests? I think it depends on your theoretical perspective, and where you come from, so for our purposes, we don't have to care so much about that we, for our purposes, we can define them operationally through their distributional behaviors. So for us, a particular syntatic category will be defined. I will say it's useful to have it if it forms some coherent group where all of these things that we call say that they belong to the same category, they all behave distributionally. Similarly. yes. Another question supposed to be applied. is it the yeah. So the question is, there's not people online. So these tests seem to be qualitative and also are there other tests for other types of constituents? So yes, that these tests are qualitative and also yes, these there you have to define different tests for other syntactic types, syntactic categories. So for verb phrases, for example. you might need to do something like, ask questions like, you did. What? What did you do? And then you have to answer with like going to the park? There are also replacements for verbs. They're actually pro forms for verbs as well like to do so. There might be also tests in involving a phenomenon called elision, where you like. remove things and you skip over them. But anyway. yeah, so it can get very complicated. Alright. So so far, we've covered syntactic categories and constituents. Next, we can talk about the relationships between the different constituents, because the it's not just that they there exist constituents in a sentence. but they also relates to each other and systematic and regular ways. And so we can talk about grammatical relations between constituents. So here are some that are as very widely known, and probably you've heard of them, and they're related to verbs in particular. But in general there are many more grammatical relations than this. but when it comes to, when it comes to verbs. you can talk about what are called arguments of verbs, so verbs can have subjects, and they can have objects, and they can have direct objects and indirect objects. Okay, so Jean-claude Van Damme relaxed. Here the verb is relaxed, and the subject is this noun phrase constituent of Jean-claude Van Damme. or the wallet was stolen by a thief. Here the verb is, was, or was stolen. and then the subject of it is the wallet. So note here that subjects an object here. We're defining them purely, syntactically. purely, structurally. So it's not based on the semantics of the sentence. so it's not based on. Oh, it was the thief who stole the wallet, so shouldn't the thief be the subject? No, here the thief is part of a prepositional phrase. Okay, so structurally, the subject is still the wallet. So it's about the where where it's located in the sentence, not about what its function is. okay. So subject is the most common one, the next, most at least in English. The next most common one is direct object. The boy, keep the ball here. The ball is the direct object. and sometimes you also get sentences where you have multiple objects, and one of them is the indirect object. so she gave him a good beating him. Here is the indirect object. and, as I said before, there are many other grammatical relations. These are just some of the common ones related to verbs that you might have heard about before. but, for example, like adjectives, can also require certain arguments, and you can talk about the relations between a determiner like the versus its noun, like like the ball or something. So there are many other relations. so verbs and usually predicates in general. they tend to impose some kind of constraints on other syntactic elements that must appear to form a valid sentence. and that's called subcategorization, so subcategorizing verbs into different kinds of verbs, depending on what constraints they add to the other elements in the sentence. You might have heard of terms like intransitive verb or transitive verb or ditransitive verb. Maybe if you've heard of those terms, those are essentially different subcategorizations of verbs. Okay, different different subcategories of verbs. For example, an intransitive verb is one that only requires a subject. So relax is an example of that. You can say I relaxed after a long day at work. relax also happens to be a transitive verb. You can say I relaxed my muscle. So you have to relax. Your shoulders to do this exercise properly, or something like that. Okay, so that in that case it would be a transitive verb that takes on both the subject and a direct object. and here, from the examples, before steel is also transitive, and kink is also transitive. But and then something that takes like 2 objects would be something like, give usually verbs that involve the change or transfer of something. Usually you need, like the the transfer, the recipient, and the thing being transferred right. There are 3 things there so often those require 3 elements, and then they're called ditransitive verbs. And there are some regular processes that exist in languages that can systematically change the categorization. So, for example, in English, there's this passive structure which changes a transitive verb into one where it only requires the subject, and it changes the mapping between the syntactic and the semantic roles of the arguments of that. And so that's a systematic thing. Other languages have different role, different operations and things like that as well. Yeah. Sorry. Do you mind to give an example of that? I didn't understand for steel, maybe. Sure. So for steel it would be something like, okay. So the active voice one would be a thief stole the wallet. Okay, so there, there's both a subject and a direct object. When you pacifize it, it becomes the wallet was stolen. and technically by a thief. Here it's not obligatory. You can add it if you want, but it's not obligatory. So the in terms of what's required by the verb, it's just the subject. yeah. And then the the other the prepositional phrase, you can add it on, but it doesn't count as an argument. Yes. What's the? Oh, these things? Okay. So with the number here. Sorry I should have explained the number. Here is just the number of arguments that it takes. and then the list here is the the the relation to like what else it requires, like the the list of argument type types. So here it means that you need to include a subject. and you need to include a direct object, and with give, you need to include an indirect object as well. So you can think of a parallel to say programming languages if it helps where you can talk about functions. And they they also have to take a certain number of arguments. So that's why they're they're called arguments in both cases. Okay. so you might have a function that is like, multiply XY, and it takes in 2 numbers x and y, and it multiplies them together. Right? Some really simple function. Well, in natural language, analogously, you can have, you can think of like, there are things like this as well where you have predicates which are like the the verbs or other things, but mostly verb, for now, and they take on arguments, and then those arguments are. They have to appear in certain positions around the sentence. and you can call them subject and direct object positions and so forth. So there's an analogy to be drawn there. And in fact, we'll make this analogy quite explicit in a model later on in the course. Yeah, there are many other subcategorization possibilities like with the verb wants. I want to learn about computational linguistics. It takes a subject. And then this is called an infinite table clause, because it's to learn. You can have something like a prize which takes a subject, an object, and also a prepositional object. The minister apprised him of the new developments. and so the minister is the subject. Him is, I guess, an object, and then prepositional phrase, object of, and it has to be with of you can't say the minister apprised him in something, and you can't simply say the minister apprised him. I don't think that sounds weird to me. and, as I said before, it's not only verbs, but other things can do this like difference, depending on your dialect. You probably want this course's difference. And then one of these prepositions. I'm not sure which one I prefer. I think I prefer from or to. I'm not sure exactly, and maybe in your dialect it's a different preposition. But but what I expected. So this is another subcategorization. Yeah. So a subcategorization is a slightly different in that. It's talking about what certain predicates expect. like the others, constituents, so there will be a constituent that fills each of these slots that fills each of that needs to fill each of these roles, and they have to appear in a certain syntactic environment which can be called subject or object, or something else, or a prepositional object, and so forth. And that's part of what helps you figure out if this the sentence is grammatical or not. Basically, that's what this is all about. Okay, so here's an exercise. Identify the prepositional phrase in the following sentence, and give arguments for why? It is a constituents. Maybe we can do this together because we're running behind time. So okay. so 1st of all, what is a preposition? Can somebody identify what the preposition is in this sentence? You raise your hand? Yes. on, on. Yes, thank you. Do you want to see? Did you want to say something else? Or Ed? Yes, on Saturday, October 12? th Great? Okay. so prepositions are these words that help indicate relations between things in a sentence, and they appear before a now phase. You can also have words that function similarly, but they'd be. They appear after a noun phrase, and those are called post positions. But in English, it's overwhelmingly propositions. There are some post positions. But yeah. and other languages prefer to have post positions anyway. Okay, so why is it on Saturday, October 12.th So now give me some arguments for why, this is a constituent as opposed to just on. Why, it's just on, not a constituent. Why is it not like on Saturday or on Saturday, October. So think about you. Go back to those tests and think about it like. why is it? The whole thing is a prepositional phrases. Yes. for whole purpose. the beginning. it would still retain its meaning. Yes, you can move the whole thing to the beginning of a sentence. That's a great test. So on Saturday, October 12.th The next assignment is due. and compare that against our alternatives. Right? So you cannot say. On Saturday the next assignment is due October 12.th I mean you could, but it would be very strange, and it means something very different which doesn't make any sense, I guess, and you cannot say on the next assignment is due. Saturday, October 12.th Great more tests. Yes. exactly. You can ask, when is the next assignment due? And you can answer. On Saturday, October 12.th I suppose you can also just say Saturday, October 12.th When is the next time to do? Saturday, October 12.th You can do that, too. So that's evidence that Saturday, October 12th is maybe also a constituent. But it's a it's a different constituent. And you cannot answer just on okay, when is the next assignment due on. No, you cannot do that right. What else? Any other tests? How about replacement? Can you replace this with some kind of pro form? Yeah. I don't know. You could say like is due then? Yes, great. The next assignment is due, then, is where then? Replaces on Saturday, October 12.th Okay, great. So now we have pretty strong evidence that this is a constituent of some kind. Alright, so that covers like the linguistic elements of what wanna wanna talk about. For now for syntax. And next, we're going to talk about the formal computational model that we're going to use to describe these hierarchical structures. Because remember, we're computational linguists. We're not just linguists, we're computational. So what is the formal model of grammar that we're going to use to account for these and other syntactic concerns. And 1st of all, maybe I should say, what makes something formal. Okay, so there's a whole field of research in formal language, theory, formal grammars. for example, who has heard of automata theory? Yeah, some people, right? And maybe some of you are taking that course or another course in formal grammars and formal language theory. So here we're going to take that machinery and that theory and apply it to natural language. Okay? And in that context a formal grammar is a set of rules and other associated things that help you generate a set of strings that make up a language. and in this context language simply refers to the set of strings that you want to accept. So, for example, you can. You can be working with arbitrary symbols. You can be working with, like the language of arithmetic. where you have symbols involving plus and minus and multiply and divide and numbers, and you want to figure out what forms like. what's the set of strings? That's a correct arithmetic expression. Right? In our case, we're going to take that form of machinery, and we're going to apply it to natural language. where we assume that our goal is again to characterize exactly the set of strings that form valid sentences of English. Again, there are good reasons to question this assumption, but we're gonna run with it. For now I don't know if I said that before. Okay, so there are good reasons to question that assumption. But we're gonna like, go with it. For now to say that we're gonna pretend that we can model English or any natural language as sets of a set of string. And you want to figure out which sets of strings are grammatical and which sets of strings are ungrammatical. Okay, so why do we want to do this? Well, a formal understanding helps us to develop appropriate algorithms for handling and dealing with hierarchical structures and syntax plus? Maybe there are implications for cognitive sciences and language learning. Okay, so there are other formal grammars you might have heard of and encountered, or use finest state machines like finest state automata and regular grammars. So Fsa's our finance data are particular ways to talk about and describe regular grammar. So there's an exact correspondence between them. And they are actually used in Nlp as well. So they're very practical because they tend to be much faster to like, compile and run and process so, but like just to use the same terminology again. and Fsa. Generates a regular language. and fsas correspond to a class of formal grammars called regular grammars. So here, then, formal grammars, you can also talk about them as like possible sets of strings that you could describe with that formalism. Anyway, in Nlp. They can be used for tasks such as stemming and lemmatization and morphological analysis stuff that we talked about at the beginning of the course. however, it turns out for the syntax of natural languages, where you might have multiple constituents and some categorizations and other phenomena. it's more natural and useful to use a more powerful class of formal grammars. And these are in particular context, free grammars. Okay, so that's what we're gonna talk about next. And before we continue, I'm gonna close the door. I'll be right back. Somebody is really upset that I'm talking about context-free grammars. Okay. so what are context-free grammars. So I'm going to start by giving you like the form of it, because I'm guessing that maybe when you're thinking about describing programming languages, you might have seen something similar before. Like, if you're talking about the syntax of a programming language like of python, or something like that, or if you're in a compilers class, or something like that. Okay? But essentially, they talk about what? Possibly they talk about our rewrite rules. They're they're essentially a series of rewrite rules where you have something on the left, which represents some elements that you're working with, and it rewrites into something on the right, which is like a sequence of smaller parts that form that bigger part, that bigger chunk. So in natural language you can talk about. Oh, we might have a sentence which we'll denote with some symbol like S. And a sentence should be rewritten into a noun phrase, plus a verb phrase. It's made up of a noun phrase plus a verb phrase. and then maybe a noun phrase. Can we write to like a word like this? And a verb phrase? We write to something like a verb, and there are some options for that as well. So here a vertical bar is just a shorthand. It means, or okay, so a verb can be right to is a verb. Can you write to kicks? However, can we write to jumps and over? Can we write to rocks? So this is already a very simple Cfg, so this is, I'm starting off with an example. and then you can use that to generate or to recognize. So to generate. You start off with a starting symbol which for us will be the S. The sentence. and you apply the rules of a Csg. To keep rewriting until you end up with a sequence of words. Okay, so S. For example, generates Npvp, there's only one option in this grammar. It's a very simple grammar. So then you get, as we writes to Npvp. And then Np. Rewrites to this, and then Vp rewrites to V, and then V rewrites to some of one of these sperm. So maybe it rules so this would be a string. This rules which is accepted by this grammar. So that's part of the the language described by this grammar another one is like this, rocks. Right? You just use a different rule there. So for B rewrites to rocks. So then, you have non-terminals, which by convention are written with capital letters. Again, this is just a convention, and the terminals or leaf nodes or words in our case, which are the yeah, which correspond to words, because we're applying it to natural language. Yes, what makes what about this makes them context free? What about this makes them context-free? So you should just try to ignore context, free the the meaning of context, free. So so what? So there's a formal definition. So here's a, so this is okay. So formally speaking, this is what makes it a context, free grammar. Here's the formal definition. If you want the intuition, the reason that is called a context, free grammar is because all of these rules you can always apply them. As long as you have the right left hand side you can always use them to rewrite, and that's what makes them context free. There are other classes of grammars. There's 1 called context Sensitive grammar. or you can add conditions. You can be like, I'm allowed to rewrite an S. To an Mpp. In the context of something else or Np. Can be rewritten to something in the context of something else. But that's not super important for our purpose. The formal definition is what we should go by. Okay, and this is the formal definition of a context-free grammar. or from now on I'm gonna start saying Cfg, to be shorter. So a Cfg is a four-tuple. We love those they have a N. And Sigma and R. And S. Where N is a set of non-terminal symbols. Sigma is a set of terminal symbols. R is a set of rules also called productions. In this particular form of something from the non-terminal set rewriting into some combination of things in the non-terminal and terminal sets. and the star here means you can have 0 or more of them. So technically, you can rewrite to an empty symbol. But that's not very useful most of the time in natural language. So we're just pretend it's like one or more, but I guess it's 0 more also works. And also, if you're allowed empty things, then it causes parsing to be much harder. But that's a that's an aside. Okay? And also you need a designated start symbol, which is an element of the non-terminal symbols. And within the system. You have a derivation to where it's a sentence accepted by this grammar. If you can start from the starting symbol, and we write to a bunch of terminal symbols. Okay. so here, for our purposes. the ends will be the constituents, right? That are larger than single words. All of the constituent, the syntactic categories. In constituents they will be symbols in the set. N. Whereas things that are words will be the terminal symbols, and then they'll be set part of the set. Sigma. Okay, so let us use this formal machinery and work with it, and use it to describe a very, very small fragment of the English language. So our extended example would be, let's develop a Cfg that can account for verbs with different subcategorization frames. So, for example, we have relax and steal and kick and give. Okay. okay, so I'm gonna start off by copying the initial grammar from before. But I'll just replace the part to do with verbs for the verbs you want to account for. Okay, so what what was it we wanted? Account for? We want to account for? Relax. relax steel cake give. And actually, I'm going to put them in a 3rd person just to make things simpler. Okay? So the way to handle this. So 1st of all, everything requires a subject. So so we're good, because, like this 1st rule of Sv. Rights to Npvp. it takes care of that. The subject is there in like this 1st element of np. so the thing we need to change is we need to change what can be a verb phrase. because we basically have 3 options. Right? We can have the intransitive option, the transitive option, or the ditransitive option. So this means we need to change this rule from Vp rewrites to V, to Vp. Rewrites to V, or Vpv. Writes to Vnp. Or Vp. Rewrites to Vnp and P. And not only that, here comes a subcategorization part we need to subcategorize. We need to come come with subcategories of the the verbs, because there are going to be some verbs that are that take one that are intransitive, so they only take the subject some verbs that take 2 arguments and some verbs that take 3 arguments. So these become 3 separate symbols in our system. And now we can change the the subcategory, so that not all of them are simply these. So v, 1 would handle relaxes. and then v. 2 handles, steals and kicks. and v. 3 handles gives. And that's how you create a grammar that that respects the subcategorization of these verbs. and you might also want to make your nominal structure more complex. So right now, all this grammar can do is say, this relaxes, this steals, this, this kicks this. And this gives this this right. That's all of the sentences currently admitted by this grammar. So if you want to make it more interesting, you can also work on the Np part and make that more interesting. But that's okay. That's like, that's not what the question asked for. So we're done with this. For now any questions so far about the example. Okay. all clear, obvious, or completely. Unclear fair? Okay, yeah. We also have to draw a tree. Or is this all great question? Do we also have to draw a tree? So you have to draw a tree anytime you actually work? You want to work with an actual sentence like a real sentence. this is a grammar right? It describes the possible sentences that can be accepted. So this describes all of the possible sentences that could be accepted. But it doesn't work with any particular sentence. So if you want to work with a particular sentence like this, this steals this, then you would have to draw the tree. and we can do that. This deals this. I'm going to attempt to use the drawing mechanism. So this is a noun phrase, right? Oh, no. I should type that. Okay. so this is an Np steals is v, 2. And this the second. This is also an Np. and then B 2 Np. Gives us a Vp and Npvp gives us an S. And then you can draw lines. Don't want it to be red. Okay, we can draw lines to indicate that they correspond to rules in the grammar like so. And you can check that. Each of these is a rule in the grammar, and since the top node is an S, which is a starting symbol, this is a sentence accepted by the grammar and everything that doesn't work like. So if you had this, this deals, you would not be able to find a derivation like this, where at the top. You have one S. Node. That's how you know it's not a sentence of the scrum. Okay? But clearly that grammar is not the grammar of English, because we say other things too. So here are some problems with the grammar. So 1st of all, is under generation like it misses a whole bunch of valid English sentences. For example, there's a whole bunch of things to do with like nouns, and that and it does that. It doesn't know about right, like noun phrases like boys and thieves and balls and wallets, and so forth. So that's called under generation. So that's something in the actual language that should be accepted. But it's not accepted currently by the grammar. You can have the opposite problem, which is over generation where it generates sentences that are not grammatical. So I think I used a different, a more complex example when I was thinking about this. So here the you can even argue like this steals. This is not a very interesting example. Right? So suppose you add, like, relax here. Okay, then we can have overgeneration. So you might say that relax and relaxes they should be. They're they're both intransitive verbs, right? However, there's something annoying called subject-verb agreement. So you're not allowed to say this relax in English. at least not in like the standardized formal version. And so then this grammar would overgenerate because it generates it accepts the string. This relax, even though it should not be accepted. If you want to account for what's actually in Ingram. okay? So we can work on this, we can gradually start to modify our grammar to account for more and more phenomena, and to fix issues with over and under generation. So it becomes an engineering project, right? So it becomes a grammar engineering project. Okay, so next time someone asks you, what did you do in class? You can say I was a grammar engineer. Okay, so let's extend our grammar to account for say. prepositional phrases. I'm also going to make the Np structure more interesting. I'm going to say that a noun phrase can also rewrite into a determiner plus a noun and a determiner can be something like V, or maybe a and a noun can be something like a foul or boy or girl, or something else. Okay, so, or wallet, I guess. to make things slightly more interesting. Okay, so we want to account for, say, prepositional phrases. Or, okay, let's do adverbs first.st Okay? So here it's like, softly. So how will we do that? How do we add adverbs to our grammar? Any suggestions? Yes. or yes, that's right. So to modify this grammar, to add adverbs. Well, first, st you need to add the adverbs. so we can do that. So we can do softly or quickly or slowly. And next, we need to integrate this into our grammar somehow. and it seems that the most obvious way is to put it in with the verb phrase. and semantically. This also makes sense, because adverbs, as their name suggests, are associated with verbs, so they somehow modify the meaning of the verbs or the verb phrases. and so we can add it like. So so now this grammar will accept a sentence like the girl kicks the balls softly or quickly, or whatever yeah. question can you have like an empty element inside? From the good question, can you have an empty elements inside. the A constituents. It depends on who you ask. So if you're thinking about this formally in the from the Cfg perspective, yes, you're allowed to do it by my definition of Cfg, because here this is 0 or more so, you can have a non-terminal rewriting into an empty SIM like into nothing from the perspective of linguistics. It depends who? You ask. I think, in mainstream. Well, in the kind of linguistics you're likely to encounter in linguistics courses here. They probably will teach you that there are empty elements, and they give evidence for that. There are some other people who say that there's there's something wrong with that approach, and that there may not be empty elements. But they're they're useful to account for many phenomena from the perspective of parsing. Okay. So now, I'm just talking about like Cfg and what they do and how you use them to describe syntax. So next, we're going to start talking about how to systematically recover these trees. Right? So suppose, given a grammar, given a sentence, give me the parse of that sentence from the perspective of parsing empty elements cause a big headache because you don't know where by definition, you don't see where the empty things are. The the empty elements are so to do, parsing properly with empty elements, becomes kind of intractable because you have to posit all possible combinations of up to infinite number of empty elements everywhere in your sentence. So in practice you probably have to limit yourself to like. At most, I will have, like this, many empty elements in a row, or something like that. And then parse with respect to that. But yeah, that's a really interesting question. Turns out to be like everything there are complicated answers. But yeah. yes, in our example we would have deleted the other Vp mapping right? There can only be one. What we develop. So we need to keep the original rules. Is that your question? It's just one mapping, that's all extended. Sorry. What do you mean by mapping? Well, I shouldn't look back. It's the wrong word, but like Vp rights to 2 different things. But it's actually just one big rule. You can't have multiple rules. So you can have multiple rules. In fact, this is just a shorthand. This is actually 6 different rules. Yeah, so this is like, vp, rights to v, 1 Vp rewrites to v, 2 nt, Vp rights to that. So these are 6 different rules just for the sake of saving space and using the vertical bar, that's all. But literally, it's just a safe space. It's these are, think of these as 6 different rules. Okay? And I think the question also asks for prepositional phrases. But I'll let you do that on your own. It's the same idea you have to say, what is a prepositional phrase, come up with the internal structure of a prepositional phrase, and then integrate that into your grammar. Okay, here's an interesting phenomenon. Consider the following sentences. The dog barked. I know that the dog barked. You know that I know that the dog barked. He knows that. You know that I know that the dog barked and at infinitum. Okay? So in general, you can describe that with some kind of grammar you can have Sp rights to Npvp. and then the Np is like the subject, so it can be like any noun phrase. The Vp is the verb phrase, and then it gives you like. you can have one branch of it which gives you barked like the dog barked, and another branch of it that goes to like know that something. But the really interesting thing here is that you can have it. You can write it so that knows that rewrites to S like your initial starting symbol. So why is this interesting? It's because we have an instance of recursion where you have a non-terminal symbol rewriting to itself eventually. and assuming that you have no empty elements, and, like you, have actual words there. what this would say would be that it's possible to have sentences of that are arbitrarily long, arbitrarily long. if it fits within this structure of like this, this described by this grammar. So you see why? Right? Because, like anytime, you have a sentence of a certain length, you can make it longer by adding, like. I know that, or or he knows that, or she knows that or something, somebody knows that right? And so this is an interesting property, and which is suggested by Cfgs, which is that in natural language there seems to be no fixed upper bound in terms of the length of a sentence. It's arbitrary, and really the main constraints are due to our processing power. Okay, we have a limited amount of like brainpower and memory, and so forth. And so in practice, the sentences are not that long. but in terms of what this formalism suggests theoretically. And and this model suggests that you can have sentences that are arbitrarily long, infinitely long potentially. Yeah. that's why it isn't. And some people think this is a big deal. and you can contrast it with other systems that maybe are fixed in some way. So, for example. supposedly animal community again, super controversial, but animal communication, like with like bee dances, you know. Have you heard, like bees, dance in a particular way to signal to their colony about the location of like food or shelter and stuff. Also, like chimps monkeys. They they make vocalizations, and you can teach them to sign. And they have properties and like whales, clicks. And that's the type of communication. Anyway, this is all controversial and potentially new, but one way that they may or may not differ from human language, at least many of them seem to differ in human language in that they cannot be arbitrarily long. Okay, they they there. There might be a fixed set and fixed inventory of symbols and signs that can be produced, but they don't have the same properties that, like Of recursion, where you can have, like. you can in principle have sentences that are very, very long. If you want. Alright. here's another exercise. Let's fix the subject. Verb agreement issue. So fortunately, English has a very limited amount of subject-verb agreements in the present tense, anyway. except for irregular verbs. So in the present tense. the only thing we really have to fix is, we need to ensure that if you have a singular 3rd person. Subject. then the verb also has an affix of s or es, depending on the verb. otherwise is just the base form of the verb. Okay, so let's fix that part of the grammar. Okay. So before we do that, we will add both forms. Okay, so let's start with this grammar and then fix it. Okay, so what is our general strategy gonna be any ideas? So the the main problem we have is right now we can generate the ball. Relax the the boy or the boy relax the the boy, kick the ball. That's currently accepted by this grammar. Right? Do you agree? Because, through this branch. We can have the ball the boy. and then through the Vp branch you can go to Vp, v. 2 np. And then you can have the boy kick because kick is currently v, 2, the ball. And suppose we also want to model like, I, okay. So now we have both 1st person and 3rd person. yeah. like, split them into different categories so that they absolutely great. That's exactly right. Okay, so we have to split them into different categories and rewrite the rules to make it all fit. So now we have v. 1. For with an S and v 1 without an s. and likewise. We'll have B 2 with an S. And v, 2 without an XS. And then we'll have. I'll try to make this more distinct. and we'll have 3 v. 3 with an s. and then v. 3 without an s. And that means we have to duplicate all of these rules. So we have versions with S and versions without S. Oh, oops! I made a mistake. Versions with S versions without. Oh, no, these are all. S's, yeah, that's right. And not only that we also have to pass that information up to the level of the Vp. So if you think about it, the agreement works between the subject and the and the verb right, and the subject is like all the way over here on the the left. and then the verb is inside the Vp, which means that we have to pass that information along somehow. Okay, so that means that we also have to indicate that this is a verb phrase, where there's an S on the verb. And that means, now we need 2 versions of this rule. We need one rule, which is like Mps Vps, and one role, which is Mpvp. and I now realize that I hate my naming terminology. Because. yeah, this is going to be okay. So we're going to have 2 types of nps, those kinds of envy Nps where you they have to take an S on the verb, and the kind of Nps where they have to not take an S on the verb, right? So, okay. so this will be part of Nps. But I, for example, would not. And for I, so you can have debt n and also I. And there are 2 different kinds of noun noun phrases, the ones that are 3rd person singular, and the ones that are not 3rd person singular. So, for example. yeah. So then. all the 3rd person singular nouns would be like a ball boy girl wallet. And then all of the not 3rd person. Singular nouns would be like balls. boys, girls, waltz. Okay. Do you see why I hate my terminology? Now, this is confusing. So I'm gonna say, 3 s. Rather than 3rd person's. Ts, how about ts. 3 s. What do you prefer? 3 Si guess. So it's 3rd person. What I mean by S, here is 3rd person singular. Oh, no. Something. Okay. Oh. 3 s. no. This is like. yeah, this is just a noun. Okay? So the so nouns that are not 3rd person singular will will be handled by this rule. nouns, and that our 3rd person singular would be handled by this rule. Okay? And then you have to ensure that the 3rd person, singular nouns agree with the 3rd person. Singular verb phrases. Okay. I will use 3 s. Like so. But but yes, hopefully, I didn't make any mistakes. Okay, does this make sense? Okay? So the general strategy then. is that we want to. Every time we create a distinction we have to think about like, okay, is it? How do we incorporate that into the grammar? Usually it means that you have to split up a category into multiple categories. And then you have to think about where that information needs to go, because if it if it affects something that's very far away in the syntax tree. then the mechanism we have for for making sure that the other part of the sentence knows about this is to propagate that information up and down through the syntax tree and through the rules described by the syntax tree. So that's the general strategy. and, as you can tell, this becomes very hairy very quickly. So then people have come up with schemes to make this a little bit more tractable. So, for example, here, we're really talking about features. right? So we have a feature, which is that a noun can be singular or plural. A noun can be 1st or second or 3rd person. and we want to be able to pass that feature information up and down the tree. And we want to have rules that check and ensure for correspondences between the features and different parts of your sentence. So then, although we don't talk about it much in this course, people have developed theories of grammar that incorporate that. And that's really cool because it's a computational model. So it can be implemented and read by a computer to do parsing and whatever. And you can specify things much more plainly and elegantly than we're doing here. through a feature-based account of a language and properties of words and grammar rules, and so forth. But we won't have time to go into that in depth. So if you're interested in that ask me, and I can point you to references. And they should take she this they should teach this in linguistics, but I don't think they do. But anyway, that's a rant for some other time and and for us, we're just sticking with standard context-free grammars, where your only option is to create these very complex symbols. Alright. So so far, then, we've talked about. Here's evidence for hierarchy and structure in sentences. And then here is one formal computational model to describe said structures. there are actually alternatives. There's an alternative which is also very popular in computational linguistics in Nlp called dependency grammar. It's just a different view of the syntactic structures within a sentence so grammatical relations, they induce a dependency relation between the words that are involved. and we have the the student studied for the oh, I should go back to full screen mode. Now. if you have the student study for the exam you can have, you can talk about how each phrase has a head word, and you can connect the head words to each other. So, for example, for the entire sentence, the head word is said to be studied within the constituent of the student. It's student within for the exam. It's 4, perhaps, and within the exam. It's exam. And in a dependency grammar, you basically draw arrows directed so directed edges to connect and describe this relation. And here's the here it is for that particular sentence, okay, so study will be will not have, will not be the child of anything, because it's the the head of the whole sentence. and then from there you can draw a directed edge to student and to 4, and you can label them. So here this is a subject relation, and here this is a prepositional phrase, argument, relation, and from student you can draw an arrow to V, and from 4 you draw to exam, and from exam to B. So dependency. Grammars are just a different view of the hierarchical structure of language. But it shares like this basic property and assumption that it that it's hierarchical. Yes. Question. yeah, how do you identify headwords? That is tricky. So you can think about like. So some of it is through which elements specify the properties for the whole phrase, the whole constituent. for example, in terms of singular and plural. It's the student that that lets you know that the entire phrase of the student is singular. So that's 1 aspect that helps, you know. What's the what's the head word more generally, it's the fact that the student is a noun that causes the student as a whole to be a noun phrase. It's not V that causes the whole thing to be a determiner phrase, although there are theories that do that. But but okay, so for our purposes, it's the student that causes the student to be a noun phrase. and it's a 4 that causes this whole thing to be a prepositional phrase. So for the exam. Again, there's a lot of complexities and different theories that do different things. But let's forget about that for now. But the the advantage of talking about dependencies as opposed to constituents is that it exposes the syntactic relations, the grammatical relations much more easily. So you can directly tell here that there's a subject relation with this arc. And there's a preposition, prepositional phrase, argument, relation with this park. whereas if you draw things out in a tree. it's not so obvious, right? How do you tell that something is a subject? How do you tell that something is a direct object. The only way to do that is to describe some here, at this portion of the tree you might find the subject as an Np. To the left of some Vp or something, and that's quite complex. whereas in the dependency grammar it's much more clearly exposed. And so if you're interested in understanding the semantic relations between words and and noun phrases, and you need it for further processing. For example, you're solving some information extraction task. People often work with dependency grammars rather than constituent grammars. Also, the other thing is that you can convert between them. So dependency trees can be converted into constituent trees. Deterministically. if the dependency edges don't cross each other. and constituent trees can be also converted into dependency. Trees, if you know what the head of the constituent is okay? So okay, let me do this example. Okay, so the students study for the exam. The way to do the conversion is to check. You start from the leaf, nodes, and you see what is associated with it. And then you you turn that into a constituent. Okay? So in the student study for the exam here, the leaf is B, and there's a student there. So you know that that's a constituent. Similarly, the exam. Okay, now that you've merged those nodes, then you can check. So, for also is associated with that right? Because now the exam is and is essentially a leaf, a new leaf note, because you've merged them so you can. You can merge it with 4 next. So then it becomes for the exam. And finally, you have studied, which takes in 2 arguments with like students and for the exam, and so that you can merge those 3 together, and you get a ternary note there. So you don't get exactly the same thing as like the kind of constituent grammars that we talked about before. So that's potentially a problem. So you might need to do a little bit more work to if you want to ensure that kind of like a binary branching trees with internal structure that we talked about before. But at least there's some mechanism to convert a dependency tree structurally into something that which is a constituent tree. And also note that there are no labels here. So you need to. You need to figure out the label you. You have to define additional rules to convert between from the these, these grammatical role labels into the non-terminal labels in the constituent tree. And it turns out you can do things the other way around, too. But I won't show an example, for now. And yeah, dependencies can cross in English. They rarely cross in other languages. They cross much more frequently. Does anybody speak German? Some. Okay, a little bit. Does anybody speak Russian or Czech? What other languages? Possibly Farsi a little bit. Does anybody speak, Farsi, and they're really into Persian poetry. Okay, okay, we're out of luck. You just have to trust me. So so you can have dependencies, graphs where the edges cross each other so. And this is an example from German. He tried to reach me. You can say either. It's like he has me tried to reach, or he has tried mean to reach. These are both sentences of German. and they're both pretty natural, I think, and and accepted. and you can draw the arrows using how the syntax of German works. And so in the 1st version there's no crossing dependencies. And in the second version, there's a bunch of crossing dependencies, so that he has me tried to reach actually has a crossing dependency because it's reached me. not tried me, but reach me. and in the second one it implies that if you convert that into a constituent structure. you will get discontinuous constituents. You have a constituent that that's broken up by other words in between, which is pretty interesting. I think we'll end there for now. So next time we're going to talk about like our natural languages, context-free grammars. And then we're going to talk about parsing algorithms. So enjoy your reading, break and happy Thanksgiving.
