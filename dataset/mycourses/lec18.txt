David Ifeoluwa Adelani: Okay, I also have people on zoom. So, okay. welcome to lecture 18. I believe you can all see my screen. Yeah, okay, so today we'll be talking about neural machine translations. That is probably more relevant to our models have been trained now than Ibm models like, nobody uses that. Yeah. okay. So today we'll try to finish off with Ibm models. And also we'll touch about we'll touch on some important concepts like, how do you actually get your Machine translation output, because when you're trying to decode from the decoder side or any other model you use, you need some authorities to decide actually what will be your finer translation. There are many possibilities. So if you still remember the large language models, language models that we talked about. When you want to predict the next world they are. there are different probabilities, and there are sometimes that's the same. That's you have different words that are equally probable. And the question is, how do you decide on what would be your candidates? Words that you will produce? So also today we'll talk about attention mechanism, actually, which actually will bridge the discussion from Rnn and Lstms to the transformer. And the interesting thing is, some of these techniques have been developed actually. in this community of. So we have some work that have been developed also at University of Montreal and And so I think one of the authors of the attention mechanism is actually still around. So maybe you can talk to him at some point and then we have transformers and landing models, and we'll talk a little bit about prompting. There's nothing technical about prompting, but there's there's some ways you can do prompting that will give you better results so just to continue from what we did last time we talked about Statistical machine translation. If you have translate if you have a sentence in English, and you want to translate to another language, let's say French or Russian. and then we say, we can formulate this using the base rule. Using, what is the probability of E multiply by the likelihood probability of F given E, and then we can take the Agmax based on the alignment model. So for the Ibm series one Ibm model, one. It was one of the 1st models from a set of 5 models, and if you remember, from the last class, the most important thing we discussed is that you want to align every word from English to another language, say French, which is our example. You want to align every word to 0 or one target word. and then you need an alignment model to be able to do this. and once you are able to get the alignment model, and you can have more than one alignment model. And then you need to pick the best alignment model to use. And for the simple case of Ibm model one. You don't need to model different distortions of the world. You don't need to care about the world or that. And then also there's no need to care about the likelihood, the likelihood of fertility, for example. caring about if a word actually translates as phrase in some other language. because maybe you don't have a direct translation of that word in a language we talk about some languages that do not have concept like grandfather and broader. And then you need to find an like an explanation in a target language for but for the Ibm version one or the model one doesn't actually treat these cases. And we said, if you don't have any training data to start with but you have larger labor data, what you can use, you can use expectation maximization. Again. where you start with counting. basically computing mle over every token in your target language and in your source language, for example, French and English. and then at the end step, you can now compute the likelihood given a parameter theta and So in practice you don't initialize this likelihood you're trying to compute uniformly if you have a reasonable size of lexicon. you might have too many parameters. So what happens is that in practice you try to restrict this only to your training data to available training data. So once you're restricted to your training with that, then actually, this kind of reduces the the number of parameters you need to compete. Okay? for all the variants of the Ibm model. when the sentence length are high, there's a need to come up with an algorithm to be able to actually compute these probabilities more efficiently. So in Ibm version 2, we don't assume that all possible alignment structures are equally possible. So so we don't have this street assumption that every word has to go to 0, or one word. and, for example. so in this in this example. so you have. you can have a word that is attached to another word. and then you can have a group of words that actually mean the same word. So you have a single word in English, and actually means 3 different words in the target language. So Ibm version 2, Ibm model version 2 actually allows you to model this. Why, Vashawan only allows water to order alignment. So version 3 and then this actually models it more explicitly. On how many words can you allow to model a single word. Do you allow just 3 words or 4 words? Then you decide and then model 5 actually makes use of dependency structure of alignments, so that they don't depend on each other. So all these ones are like the series of Ibm models that have been developed. But in practice nowadays what we we usually do before the neural models is what is called phrase-based, statistical, empty. And after that we have the newer-based statistical engine. So for the phrase based status. Look at empty. The idea is that you can leverage the knowledge of of your status to put Lm and actually use it to do machine translation directly. And there are some times that this is effective, especially if you don't have a lot of data. So if you have a pair of language that you don't have a lot of data, then typically people advise phrase based statistical, empty right? But by the end of this class. I'm going to tell you how to address pairs of languages where you don't have a lot of data even in Europe, based empty. Well, previously it used to be. You don't have a lot of data used, statistical, phrase-based, empty. You have a lot of data used in your basement because you have a lot of training data to have a good model. So here, for the phrase base, you can have different possibilities. You have a word that means another word, and then you can have a phrase that can also be translated to another phrase. So rather than having a world based statistic, statistical, empty. The idea is that what if you can model immersion translation even with context? That is the idea of the free space smt. And here the idea is that instead of splitting the sentence into words. you actually just split it into phrases. and then you will be able to actually compute the probability of the target phrase given the previous source first, st and then you could add some distortion. This distortion is important, because. depending on the language you're trying to translate to the word order might be different. So you can have a group of phrases that will. That's that is supposed to be normally at the beginning, and then this is kind of shifted to the end. So you need this kind of distortion parameter to be able to address this. So whether you are using the statistical Nt or you are using the neuro based empty. you will need to do what is called machine translation decoding. Because when you're trying to decode you're trying to produce the translation of a sentence of a sentence, 1 1 word at a time in the case of an Lstm and a group of words at the same time. In the case of a transformer architecture, you will still need a decoding algorithm to actually search for the best possible translation that you will give back to the user. So there are some parameters. For example, if you use the beam search. it's very common that people will specify a beam size of either 5 or 10, and usually with the with higher being beam size, you'll be able to get like a better translation result. And the idea is that, for for example, the deal search the idea is that you have, like maybe 5 candidate translation. And you need to decide which is the best right. And how how do you do this? So we have a couple of algorithms, such algorithms like sh greedy hill climbing beam search so for the Greedy Hill climbing. So the idea is that you actually want a single translation from different candidates. But this actually start by creating a 1 complete candidate translation. When you translate word for what the question is, can you get what is the best candidate translation for this word out of the different, many possibilities. So you have this word in German. So and then this sentence is being translated to English. What the English translation it's not very grammatical, as you can see, because you're translating word by word you have this week is the grain which at home. But so the question is, how do you make it? More grammatical? And that's where you have the E climbing. and for the hill climbing, the idea would be is that, can you change the translation of a word to a phrase? And then you can now combine them. Basically, you're trying to formulate a more grammatical sentence. and when you combine the translation of 2 words into a phrase, you can split up the translation of a phrase into 2 subphrases, and rearrange the part of your translation. and instead of having a very strange grouping of sentence because, after translating by word, you can then have a better translation. and the the same thing. I mean, what we discussed in the Ibm model is still true. Basically, you evaluated by computing the probability of E, a probability of FA. Given E, where your translation depends on how you are able to align from the source to the target ports. Do you have questions before we move to the neural machine translation. So for the neuro-based machine translation. neural network, one of the the most appreciated successes of neural networks is application to machine translation. because machine translation is a very, very difficult problem. Why? Because you need to translate from one language to the other. and these languages can have different preferences. Yes, actually sorry. May I ask a question about hill planning? Yes, so I understood. The greedy step is to take the change that produced the maximum probability. Yes, but I don't understand. How changes are applied. Are they applied systematically in a certain order. or is it random until you stop improving. Yeah, you can have something like that. Basically you try. It's like a search agreement. Right? So basically, you. You try this, and then if this is not a proper match, then you replace it with another one. There's a better match. and so it's you're keeping track of everything you've already tried. Yeah, it's like a heuristics, is a heuristic way of actually getting the the best translation. Yeah. okay. So for the neural machine translation this often leads to the current state of the art model. But also this requires a large amount of data to be able to train immersion translation previously for any single pair of language. You need at least a million parallel sentences between. The initial language you want to translate from to the target one. and which you agree with me that this is not available for many languages of the world. So one very successful approach is to use recurring neural networks. which by now I think you're familiar with it. On we use the recurring neural network in an encoder, decoder manner. Where you have the encoder, you try. You have the memory for every input that you can have. So you can have input, a BC. And then you have the Rnn States. you use this state and memory information to compute what would be the outputs that will be sent to another state, and then you can compute. So every Rnn state can actually output something. But you can design it in such a way that it doesn't output anything at the except at the last stage. So in the earlier lecture we gave different forms of Rna, that is available because it's very, very flexible. So you can use. You can use Rnn for different tasks. You can use it for a for a topic classification where you just need a single output, so it does not need to output values at every single state. And then you can have another task like language model that you need to output of every State, because you need to produce what will be the next word. But for machine translation. What you need to do is that you need to understand the entire sentence right? So for you to understand the entire sentence. So you do need to output at every state of the Rna. So at the last layer. which is C, you can now produce. you cannot produce what is called like a context. Vector this context vector produce kind of summarize all the information in the of the sentence as a. Vector so once you have this information, you can pass it to the decoder side. And when you cannot actually generate a translation of the sentence. is this clear? A little bit cool? Same vector passed to every cell in the decoder? Right. the encoder, when once the context vector is encoded by the encoder. Yes, it passed to every cell in the decoder. It's the same. Vector no, it's passed to the next cell in a decoder. and then once you produce the next the outputs add the next cell, which is W. They need to also pass it to the next one. Will it modify it? Yes, of course you modify as you go. Yeah, that's the way it works. But I'm going to show you another approach where you don't. You need to send every outspot. And this is where we'll talk about the attention mechanism. But for now, all you have to assume is that you are only interested in a single vector, that summarizes everything in in the source text. and that single vector is now partially decoder for decoding. But now you can reason about some issues with this approach. And what can get wrong? And we'll talk about a modification of this, we where actually an attention is introduced. So basically, you encode every single sentence. And then you have this context, vector, Z, and then you pass it to the decoder side, where you actually decode it. One word at a time. Okay, there are some tricks to improve where you can reverse the order of input sentence. Or you can use a bi-directional r and n. to have a better encoding of this single vector. and then you now pass it to the decoder side. Also, another one is that you can train an assembly of translation models and decode by averaging their output probabilities. because you can have all these outputs and then then use one of those algorithms that we talked about to actually determine what will be the translation. But what is the problem with this? The problem is, if you go from one Rnn state to the other. As you move across the memory stage, you tend to forget one or 2 things right. And once you forget some some of this information at the decoder side. the problem is that the decoder doesn't know what is the most important part of the sentence that needs to translate. There are some sentences that if you don't get the say the right word or the anchor word correct correctly. it doesn't make any sense again, like. So we had this thing a project that we're doing recently. And then the translation was an apple. A day will keep you away from a doctor or something like this. Right? What is the most important word there? Right. It's maybe apple and a dog. So if your mother doesn't know what to pay attention to so we tried it for a language West Africa, and then it gave banana so the translation was correct. But the only thing that was missing is I changed the word apple to banana. So I'm actually just this gives you a very useless sentence, because it's not a banana idea that would. That will keep you away from the doctor. So the question is, how can the model learn to actually focus on the most important one, which is apple right? And and you can sometimes you also have a sentence where you can use a pronoun. and then you use it to refer to another word. and when you will be referring to it when you refer, when you use a pronoun for a noun? The model already completely forgot about the word you are trying to refer to. So the question is that is there a way, we can actually model this? And the answer is, Yes, we can model this. And the short answer is that from from this encoder decoder instead of just kind of throwing away the outputs at every encoder memory cell. Because now we keep just one output. You need you. You need to keep the output of each memory cell. And then you use this to compute what is called the attention. So and this is why you have this interesting diagram. The actual issues. You? Oh. sorry. I didn't know I was blocking. So that actually shows you. So that means you need to gather because the X is your input. You need to gather the output at every. at every memory cell. And here they are using a bi-directional. Rnn. For encoder. so that the encoding is stronger. Right? And then the outputs you use this output at every so every Rnn memory cell to actually compute the attention so you can think about attention as trying to compute what is the dot product between 2 vectors. and then it's a measure of similarity. And then, if you multiply them together. You know what is the most important right where you do like a dot products. And this is the way it works. So this attention will now be used to multiply the final output you got. and then what will happen is that because you run the attention over a soft Max is going to give higher weights to some important information and give lower weights to information that not very relevant. and by this once it gets to the decoder side. The decoder knows what is the most important information to focus on. I hope you get the idea. It's I mean, of course, you can read the paper for the more details. so you can see attention as something as a salt retriever. So in in a retriever system, you have, you want to query. a knowledge base. And then you need a key actually to access the information, and then you add the value. So attention can be seen as a soft version of a retriever system from a memory which has a key. They used to unlock the value. So you have the query which is a representation of what we are looking for. So in a Ir system. you're looking for a document. and then you have a query, and then you want to check the similarity between the query and a document. Right? So here it's very similar. You have a representation of what you're looking for as a query, and then you have the key, which is a representation of an entry in memory. and then you have the value, which is the information that is being stored on general, what we use to formulate. These are just actually random mattresses that we give different rules and one random matches we serve as the query matrix and another mattress will serve as a key mattress. Another one will serve as a value mattress, and then by construction, because of the way the attention has been computed, you assign different rules to the different mattresses to actually give you to model what is called the attention. All right. So you can visualize the attention. If you have the sentence. This is one of the most famous example that you see in attention. And this is from the newspaper, and here you have, what is, what is the model supposed to focus on? So the agreement on the European economic area was signed in August, 1992, what information are important and what information is not very important. Right? So, and if you look at the matches, the visualization of the of attention. you will see that The agreement seems to be very important. and then you have European seems to be very important, and then you see that even though it was swapped in the French sentence. You see that the model was able to pay attention to this, even though there's some reordering happening. the model is able to pay attention to it. And the model sees that August, 1992 is also very important. And you see that you see that it's very bright. And you have another example here. And by this people started thinking about, oh. that means we can interpret our neural network models by computer attention. But that's a lie. That's a lie. You cannot. It's not attention. Does it give you a good interpretation of what is happening in the neural networks? But this actually gives you an idea of what the model is trying to do. So this visualization of attention gives an idea that the motor is paying attention to the most important part of the sentence. Yes, sorry. You might repeat what you said about whether or not attention tells us what's happening in a network. Yeah. So I'm I'm saying it's not an interpretability tool. So if you just analyze the attention on what. so you can have an idea of what's going on. But it doesn't give you a complete picture of what's going on in the neural network. But it's a black box, right? But at a certain step it shows you what the model's paying. Yeah, there are some times that if you analyze the weights. you actually have an idea of what's going on. And then sometimes that's just completely random. does it depend on the model or or it's random within a model. So every model has some structure going on. So, for example, if you take a birth model and you analyze the the lower layers, it has been shown that, just analyzing the lower layers, you can see that it's capturing some synthetic information taste like part of speech. Right? But there's sometimes, if you analyze some other parts of the layer, you don't see anything going on. So that's why I'm saying that you cannot use it as a complete interpretability tool. Sometimes it helps to give you some additional information. Sometimes it doesn't help. Yeah. But in the case of Martian translation, actually. I'm sure if you visualize for some other sentence, you may not be able to see a very plain trick. Okay? And with his attention getting popular. we we now have what's called a transformer architecture. And the transformer architecture is very simple. Let's do away with recurrence. Let's do away with what is everything. We still fix the encoder decoder part. But let's do away with recurring network, and let's just use attention. And let's see what we have. And it seems to work. basically, we're still using attention since 2,000. And we're still using transformer architecture since 2017, and the title of the paper, which you know attention is all you need. and I remember there are many papers that says, this is all you need. This is all you need. But yeah. yeah. But at least we see that, for now it still seems that attention is all we need. And the interesting thing with the attention, architecture, or the transform architecture is that it has been applied to different modality, to vision. It seems to work to. Biology seems to work. So I think it's really good. Although we have been trying to replace it. So far no one has been very successful. Maybe you can try to replace it in that alright. So the idea is to actually replace Rnas because Rnas has a lot of problems one of the bigger issues of Rnas are things like vanishing gradients, right? And exploding gradients. But there's a way you can address that which we already discussed in previous class. But there's a problem. Because if you have a really really long context. Rnas, which they struggle because, you know, you're passing information from one cell to the other, another cell, and at some point it's the the length. The context length is just too long, and it has forgotten a lot of the information. So it still has some issues in modeling long context dependencies. So another problem with recurrence. because you are passing information one time, one step at a time is also very difficult to train. and it's difficult to paralyze what attention you can paralyze everything in training. and then you can allow flow of information from one world to the other, because everything is like you're computing just mattress multiplication. So they are very cool ways of paralyzing matches multiplication. And then it's like a computing those multiplication trying to understand how with vector, is similar to the order. So all this can be easily paralyzed, parallelizable compared to Rnns. So for for the attention in transformers, you have the sentence. and then you have an embedding. and the goal is to compute the next layer of water representation at Layer L. And what we are trying to produce is that given X and the embedding of of the words so given the word and the embedding of the word so. and by now you should know how to compute the embedding of the word, even if you don't learn it yourself. A lot like using what's back and all that. You, the motor, can also learn it by itself. So you have a sentence, and every you have a sentence, and every word in the sentence also has its own embedding. And then the the next thing is, how do we compute Z. So the idea is that we can actually just do mattress multiplication. using the structure of attention and actually learn what is Z. So we want to learn a distribution over words to decide how important each word is in order to compute the representation of the next layer. It might be that when you compute the next the next word representation is just going to tell you the same word is important, but sometimes it's going to tell you that another word is more important than the word. I'm so imagine that you have a representation of w. 1 w. 2 CW. Head, and then you want to know which of these is more important to w. 1, and then you do the multiplication across the embedding. and sometimes it tells you, for w. 1 w. One is the most important. Then sometimes it tells you, W. 8 is the most important right? And then you want to learn this. And the way we learn this is using the query, key and value. Ps. so in the query you you want to use use of this word as a query. basically because we want to compute the representation at the next layer. So we always need a query for the key. We want to use this vector to decide how important the world is to another world as part of the attention computation. And for the value. This is the value that is being stored. The vector stores. This value associated with the key. And this is how you compute a value. So each view is associated with its own. Vector but actually in practice. What we do is just we just randomly initialize. actually different vectors. And then we just do the matches. Multiplication based on on the attention computation. Yes. so is this self attention. Yeah, this is same attention. Right? You because you save attention is that you're multiplying it by itself. Right? Yeah. The queries and the keys are those words from the same sentence, yeah, the query. the key values are for the sub centers. So that means giving a sentence. Okay, so imagine that you represent an entire sentence with a vector. right? That is your vector then, you now need to initialize different information. You have like a vector representation of query, vector representation of keys, vector representation of of values for the same. For the same sentence. And then you multiply them together. So in practice, query, key and values are just randomly initialized vectors. Right? Initially, they don't mean anything. So until you train the model, they don't mean they don't capture any information. So basically what the embedding do capture information because the embedding is connected to the word. But these other values queries, keys, and values. They don't. They don't mean anything at the minute. At the beginning they're just randomly initialized. But once you have the structure of computing the attention which maybe I can pull up the formula. So I want to recommend reading the Illustrated Transformer. Now, when it 1st came on, nobody understood it. But this tutorial was very, very helpful. and it's still relevant to today of like, more than 6 years ago this was prepared. The and the guy actually had a very good visualization of what they're trying to do. But in practice is that you have a good representation for the, for the, for every word. and then you have a representation for what it means for the query keys and values, and then you just multiply them together. So for this example. we want to know that if you have thinking machines. what is the most important thing? Is it thinking or machines? This is what we want to compute. and it's safe attention, because you are doing attention to the same sentence. Right? You want to know which part of this sentence. or which word in this sentence is actually the most important. And here, when you compute, you multiply the query with the key and then you have this value. and then you do that for for machines. You do that for thinking you do that for machines. and then you now normalize it. and then you apply softmax. This model is telling you. This calculation is telling you that thinking is attending to itself. It believes that thinking is more important, right? So this word, when attending to itself. It believes that the same word is more important. But there are some cases where actually he believes that this, that itself, is not that important. Do you understand? So, for example, if you have a pronoun like it, it might mean that. Okay, although you are trying to attend to to me. But I have another word that was referring to me. That is more important. And you are able to get this just doing this dot product computation. And in this case you. You have the value. You multiply the softmax with the value. and then you take the sum you sum. I know you have. Yes, how important is the quality of the embedding of the word to the results of it. It's very important. Yeah, because you need to be able to encode the the word very well, and it's either from a separate model, like word to back, or the model like starts with one hot encoding and learns its own. Yeah. But what we do normally is just to learn everything. So everything is lined is the initial input, one hot encoded, then everything. Yeah, it's 1 encoding. So everything, including embedding the queries. keys and values are just lined. So there are 3 parameters that you have today. hey? Do you have other questions? Yes, wouldn't it be faster? Instead of starting with one hard encoding, we start with the embedding of a different model. Yeah, initially, it might be faster. But people find if you just learn everything, it's it's better because, the 1st application of this is machine translation. And like I told you, machine translation is trained on very large amounts of parallel sentences. for example, like a million sentences. So you have enough information. In your training data to actually learn a good representation. So there's no need to, you know. Try to improve another thing. Yeah. so and then you have what is called multi-head attention. So you have self attention, you have multi-head attention. So the idea of multi-head attention is that you? The way you have done it for a single sentence. Now do this n amount of times right? Maybe like is a parameter. So you can do this 8 times instead of one time. and then you just have everything is just like master's multiplication. And and then the model figure out what is the most important. So it's like, you are having multiple ways of viewing the data. Right? So you, you have one way, if you do a single self attention, and if you do it 8 amount of times, then you have 8 different ways of viewing the data and then you can now concatenate all these results together and pass it to the next leg. And this is the idea of multi multi-year edition. Yes. Are the weights shared between each head? Or are they different? They're different. So you're learning 8 different key value queries. Yes, what about the embeddings? Are they the shared? The embeddings are shared. Yeah. yes. Okay. good. So now we have transformers in Mmt, so initially, Google translate was based on Rnn end. And then and they've moved to transform our architecture. And they achieve very good performance. So basically, they didn't change everything. The I try to just change the transformer encoder. And then so this is transformer. This is what you'll see. So you have the encoder parts. and then you have the decoder part. So different models. They use different parts. For example, the batch model only use the encoder parts. They don't use the decoder part. The gpt model. Only use the Gp. The decoder part. They never use the encoder badge. because you see that every model, whether you're using encoder only, or the decoder. Only you always have the embedding. You have to learn the embedding, so you can just ignore the encoder path if it's not necessary. So for bird's model. He wants to understand what is in a sentence, to be able to make a prediction, to focus on the encoder path. Gbt wants to make a prediction and generate text. They focus on a decoder part T. 5. Model tries to use both. It wants to have a good encoding and also do a good prediction, so they use words. encoder, decode architecture. So for machine translation, you need a good encoder to encode the entire sentence, so that you can pass this information to the decoder side to translate. So we need a good encoder. So Google translate, they did away to just do away with the they added an encoder, and then they have a better encoder, which is a transformer encoder. But they still retain the Rnn. Based decoder, who has an idea why they retained the Rnn. Piece decoder. Do you have an institution? I mean, now they probably just use transformer everything. But then why they do this? Do you have an idea? But it comes from the line that it's less expensive. Let's say. why is it very expensive? I I know you can say, but why is it very expensive? So this computation is N squared. right? Because you're doing multiplication. So it's really really expensive at training, you can paralyze things and just wait on different gpus. And it's gonna trade. But at the prediction time you still need to compute all these attentions, right? So it's really really expensive. But Rnn is very cheap. You just need to predict one world at a time. So what they did initially, just to yeah, I mean decode Rnn is cheap for decoding, so they kind of just use the transformer encoder opposed to the decoder to save cost. Right so. But now I think we are making a lot of improvements in terms of developing more powerful gpus. And now I guess. When Transformer 1st came. This was really a big problem, because it's very expensive at decoding. And there's a lot of research actually focusing on how to improve the efficiency of at the decoding time. How to minimize this N squared to something. Maybe n, times log n, for example, to actually reduce are this expensive inference. So there isn't very simple. Because Rnn will feed one token and predict the next token untransformable. We're just feeding the entire context in order so far to predict the next token. So. And this brings us to another architecture that's really really famous. So it's like we had this transformer era we have. and then we have the bet era that's kind of rain for like 4 years. And Chatgpt came, and then everybody forgot about that. And the idea is we can use the same architecture of the transformer. We're now focusing on the encoder only side which is on the left. And then. and the way this model has been trained is also a language model. But instead of predicting the next token, what you actually predict is a Max token. So you have. This is a world Maxed, in this sentence, right? And then you have to train a model to actually predict what is the Max token. and it will also train on what is called next sentence prediction. Does this sentence follow this sentence, and the way it was trained is very simple. Imagine a Wikipedia text. and then you have an entire paragraph. You have a sentence by sentence, and then, if 2 sentence follow each other, you'll see a kind of continuation. It's very easy to predict if this sentence will follow each other. So if someone is talking about a sentence in economics, and there's another sentence in out. Maybe it's not very likely that you follow each other. And then with this task we are able to know. to predict if the sentence follow each other, and combining this Max language model with next sentence prediction, you can actually create a very powerful encoder that can be used for many natural language, understanding tasks from sentiment, classification to topic, classification to any classification task based color. and it was strained on a large amount of data. Then it used to be large, which is like 800 million watts. which is called books, covers and Wikipedia and it's up to 3 340 million parameters. and apart from this, they also train a multilingual model on, I think, over 104 languages. which we also have a multilingual birth model. But people have made different arguments that you don't actually need this next sentence prediction. So the Roberta model, which never got accepted, actually just did A did not use this nest sentence prediction, and they were still able to have similar performance. But they also increase the amount of data they train. So that means if you train on more data you don't need. You don't need this second task of next sentence prediction. Yes, I'm sorry. Why didn't Roberta get accepted? Because, sometimes reviewing is a very noisy process. So this is stochastic. Sometimes you have a bad review, and then the fact that's rejected it. It helps okay? And then. but with the multilingual version got accepted. So we also have, like a Gpt. 3, which, you probably know by now, is from open AI. And what they did was to just scale the amount of data I told you about Roberta scaling the amount of data, and they had better results. And they can even do away with one of the pre-training tasks, which is the next sentence, prediction for Jupiter 3. They actually train on more more data, 500 1 billion Watts this was published in Europe's, and it has like 175 billion parameters. It's really a big model. And at that point it was too big to be released. But nowadays we also have, like more than a 75 billion models that have been released. It's just very difficult to to run them. And now they put it behind. Apis, yeah. So in terms of the successes the idea is that if you train on on what we call a self supervised pre-training. self-supervised training means that just using an unlabeled text, you construct a supervised tax to train your model on unlabeled text, and you have what is called self-supervised training, and the task for the for the birds model is just predict the missing token, which is a Max token. And if you're able to do this, you can create a very powerful encoder that can be fine-tuned on many downstream tasks. part of speech, tagging anyhow, sentiment, classification, and so on. So, and also Gt. 3 came up with another idea which is really cool, which is, you can do what is called 0 shot learning. Basically, you can just predict. try to. So the model has been trained on just simple next sentence prediction. But if you prompt the model. Now you can try to get a you can try to solve in natural language, understanding task by just prompting a generation model. So the idea would be if you want to know what would be the sentiment of a text. you can say, this movie is great a name you. You can tell the language model that the sentiment of this will be positive, and then you give it another example. Then language model just with that single example which we will call one shot. The language model will be able to solve the task of sentiment classification. You give it an example, then it has an idea idea that oh, the movie is great means positive. So if you give it another example. this movie is just a waste of time. They knows that this is a negative sentiment. So the interesting thing is that almost all Nlp tasks, I would say, oh, I don't see any tasks that cannot be converted. To just prompting the language model to give you what will be the performance to give you the result. You can do this both for 0 shot and few shots. with a few examples, and the model will be able to answer him. So other interesting architectures that I think it would be nice to mention would be like architecture like T. 5, model. And T. 5 is quite interesting because they use both the encoder, part of the transformer and the decoder side, and it's very similar to the bets model, because they also use what is called span corruption. You corrupt some part of the sentence, and then the model kind of reconstruct it. It's kind of like a denoising task. So you kind of apply a noise function to your text to mark something out. and then the model is supposed to reconstruct the entire sentence. So, however, for the T. 5, they are actually not reconstructing the entire sentence. What they are doing is actually just providing what is missing in the input so you have this example in the original paper. Thank you. X. Me. To your party. Why weak? And then it also retained the sentinel tokens. but actually predicting what was missing. and also retaining the another sentiment token like the Y, and predicting what was missing. And then they have a final token. So this was how T. 5 was created. So strain on the C 4 corpus. and it has, like, over 34 billion tokens. and they scale it up from a very small model to even 11 billion parameter model. And the idea is you can cast any task as a text generation task. Right? So you have a translation task translates English to German, and then you send it to the model it can translate. Then you can give it another sentence. Like an analyze sentence. You have 2 sentences, and it can give you what would be it can give you the rating. Okay, this one is I think similarity tasks. So you want to know if sentence one and sentence 2 actually similar. And then you can have a grammatical check task which is a caller data set. And it's gonna tell you if this is acceptable or not, and you can have another generation task like summarization. And it's gonna give you so every task. And it will give you it will generate what will be the outputs. But one issue is that it's sometimes it's tricky for a classification task. Those intoxication tasks. You can model it to just give you for example, 0 1, 2, 3, 4. If you have 4 classes. Well, now, you force the model to always generate what would be the prediction? Right? So if you have a topic classification like, categorize this into business news sports, news. entertainment, news, or political news right? Sometimes it can generate different things. It can generate political or politics. Government related news. So and then you have to be able to Co. To convert what has been predicted back to the class you are interested in. But this is very easy to address. So we also have a multilingual version of this that was trained on, I think, one on one language and a lot of data, 6.3 trillion tokens. And, for example, you can use this for the machine translation task. because it's a multilingual model. So it can do what is called crosslingual transfer. Basically, you have a language in English, and then it will be able to translate it to German. Some, the pre trained model can already do that, but it's just that. It's not very good, but if you fine tune it explicitly to do this, it will give you actually better result. Actually, this was very interesting to people working on multilingual nlp, because now you can do this for different languages by starting with the multilingual model. So we have other text to text models that will be interesting to mention, but model, which is probably the topic of your reading assignment, which is similar, objective as the birds model, but with an encoder, decoder, transformer architecture. And we also have the multilingual variant of those models. We have the bat model with 25 languages, and we have another bat model with 50 languages. And now you can check the 2 architecture and see what is the difference? Actually, there's no difference in these 2 architecture. The only difference is sorry. There's no difference in the pre-training task they are doing like a Max language model. Well, the difference is that bass is in an encoder, decoder architecture. and they also try to do different things. Interesting things with the noise function to see what part of the sentence should they, Max? Out? Should they, Max out just one word or a phrase, and then the model reconstruct everything. Okay. And now we also have another branch an another idea. models like embat or T 5 has been trained on only on label text. which is no supervised learning. Right? This is just on on label text. But you can also train on massively amounts of data. So you can train on if you have a lot of data on so many languages. This can also be trained right? And in this case you are not training on just not you are not training on monolingual text. But you're training on parallel text. So M. 2, m. 100 was created. I I forgot the citation, but now they were created by I think Meta, and discover, like over 100 languages which they have trained on parallel examples in all these 100 languages. Unlike the Mt. 5, which is just based on which is not based on parallel data, basically. And then you have another B 200, which actually, they scale it from 100 languages. Now to 200 languages, and this seems to still be the state of the arts for machine translation since the last 2 years. and they also release an evaluation data set. That also is very popular, which is called a Flores 200, that covers like 200 languages. And this picture shows you an example of the different language groups or regions that are covered. in this model. So lastly, I wanted to share one interesting finding, which is even if this pre trained model has not been trained on a language. there's a way you can actually just quickly adapt it to that new language. So if you have a few 1,000 high quality translation compost for a new language, even though it's not trained during self-supervised pre-training or large machine translation per training, you can still quickly adapt it to a new language. So this is an example of a language rule. In, I think East Africa, where so hilly has been covered in the pre-training blue has not been covered. But if you take, like an original pre-trained model, like empty 100 for Swahili. I think it already achieved 20.1 blue score. and for I mean that's from English to Swahili and Swahili to English achieve 25.2. But you cannot use this model to generate for a language called Luo, because it's not covered valuable training. But if you fine tune it on high quality compost you can actually just have. you can use it as a translation model, right? So, which is very cheap. Suppose you have, like 5,000 parallel sentences, and then you can fine tune this pre-trained model. whether the one that has been massively portraying on unlabeled data or massively portraying on parallel data, and then you can see some huge improvement in performance, whether you use Mt. 5 or you use Mbat. And oftentimes, if you use a machine translation model you often can get slightly better performance than if you use or models that have not seen parallel sentences. So I think it was almost kind of wrap up there. The last thing I wanted to talk about is current trends now for Nlp, including for machine translation, is that you can just prompt a language. Model this guy, you can come to language model. So. And now sometimes there are interesting things you can also do is that you can provide instruction, you can provide a demonstration which kind of like an example. and then you can now provide a query, and then it will give you the results, and I believe all of you are already aware of this. Another thing that can boost the performance is what is called chain of thought prompting. And it's like you ask the model to kind of explain the thought process and trying to predict the output. So for the standard prompting you. You give it like an example. And this is the answer. But there's a way. You can also prompt it where you provide an example, and then you provide an explanation on how it arrived at the answer, and the explanation doesn't have to be much. You can provide just one example. the already give the model an idea on how to solve the problem. or you can provide more than one example. For example, this Mgsm was I think. the the few. Short example is like 8 example. and by this you can have a better performance of the model right? Because the model also will try to replicate this chain of thought idea to be able to produce better results. So this is like teaching the language model to reason before it produces. Yeah, so okay, you have questions. Okay, alright, and on Monday we we're gonna have another class on multilingual nlp or crosslingual. Nlp, and yeah. good. Thank you.
