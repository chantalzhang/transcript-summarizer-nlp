Jackie Cheung, Professor: It's the same. So I don't know if it's because they want to like you're saying in the last week 10 response for exactly like, is it? 10 spots wrong? And then there's like interesting that certainly doesn't. Hi, everybody! Can you hear me? Is the microphone working? Oh, okay, great Hi, so welcome to natural language processing. This is Comp. 5, 50. Hopefully, we're all in the right place. I am Jackie Chung. and this is David Adelani, we're going to introduce ourselves. Do you prefer the captions, or are they distracting? They're a bit distracting. I think there's a way to keep them, but not show them on screen. I'm okay. I'll hide them for now. alright. So we're going to talk all about what this course is about the topics and so forth. But 1st I thought we thought we would introduce ourselves. So I am Jackie Chung. I am an associate professor in the School of computer science and I'm also affiliated with Mila. And I've been at Mcgill for a while now since 2,015. And in my lab, we do research on obviously topics in natural language processing. and that includes natural language, generation, automatic summarization. And then some terms that you may or may not know so computational semantics and computational pragmatics as well as applications. But by the end of today's lectures, at least, you should be able to know what these terms mean, like semantics and pragmatics. If you don't already. Okay, so that's me. And then I'll turn it over to David. You so alright. So David. So I need my patients at Zailand and a postdoc at Ucr, London. and I resume as a assistant professor at the School of Computer Science this fall. So I work on topics around multilingual Nlp machine translation, representation Lane. Speech processing. So I'll take few slides before adding over to Jackie. So preliminaries. So of course, you already know your instructors. If you're here today, you I'm I'm very sure you know the time. And also you know the building. So we have the following office hours. if you want to see Jackie. So you should come on Monday from 2 to 3 30 pm. On a Wednesday. You can also come see me. so we have the following, Tas Shira Zilling, Guaraff Shi, jun. I'm sorry if I didn't pronounce it. Very well, I'm seeing. So for the evaluation we have 2 programming assignments, which will be 20%. And we have. We're going to have like 4 reading assignments, 5% each and then at the in the middle of the semester, we are going to have a midterm which will be 25% of the average. And then we have a group project which could be a group of up to 3. Okay, so the textbook will be using. So this guy, the slice may deviate some sometimes a bit. But it's more consistent with Juraski test book on speech and language processing, and it's publicly available. I'm not Charlie Drasky. Released an updated version this August. So you can check the new chapters that have been added any questions so far. Okay, yeah. There's no fine for this one. There's no one, is there? Is there no violence? No. So it should be. The the Gopaja will be the final exam exactly. hey? So on the assignments we have like 2 programming assignments. You hadn't the assignment online through my courses. How many people know my courses? Yeah, okay. so I'm programming needs to be done in python. We have some prerequisite for this call. So I believe many people here should be familiar with python programming. And also we are gonna have, like 4 reading assignments, 5% each. which will be very similar to some of the things you have been taught in class. or a little bit of advanced materials that we are not able to cover. Okay for the midterm this will cover 25%. So it's 25% is. So you should take it very serious. So this gives us the opportunity to really test what you have been taught in class. and it covers, like. a quarter of your grid. So the tentative time, I think, is finer. Yeah. Oh, yeah. November 6, th 2024. And of course, we're going to give you more details as we approach it. So for the final project. so basically, you have to. This is an Nlp class. So you need to work on a language data. So it's very important. There's language data. So if you work on all the images except as multimodal. I think this is also acceptable. We. We need to summarize the and review relevant papers, report on the experiment and must be done in a team of 3. So I think there will be a template. which we've been nothing. Yeah. So you are supposed to come up with a new idea it could be. It doesn't have to be completely November. Well, but it should be at least a project that could extend an existing model to solve a particular task and work on a relevant topic of interest. If you're in this class, you already know some of the odd topics in Nlp. So a topic like that and consult a list of suggested projects to be posted. You can pick from this, or you can also deviate from this a bit. So for your project steps. paper project proposal, and then you have the progress update and the final submission. We are going to announce the due date. Later. So for the general policies. if you are late to submit your assignment, you have a grace period of 24 h, which I think is very generous. If it's more than 24 h. yeah. we will decide on what to do of plagiarism. I'm sure you all know the university rules on this. Please don't do it. Don't copy your colleague. So also, in terms of the language policy. We're in Quebec, so I think you could use English or French to write your essay are susceptible. hey? Yes, this is a very important one. I know you have have the question, can I use Chat Gpt, or any other language model for the exercise? I think you can use it if it assist you. but it's not that you copy the entire code for your project from chat to pick. or you write an essay. A complete paragraph using Chan Gpt. If it helps you to understand course materials to summarize it. Fine to search for information or brainstorm, I guess. Or maybe you're right or make your essay better. I think that should be fine. But please do acknowledge it if you use this technology. So it's not okay to use this as a primary means to complete your task. If you only generate all your solutions. Reassignment from this we're going to detect. and there's a procedure on how to go about this if you are caught. Also don't use it to generate your entire report. So we are going to be using different platforms. Some of you know my courses. I think we'll also be using the ad platform because he has some advantages. And then we'll be releasing most of the details on these platforms. But for the assignment and project submissions will be on my courses. The meet I'm also will be there. I will release your grades in my courses. Okay, administrative things are over. Do you have questions? Yes, that's important. Yeah. So we're using zoom today as like a backup lecture recording, although it's supposed to be automatically done. So in general, we we're we do not encourage online attendance. There will be lecture recordings. and if you're sick, then then you can ask us for the zoom link. Because, yeah, if you're not doing well, don't come in and spread it to everyone else right? But like otherwise the, we won't make the zoom link just publicly available. Okay, thank you. alright. So let's go into the lecture. I'll I'll just take few slides before handing it over to Jackie. So we want to make a distinction between computational linguistics and natural language processing. If you attended the Acl Conference, or you have read about what happened there. People still do not agree on what is competition, linguist linguistics? And what is Nlp. so even for big profs and well established researchers, thirsty arguments on what is this? So if you have confused, you're not in yeah. You're in the same boat with many other people. But we try to make a distinction here. and I'm happy. My colleague here has more background in computational linguistics than me. I'm more from the Nlp side. So you can also see some balance in the lecture. Okay, before that. We want to clarify what this lecture is not about. We all know about Llms Chat gpt, and so on. Cloudy and Gemini. whatever is your favorite? And they have impressive performance. Even for difficult tasks. So when I was doing my Phd question, answering is a very, very difficult task. is it? Now? I'm not sure. So that many labs just focus focus on this also, we have code generation that is not very prominent in social engineering, essay. Writing. Summarization. Also, back in the days was a very big topic and very difficult task. and our our older models, or which I will call inferior models. I'll find the task of summarization. I've gotten our lstn very difficult. So. And also we have commercial uses use cases from customer service, personal assistant and healthcare, and also for entertainment. And sometimes it's even used in setting disputes. I don't think it's the best way to set a dispute. So this man, Tom Scott. in 2020. Then you can tell me if this prediction is right or wrong, artificial number processing remains 10 years away. 2020, when the charge pity come out who remembers 22? Yeah. So our artificial Lambo process still remains 10 years away. just as it has for the last few decades. Okay, in 2023. What did he say that this new technology, the thing that was going to change everything was starting to actually change everything. So now he's not sure. 3 years after what happened during this time? that you probably know. Okay, so how do language model works? So language model is a very, very simple concept that has been a statistical Nlp for many, many years. and we even have statistical language models before the neural language models where you can pick a small chunk of text. whether a storybook and you can train a language model to just predict what is the next work. So the idea is, predict the next one. It's as simple as sentence complexion. And the key insight is that you learn correlations between words. In context. given, a context predicts the next one. Another definition could be, what is the probability of the entire text, and also be referred to as the language model? So if I say, Mary had a little lamp. Accident? Very. And up. What's a good prediction? So you have a probability distribution that kind of. And then you can rank the prediction and say, Okay, which world is more likely to be predicted? Is it a law or accident? I think in this case you have 2 good choices. So for many page generation capacity. For you to predict the next word you have to decide using different algorithms to say, what next word to predict? Should I predict, Lamb. or should I predict accidents? And you can have more than 5 different words. That would be a good completion but these days it's more, of course. It's not as simple as just. Training language model on storybook. So you could take the entire text in the on the web. So, for example, all the text of Wikipedia, English, Wikipedia, and training language model. And nowadays we even use a bigger, bigger purpose for that. Okay, what this course is not about. This is not about large language models. How did we get to land number models? This is not a cost about it. What was the progress of the field of Nlp. Why did people try methods? Sorry. I think, what this course is about other. How did we get to land? Longer models dominating the end of your research. We started from somewhere, right? So we started from statistical language model and statistical Nlp before moving into the new architectures. So what was the progress of the field of Nlp. Why do people try the methods that they did? What are some of the common tasks and paradigms involving natural language, including very basic classification tasks and linear models will be covered in this course. How do we evaluate and analyze Nlp systems? What are the metrics that are used. including very basic metrics like accuracy. f, 1 score that some of you may be already familiar with so more application specific ones like Broscore or Krf. Hey, our! The properties are much no longer reflected in an open search. What this cost is not about the latest techniques in large longer. I'm sure there will be other courses. In my view, that covers some of this deep learning machine learning as a primary focus. This is not the course. Of course we all use machine learning in some ways or the other. and of course. for your projects, you will probably also use machine learning. But this is not a focus of the cost languages everywhere. Covering different applications and has a lot of importance both for a daily task and in different industries. So and I want to also emphasize that language is not all only working on English language. One of my research area is multilingual nlp, we, because we have so many languages in the world. The word language can be translated or can be expressed in different languages in the world. is French. German. in Chinese, and so on. But we also have so many languages in the world. I try to. The largest proportion of the languages in the world are in Asia, followed by Africa, and then the Pacific. and then in the Americans and European languages which. Actually dominates most of the data on the web. just like 286. So I would encourage you to also have projects that extends more. That is more than one language that works on more than one language. or that scales to other languages. So many of these languages are underrepresented, so we can only say only 7% of the languages are can be categorized at institutional that are used for many, many different things, including education and business, and so on. close to half of the world, languages are actually endangered. So Nlp should be beyond English. Okay? What's language? It can be referred to as a form of communication. arbitrary part pairing diff between form and meaning. So. But we can also have other kind of language. That's are not vocal. Of course, language is primarily vocal, but some are not vocal, like the sign languages. And in language, one thing that is important is how it is highly expressive and productive. You are producing some form of data. whether voice or text, or so on. And it is nearly universal. We want to say, this course, is focus on natural language, but we also have other forms of languages like programming languages. which is what you use to code vocalization by your favorite animal, like a dog or cat or written English. But of course, we would be focusing on the natural language, not program languages. Okay, so computational linguistics to distinguish between Cl and Nlp. Modeling natural language with computational models and techniques. That's a simple definition of computational linguistics. The domains of natural language can be acoustic signal for names, words, signal syntax, semantics. including speech. Alright. So it involves natural language, understanding, or comprehension or natural language generation, I think. Early on. There's a big distinction about whether you're working on Nlu task or Nlg. Task. But nowadays we tend to be the model. That kind of work for both both Nlp and Nl. Oh, sorry Nlu and Nlg. And this distinction is quite interesting that in both core Nlp. Task and in speech processing they also have this distinction for Nlu and Nlg task. So the goal of Cl is language technology applications and also scientifically understanding how language works. It could be as how people develop a language it could even be! How a baby leg. a new language! Oh. try to understand the language so it could be how language works in general. So the methodology and techniques, of course, is very standard, which might also be similar to Nlp. We often gather data which we call language resources. We do evaluation. Also a lot of statistical methods and machine learning. and sometimes we also have rule based methods. So Nlp, on the other hand. although they are used inter interchangeably. But there's a slight difference. So I don't be focuses on more practical technologies. here we have. Cl, can we can refer you to as more science or Nlp is more engineering because you just want to make things work for a large number of people or for practical use cases. So in terms of natural language, understanding. so natural language, understanding. You want to know, really understand how language works? How they can be usable for machines and humans for different different tasks. But in generation you really want to produce text but also to be traditionally referred to as semantic formalism to text. But more recently, it's more about you're giving a tax, and you want to generate something it doesn't have to be your provided input and text. So the original input might be images. But the output will always be text. So image captioning can be also referred to as text generation task, even though the infuses image. And Esr, the input is is voice or audio. And the output is text. Okay? So most work in Nlp is an Nlu for even very basic tasks, like tagging battle speed, tagging name, density, recognition. They can be considered as primarily nlu task. is this what I'm supposed to stop? Or, yeah, okay, I think a few more stuff. So one example of understand is call a taxi to take me to the airport in 30 min in any of your task. For think about a dialogue system. You want to know what is the intent of the user. So it could be. What? Okay? If I ask you, what is the intent of the user here, call a taxi to take me to the airport in 30 min. and then you can have different categorization of the intent of user. It could be current or can be an intent it could be transport to the airport can be an intent it would be. Wake me up like alarm can be an intent wake up time, and so on. So and another one is, what is the weather forecast for tomorrow? This is talking about another domain, but has another index, but generation is different. Where you have to produce a text, it could be in the same language, or it could be in another language like, Here you are. You have a translation of I like natural language processing in German. Which is, I I hope I still remember my German very well. Automatic sprouts. Okay. okay. alright. So computational linguistics. Besides new language technologies, there are other reasons to study Co and Nlp as well. Here we have the nature of language. And probably all of you know, chomsky. yeah, okay. maybe 1st language accusation you want to wear cum skip. Propose a universal grammar for language on are here. So we are trying to understand what is the innate knowledge. That's what init knowledge most children already have in order to learn their mother tongue. And this is an interesting research which you can study even from a baby acquiring a new language. So the nature of language language processing some sentences are supposed to be gammatical, correct. but are difficult to process. And here, in incl you want to have a formal mathematical model to account for this. This is one example the rat escaped, and then you have the rats. The cats caught escaped. You know that you all agree that the second sentence is not very grammatically correct, and then you have the rats, the cat, the dog chased, caught escape. So in language processing, you know, some sentences are supposed to be what grammatically correct. But if they're not grammatical, grammatically correct, it's very difficult for us to process. And also there is a way you can detect this by having a structure for each sentence. So if you have a structure for a sentence, you can know what is broken in the sentence. because every language has a structure some languages the verb has to be in the middle, some languages the verb is in the front. and and so on and so forth. yeah. so the last slide here that will cover the mathematical foundations of sale, mathematical properties of formal system and algorithm. And the question is, can they be efficiently learned from data or efficiently recovered from a sentence? And also we do have some complexity analysis. Why design designing the algorithm. So I think I will stop here and you can pass. Thank you. Alright. Thank you, David. Yeah. So I I gather it's cool these days to like, hold the microphone in your hand. Right? So I'll continue to do that rather than to like work. Okay? Yeah. So so yeah. So by now, then, you have at least some basic sense of what the terms computational linguistics and natural language processing mean. And and we've also seen some of the goals of the field. Now I'll talk a little bit more about what we're analyzing. So, for example, in terms of the the type of language. And then a little bit after that, we're going to talk about the different ways in which you can break down the phenomenon of language and potentially look at these phenomena separately, or to look at the interactions between these phenomena. Okay, so in this course, we're primarily going to be focusing on text. But, as David pointed out earlier, this is different from the the perception, at least by traditionally in linguistics, that the the spoken form is the primary form of language. However, Texas is just so convenient. And and and it's it's like it's discrete. It's it's really simple to to have come up with a scheme to store text, and then we can grab all of it from online, and we don't have to do any like messy signal processing to convert acoustic or speech signals into like a into a symbolic form. And so we're primarily going to be working with text. And that's also how the field is, it's the the vast majority of the work is on textual data. However, you do lose some information by working with text rather than working with speech data. So in some sense it's an idealization of the spoken language. For example, one thing you might lose is like tone of voice, and it might be more difficult to convey, like your attitude. Right? So how many people have had a misunderstanding because, somebody you were texting with didn't understand. You were being sarcastic. Right? Yeah, probably many people, right? And in fact, that's there are theories that like this kind of loss is why emojis and like Smileys and things became be became popular, became used right in terms of informal a textual communication, and that we're losing something which is actually important for communication, and so we add it back in so in nlp, then, a lot of the older work has been on clean, formal standard English especially English, because a lot of the researchers back then were from, say, the Us. And Canada, Uk, and so forth. But that's very limited, and the things have changed quite a bit in the past few years, and that now we work on many other languages as well as David also does. And also now we work on a more diverse set of text, not just formal text, but also these informal texts that I just talked about like text messages and online communications. And so on. So working with speech is also a research area, very important. It's just that there are additional issues there or or different issues there that we won't cover in this course. So much so, for example, speech has disfluencies. So you know, like broken lines of thought and trains of thought, and so forth, and that that you resume so so then there are different techniques to try to deal with, that there are also non standard language, which is extremely interesting. but it means that you might. There might be more diversity, and you might have to account for that, and people also speak with different accents. and some of the tasks there in speech processing include automatic speech, recognition, and text to speech generation among others. So ASR, so automatic speech recognition is we. We've also had tremendous progress in that by using deep learning techniques there sometimes is a perception that is a solved problem. But it's by no means a solved problem. Just try to do use, do ASR in like a noisy environment where there are multiple streams of speech, and you'll quickly realize that. No, there's still a lot of work to be done. Okay, so now I'm going to talk about how linguists, at least, have traditionally divided up language, the phenomenon of language, into these different subdomains. I think this is really important and useful to know. So. even though large language models work by just doing next word prediction. At least, that's the core. It's not the only thing. you might still be interested in analyzing these large language models in terms of, okay, so you train this model. How do you characterize which aspects of language it seems to be handling well in terms of its behavior, and maybe which aspects in which it's not handling so well. And so then we can turn to these divisions from linguistics. So in linguistics. I think some linguists really believe that these divisions are cognitively, somehow real in our minds, in our model of language. Here we are, maybe more neutral. We don't. At least I won't take any stand about whether these divisions are actually there, but at least they're still useful to it's still useful to have these divisions for us to be able to more easily talk about different phenomena in language. So we have phonetics, phonology, morphology, syntax, semantics, pramatics, discourse. That's a lot. And and they're roughly organized by the smallest units going on to bigger and bigger chunks of units. So let's take a look at this. Okay? So the 1st division here is phonetics. So phonetics is the sub area of linguistics that studies speech sounds the the speech sounds that make up language, or, in the case of sign languages, is like the way which she articulates with with with parts of her body to to produce sign language outputs. And so there's issues to do with articulation and transmission and perception that you might be interested in and so. so more concretely. Here's an example. Suppose we have the word peach. You can have a relatively low, level transcription of the speech sounds that make up the word peach. There's a p sound. P. There's a puff of air, there's like a long I sound e, and then there is this something called an African, which is like. sure, this, this sound, and and very specifically what goes on in your mouth when you say the word peach, right? So, for the p sound that involves closing of your lips. Okay, your lips have to come together. You build up some pressure in your oral cavity, and then you release it with a puff of air or aspiration. So you can have these very detailed studies of speech, sounds. and vowels can also similarly be described in terms of formants, so the e can be represented in terms of the formants that that make up the the vowel sound. And so vowels are more continuous. There's usually this a 2D diagram of like, and you can talk about vowel heights, and whether the vowels near the front of your mouth in the back of or the back of your mouth, and so forth. Okay, so this is phonetics. Phonology is something that's often confused with phonetics. But it's a different area of study that studies the rules regarding how sounds are patterned in a language and how they're organized with each other. So now we come to the interactive part of this lecture. So here we have 3 related, similarly sounding words, peach and speech and beach. Okay? So so in the spelling, at least peach and speech are both spelled with P. So I'm guessing that for the vast majority of you you think of these as like the same right? They're just peas right? Whereas Beach is different because it's a b sound. Right? Well, actually, all 3 of these sounds are pronounced differently. Okay? And there's a phonological rule that tells that in in, if you speak a a. a native version of English, then there's a phonological phonological rule that changes the way that the P sounds in different environments. So specifically, the pea and peach is supposed to be aspirated. There's this puff of air, the pea and speech is unaspirated. There's no puff of air after that. and the B and beach is just a different sound, and there your vocal cords will vibrate. So what I'd like you to do is to say peach and speech, but put like, put your hand in front of your mouth so you can feel the puff of air. Okay, you say, Peach, you can feel it. Okay. If you say speech, you should not be able to feel it again. If you're not a native speaker of English. This might not work so sorry about that, and if you say beach there should be no puff of air, but your vocal cords should vibrate a little bit earlier. Okay, so then, you can like study this, there's a rule here. Okay? So whenever P appears after an S. Its pronunciation changes in English. And this is like, this is a rule in English. So it's not universal. Yeah. So then we can say that the P. And teach and speech are the same phoneme. So they're the same abstract. They belong to the same abstract category. But they're actually phonetically distinct from each other. Here's another fun. trivia fact. If you speak French, if you're a native French speaker, your B's and your P's are actually different from the B's and P's in English. In a very subtle way. This is one of the ways you can tell. So whether someone grew up speaking a language in that in French, the the B's. They. You start by rating your vocal cords a tiny little bit earlier than in the be than for the B's in English. you can. You can check. You can check that at home and ask your friends or something, whereas, and also with the peas in French, the puff. The little puff I talked about tends to be much weaker or non-existent compared to a native English speaker. So even sounds that appear to be the same sounds across languages there, there might be slight phonetic differences between them. Okay? So then, we had phonetics and phonology. So next we have morphology, which is about the study of word formation and meaning. So here's a really long word in English anti disestablishmentarianism, and you can break that down into these different chunks which we can call morphemes. There's anti. And then this, and then establishment Aryanism. You can even analyze it, starting from the the stem, and you keep adding prefixes and suffixes, and each time you change some property of the word, so you have established, which is a verb. Then you have a establishment which is, I guess, turns it into some noun related to something that's being established. Then this establishmentarian, I guess, is somebody who is pro establishment, perhaps. and then you have Establishmentarianism, which is the the philosophy or belief related to us, being establishmentarian. And then, if you added this, then it changes the changes. This so that you're, I guess I don't even know anymore. And then you add an anti. And then you're against that. Okay, so there's but but the point here is that there's some regular structure. Okay, there's some correspondence between how you word pieces are built up. And how they affect the meanings of words. Another. I guess linguistic trivia point is that English actually has a very simple, relatively simple system of word formation. So English morphology is actually relatively simple compared to many other languages. and so actually, we won't spend that much more time on morphology in this course. I guess we still have an English bias, but in other languages, for example, like Finnish or Czech or Swahili, many other languages have very complex morphologies, where one word has many, many parts, a lot of them involve conjugations to express meanings that in English we would use a separate word or an adverb. For okay, then is syntax. So syntax is the study of the structure of language. and how you put words together to form sentences, and how those sentences end up being end up, being interpreted as being a string of that language so grammatical or not a string of that language so ungrammatical. So in English, we have strict rules about the orderings of words, and how you're allowed to put them together to form a sentence. So if you say IA woman saw a park Indie, that is not an English sentence, that's not a grammatical sentence of English. You have to arrange them in a particular way, like I saw a woman in the park. and so these are technical terms, grammatical or ungrammatical. So ungrammatical means. It's not a string in the language and grammatical means is so. Why is that so? You can study it. So, for example. traditionally, then, people have drawn tree structures to represent relations between different parts of a sentence and then put those together. And if you're able to put them together according to some grammar, then it's a valid sentence. Another interesting thing is that these phenomena and language there are aspects of them that cause. The same sentence to have multiple interpretations. Okay, so here is a comic. So if you I just ran my 1st Marathon in 6 years. The other person says, 6 years. That's pretty slow dude. And then the 1st person says, No, no, I mean I don't run them very often. And then the second one says, Yeah, that's fairly obvious. Okay, so okay, where does the ambiguity here come from? Yeah. Do do you see it in 6 years. Yeah, that's right. So you can analyze this syntactically. So if you draw some tree structure, the ambiguity comes from the fact that this prepositional phrase, in 6 years it it might modify different things. It might modify that the marathon is the 1st one in 6 years, so it attaches to the noun, which is the incorrect interpretation, or that you, you ran it in 6 years so that it was. It's the last time you ran, it was 6 years ago. Okay, so that was syntax. You have some. It's like some structural analysis of sentences. Then, next, we have semantics, which is the study of the meaning of language. And so what does that mean? So it can mean various different things. And there are sub areas of semantics as well, so a big one that we've done, we do a lot of work, or at least we used to do a lot of work on and still do in computational linguistics is something called lexical semantics, which is the study of word meaning. So just like a sentence, can have multiple interpretations and be ambiguous. Individual words themselves can also have multiple meanings, multiple interpretations. Typically, they're called multiple senses. Here we have 2 senses of the word bank. It can either be the bank of a river or the bank, as in the financial institution. so a very popular task, at least last decade in Nlp has been something called word sense, disambiguation. So you see all of these words. how do you figure out which sense of the word is the intended sense in that particular context. And people have devised algorithms for that and come with data sets and evaluations. So that's lexical semantics. Here's another one. This is this one is less obvious. So I wonder if you can figure this puzzle out. So I claim that there are at least 2 interpretations of the sentence. Ross wants to marry a Swedish woman. and and somebody figure out, give me like one of the interpretations. Yes, the specific woman that wants Ross wants to marry is Swedish and Ross Ross wants to marry any Swedish woman. That's right. So yeah. So here one interpretation, which is hopefully the more common one is that there's a specific woman that Ross wants to marry who happens to be a Swedish woman. And then the other interpretation is that Ross really has a thing for Swedish women and really wants to marry one of them. Okay? So you can see there's a there's a difference, right? And actually, it's caused by different senses or interpretations of this article of a okay. So don't dismiss short common words, because those are sometimes the hardest to model in terms of their their meanings. Okay? And then next, another area which I I like to study a lot in my lab is pragmatics is the study of meaning of language in context, and in particular, there's often a difference between the literal meaning which has been put in the semantics bucket versus the meaning in context, which is often put in the Pragmatics context. Okay? So we come to another web comic. So this one is Pinocchio was cursed so that his nose would grow whenever he lied. However, this curse, apparently this curse is, it was cast by a a wizard or witch which does semantics. Okay, so it's a it's a semantic curse. It's not a pragmatic curse. Okay? So the curse is only in terms of literal meaning. Okay, so this curse is pretty pretty easily managed. If you just phrase every sentence so broadly that they can't be false. So Pinocchio, did you bully that boy at school? And Pinocchio says there are people who would dispute that perspective. or Pinocchio, did you egg my door? If you're accusing me of such a deed, I have nothing more to say. or even some. This one is more innocuous, sir. Would you like any dessert dessert would be delicious? So you can see that in terms of the literal meaning none of these statements actually evaluate to false. But it's clear that there is a interpretation that any reasonable speaker of the language would give. And so and this one is actually pretty common, right? This one is actually something that you one might say right, the dessert would be delicious. You're not actually saying yes. in fact, you could say something like, Oh, if someone asks you, would you like any dessert? You can say, Oh, dessert would be delicious, but not today for me. Thanks. Right? Okay? So that that illustrates that there's a difference between the literal meaning and the meaning in context and intended meaning in context. There are other issues in pragmatics which are quite interesting. This one is called Dixis, which is that the interpretation of expressions can depend on extra linguistic context. So one prominent example is pronouns. If I say I think cilantro tastes great, then the I there is pointing to me as the speaker. But if you say it, if you say I think the cilantro taste great, then I then points to to you. This is pretty obvious, right? But if you you have to come up with formal analyses of these things. Right? So how? In fact, it's quite likely that Hgbt and large language models don't understand these distinctions. I would claim, although you can dispute me. Okay on that. So the entity referred to, the antecedent depends on who is saying the sentence. okay, and then discourse. So this discourse is the study of the structure of larger spans of language beyond individual clauses or sentences. and that that's also an area of study. So, for example. So so why do we say multiple things? Because usually because these multiple things have some relation to each other and they help us complete some communicative goal. At the end. Right so and so, then that means that quite naturally, most things, most utterances, and most sentences that you say one after the other, or you put in text one after the other, there'll be some logical relation between them. And so that should be reflected by the text that she generates as well, or that that attribute generates, or in terms of how they interpret it. So if you say something like, I am angry at her. She lost my cell phone. This is coherent because it's clear that there are. There's a relationship between these 2 sentences. If you say I am angry at her. The rabbit jumped and ate 2 carrots. There's no relation between them, and so it doesn't make any sense, and this is not coherent. And, in fact, there's a whole branch of comedy like absurdist comedy, is based on putting things that are unrelated next to each other for a comedic effect. Right? So it's sometimes you can violate some of these common properties in order to make some point, but by default we assume that there should be some logical structure within our discourse. And you can analyze that as well. So yeah, in this course, we're going to cover many of these different areas, maybe 2 different levels of depth. But we're going to try our best to cover some of the basic distinctions and also computational models and algorithms related to all of these areas. And and so then that means that we're gonna talk about the technological perspective. But also the maybe it's linguistic or scientific perspective of all of these phenomena. So in modern Nlp, there's often we often think about modern Nlp as some combination of some pre specified knowledge, and also machine learning from data. although, to be clear. Not all Nlp methods necessarily have to be machine learning based or machine learning methods, but still some of the the ways in which we investigate these issues might include thinking about. say, how you collect the data, how you work with the data itself. And what form and representation the data should have as well as the the algorithms and methods and models that you construct and train on top of this data. And if you do both of these things. Very well, this is how you can get very high levels of performance and potentially useful or harmful technology that results from this. And so what we do in Nlp is, we do a lot of work on problem specification, on thinking about machine learning algorithms, on eliciting annotations from humans, especially when we have a particular kind of representation that we want to parse it to, for example. and also through, say, linguistic knowledge, through linguistic analyses of languages, and we combine that with learning from data, from websites, news articles, whatever's on the Internet. also structured kinds of data. So a lot of data is in some non human readable form. Right? So there are a lot of, say, meteorological observation stations. And they collect a bunch of data there. And that's also useful data. And we might want to interface with that data or work with that data in some way. And that's also part of Nlp, is that interfacing? So yeah. So some of the major paradigms that have been pursued in Nlp, which are not necessarily mutually exclusive include rule based systems. and traditionally, they have been hand engineered systems. For knowledge about language, like in French, translates to happy, maybe some of the time most of the time machine learning like, giving examples and letting some statistical method figure out the associations between those examples. And so that's what we're gonna talk about first.st Actually, so classification. So is this email, spam or not spam. It's like one of the poor parts of our modern existence. Right? But that's a that's that's an Nlp problem, email, spam classification or another one is sequence models, which is where you're not just making one single decision like, is this one email is this email, spam or non spam, which is one decision. But you might have to make a sequence of decisions like about every word that you have in your document, for example, and there are many other paradigms, and deep learning is also one of the algorithms that fit within this top high level topic of machine learning. also knowledge, representation. So how do you formally encode information in a way that's useful or interpretable in some way? So there might be formal structures that you can posit for this and you can use some kind of logic. So in the middle of the course, we'll talk about using logic to represent the the meaning of sentences. And and we can talk about pros and cons at that point as well, or with, say, neural networks. A a lot of the activations within, like a neural network model. Those are continuous valued numbers. Right? So that's how do you? How does that correspond or map to meaning, that's pretty interesting. Think about. okay? So then, yeah, so the high level, then structure of the course will be that there will be some correspondence between the Nlp topic with the linguistic layer, with some of the techniques. So, for example, we'll start with text classification where we mostly are concerned at the granularity of individual words. And we're going to look at machine learning techniques for it within the classification paradigm. Then the next topic will be something like language modeling and part of speech tagging. And then the linguistic layer is also words, but potentially also syntactic structures. And the machine learning techniques involved would be models that are designed specifically to handle sequence data. And then we might look at syntactic parsing. And then where the linguistic layer is the syntactic structure, and then the techniques involved would involve structure, prediction algorithms and also various dynamic programming algorithms that can help us more efficiently explore all possible parses of the sentence. And so on and so forth. So we'll have semantics and reference resolution, which is at the meeting level. And then and then we're going to look at techniques for representation of meaning, like logic and also machine learning techniques like semi-supervised learning and neural models. And you're closer to the end of the course. Then we'll talk about machine translation, summarization and potentially other applications potentially other issues with evaluation and ethics of Nlp. So you have any preferences. Or if you have any topics you would like to explore that are more in in that area that may be more applied or advanced. Then please email us. And we can see if we can incorporate that, because the last few lectures of the year they change every year, and we're able to tailor it. If you're if you send us suggestions or recommendations. Okay, any questions? Okay. cool. Alright. Yes. I think I already talked about this. Okay? So then, in terms of the learning outcomes and course objectives, then the goal of Comp. 5, 50 is to help you understand the broad topics and applications and common common terminology of the field is to help prepare you for either research or employment, including internships in computational linguistics or Nlp. So we're going to learn some basic linguistics along the way, we're going to learn some basic algorithms. And hopefully, you'll be able to read an Nlp paper and understand the challenges that are in computational linguistics and analy. and at the end, hopefully, you can answer questions like, is it easier or hard to do something? I think this is a really important skill to have, especially because anything related to AI these days is so prominently in the news, and everyone's talking about it like my parents know what Chat Gp is okay they asked me about it. And then, some people are like, Oh, it's AI gonna take over the world. And that's a controversial topic I might give you one view of it. You can feel free to agree or disagree, but at least by the end of the course you'll be able, you should be able to better argue that for for your point of view, like yes, I think AI will take over the world because or no, it's overhyped right? And and why is that? Okay? So yeah, any last questions comments. okay, so if not, then, yeah, we'll end up early today. So the next lecture is next Wednesday, because next Monday is Labor Day. So hope you have a good week.