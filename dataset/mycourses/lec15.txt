Jackie Cheung, Professor: Hello. welcome back. Okay? So today, we're going to talk about compositional semantics. But first, st it'd be good, I think, to just quickly summarize and recap what we did last class. And even before that, just some reminders. So the midterm is next week. If you have a midterm conflict, please send me email, like by the end of today. So we need to figure out if we need to book a room for midterm for the makeup midterm and so forth. So please do that as soon as you can. So yeah, the midterm is, I think, next Wednesday, if I remember right. So then, there's no class during that time. But I'll I'll have extra office hours in my office during class time. Also, I assume that you've already started working on reading assignment 2, and then programming assignment 2 is out as well. Okay, so here are the algorithms from last class. So last class, remember, we were still talking about lexical semantics. And we discussed Lusk's algorithm and Yourowski's algorithm. So do you remember what tasks those algorithms were for? Yeah. be like. thank you. Not quite. It's not not about the meaning of 2 words in a sentence. Yeah, this is yes, that's right. It's word sense, disambiguation. Okay, so it's for a word that might have multiple senses and figuring out which one it is. It is intended in that context. And then we after that we talked about hearse patterns and also bootstrapping with these patterns. And that's more for figuring out. If now, this is, these are the ones that are about like our 2 words, do they fit some lexical semantic relation? Okay, so we did a lot last class. But there were a few topics that we didn't cover in enough depth. So I just wanna quickly go through that a little bit more. So one important concept from the second half of last class is this idea of a term context matrix where you can build up a representation of of the words in a corpus by looking at their co-occurrences. So this is the idea of distribution of semantics. Right? You're using the distribution of words in order to infer something about the meanings of those words. And so here, in the term context, matrix, you have the target words which are the words whose meanings you're trying to model. And you have context words which are the words that appear in the context of those target words in the Corpus. And you can define context in any way you'd like. Maybe it's about appearing within 5 words of each other or something, and then you can report the counts. So one thing that I didn't really spend time on which we should talk about is like, do you just record the counts? Or is there something better you do because it turns out that there is something better. And I should also say so. Why do we care about this? So I mentioned that like, this is kind of the beginning. This is like the 1st kind of iteration of this distributional approach to modeling meaning, and which eventually led to large language models. But also, even now I would say, a lot of search engines are still based on word of co-occurrences. And so it's still based on these fundamental ideas. So it's important to learn that this for that reason, too. and in particular, with search engines, for example, you can take this approach where you model every document and every yeah, every documents, by looking all the words in that document, and then you can compare the similarity of documents with the query terms, as you might type into a search engine, and you can use something like cosine similarity, which, remember, is the, it's like the cosine of the angle between 2 vectors. Okay, so here's the thing I wanted to also talk about. So this has been a very important, influential technique, which is called point wise mutual information waiting. And the idea is that rather than use raw counts. You instead record your term context, matrix. some notion of how much more likely or less likely than chance. that that you observe. Those 2 words co-occurring compares to observing them separately and individually. So it's no longer a a natural number. It's no longer like accounts. Now, it's just some kind of like a score or or some kind of a a log ratio of ratio of likelihoods. Don't play integration. I'm sorry. Okay. So for example, here so well, here's the formula for point, wise and mutual information. So you have. Excuse me. You estimates the probability of both of those words occurring. and you divide that by estimates of the probabilities of each of the words occurring separately of each other so independently. so. So this looks a lot like some formulas you've probably seen for looking at independence right of random variables. Right? So if if it turns out that w. 1 and W. 2 are totally independent of each other. like the random variables. then the numerator and denominator here should be the same. That's the definition of 2 random variables that are independent of each other. So which that means that this ratio is one which means that the log of it is 0. So that's kind of like the chance probability. Okay, that's saying you have 2 words. If you see them in a corpus as much as you would expect by chance. then they're not really especially related to each other, or unrelated to each other. And so, then, that you should record a a score of 0, a Pmi value of 0 for that pair of words. If 2 words happen to occur with each other more commonly than you would expect by chance. that means the numerator here is greater than the denominator. So then this score would be this ratio would be greater than one, and your log, the log likelihood ratio would be greater than 0. On the other hand, if 2 words co-occur with each other, less likely than you would expect by chance. then you have a negative value. So this is the idea. So rather than occur recording the raw count in the term context matrix, it records a notion of association between the target word and the context word. So let's look at an example. Suppose the word be occurs a hundred 1,000 times in a corpus of a million tokens. and then it co-occurs with the word linguistics 300 times. and the word linguistics occurs 2,500 times in total. Well, then, you can compute all of these numbers. Okay, so you can compute V and linguistics. So because it co-occurs with linguistics 300 times you get this joint, probably. So that's 300 divided by? yeah. So yeah, by a million. Yeah, that's right, because it's the probability of both of those things happening. So it's 300 divided by a million for the though it's 0 point 1, because the is a very common word. So it's a hundred 1,000 divided by a million. and for linguistics is 2,500, divided by a million. And now you can plug these numbers in. So we have the point. Oh, oh, 3, divided by the product of point one and point 0 2 5. So this gives you some Pmi score there, which is slightly greater than 0. So that means that these 2 words co-occur slightly, more often than you would expect by chance. And that's great. Yes. we just ensure that somewhat normally. Yeah. So why are we dividing 300 by a million. It's because here it's kind of the probability of both events happening. So it's so so yeah, this is the approximation there. So it's like 300 divided by a million. Because you, it's you're looking at the the entire corpus, and there are a million tokens. And so. So yeah, I think that maybe it's not exactly the right space of events that you're considering. But I think it's it's close enough that like. you can just use that ratio as the probability. Yeah. yeah, does the base of the logarithm matter? Does the base of the logarithm matter? not really because if you choose a different base, that's like just multiplying all the numbers in your entire term context matrix by a constant. which doesn't matter if you're using cosine similarity. Okay? What else? Yes. Another thing that people often do is that they often discard negative values in the Pmi. Like, if you get a negative Pmi, often they discard it, and they just record a 0 instead. So this is called positive point wise mutual information. And so it's something you can try if you'd like to see if it helps. The reason for that is that the fact that 2 words occur Co. Occur less likely than you would expect by chance, is not very strong information of the anything of the meanings of those words. So, yeah, people often just discard them. Okay. yeah. So this is one tweak to just doing the raw counts. And if you use Pmi, and there's actually like, slightly, there's slightly different versions of this. But you can use Pmi, and you can get pretty good levels of performance if you implement some kind of ir system like some kind of basic ir system. Okay, another thing you can do is you can do singular value decomposition. So here in this nlp, course, we have limited time, so I won't really talk much about all the math behind it. But I assume that you have, or will see this in another course. Actually, how many people have seen singular value decomposition in some course. Yes, a lot of you, right? So it's a really famous technique. It has, like 3 or 4 different names, because there's slightly different versions of versions of it that have been discovered in like different contexts. But yes, so you can talk about singular value decomposition. You can also talk about principal component analysis. And as it's applied to information, retrieval sometimes called latent, semantic indexing, or something or latent semantic analysis. so in truncated Svd, the idea is that you want to creates a version of your term context matrix. which has lower rank in the linear algebra sense. And so the way they do that is, you can apply this Svd method. So there's a standard method to do that that lets you factorize the original term context matrix into the product of 3 matrices. And these are the dimensions of the matrices. So you have your original term context matrix, which has dimensions of the size of your vocabulary by the number of context words. So I'm indicating the size of your vocabulary. So number of target words here as the size of B, and I'm indicating the context word size, the number of context words as C, and then you can factorize it into the product of these 3 matrices. And here this Sigma K here is a special, because there it's a diagonal matrix that contains the singular values of this original matrix which you can think of as some kind of characteristic way of of of looking at the properties of that matrix. And it turns out that you can throw out some of the values of the in the singular yeah, in this, in, in this, in this matrix, and also throughout the corresponding rows and columns in the other matrices to come up with an approximation of the original term context matrix. And here, this is kind of a dimensionality reduction technique. You're basically taking the dimensions of the original term context matrix which explain the least amount of variance. And you're just getting rid of those dimensions and projecting your matrix to the remaining dimensions. Yeah, sorry. What is? M, yeah. So the idea here is that so suppose M is the original rank of the term context matrix. What you do is you pick a K which is much smaller than it. And that's how you can get an approximation. So you're getting in A, you're getting a smaller. Well, you're getting a matrix with lower rank than the original. And in practice doing this truncated Svd often improves performance, because you're removing some noise and preventing some overfitting of the model, because you can think about overfitting in this context, as like you observe, some counts co-occurrence counts which are just, random, and not predictive of the future, and so truncated. Svd. Is a method to help you get rid of some of those. You can also prove that this view of truncated Svd is optimal in a technical sense, in this very particular sense, which so suppose X was your original term context, matrix and xk, is your approximate term context, matrix of rank. K, you can prove that the the matrix you get by applying this as truncated. Svd procedure has the lowest amount of error. So you're preserving as much as the most information you can. Here we have. This. This is like a norm. This is an L 2 norm. So you're looking at, how much is the difference between your original matrix and the approximate matrix. And you can show that this is the best possible approximation among any matrix of this rank. Yeah. So this is just to say, this second point here is just to say that it corresponds to something else called principal component analysis. Maybe it'll be helpful to look at this graphical intuition of what truncating, truncated Svd does, or principal component analysis. So suppose you have just 2 context words. So your original term context, matrix has 2 dimensions. And then you plot all of your word vectors in this two-dimensional plane. What truncated Svd or Pca does is that it finds all of the vectors. It finds, like some axes that best explains the variance of the points with respect to each other, so the points here are the closest, as close as you can get to the axis, to the axes as you have. and then, when you are truncating, when you're removing some of the singular values and removing some of the dimensions, you're kind of squashing everything into just a lower dimensional space. So graphically, this is what's happening when you're doing this approximation and and this one dimensional approximation of the original 2 dimensional make. A space is this is the this is the squashing. This is the approximation that preserves the most information. Basically. So again, I have. I don't have time to do this topic justice. But there's a lot of math and linear algebra and and algorithms and people have come up ways to do this very quickly. So you should look into that if you're interested in this and this idea of Svd. And truncation and principal analysis, you find that all over the place. For in many applications like this block. okay? And finally, the other thing, of course, is that people also train neural models of word embeddings for lexical semantic for lexical semantics. And I mentioned last class that word 2 vec. Is a famous model that does this from 2013. So I thought I would just briefly show the neural network figures for word 2 vec. So the original model actually proposed 2 versions of word 2 vec. In the 1st version called the continuous bag of words model. You look at representations of. So so 1st you sample a particular position within your training corpus. and then you look at the words, say 2 words before and 2 words after, and you take the rep. The the embeddings associated with those words which are parameters of the model. Then you sum them up. and then you use that to predict the middle word that's missing. So that's how the that model is trained. And this, yeah. And this latent representation of what you predict. That's like the representation of the word. That's how you get a representation of the word so that's the continuous continuous bag of words model. the the more, maybe more popular one is the skip ground model, which is that you take the middle again. You sample a position in your training corpus. You take the middle word, and it's embedding. and you use that to predict one of the one or more of the words in the surrounding context. So these are by modern standards. These are relatively simple architectures and ideas. But but yeah, sometimes they still use word 2 vec, so word 2 vec, although like, maybe they're not popular anymore for English. Sometimes they're still used. If you have, like a highly specialized corpus where you don't have a lot of data to train a transformer model. Or if you're working with a language where you don't have enough data to train like Lstm or transformer model. Sometimes people still use word embeddings of this time. Yeah, does the skipgram have a lot fewer parameters? Or is the just the embedding of the word. Longer does the script skip ground have a lot fewer parameters? I would have to think about that. I don't think so, because, like, it's so because the parameters are shared and the sibo. So it's not like you have a parameter. You have a set of parameters for a word, 2 positions before, or something like that. It's it's still the same. It's like shared parameters. So you just have one set of. So suppose your representations are size 100 or whatever. Then you have 100 parameters for each word in your vocabulary in both cases, I think. okay. But yeah. So all this to say, this is kind of been. This has been how distributional semantics has evolved. So you start off with like some kind of count-based approach. And then people have used linear algebra to try to like reformulate that and do a little bit better. And then there'll also be neural approaches, and, as I also mentioned last class, it turns out that you can prove that the Skipgram model and the Svd based model. You can prove that they're equivalent to each other in some sense. Okay. any questions about my school sematics? Yes. With Sv, do we need to like? Is it good enough just to know, like the high level idea that we're essentially suppressing it, and only keeping the relevant information or the ones that don't. Well, only keeping the information that does affect variance. Yeah, I think that's the high level intuition. Yeah. like to to. Yeah. So we don't have time to like, get into any more depth. So for, say, for the midterm, for example, it's enough to know that it's about that. That's the high level intuition. And it does this by applying this matrix factorization algorithm. Okay? All right. So distribution of semantics and lexical semantics done. So. The next topic is the, I guess, the final major topic for the midterm. And it's about compositional semantics. So the thing with lexical semantics is, it's only about the meaning of words, or maybe small phrases. Right? So remember, what does Lexical refer to? Lexical refers to that? It's something to do with your lexicon, which is this idea that you have some abstract like list of like almost like a dictionary in your mind that lists out all of the words and phrases that you should know for knowing a language. But language doesn't just happen as individual, flexible items. Okay? Language happens in well spoken language happens in utterances. Written language happens in sentences and in even bigger chunks, which we'll see if we have time to talk about later. So in compositional semantics, what we like to do is we like to talk about, say the sentence level where sentences have meanings that you can derive by looking at the parts of the sentences. Okay, so we have to talk, then, about the principle of compositionality. So. Compositionality is exactly this idea that the meanings of sentences is not just some. It's not arbitrary, and you can derive the meanings of a sentence by looking at the meanings of the subparts of the sentence. So in particular, the meanings of all the phrases within it. Okay, so that and or or just or, more broadly speaking, the meaning of a phrase depends on the meanings of its parts. So if I say to you, you may disagree, but comp. 550 is a fantastically awesome class. then that sentence has a meaning. Again, this meaning could be true or false. but we understand that by looking at and breaking down like the meanings of like like. What does pop? 5, 50 mean? What does fantastically mean? What does class mean? And so forth? What does a mean? And it's how all of these words work together that you get the meaning of the whole sentence. So whereas lexical semantics might give you the meanings and behaviors of each of the individual words. We still need some procedure to build up the meaning of the entire sentence through composition. And so this is really important. This idea of compositionality is really important, because you could argue that this is what lets language be really, really flexible and useful, and you can use language to talk about new situations and so forth. Okay, so people make a big deal of compositionality for good reason. And so if you if we encounter a new situation where we discover, like previously extinct yellow dinosaurs that have been revived, or whatever I can talk about that, and maybe that has never happened before, but because you know the meanings of each of the words, you can combine them in new ways, and we can talk about new ideas and new events that happen. yeah. okay. So we should also talk a little bit about some other properties of the meaning representations of a sentence. and within this module what counts as a good meaning representation. If you think about it. In the last lecture or 2, when we were talking about lexical semantics. We kind of talked about 2 different views of it. We talked about the relationships of words to each other, right? The relationship of words to each other like with all these like somatic relations, like synony and autonomy, and and some autonomy, and so forth. And we also talked about relating the meanings of each word to the things in the world, right so to to the references, and also like to the sense of the the word. So that's 2, or maybe 3 different views of meaning. So what about at the sentence level? So at the sentence level. you can still talk about the meanings of the sentences with respect to each other. But what about the aspect of the meanings of sentences with respect to the world? Right? So so how do we think about that. So one way to do this is to really use some kind of ideas from logic. And just look at how they do things and then apply it to natural language. So, for example, to relates the linguistic expression to the world, one thing you can do is that you can assert that a proposition. So this is a term from logic is either true or false relative to the world. Here's another potentially new sentence. Pandas are purple and yellow. Right? So what is the meaning of pandas are purple and yellow. So to figure that out, you have to take the sense of that, whatever that means and evaluate it against a world. So in our current world, in in this particular universe, with this particular earth, with this particular kind of pandas. it evaluates to false right? I guess, unless you like, die a poor panda or something. But but the the point here is that you're able to do that evaluation? Okay? So the meaning of the sentence is, the is the thing that lets you perform that evaluation to check. If it's true or false. With respect to the current world. So that's 1 potential view of like. of sentence meaning another is to convey information about the world. So this is closely related. I don't think this is actually true thankfully. But suppose I say it will snow tomorrow. Then what that's saying is that it's conveying something about the world. and how the world is or will be right. So that's another way to connect the the language to the world. or it could be a query, as well like, What is the weather like in Montreal next week? Then it's a query. Maybe it's implemented as you. You turn this into some logical form, and then you can evaluate it against like a database that's connected to some meteorological service, and then you can return. You can retrieve the weather for Montreal next week and then give you an answer. So it's this kind of a sense of meaning that we're going to be talking about. It's about relating linguistic expressions to the world with these sample tasks of how we might want to do that. So we talked about compositionality so just very quickly. There here is also a very high profile violation of compositionality. So language is compositional, but it's also not perfectly compositional. So in particular idioms or expressions whose meanings cannot be predicted from their parts. Okay, so so, for example, you might, you may or may not know that kick. The bucket means to die. Okay, but that has that is not really derived. You can't derive that from kick the bucket. or if you see something, it's the last straw 99% of the time you're not actually talking about the last straw. Right? You're talking about. This is the final thing that really set someone over the edge, and they get really mad, or something like that. Right? Likewise, piece of cake is not always literally about the piece of cake it could just mean something is very easy hit. The sack means to sleep, and so forth. So these are clear violations of compositionality, because there's no regular like function. There's no function that you can apply here to derive the overall meaning from the meanings of each of the parts. Interestingly, though it's not like arbitrary like, there are still some commonalities in the meanings of like each of these parts, and the overall meaning. So, for example, the type of events is usually preserved like kick. The bucket is like a thing that happens. and then it's kinda over right like, which is, I guess, the same as like to die. So it it's and it would be very unusual if if there was an idiom where the event type is something like I don't know relaxing at home, and then it means to die, because then one of them is like a it's like a state of being like, whereas like die is like an event that just it's like a point in time. So usually there are still aspects of meaning that are shared between the idioms and the overall meaning. But it's still a composition is a violation of compositionality of root. And here's another more subtle violation of compositionality or arguable violation of compositionality. which is that things are often relative to each other and and not strictly about. The meetings internal to each of the parts. Okay, so so this is the idea of co-compositionality, which is that this is still like compositionality in the sense that the meaning of the whole is derived from the meaning of the parts. It's just that the function that you use to compose and get the overall meaning is complex and is dependent on the parts themselves. Okay? So I'd like you to like. Imagine in your mind what the meaning of red is like on its own right, like, just think about Red! What does Red mean to you. I'm guessing. For most people it might be like some very vivid kind of like red. Maybe the the RGB value of like 2 55 0 0 or something like that, right? But it's like that. Might be your prototypical red. But now think about red in the context of all of these words, like a red rose. What kind of red is that? It's not? It's probably may maybe that very visa red, or maybe it's something slightly darker and more purpley right? And red wine is red wine. Really the same kind of red red wine is very dark, right? That color I don't know what to call it. but is that it's not magenta violet? I don't know. Purp again, some kind of purpley red. or like red cheeks, right red cheeks. If you actually analyze the RGB value of someone who's blushing. Probably the the RGB value. Well, it depends on their skin tone, but it's almost certainly not red. If you just look at it in isolation. but you can still talk about somebody's cheeks being red, right? Or red hair. People who are said to have red hair are also, you know, it's not exactly red, either it might be there's variation, but it might be more orangey, or whatever, or brownish. So then the idea here is that the meaning of red actually is influenced by what you're composing it with. So it's not just that there is like a fixed, static red redness that gets added to the noun that you're composing it with. Instead, you have to. There, it's contextual. And and when you put words together, their their meanings like affect each other as well. Okay. no. So the tradition that we're going to discuss today and next class is to use logic to model sentence meaning. And this was a tradition that started in 1970 with Montague. And hence the this approach is called Montegovian cement. and it's the idea of using a logical formalism to represent the meaning of a sentence with a tight connection to syntax. So Montague says, there's no, in my opinion, no important theoretical difference between natural languages and artificial languages of magicians. Indeed, I consider it possible to comprehend the syntax and semantics of both kinds of languages, with a single natural and mathematically precise theory. So okay, these days, with like neural networks and large language models, you may or may not have thoughts about this. Okay, after reading this quote. but this is the assumption and the approach behind Montegovian semantics a dash we can use that that natural language can be made as precise as like logic, logical languages of our logic. and you can model the former with the latter. at least in at least the meetings. So there is an advantage of doing things this way. which is that you can then talk about natural language inference as applying logical rules of inference. So if you pick a particular logic as we will, then, whatever inference procedures are defined by that logic, you can also apply them to natural language after you've converted natural language to that logical form. So this is the idea. And I would also say that, like the other reason to pay attention to this stuff is that we can never get away from something symbolic and logical, at least not completely. because we still have lots of observations that are non linguistic again, like, say, weather data, right, or like other, in sources of information from natural processes that people gather and put in some database. and you have to access it through some database. Maybe you have to write some SQL. Query to access it, and all of that involves like manipulating logical forms and logical queries. So and so that means at some point. You you're you still need. You're still going to have to translate between that and natural language. If you want to have a natural language interface to all of that information. Okay, so then, what is inference? Inference is to make something explicit that was implicit before in language. Okay, so if you say something like, I want to visit the capital of Italy. and the capital of Italy is in Rome. Then you can make an inference, which is, I want to visit Rome. If you say all wugs are blorks and all blorks are cute, then you can conclude that all wugs are cute. and so these are cases of semantic inference. So maybe unlike in your previous logic classes. Now, it's like, in terms of natural language sentences. So this is like natural language inference. But we're gonna still turn this problem into perhaps a problem of logical inference by defining a procedure to convert the natural language sentences to some logical form. Okay, so so the logic we're going to pick is 1st order, logic or 1st order predicate calculus. So how many people have seen? 1st order logic before. Okay, that's surprisingly few. I was expecting most people to have seen it. So this means you should ask me questions. If things are confusing. because chances are, it'll be confusing for most of the rest of the class, too. Okay. so in 1st order, logic. 1st order, logic has these components, and so we need to define them, so that we can talk about translating natural language to 1st order logic. So first, st order logic can be defined as having a domain of discourse, which is a set of entities that we care about. So this is the side that is more like a semantic. It's about like, say, the students in the class or the topics that we study. or the classrooms and courses that might be one Mini scenario we want to model. So there's a domain of discourse of entities. then 1st order. Logic also has variables which are by convention denoted with lowercase letters, and they stand for potential elements of the domain of discourse. It has predicates which map elements of the domain of discourse to truth values. and you can have different valences which means different numbers of arguments. So, for example, I can define the predicate in course XY, which takes in 2 elements of the domain and returns. True, if X is a student that is in the course Y and false. Otherwise. yeah, should we think of a pedicut as kind of like an identity function in a way that as predicate and identity function, what do you mean by identity function like if it takes like or an example in the case, like, in course, Xy like it just checks the identity of X and returns like yes or no so predicates. So if it's a predicate that has a single argument, often it does give you like the category, it checks the category. So if you have, like a student X, it's usually in natural language, it's like checking whether X is a student or not. So in that sense. Yeah, it gives you some information about the properties of that out element. If you have something that takes in multiple arguments like this, then it's usually about some relation. So here, in course, is about a relationship between the student and the course. And the student being in, enrolled in the course. Yes. So by contrast. you also have functions. So functions, map elements to other elements of the discourse. And, for example, maybe you have an instructor of function that takes X and returns and other elements which corresponds to the instructor of the course. And so the difference between predicates and functions is what they return. Predicates give you a truth, value like true or false. In 1st order, logic functions give you, like a elements like other elements of the domain of discourse and a valence, 0 function would be a constant. So, for example, I can say, like, I can create a function called Montreal, which takes no arguments, and it just returns an an element of your domain of discourse which corresponds to the city of Montreal. The the idea of city, the city of Montreal. And then you have logical connectives. So please tell me you've seen these like. okay, good. So you've seen like not, and and or and implies and bi-directional entailment. And so forth. Okay? And also, okay, how about quantifiers? Have you have people seen quantifiers? People have seen quantifiers? Okay, so you have seen 1st order. or maybe most of you. Okay, so then there's an existential quantifier and the universal quantifier. So the so yeah, these are the basic elements of like 1st order predicate calculus. So it might be easier and more concrete if we look at more examples. So here's an example. So we might want to define some procedure that converts a sentence like the capital of Italy is Rome into some logical formulation. So here I have capital of which is a function. and then Italy is also a function. It's a constant and then equals. It's a is a predicate. So I'm taking. Here's another constant Rome. So this sentence and this formulation is asserting that there's a something called Rome. There's something called Italy. and then you can have a function to get the capital, from Italy, from the from Italy to some other entity, and the other entity is the entity denoted by Rome. Okay. So here Rome and Italy. Just think of them as like names. so they're just names. And then there's also this abstract entity which is like the entity of Rome and the abstract entity of Italy. and then the capital of converts that Italy entity into the Rome entity. Or here's another logical sentence that all wugs are blorks. We can convert it to this form, for all X log. X implies block X. And so here, wug and blog are these are predicates. Okay? So they take an element. X, and it checks. If it's a log, then it should be a port. And this X here is a variable. This X ranges over all of the elements of discourse. So any questions so far? Yes, I guess, for the second sentence are we essentially saying for every X, then applying the predicate of like, the one thing is also a floor. I guess I yes, basically. So you're for all X means that you're so. The way to think about this is usually you're modeling some situation in some situation you might have like 10 critters or whatever. And then, when you're doing for all x, you're like checking for all 10 critters and for all 10 critters. You're asking, is this a Wug? And that's what Wugex is doing. and if it is. then it should also be the case, that if you ask, is it also a blurb? It should be true. But if it's not a bug, then you don't care then. and then it can be a block or not. And and the implies arrow here. It's a it's a logical connective, right? So it takes in 2 arguments, 2, 2. Truth values, the one thing before and the thing after. And then you can draw a truth table for that so hopefully. Remember that right. So if the left hand side is true. then the right hand side has to be true. In order for the whole thing to evaluate to true. If the left hand side is false, the whole thing evaluates to true. So so these all have precise meanings. And you you might have to review logical connectives. Yeah. or Boolean connectives, if you prefer to to remember what they mean. And there was another question, yeah. the function of cat. So it's existing function. Or we create this function just because we want to analyze the simplest of the capital of Italian. Yeah, that's a great question. So the question is like, where does this function of capital out come from? So here is just an example. It's just, I'm just defining that this could take the form of a function to represents capital of. but in general there will be. There are conventions and procedures, so that every word in English you can associate it with some piece of logic which might involve defining functions or might involve defining predicates and so forth. And then we'll also define a procedure to combine those pieces of logic together in order to form the overall meaning of the sentence. Yes. Is there any order of the old? Yeah. Is there an order to it. So here it's yes. formally speaking. Yes, there, there is an order to which you apply things, so that this whole thing is within the scope of that universal quantifier. But if it's ever unclear, you can add parentheses to make clear the scope. Okay. so this part is really confusing. because it's not intuitive, and the 1st time you see it is confusing. But so the reason, remember that the reason we're using a logic ultimately is to relate language to the world. Okay? And so that means that to interpret a 1st order logic. you have to interpret it and evaluate it against some world in the end. Okay, so a particular instance of a 1st order. Logic consists of the predicates and function names and arity. as well as a set of sentences. In 1st order logic using those predicates and functions. and then an interpretation of it, or a model of it, is to apply it to an actual scenario to check the truth and false values of all of those logical sentences, or anything else that you might care about. Okay, so so, and and your interpretation involves specifying the domain of this course. and it also means specifying the functions of predicates. So how do the functions actually map entities to other entities? And how do the predicates actually map entities to true or false? Okay, so think about it this way, like the just, the logic and the sentences themselves, although that's what you spend most of your time like working out. That's just like general statements or constraints about like things that may or may not be true. Whereas an interpretation is where you actually interpret it in a particular scenario. to see if that if if those sentences are true or false. yes. here arity means valence. So the number of arguments. Okay, so I'm we should do this exercise together. Because I think it's this is, this can be difficult. Okay, so we're going to come up with a 1st order. Logic, characterization of the following students who study and do homework will get an a students who only do. One of them will get A, B and students who do 90 will get a C. So we should list the predicates and functions that are necessary, and make constants for the grades. and then we'll come up with an interpretation of this 1st order. Logic. where you and 2 of your friends are the elements in the domain of discourse, such that the above 1st order, logic formulas are true. Evaluate to true. Okay, so let's do this together. Alright. Yeah. before we get started, I'm going to close the door. Oh, yeah, sure, that'd be great. Thank you. Thanks. Okay. So let's let's do this. So students who study and do homework will get an a. So what do you think? What would be? So we need some quantifier. Okay? So because we're talking about students. So are we talking about? Yeah. So what kind of quantifier do you think we need? Yes, that's right. Yeah, because we're talking about some general property that should apply. think there's a way to have equations. Okay. okay, that's not helpful. Okay, so for all x for all students, how we do that is we write. okay, why did it create a new text box here? An idea. There we go. Okay. So for all x. if X is a student. then. okay, students who study and do homework. So okay, so they also have to study. Wait, wedge, yeah. And they'll get an A, how about we do sometimes it just. And to be pretty, to be more clear, we can add a parentheses. Okay. here. Okay, are we okay with this? So students who study and do homework. They're great as 8. Yeah. Do we need the.do we need? Which? Oh, you mean this dot? Yeah. I think technically, yes, but it's not a big deal. Yes. Do you think of wedge as like union? In a way, wedge? No wedge is closer to intersection. Yeah, wedges and end. Yeah. so this, the the wedge is just like Powerpoint. Speak. This is just the and symbol. Yes. is this? Okay? Yeah. So I can. I can add another one, too, to make it clear. I just want to make it clear is that the conjunction of these 3, which is in the conditions for the implication. Yeah. Yes. So the students study and do homework. Those are all credit between the grade up as a function. So then I'm wondering, can you just do alright rolex students? And so, yes. And the homework goes to just say, like, you need the greater touch. Right? Okay, that's a great question. So here we just have. We have like 3 predicates, and we have grade of X is equal to a why can't we just have 8 here. So the reason is because the types don't work out. Okay, so so this is a logical, this is a Boolean connective right? It takes 2 truth values and returns another truth value. So on the left hand side here, this is like a logical expression, and it gives you a truth value in the end. But just a a is a constant, so it gives you an element in your domain of discourse. So it gives you just so that just the constant. A, by itself gives you the abstract idea of a. It doesn't give you a truth value. It is a function. It's like a constant. So you need to like, you need to like Ref. You need to have that relates to the X to the student in some way. So the way that I'm doing it here is to say that grade of is a function that gives you the grade of the the student, the X entity. and that's equivalent to a. And then this is actually a predicate in disguise. So it's like a comparison. Yeah. And it gives you a truth value so great of X equals 8 gives you a truth value. Yes. Do we have to bracket the entire thing. So I'm thinking of for every x, then we apply the if condition to the X, and then if that, if condition is satisfied, then we get the grade. I don't understand the the question. Oh, like, do we essentially need the brackets at like in front of students and ending at a Oh, I think technically you don't. But just for clarity, I'm adding. but it's the same. and the reason you don't is because I think, the quantifier takes precedence. No sorry this implication takes precedence over like the quantifier, or something like that. But if you want to be just super clear, you can just add the parentheses. And it wasn't worried about it was like 82. Yeah. So you're asking about the interpretation. So we're not there yet. So so for now there's no interpretation. This is just saying that this is just an assertion assertion that all students who study and do homework get an a. But since we haven't applied it to a particular group of students and their results. Yet we can't tell if it's true or false yet. So there's no interpretation yet. Okay, so the second sentence to translate is students who only do. One of them will get a B so you can do it as like 2 separate sentences if you want. And so students who don't study and do homework at A B students who study and don't do homework at A B. You can also combine these 2 and have like a more complex expression here. That's fine, too. Yeah, that would be really interesting. that's right. yeah. If we had an or like students who study or do homework. wouldn't that contradict the 1st statement that students who do both get an a yeah. So no. So so you have to be careful how you write the or so the or has to be over those, both the conjuncts, the con, the conjunct of both. Yeah. So so you cannot. Okay. So okay, so let me show you the wrong thing to do. Okay? So so this would be wrong even if you add a parentheses here. So this would be wrong. Okay? Because or is in logic is always inclusive or so it's 1 or the other, or both. Okay, so I'm gonna delete it. Yes, Xor Xor. So it's not defined by the set of logical symbols that I put earlier, like logical connectives that had earlier. But if you define it with a truth table. Then, yeah, you could use Xor. yes. Oops. Oh, okay. But yeah, Xr would be correct. You just have to define what it means. Okay? And the students who do neither get a seat any questions about this. Okay? So once again, to illustrate the difference between just having these logical expressions and truth like, we don't know if these evaluate to true or false these sentences. because we haven't applied it to any particular situation or scenario. So to actually figure it out. That's when you need to talk about interpretations. So how do we interpret this logic within a particular context? Okay, so then, so okay, so then come up with an interpretation where you and your 2 of your friends are elements in the domain of discourse, such that these 1st order logic, formulas evaluate to true. Okay, so this is, you need a domain of discourse which involves you and friend one and friend 2. And we also need the abstract concepts of like the grades. So we also need the abstract concept of the grade of A, the abstract concept of the grade of B and C, okay, so this is probably going to be your domain of this. This is like the minimal domain of discourse that you can have to to get this to work and satisfy all of those requirements. Okay? But once again, like, it's important to like, distinguish between like these 3 and the names of A, B and C here. So these are constants. Okay? So those are just names. whereas, like A, B and C, here are the abstract notions of those grades. So you can imagine that. Okay, next year Mcgill decides to change the names of all the grades. So now you get like Unicorn and like, I don't know goose, and I don't know duckling instead of A, B and C, in which case, like you can change the names of the constants to like Unicorn and goose, and duckling, or whatever. but the abstract notion of like having grades that are better or worse. can still be there. Yes, is the domain. Is this the domain of all variable factors. That's right. This is the domain of all variables. Next, so technically, you can have student a student. B student. C, yes, you can have student A, you can have student, and you can pass in A and B and C, it's just that they'll evaluate too false. Yeah, that's right. You can have like, yeah. So okay, so we're not done. So we have to like, specify the meanings of everything. Right? Okay? So so for student, for example. So I'm gonna put that you and friend one and friend 2 will be the ones where, if you pass it to students, you evaluate student of that re values to true and otherwise it values to false. And maybe And you're a very good student. So you study, and maybe so does friend one. but not for 2. And again, you're a very good student. So you do your homework. And but maybe your friends don't. And yeah, and then creative. So, Grena, this is a function. So maybe you define it so that it turns out that you got an A and then, friend, one gets a B, and then, friend, 2 gets a seat is something like this. So this would be an example of a possible situation or world where all of those logical formulas evaluates to true. And you can check right? You can check, like for all X student and study and do. Homework means that the grade of X is a you can check. So you it applies for you right? Because you study and do homework, and you get an a oh, sorry. This is wrong. This should be lowercase and then your friend, the condition doesn't apply, because, your friend doesn't do, friend, one doesn't do homework. So the condition this condition doesn't apply. And same with friend 2. And for all the abstract notions of like grades that also doesn't apply, because, like they're not students. and you can do the same thing to check for like this sentence and this sentence, and this sentence, this sentence. yes, so in this case the uppercase. A is kind of true if it's the grade of a rather than like a lowercase a. So the uppercase a is just a name. It's a function. And and so it's a constant that gives you a name. It's kind of like, maybe you have a name, and maybe you have a nickname. But both of those points to the same. You right? So then, you have this abstract idea of like you as a entity, and then you have different names for it. So here, capital. A capital B, capital. C are just names of the abstract notions of like those grades. Yeah, yeah, exactly. Oh, yeah. Yeah. You're right. So a returns. hey? I, B returns B, and see returns, see and equals is like a special predicate, and if you pass in like the same element of the domain of discourse. It returns true. Otherwise it turns false. Okay. yeah, that was my question. Is there a way to define functions like that that like, if that are defined beyond this input gives this output. Are there ways to define functions that are not just like this input, gives this outputs. so in order to not confuse things. For now just think of it that way that you can define functions. You define functions as this input gives this output. Maybe in some other cases there'll be some regularity in the data or in the in in the patterns, such that you can check with in through some other means. but just for the sake of not confusing us. For now the the function is some arbitrary function that takes in elements of the domain of discourse and gives you elements of the domain of discourse. Okay, so what would be an interpretation such that the these sentences don't evaluate to true. Yeah, we change the grade of any student. Yeah, we change any of the grades. Yeah. like, if your friend somehow gets an A does, despite not doing homework, for example, then this will be, then the one of those formulas would evaluate to false. Okay, so does this idea make sense. So in practice. You almost never go to this level of thinking about particular scenarios and and defining the particular scenarios. But it's important to understand that fundamentally, this is how truth works in 1st order, logic and interpretations work in 1st order. Logic. It's like you have to. You apply to some scenario with this interpretation, and then you check what's going on in. In practice. You don't have to do that because you can. You can talk about. You can look at logical rules of inference and derive conclusions by using logical rules of inference. Anyway, that would still be useful without having to go through this layer. Okay? So we we're almost there. The other main piece that we need is we need to have some. okay, we need to have some algorithm for constructing these logical formulas at the sentence level from the its parts. Okay, so we need to build these meaning representations compositionally as because that's like kind of the goal of today and next class. So in order to do that, we need to use another tool from computer science and whatever. So we need to use lambda calculus. and that will help us to define a precise algorithm to do so. So lambda calculus again, you might have seen this in another course. allows you to describe computation, using mathematical functions and the computations that we're going to be doing is we're going to be building up a 1st sort of logic sentence as the as the meaning representation of the sentence by using fragments of logic in the subparts of the sentence. and lambda calculus can also be formally defined, and it can be defined recursively. So there is some variable. This is a different notion of variable compared to the notion of variable in 1st Square logic. But there's a variable there X, which represents some piece of lambda calculus. Then there's lambda X of T, where t is a lambda term. and then there's Ts, where T. And S are both lambda terms. And then there's functional application. So there's a lot. But we need this because we need to be able to store partial computations as we're building up the meeting representation. So function application also known as Beta reduction takes this form. So you have some lambda, X some lambda expression lambda, X of T, and then you apply it to some expression. S, and what it does express. Yeah, what it does is it replaces all instances of x within t with the expression s. So, for example, lambda x of x plus y apply to 2 simplifies to 2 plus y, because you're replacing everything that's X here with X with the with the 2, you can even take lambda expressions and plug them in as well as like arguments within the the body. So if you have lambda, XX, lambda XX, you can simplify that. So this is 1st thing. Here there are 2 copies of X. So the argument here is this identity function. So this means that you have 2 copies of the identity function. And then you can further reduce this by taking one identity function and passing it to the other identity function. And you end up with like one identity function. If this was confusing one thing you can do is you can rename the variables. So rather than lambda XX. And then Lambda Xx. You can rename one of them, say, rename the 1st one to be lambda. YY, okay. So something that takes in a Y and returns 2 copies of it. That's basically what this 1st function says. and then. and what you're returning 2 copies of here is the is the identity function. So one thing to note is that function, application is left associative. So if you have, like 4 lambda terms A, BC, and D, you 1st simplify a B, and then you take the result, and then you so do Beta reduction with Z, and then you take that result. And then you do beta reduction with D, so we only really, we're not really using, like the a full power of lambda calculus, that much we really only needed to store partial computations. So I've only defined this these terms a little bit loosely and intuitively, but you can go and be more precise about it, and can read up on work in that. Okay, so why do we care about partial computations? Why do we need lambda calculus? It's so that we can look at each of the subparts of the syntax tree. and we can look at the syntax syntax tree and build up the meaning representations bit by bit, until we get to the sentence level. When we have the entire logical expression. Yeah, like, suppose you want to measure you want to produce the representation of whiskers, disdain, catnip. The general idea that we're going to pursue is that each of these words. So each of these lexical items. we'll have some fragment of the meaning representation with things that need to be filled in. So, for example, for the verb disdained. maybe it's a predicate. And the predicate takes in 2 arguments, Y and x. because you need something that is disdained. And you need, yeah, you need to have something that's disdaining something else. And when you compose distained with catnip. then what happens is that you take that expression which you think of as a lambda term. and then you take whatever representation of catnip that you have as the argument, and then you can do beta reduction. So then you can make sure that catnip goes into the right place so that it's the thing being disdained. And then it's still missing the the the semantic agents is the thing doing the disdaining. And then later, when you see whiskers, then whiskers becomes the argument, and then it gets plugged in into the Y, and you get like something that's but here it's like some kind of semantic representation of Whisker's disdained cabinet. So this is the high level idea, which is that you have these words. They have pieces of logical representations with things to be filled in, and then you, you express those things to be done using lambda expressions. And then you use lambda calculus, and you do composition up the syntax tree to get everything to go into the right place to get the logical representation of the whole sentence any questions about the high level idea? Yes. Is this kind of to assign things different words like. So, for example, cabinet being disdained by whiskers. And this lambda stuff is so that we say, like, what object is applying to something else. Yeah. Yeah. So it's it's to have some regularity and and structure, so that, like ideally, you want all transitive verbs to look similar. So they all have this form where they need. They're looking for 2 things within some predicate. And then and the lambda terms, let you express that that like it's missing 2 things, and then you're going to find it somewhere else in the sentence. And then the procedure of actually finding it is is by doing composition. using the syntax tree as your guide kind of like a placeholder until we reassign. That element sort of thing. Yeah, it's like a placeholder until you have yeah. Found it and resigned, reassigned it. Yeah, that's a good way to think about. Okay. I'm going to post the answer to these exercises. So you can check online. Okay? So then what we'd like to do is we're assuming that the syntax tree will be useful for helping us derive these meaning representations. So what we're doing is we're doing syntax driven semantic composition. So what that means is, we have to go back to Cfgs. Remember those. and we have to augment them with lambda expressions with the idea that every time you're doing a syntactic composition. So you're putting 2 things, 2 or more things together, using a production in a context, free grammar. You're also correspondingly doing some function application in lambda calculus, in order to simplify in order to in order to take these pieces of meaning representations, and like merge them together to make a larger, a meaning representation of a larger bit. Okay? So then, syntactic composition goes hand in hand with this, like semantic composition. According to this theory, this approach. and you can write it down formally, as like your semantic attachment. So this is your Cfg rule. where you have, like some left hand side rewrites to N right hand sides becomes some function where you're taking the semantic representations. Here, I I'm using dot sem to refer to each of those the partial the pieces of logic representation there, and you're combining them in some way in order to like. gets the overall meaning representation of the entire constituent, the the larger phrase. okay, so let's take a look at some examples. Okay, so if we do this bottom up. and for the small pieces it's like for the lexical rules is pretty easy. So okay, so proper nouns. 1st of all. they will be constants in 1st order, logic. So Comp, 5, 50 is a constant and actually, for reasons we're gonna type, raise them so that they're gonna be lambda expressions apply to constants. So so here's gonna be what we store. So Comp, 5, 50 will be lambda XX applied to the constant Comp. 5, 50. And then the associated rule, for, like Np. Rewrites to proper noun would just be the take the semantic representation of the proper noun, and that will now be the semantic representation of the Np. So just pass it up common nouns. our predicates inside a lambda expression of type E to T. What does this mean? So this means that it takes an entity so like an element in your domain of discourse, and it tells you whether the entity is a member of that class. For example, the rule of N rewrites to the word student is associated with this semantic attachment of lambda X. Student X. So it's a predicate applied to next where X here is like a variable in the lambda expression. and then intransitive verbs. So how are intransitive verbs handled in this framework? What you do is you create an event variable? E, and you assert that there exists a certain event associated with this verb with arguments. So suppose you have the verb rules. then its semantic attachment looks like this. There is a it looks like there's an event, E such that e is an event of tight rules. and there's a ruler which is the X to be filled in. And then finally, well, for now the composition rule of S. Rewrites to Npvp. Is associated with this semantic attachment of Np, dot sem apply to Vpsem. Okay, so that was a lot. But let's look at an example. And then hopefully, it'll be a bit clearer. So let's derive the representation of the sentence. Com 5, 50 rules. And this is what it would look like. Okay, so okay, so 1st of all. Ignore all of the logic. Mumbo jumble. We just have a syntax tree. Okay? So this you can remember from before right, as rewrites. The Npvp Np. Rewrites to a proper noun, and then proper noun rewrites to the word comp. 550 vp. Rewrites to V, and then V rewrites to the word rules. So so far. that's just the syntax. And then the semantics of it all is that you need to associate. It's you go bottom up. Basically so for each of these rules you look up. The semantic attachment associated with those rules which you can find from the previous slides will look like this. and then for each rule you then apply this like you look at the semantic attachment associated with those rules. and then so for these unary rules, you just pass up the semantics. And what we're interested in is we're interested in the logical representation you build at the S level. So at that S level. Okay. So the rule was, Np, dot sem applied to Vp, dot, sem. so that means np, dot, some would be lambda XX, comp. 5, 50, because you get that from this, from its from the subject. Np. and then vp.com. You get from the Vp semantics. And here is where you do, beta reduction to simplify this expression. Okay, so you take this entire. So the way you do this is, you take this entire expression, you plug it into where X is. and then this is what you get. After that one step you're you end up with. Lambda. X exists. E. Rules E. Ruler EX. And then you take Comp. 5, 50, and you plug it in where X is within the smaller piece, and then you get this expression of there exists a. E. Events which is a rules event, and the ruler of E is whatever is denoted by the constant comp. 5. 50. And this is the logical representation of the sentence. Any questions? Okay? So then, to summarize at a high level, the idea here is every terminal rule. So every rule that results in a leaf node, so, involving a terminal. you take those semantic attachments, and then afterwards is a mechanical process of looking at your augmented Cfg with those semantic attachments and then running those procedures. And then we needed lambda calculus to store these partial computations. And we need the logic, because that's what we're aiming for. So there's some logical representation. which is, which is what we're aiming for in this whole exercise. Yeah. So it's like, that's why associated population, it always applies another qualified into the inside. Okay, so the the questions are so the grammar does not have to be in Cnf. that's right. And then, in terms of the order in which you apply things. You have to look at the semantic attachment rule. So that's part of designing this augmented grammar. So here it says, Np, dot, some is the Punctor and Vp, dot, M is the arguments. If you have a different rule that works in a different way, you have to follow that. Okay, I think I'll stop there and we'll continue next class.
