David Ifeoluwa Adelani: North campus. Alright. So okay, let's let's start. So today we will be discussing Engram language model. So I will skip the review of last time. just because of time. so I'm on Slide 2 now. So the view of language, so far, I mean in the in the past lectures we have examined. How? How can we model language so far in context of test classification? That is what we have been doing. But you would also notice that we do not fully care about the context of the of the words, so we only treat them as bag of words or bag of engrams. you have a question. Yeah. So where's the zoom link? And then are we also recording the lecture? Since it's not recording? Yeah, I'm recording. I don't know. Some people are able to join. So there is a there's a zoom table. Okay? Okay. alright. So so so far, we have only cared about how to. how, how to do well, on downstream tasks. And then we have examined. Okay, could you please not join on Zoom? If you are already in class at least difficult here. or you at least mute yourself. Okay, so common feature instruction strategies destroy much of the information in the text message. Because we use like bags of engrams. So in the in the case of knife is very easy to explain. We don't really care about the order. We just care if the world East in the what do you call it? If a word is in the sentence. so we can use and grams instead of considering one word, we can consider 2 words that follow each other. But we don't really care about what is the context we don't really care about does it depend on another world? And here we are going to try to move to something more realistic in how do we do language modeling so that we can also factor in the context or the previous words before the word we're trying to predict. So does that word depend on previous words or not. So the 1st thing we want to look at is how words are distributed. What are distributed are following what is called Zip's law. and then it's a very simple law, which I'm going to show you is just trying to say, what is the relationship between the frequency of the word and the ranking. Then we're going to examine more statistical language modeling based on n grams, and then we'll do or maximum likelihood by relative frequency, and then we'll touch on how to evaluate our language model. One of the most popular technique is what is called a publicity. Until today publicity is still being used even for the big models. So I want to remind you of what is the word which you probably know? Award is the smallest unit that can appear in the context. So I forgot to say, we are at Slide 4. so what is the smallest unit that can appear in an isolation. So, but the problem is that it's not very easy to determine sometimes. Especially if you move from one language to the other. If you have a world like football? Is this a? Is this 1 1 word or 2? Is it foods plus ball? Or is just football? Also we have peanut butter? Is this one word or 2 words, and if you translate it, for example, to German, you have something like Foosball or endless person. So so and then that's that's the. It's always difficult to have a clear definition on how to separate. What's the word from? The other. So it's even gets more complicated if you move to some languages when slide 5, and when we consider the Chinese example. and I don't speak Chinese. But if you you're a native speaker. You can clearly see what we're trying to say in the 1st world we have like 3 words. And but the problem is that they are not really separated by space. This is more character separation. And this has caused a big problem in Chinese. And I'll be because it's a 1 of the 1st acts they have to tackle is how to do word segmentation in Chinese language. and if I speak of the language will be more obvious, because things are kind of stuck together by characters. But do, is there a notion of war segmentation that you can actually agree on for Chinese? So on slide 6. We go to autography, word types and tokens. One of the best way to separate your words in the sentence is just to use what's the space. I think by by now this is very obvious. But when I started working on Nlp, this seems to be very trivial, right? Just separate by space. But there are some times where you have things like a poster fee, and then you have to determine how do you separate them? So if you will have a word like, and then most of the time. The apostrophe and tea are often separated from the can. Because also the apostrophe and tea appears very frequently in our corpus on how do you even ask a question? How many words are there? For example, the cat sat on the mat. and then we have the the following word, tokens, which, if you count every instance. then you are going to have, like 6 6 words or 6 word tokens. But we could also differentiate between token and types where types are unique occurrence of the world. And here, in the 1st example, cat marks on Saturday you have 2D's, and then and then, when you're trying to say, what are the, what types that will actually go to your vocabulary. and then you can separate. You. You can ignore every other occurrence of the. And then you have like 5 war types. Okay? There are some cases that are not very clear. In Slide 7, we have the Fuzi cases. and the question is, how do you separate? Let's assume you want to build a vocabulary. I have the word like run and runs. They're very similar, all right, but they are. They're going to have different ind index in your vocabulary. The question is which we also treated. Probably the second class is, how can you merge them? And one of the ways you can match them so that you don't have very similar worlds that are talking about. The same thing is to do what's called like lemmatization and stemming. And by now, who can remind me about. If you want to combine, run, and runs. What are you going to do? Would it bematization or stemming? Or let's say, if you want to convert Rons to run. What are you going to do without a dramatization of studying? Yes, Barry. to be honest, it could be both in that case. For for the 1st example. Because then we are just kind of truncating the the suffix for the second one. Then maybe you can. Yes. for the second one. Semi! Is that is that scaling or lemmatization? Nope. What it says of democratization, because happy and happily are different. Yeah. Now, this gets more tricky. Yeah. So if you want to go to the root word. Okay, what if we have like happiness? Yeah. Happiness. Happy if you have happiness if you want to cover happiness to happy. Yeah. that would be limitations. To be honest, I'm not even sure. Again, if Apple will cover, because we even have different kind of rules. We have different kinds of stem up. So we treat this as a stem. Some will not. and then we also have the way where you could combine and normalize words like realize. Which is more. Is it the American way of writing it versus the British way of writing it. Yes. I'm doing the same, for happily we just remove either way. Yes, you just like to chunk it. Yeah. The last few words at Carras. and also you have cases like, apart from, you know what is British, English, or American English. We can also have a way of capitalization. So if you take like the sets of all rewards you have we, which is, which is a calculator. W. And smaller W are going to be separated. So if you before you do before you compute your. but before you build your vocabulary, if you lowercase everything, it's better than if you don't lower case. Similarly, you have things like fragment and fragment on on our cases. In slides number 8, we can compute what is called word frequency, or we, the popular word that we use is what is called time frequency, where you want to compute the number of words in your compost. S. And for this to happen. We have to say. how many times does, for example, in this example, how many times does the word cat occur in the card sat on the mat. This is very simple, it appears just once. but you could also have different occurrences. So how many times does the word d or call in the sentence, if you are still following. I haven't lost you 2 times, and for the relative frequency you have to just normalize this by the size of your sentence. which I think is correct from the world. Relative frequency. Okay for you to do any calculation. You always need a corpus, and of course you could also have several copperam that you will combine to form one gigantic text. Let's assume you want to analyze words in general in the English language. The question is that what kind of corpus can you use? What is the representative coupons for English? If I throw you that out to you? What is the representative corpus for English? If you want to analyze all words in English. Yes, the Oxfordination. Someone says the Oxford Oxford Dictionary. Okay. Probably another person will say it can be dictionary. That is one answer. But you could also just have a very big compass. A very good example that can be representative will be like the English Wikipedia. English Wikipedia has been contributed by so many people. and I think it has maybe the gigs of text that is big enough to kind of capture different cases in English. Of course this will not capture all the social media slants and words that have been used. so probably it's not the best, but it's still one of the one that is representative enough or English. So we have other examples that are previous previously used like a brown corpus. Oh, British national couples. And the Wall Street Journal couples are so many of us. So this is like. okay, this one is very simple. You just want to connect the frequency of what type to the rank of the world type. So we are saying, the frequency of war type is inversely proportional to the rank of what type. and it's very easy to see if you look at in slide Number 11, you have the rank which is one, and then you have. That's run for one also is connected to the world with the largest number largest occurrence of the largest frequency in our campus. and I think by now you understand the relationship inside 10 that will stay. The frequency is inversely proportional to the rank. I I believe there's no question. This should be clear. alright. And then, if a word is infrequent also, what will happen? That's what is going to have a lower rank. Basically. you have the word deed that is around one. The word that is infrequent might be around 30,000. Also. we can generalize this to see if mandible it's law which here they are basically adding a constant. So if you have this inverse relationship, that means you have to introduce a constant. This is basic math. So you have to introduce a con, a constant that actually relates F to R, and that is B, I think that should be clear in slides 12, and after that for us to have this Mandel Broth's law, which is a generalization of this law. We had 2 more constants. which is is a file and B, but typically what is being used is that B is equal to one. So it's not very complicated. And some people just say, this file is depending on the language. This would. this will be different. because it's like you are trying to model how every language, what is the a law that gets every language. And I'm going to show you in the next slide that it. It differs for different languages. because different languages have different structures. and if you add a log, so F equals to P over r plus 5 raised to power B, and you are going to have this notation? Where? We should reclaim from the law of login. Yeah. what was the motivation between adding the additional parameters like this? Was it experimental? Or was there? Yeah, that's a great question. So initially, when they did this, since there is work perfectly for English. and then when it moved to other languages, you find that? This is probably not so true. And then we have something that can analyzes that if you can provide the right file and right. B, you may be able to model any language. Okay? the long tail. Practical implication of this most war types are very rare. Depending on the language. so. And the problem is that some words we only appear once, why? So we appear 10,000 times right? So we have to know that, and that's why we have a long tail and sometimes this long tail are very important that if you miss them you will not be able to. There's no way to recover this if you missed that. So a small number of waters make up the majority of waterkins that you see in any compost. So these issues will often cause problems for us in terms of designing models and evaluating their performance, as we will see. So this is what I was talking about. Thanks for the other question, where we are trying to see what kind of equation works for different languages. For example, in English. you have. 40% of the words appear once in a corpus. and then you have a handful that come very, very many times like you have a lot of stop words, a lot of preposition that appear so many times, and you have other words that appear fewer times in Bulgaria which I don't speak. You have same number of water types, or call for fewer tokens. which is very different for that of English. and in in lips you have 80% of words appear all at once. That's on the Sgm end. And why is because the world is morphologically rich. Do you have an idea of morphology? I can explain. So basically, you have a root word, and then you can append to the left or to the right kind of how that some language is, just keep, append it to this suffix, just skip. adding more words to the right, and then it becomes longer, and there are many, many languages like that in the world, for example, subun languages in Africa, you know, you just keep adding. And then for this kind of thing, it's really difficult to. So you will just have many words that are purchased very few countries. Alright. So here we yeah. So here I want to talk about the phone to work ratio, which is very simple to how many phonemes are in in this sentence? For the 1st one, I think it's clear that we have 6 warnings. and then you can also divide it by one. and for the second one Cantone east. you have 7. You have like 7 phone names, but here you only have what? So also we have several, what types! And similarly for the French word. And you see that for the initu, too, which is very, very different. Here you have 8 phonemes. but it seems like it's the same work. Very, very long word. But when you pronounce it. You actually have 8 footnotes. Why do we count words when you come to what's very important for actually building? What is called Lovewood model? Because statistical language model is just by counting how many words appear in your couples. And then you use this to build probability distribution. Okay, so this, what frequency are important for test justification, as you have seen in previous class. When we are trying to classify whether something spam or not in information retriever, and in so many tasks so remind you the task of language. Model the task. The task of language model is like, you're trying to predict what is the probability? Of the next world? Given some context. that's his language model. So you could also see it. As what's the probability or trying to find what's the probability of the entire couples. So if you are asked to put it in next world, given this context, may we add a little. you find that both alarm and accidents are actually good consternation of this one. So let's try to view this problem realistically. So, probability of reward. I mean, where we define a random variable W equals a small W. Given a context C, so the random variable here is W, which can take any word in our dictionary. and then. the smart of you represents that the value it can take. And then we have a context. What is so? What is the probability of the world equals Lamb. Given the context merely at a little. you can compute this probability, but sometimes, when we are working on this, we often ignore this random, variable which the statisticians may not be very happy with us about. But anyways, we would do. We do this okay, equivalently. You can say, language model is the probability distribution over a sequence of words. Just imagine you have a sequence of words. Combine all the text in English booking period into a gigantic text and then complete. What's the probability over all the words that have been that's not worth it. And you can decompose this probability. This probability. Using this chain rule of conditional probabilities which I can explain. using the book and maneuver of the country with all those things abandoning CPU of w. 1, even W. 2 W. 3 WN. What's a plan for? For? So we need to. probably. And then we can also decompose this from W to. So you can multiply by a little confusing. I need to review then, this topic that I realized $4 ready to, you know. So if you. If you apply the chain rule continuously, basically, you just use the law of conditional probability. probability of the blue one. Given all this. and you can also reverse it because it could be probability of WN. Given the rest. and then you can also do it this way. Probably the way our group expressed, this is probably not the best. Okay? Alright. yes, yeah, it's kind of equivalent. So because using the law of probability again. So so using a lot of probability. This can also be. Express the hospitality on document I know you are. You hang on as well. so they are kind of looking back. hold on your phone 2, 1. These 2 are incubarents. So if you have like. in bidirectional oil, you can do it both in the forward direction and the backward direction. Well, this is probably the this is the better formulation going down here for what you are describing. Yes, per student, the product term be from I of one to N, and then WI, plus one that I minus one. did I miss something it's starting from. I equals 2. Yeah. So for the 1st formulation, I did, that is correct. But for the second formulation we have to modify. Basically, you're going to see it should be W. So basically, it's with Wk, and then you start from one QWK. Minus one to your hospital. All right. Alright, example, a good language model should assign a higher probability to a grammatical string of to a grammatical string of English. For example, you're wearing a fancy hat and a lower probability to ungrammatical strengths. for example, fancy you our hearts be wearing sometimes you can have something that is. has a good probability in terms of language model, but it's not very grammatical, like the example here in Slide 21. So also the length, the length of the sentence, and the reality of the words in the sentence affect the probability. So you all agree with me that the probability of I had the would be greater than the probability of. I had the cake. because I heard they might, I would appear more frequently because it could have more combinations than I had in cake. so what would language model capture. They do capture some linguistic knowledge. Interestingly, they also capture many facts about the world, because if it's a correct fact. then it should be able. It should be easier to predict. Basically, if it's a correct answer because it will occur, it will co-occur with the context, multiple times in different sentences. and then you cannot refer to this as a good fact. And that is why, a longer model like Gpt. 2 or Gpt. 3. It's able to predict a good fact and a correct spot. but sometimes it can also produce incorrect facts. just because it seems to it, as if it has a good probability. and I probably believe this is the topic of your assignment to actually determine if the fat is true or not, and then we across fire for it. The longer model can also predict an incorrect fat. Also you can use it to capture what is the correct syntax, and also to it can also be used. For discourse analysis. There are many applications of language model. When you use your mobile phone. you prefer that it suggests what's for you when you're typing. It's an example of a simple language model which is sentence completion. automatic speaker automatic speech recognition. The idea is that the way I'm speaking now is supposed to transcribe the text, and if it's a good ASR model one of the models, there is a language model after you have really acoustic model. You also need a lot of model to be able to go one after the other to generate what is the next thing after the purpose. So ASR is a very a big application of language model and also machine translation. Okay, typically we find a solution that maximizes a combination of tax-specific quality. or they'll add the language model probability. If you are just shooting for tax specific quality. That is okay. But also that task may not realize. I mean, you may not be able to generalize across many tasks. and if you just focus on having a very good probability distribution, it may not work well on different tasks. So there's a trade off for this. So building models giving lots of data from real world. The idea is that we want to find what is a set of parameter that describes the data. So in terms of status quo nlp. if we are looking at what we are talking about the parameters. It just means you have to estimate every single probability for that text. And if it's a neural network that means you have to find a set of parameter that actually can model everything, the entire text. Okay, so the model is, what is the probability of the world? Given the context. and you can mathematically, actually calculate this for either a unigram model or a diagram model. So how do you build your statistical language model the 1st thing you are supposed to do is to gather a large representative training purpose. Something like the English will give you there and then you learn the parameters from the couples to build a model. If you just want to build a statistical model. That means you have to estimate all the probabilities. For example, all the unigram probabilities, diagrams, triangle probabilities, and then use this to build a language model. For neural networks. That means you have to learn a parameter theta that actually is able to predict the next word given the focus work. and once the model is fixed, you can then evaluate it on the text data. So Rdb, learning model. So what's the probability of the world? Given the context? So. or M. Graham. we make use of this assumption, which is called conditional independence. It's very similar to what I've explained on the Board for unigram probability. What is the probability of the next word given the context for Unigram. There is no context. right? Because it's a unit of distribution. What's the probability of every single word? That's your name now. So basically, there is no quantum system. For diagram. What is the probability of the next one given one single. we are smoking. so that's probability of WN. Given WN. Minus one and trigram. What is the probability of Wn. Given Wn. Minus one comma WN. Minus 2. So if you say the cats is sitting on the mat. And then you want to predict the math. Given. The last 2 words that proceed are okay. The last 2 words of the format. So that is like a trigram distribution. And if you say the the cat is sitting on the mat, if you just say what's the probability given just one previous word. That means you'll say, what's probability of Mat? Given the. So that's a simple example. I have an exercise for you to processing very soon. So an example here is, what is the probability of cuts that's in Slide 29. That will be the count of all the times. Hats appears divided by the count of all the words in your compos. that's a unit gamble of religion. So the byground probability of cats are given. D, so that's the count of how many times the cards or call. Don't change the order, please. How many times they can cut or come like that. just like that! That means you have D, followed by what the cat divided by the Count of B. That's the background distribution and the diagram probability of the cat given P to D. So that is is, if you want to model feed of the cats, that will be the probability of cards. Give 1 50, and then that will be the count of the feeding cat. How many times you have feed the cats like that occurring in a couples divided by the counts of feeding. Okay? So all these are what we call the embali estimates. The maximum likelihood estimate. Do you have questions on this? Because you wouldn't need this for the exercise. Yes. word data in no context, no context. So probability of each word. Yeah, that's the unit distribution. Yes. Oh, so you're counting one types. Yes. devices was so. How many times the occurs multiple times. I thought this was like so like in part of space. the web like would be like a type. so verb will be so. It depends on what you're counting. The type are in your list of words. It's unique. unique words that appear in your in your couples. Yes. Sorry world of the world, just our own. the unique occurrence. Yeah. So that's what they can occur 10,000 times. Well, you're just saying, how many times? D, of course. do you have questions? Okay, this is your exercise. Can you compute a unigram and background language model, using the following sentence. So let's start with the unigram. So what is the probability of that's what is the probability? Okay, yeah, yeah. Sorry. I just have a question about unicrons. Yes. if we count the probability of each word based on all the words in the purpose, aren't you always going to put those V on every word, because we use no context and use the most frequent words, which is the yeah, but usually for a standard language model. You combine this difference. You combine the diagram, the diagram, and the unigram together? No, you don't know. Yeah, alright. I know that when you tokenize we can tokenize into that. We just need those. the matrix into the model. Right? We don't have to. So so you said, if you are using a tool, a library, yeah. Oh, for for the, for the assignment. I think you're supposed to generate from right. No, no, for any level, I mean, like, once you have the data. Yes, you have to tokenize it right, and the choice of tokenization can be. You can split it word by word, or you can do some single one. Yeah. And then once you have those those grams, you have the matrix, and then you feed that into the model. But you don't really have to do anything right? You don't need to do it because it works. Yeah, because you're focusing on just classification. Here, we are focusing on a more general language task. Yeah. alright. So what is the probability of that? Yes, I agree. 5 over 15. Okay. What's the probability of is. yes. 6 over 15. Okay. What's the probability of? Not what? 2 about 15. Okay. what's the probability of what else it? 2 or 15. Okay. So I guess that is the unigram. So you have completed your Gram language model. So for the diagram. how would you compensate? So you want to estimate the probability of that's that right? That is. it should have been easier if I'm projecting. But so you have probability of that. That is is probability of is that basically you are going to take them 2 by twos. and then you have. That's that, that is is is that that that is. is not, not, is, is not, not, is is that? That's it. It's it's it is okay. And then, if you want to compute the probability for every unique types there and graph types, what are you going to get. What's the probability of that? That? Yes. So 5. How do you compute over 5? That's called my Internet? And and that did. And so by 5. Yeah, so it's the probability the count, of the number of times you have that that which is 2 times divided by the what the count of that right. the count of that. And how many times do you have that? You have that 5 times right? And that is how you have 2 over 5. So what's the what's the background probability of that is 2 or 5. Okay. yes. 5 years. 1st word, or the second word. So in the case of that is, are we dividing it by the occurrence is, or the occurrence of that? Yeah, it's not divided by the occurrence of the 1st one before the second. Yeah. yeah. The 1st word these words, it depends on the second, the second one. The second word always depend on the 1st one. Yeah. and then for that is, that will be the count of the number of times you have words that's East. divided by probability, or the count of the times you have that right? And that will be again. what's the answer? Also 2 or 5. Okay, what of is this? Yes. Okay. So what happens whenever the denominator like that's in regards nothing. I'm not very good. happy end of the sentence. So at the end of the sentence, if nothing is happening, so this is more like a unigram. Right? So you can skip the last discarded, because there's nothing. Yeah. 6 out of 5 not out of 6. No, it doesn't affect the calculation. It just affects how you group damage to tools. Yeah, yes. yeah. So typical. We always have, like another end end of sentence token that will always support that. For this reason. Yeah. So what's the probability of is is one over 6. Okay, probability of is that 2 of us should be part. Yeah. Okay. Probability of, if not 2 of us. 6. Yeah. Okay. Probability of not his. One. Yeah. A probability of it's eat one of our 2, and the probability of it is. what what about you? Right? Alright. I guess we almost covered everything. Yes. it's that we are having is always something. Is this, is that, and say, I think included. But I felt like a factor identified so like as the vibrant is, and then and 7 isn't like an additional option with like one or 6 or 12. So you mean for the background? Yeah. So I think the thing is that if you have the end of the war token. there's something where the cover which is back off. So you. if there's nothing you can compete again, you can back off to from diagram to Unigram. and then you can back off from diagram to buy ground bylaw. So your background. But this this basic knowledge is probably enough for this lecture before we make this more complicated. Yeah, alright. yes. San Francisco. Chance of a child. I I don't get a question. So if you add end of end of sentence token, you already added for the entire purpose, and then you compute it. Right? Yes. yeah. So I I think all of you are more focusing on what's the problem, what we have go to the next one. Well, our goal is that we want to just estimate every probabilities we can estimate from that text. From what we can see right? So the idea is that, can you estimate the probability of every word. every background and every diagram that you can see in this in the couples. So that is the goal of language here. Yeah. Okay, so the last sec section is that once the model is fixed. we can now use the model to evaluate it and test the data. Okay, so normally, if you if you're trying to test very similar to test classification. You need to divide your data set into training data and the tested data. But you could also have the validation data. So for us to test like, if you remember, what we are trying to do is to estimate what is the likelihood of generating the testcos. So after you have feed the data using parameter theta on everything on your training purpose, how do you compare? What is the probability of the test compost. and how do you evaluate it? So there are 2 ways you can evaluate. 1st we can use cross entropy. and then you can also use publicity. And there's a nice connection between cross entropy and publicity. Because publicity is just too. So what do you call it? race to power the cross entropy. Yeah. So from information theory. consider some random variable X distributed according to some probability distribution. Of course, we can define the information in terms of how much certainty we gain from knowing the value of X. And the more information that you need to know. This is what we want to measure if the information is very trivia. so there's no need to waste a lot of that beats to encode that information right? That is the idea from information to. So let's talk about likely or unlikely outcomes. If you are observing a likely outcome, less information is gained. like observing the word gay d. But if you are observing a more rare information, more information is gained, and we can now use this. Go back to information theory to actually develop a metric that we can use to estimate how much information in bits. So information of X in slide 36 equals the log of 2 of one over B of X. This is by definition, I'm not adding anything. and entropy is just what is the expectation? Sorry? What is the expectation of Ajax the expected amount of information we can get while observing a random variable. So an expectation of Ix would also give you this formulation, which is, you sum over all the pis log of lot of Api. and if you do, the math one over pi will give you negative. Yeah. If you had a log within. So you're gonna have logarithm of 2 2 raised to power 0, minus log of 2 pi, and then this will give you negative. Okay? So I have an example in Slide 38. The plot of entropy versus the coin tos fairness. If there's maximum fairness that means you cannot easily predict what will happen. Then you can have a very high expected value of the information which is the entropy eye entropy. And then, if it's biased, then you can easily determine if the probability will be 0 1. So there's no need to. sweller of this and code information. Yes. Do you want to quickly explain how information and the logarithm are related in pivots. So the the information, why is it locked to a 1 over? Pdf. oh. okay. so you are you talking about slide? 35, 35 or 36. Yeah. no, it's the 6 30. So that's the formal definition of what is information in terms of this is the definition from information to Europe. Why is it one over the 1,002 in the law? Or we don't need to know. Now you want me to talk more about this. This is more. I'm just taking this from definition. Of of how information is defined. And then we're trying now trying to borrow the concept into into Stats square of you, basically. So here in the cross entropy, yeah. The idea is that entropy is the, of course, entropy is the minimum number of bits in order to communicate some message. but if we already know the probability distribution. The message is drawn from then we can compute what is the cross entropy. and the cross entropy is also defined this way. But the basic idea is that if you want to know what's the cross entropy between the true distribution P and the model distribution in Nlp, what happened is that we don't know the true distribution. P. But we can. We already train a model cube which is our model distribution. And we want to use this model distribution to determine what is the probability of the test. The starter does it so, and because we don't know the true definite, we don't know the true distribution of of fee. we come up with an approximation of what is the how to estimate the cross entropy which will be defined as minus one over N log of 2 of the probability of the model you have trained. So at test time, what you are going to use is you're not going to use, say. the true distribution at all, because you don't even know it, and you all all focus your attention on estimating having a good estimate on the model distribution. And if you do this over if you are able to, basically, you're trying to say the if you have a good model. The model distribution should be close to the true distribution. right? But since you don't know the true distribution, you you use the model distribution instead of the true distribution. And then you compute the cross entropy based on this. and from this we, this leads us to publicity and in publicity. We are just saying that this will be 2 ways to power. The cross entropy. Some people formulate it differently, which would be if you assume log of E. This can also be the exponential of the cross entropy. You have questions that that's the end of luncheon. Yeah. Sorry for it. Every caffeine is beyond my control today. Yeah. Sorry. I'm sorry. Go for me. Yes. true. Yeah.
