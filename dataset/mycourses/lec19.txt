David Ifeoluwa Adelani: Not okay. So there are over 7,000 languages in the world, and or like over 400 of them, are spoken by 1 million speakers. But we don't have a single technology that works for 400 languages per world. So we have some language models that they try to scale, to like 500 languages, but in terms of performance we I don't think we have a good luck with that. How it asks to the force all the languages spoken by 1 million people. So that means there's still a lot of work to be done. and over 1,200 languages are spoken by under the K people. and with 2,000 languages, you have, like a fewer number of speakers. So the question is that if you look at the distribution of the languages in the world. Actually, you have more languages in Asia. followed by Africa, the Pacific the Americas, where we are located, and Europe and European languages have been more favored because they probably because they are one of the early adopters of the Internet technology. And then they have a lot of materials on the web. so we could also speed them. According to Achnolog, you could also categorize them, based on which languages are institutional, which maybe they are used by different government activities, like schools, mass media but in terms of language technology, even less are supported by language technology. So how do you define under resourced languages? So it's a very simple under resource. Languages are languages with various resources, but in terms of resources, it will be classified in terms of the data that is available. How many unlabeled data are available on the web? How many was the size of Wikipedia. So what's the size of if you crawl all the available web tests in that language? What's the size? That's an example of a labor data labor data. Do you have, like part of speech data sets for this language? Do you have set my translation data set for this language or so. and also in terms of the research that have been carried out on this language. Do you have representation models for this language? Do you have language models that support in this language. And do you have even basic linguistic tools? Some languages are not supported by keyboard spell checkers, morphological analyzers and dictionaries. So because nlp is data driven. So if you have a lot of data on the web. you'll be able to, for example, create a better representation models. Oh, better tax specific one. So data is the key. So one of the most popular ways to categorize languages is based on Jewish classification. which is a Sys class categorization of languages based on how much unlabeled data are there on the web and labor data are there on the web? So class 0, these are like. languages where you don't really have a lot of you don't have any text at all. So, and class one would be languages that have few texts. For example, maybe there's a Bible Corpus, or some linguistic activity documentation available in this language. And so I know if you increase the Joshua class Joshi Class 2. They have some unlabeled data. and they have a little bit more on label data and a few label data. Those request 3 has more on label data than juicy class 2, but also still less labor data. And then the winners are kind of like languages with sufficient amounts of labor data and legal data. And that's why you will see it's easier to categorize languages like English, Spanish, Chinese. Japanese, French, German, Us. Like win us in terms of the amount of data that is available. So because most of what we do now is based on what is called language model. So there are different kinds of language model, as you can see here. And then on the left hand side, you have, like encoder holy models. Why, on the right hand side. You have, like decoder, only model. So an example of encoder only models would be like birth model. I think we discussed that last time where you could. You want to encode your text properly. and then you can also send this to a class classification head or to a classifier to do the categorization of what you want to do. And also we have, like encoder, decoder model like T. 5 model is a very good example. And also we have a bat model is another example, and decoder. Only model would be like something like Gpt. One Gpt. 2 Gpt. 3. And I believe the current model is still also based on decoder, only model. And, interestingly, all of them must be based on the transform architecture that we discussed. Yes. sorry. When did decoder only models become the most popular great, because, when we discussed the transformer, we said the encoder, because this is based on sequence to sequence model the encoder parts. The focus of the encoder parts is to have a very good representation of the model, to compress all the information into a single vector right. For example, in the case of sequence, to sequence Lstm. And even with attention, is still trying to do the same job. But better job of passing a very well encoded information to the decoder decoder is is all used for generation. Right? So you can generate a new, a new text. For example. Yeah. so it's also embedding. So whether you use an encoder only or decoder only, you still have to learn an embedding to project the world into a lower dimensional space. Yes. So in a decoder only model, the decoder itself learns its own embeddings. So you learn, both embedding both. Yeah. Input. Embedded and output embedded. And there's a way you to also tie them together. Yeah, there's no encoder. But since it's also supporting output embedding, so you also have to learn an embedding. So there's no way to skip this process right? Because the the vocabulary space is very large, even if you use a word. So you need to kind of actually go put them in a lower dimensional space. Yes. the left hand side part of the architecture of the transformer only. Yeah. But it so well, you could. You have to also construct a task, an example of a task used to pre train and then go down. The model is the Max language model? Yeah. yes. Can we say the decoder? Only modules have, like internal representation, that we don't have access. Yes, you also have a power citation. Yes, yes. I think it's the same branch. Okay, I'm not focusing on the sub branch. So I'm just focusing basically decoder. Only. So I think maybe the way they did the categorization more is the current chart based instruction to Russian are probably categorized separately from the others. Yeah. alright so. But what we do typically in an OP. This is the validity today. Since 20 2013, plus, you have, you learn a self-supervised model right, based on an on based on the large, on the text. It could be water vec, right? It could be a birth model. It's just the architecture that is different. It could be Elmo, it could be T. 5. This is an example of a birth model where you want to predict Max token. and then you cannot fine tune this on a label example right? And then, after the fine tuning stage, you can add additional layers so you could have a best model and attach it to Lsta or a single layer. and then you can use it for different classification. Tasks like sentiment, classification, or question answer. So we call this pre trained, fine-tuned paradigm. There's another paradigm, but it will not be the focus of my lecture today, so I won't be talking about prompting a lot. So it's more on multilingual nlp and crosslingual transfer. So for you to use this, there's a problem. So the 1st problem is. There's lack of legal data for downstream task and also for many languages. Because now we are, we're in the multilingual space. So also there's not a large enough on labor data for many, many languages. So the problem is that you cannot really do have a good, successful, self-supervised training. If you don't have a lot of data. So but you can leverage what is called transfer learning, which is very popular in the last 3 years maybe less popular now, because we're prompting. But the idea is that you can transfer the knowledge of a model to another task to another domain, to another language. So that's why it's called transferring. And so this is the area. So you have a model here where you know that it has stored some information. It has learned some information on a large amount of text. And then you want to try to pass that knowledge to another model. B, right, that is, focusing on a different task or domain. So this is inspired by what happened in computer vision, where image net has been trained on a lot of images of cats and dogs and everything. And then you cannot transfer this to what? To order conservation tasks, your segmentation and order? Yeah. So you could also do the same thing for Nlp, right? So you can have same task, and then you transfer it. So given the same task. So you can train a model on the same task, and then transfer it to the same task of a different domain and of a different language. Is that right? So then, it's very easy to transfer another one that is even more popular is different tasks. So you can train your model on a different task and then use it for another task. So and this is what we do. So you do mass language model on, on a larger number of text, and then you cannot find some little sensory classifications which is like different tasks. So yes, that's right. Trust me. so one of the ways to actually incorporate as far, and it would be like fine tuning. Find that? Yeah. yeah. So it's like one of the thing you would do. But it could be more complicated like that you could use. You can modify the Internet structures to do transfer learning. There's a way to do parameter, efficient transfer line, and and so on. So. And this is one way you can do. Transferring one example will be worth future instruction. So you can extract sentence embedding from the language model like Bert model T. 5, model. And then send this to this because it's well, is a rich representation. Send this to another architecture like By Lstm. Or Cnn. And then use this for classification. So this is very popular. For example, this was our model was trained. All the weights are frozen. You don't need to modify the weight. The second approach is what you mentioned, which is fine tuning, and here you do end to end fine tuning. You modify all the parameters of the model. and it has been shown that this often leads to better performance than doing feature extraction right and selling it to other architectures like Mlp. Lstm, and so on. So in terms of crosslingual transfer. So so suppose we want to transfer the knowledge from English to another language called Americ. What's interesting is the language with a different script. So the transfer is really difficult. You could also have the same experience with transferring to Arabic or some Indian languages that use very different. So you do the self training. But now there are 2 ways you can actually add there are 2 ways to fine tune. So if you have labeled data, you could just like fine tune this multilingual bits. And now we move to multilingual bits. So if you want to do multilingual transfer posting transfer, you need an encoder that is multilingual. The difference is now mass language model has been performed on multilingual text. So you concatenate the language of German, English Swahili and combine them together. Just append, and then train the same. And then, if you have trading data, you can find some directly in Americ. And if you don't have training data and one thing you can do, you can. You can find some language that has training data, for example, English. And then then just do 0 shot transfer to the language. I hope this is clear. This is very, very, very common. Yeah, yes. Say, 0 shot, 0 shot transcript. You just mean very good question. So I think once you just you just predict that's it. So you train a model for English. And then you run prediction for another text which is in another language. So that's why we call 0 short transfer. Okay, is that clear? Yes. so it could be any task. Suppose sentiment let's think about Amazon reviews. And then you have a lot of text Amazon reviews in English. But you don't have anything in Spanish. and then you train on English. and then you just evaluate some Spanish text. Yeah, yeah, but you're not generating it this time. So this is more for classification tasks. Yes, and the reason is because you forced to mass language model on a multilingual text, right? So there's a way you can do transfer learning on monolingual model. But that means you have to 1st kind of modify this monolingual Lm before you can now adapt it. Yeah, but that's a little bit more complicated. But this should be clear. Right? Yeah. alright so. But what we know that most preaching language model do only cover, like on a hundred languages on the maximum. But now we have people that have tried to scale this more. but, for example, the 1st version of the models we have like multilingual Belt Xml to support, like 100 to 100 languages. And then recently, I think last year we had 500 from Lmu. That they try to, you know. Just do adaptive fine tuning to 500 languages. We have Serengeti that was trained for 500 African languages we have. Nllb. That also can be used as an encoder. So, of course, lingual transferring is attractive. Why? Because you have so many, so many languages in the world, and then you are not able to have the data set for everything. Right? So you could just do the research transfer if the multilingual model is really good. And I've seen a lot of text in that language, even though there's no labor data, you can still have a very good transfer, especially if the languages are very related, so could have a very good transfer from Spanish to French, because they're Latin based. and the languages are very similar. So the 1st challenge which we'll discuss so all the rest of the talk will focus on these challenges, and I'll try to present some of my past work on this. And and also interesting question that you can focus on. So the 1st challenge is, labor data sets. You don't have a lack of data set for many of these languages, and also this virtual language model. Only cover a few languages, like only languages. from, let's say, over 7,000 languages in the world. Another issue is that you have languages that actually may use different scripts. So transferring from English to American is really really difficult because they use different script. And also we have issues where you have catastrophic forgetting, you know, if you do this fine tuning, and then you forget the previous knowledge while trying to adapt another task. We also have issues of parameter inefficiency. Where, when you fine tune, it just creates new copies of the model. Right? So if you prompt now, you don't create new copies of Gpt. 4, this will be completely unscalable, right? So previously for the birth model. It's so nice. But once you find zone, it is another copy. So yeah, if you have 500 Md of okay, it was a size like 500 MB. Size. And then you train 200 models. You're going to have 500 MB. Types, 200 checkpoints stored. It's kind of no privacy efficient. So people were looking at ways to make it more parameter, efficient. And there are also issues of what would be the best source language. If I want to transfer from English to Americ. But if there are other languages to have legal data most, I always use English. or or can I use languages that are more similar to the target language. So the 1st one, where we talk about developing data set for low resource languages. First, st I will start on what we did for African languages back in 2019 so we don't really have a lot of data sets that were human generated for African languages. And by working with different African communities, we're able to create different data sets that covers different tasks from question answering text to speech sentiment, classification, machine translation, news topic, part of speech, name, density, recognition. So Mascani, for example, this is an African community I collaborate with which is a Grass group, Nlp community for Africans and by Africans. But I must also tell you that there are other communities around the world. For example, there's a comment in India, AI for Bharat. They're also working on similar thing. There's a Southeast Asia initiative. and many people just collaborate with native speaker to develop a level data set for their languages. So the task of named entity, which you should be very familiar with, which is classifying entities into personal name, location, organization updates. And then you have this bio tags that you're probably familiar with. And so we have this kind of collaborative projects where we have native speakers and Mls. Platform around the world. And then we have 2 versions of the Masaka Project. First, st we created Nameless recognition for 10 African languages, some in West Africa, some in East Africa, and later on we find out that I mean, we know we're excluding the southern part of Africa. But actually, more compelling reason is because they have very different linguistic properties which I'm going to show you. and that means the transfer from this East African languages to West African languages doesn't transfer very well to the one in Southern African languages, because those ones are more morphologically rich. So so I give you an example of some of the languages and the different linguistics structures. So this is English. Imagine you want to do a transfer from English language to Americ. You see a language with a different script, so this can already pose some challenges. Then you have languages like Rwanda or Swahili, which have a lot of morphology. I think we did an example of Swahili in class where you're trying to decode, where you have different prefix. And then you want to try to decode. What is the root word. and we also have languages like it's called Size Zulu, that apart from using rich morphology, they also have what is called now classes. So there's a different prefix for a personal name, for a location, for different entities. and also who can tell me what would be the challenge for? Nr, if you want to go from English to this language. there's a result. But I didn't put it in the slides. But if you can think of what would. What can happen with any our model train on English work for? Is the cost always as well? And why not? Yes, like. There's no translation of like some words in English language. Yeah. But the models are smart enough to bypass that. Yeah. Just looking at a linguistic structure. What can I pose a challenge. Remember, I told you about not this long. Okay, yeah, thank you. No. So the answer is on the screen. But yeah, yes. these languages. One word, and they're a lot more complex and translates to a phrase, yeah, but our models are smart enough again for that. Yes. yeah, it's very similar. Okay? The answer is like, the issue of now classes. So if you look at entities here, you have something like using right? You have something like in Nigeria, so one feature of ner is capitalization of entities. Right? Of personal name is capitalized right at the beginning, is capitalized in Zulu is not capitalized the beginning as something. So this capitalization is absence just makes the annual model fail, or personal names, because it kind of fits into this idea that if you have capitalization it's more likely to be an entity. And now, when you go to a language that doesn't have this feature, it's just kind of yeah fails very quickly. And this is known. Because if you want to do with the crf, if you want to be a features for Crf model. One of the features you would do for any R is capitalization. Do you have capitalization which is humanified feature? But the models also fits on this kind of feature. Yes. sorry. This is maybe a more general question. But in transformer based models which I'm word embedding, how is how is something like capitalization represented. Is there a different word embedding, or a word that starts with a capital? Yeah, I mean. there, it's not that we have a different word embedding. But if you look at these 2 words. the even the most the tokenizer, we treat them differently. Right? So if you want to split zang. How would you split it? Maybe he's going to split it into one word or 2 words. Right? If he's going to split Kuzang. it's probably will may or split it differently. Right? And then you might have issues where actually it actually fails to capture this information correctly across different languages. So but there are results also showing this also. For example, in the paper, where we show that the performance drops very good. and we have languages with diacritics. And then the what's our representative? A little bit differently in this language is with the acritics, and then it's tokenized differently, and then it also fails for the Ndr task. So this one I will just really brush through about it new topic classification. The idea is that this is a language in Nigeria called Nigerian pigin. So you have when you pivot and decode. This is like a clear language, and then you have because it's very similar to English. The 1st word is more like a business news the second one is talking about an entertainment award which is like an entertainment news, and the 3rd one is probably on health news, and the last one is sport news. Even if you don't speak this language. I think you're probably able to record this. So in terms of. We also did very something very similar to the Masa Project, and we create a new topic classification for these languages. So one major issue, and why we have decided to do this is that when you try to scale to develop new models for different languages, if there's no label data, there's no way to evaluate how good the models are. Right? So and that's why we have Masaka and then Masaka 2.0. And then we have this new topic classification. So this is based on BBC articles and voice of America. And then we have some local website where we called news articles, and we label them. So, for example, this is BBC, and they are predefined categories. For example, health news is we? So you can just give your cry everything from health news, yeah, likely to get mostly else related news. Or we have business with politic news culture sports and all that. So and also we are like religion because it's very prevalent in the African newspapers. So and then this is how we actually develop this Masaka news data set. Also in the interest of time. I'm not going to present results for this, but I'm going to present results for the last one, which is Srb. 200. And again, then I think in 20 what? 2023 so we had this very. I had this very, very small idea. And the idea is that is it possible to repro repurpose. An existing data set so creates a labor data set for many, many languages. Right? So when you work on low resource languages, one of the major challenges that there's no evaluation data set. So the available evaluation data set like we have taxi, 1,500 from Lmu university. But this is based on the Bible on this part. A little bit biased to the religious domain we have any which covers 1 76 languages. But these are not human annotation. This will just automatically annotated by some rules and heuristics. So you cannot trust the result from this data set. We have ud dependency passing, which also has, like dependency, passive part of speech. But this is a very expensive annotation process, right? Some some languages. It it would take them like one year to have a huge benchmark. and and now the community has grown it. This project has been on for more than maybe 5 to 10 years. and they are slowly adding maybe 2, 3 languages every year. and maybe it will go on for the next 20 years. So actually. Oh, it requires a lot of expert annotation. We have belly belly. That was actually probably this. The closest to our work is Beli. That was used for question, answering task or reading comprehension. Given an article just like reading comprehension in high school, you ask a question about the article, and it returns the answer but here they do not provide any training data. It's just an evaluation. We also have a a massive from Billy believe was from Meta, and massive, was from Amazon. And we just have issues of not covering many low resource numbers. So here we actually develop Sib 200, which is based on topic classification. It's general domain. And what we did was to leverage an existing data set called Flores, that is based on machine translation. So these sentences already have translation in over 200 languages. and what we did was, if we label correctly the topics of the text in English, we can just project this to the rest of the 200 languages, and that is how we created Siv 200. So it's very simple. So here we have text in English. and then we annotate them into these different categories, and then we project that into Arabic. We project it into Yoruba. We project it into Maltese, and that's it. And that's how we created a benchmark very quickly for 200 languages. This is clear. Right? Yeah, yes, yeah. The exact translation into this different languages by humans. There might be some translation errors. But that's not the focus from this time. Yeah. all right. So the so when we did the annotation. The phrase Kappa score was pretty low, just because we're dealing with very short sentences, and people disagree a lot, even for these short sentences. and then you can have a sentence where some people will label this as geography, and another person will label it as travel, because someone is going for hiking, or something like this. Alright, so we have this different category category. So we have sip easy and hard where we have the 1st sip had, like 7, I think, 7 categories of labels. And this is a distribution of the categories. We have more science and technology articles, politics travel than some other categories. So we also distribute this by language family. So you can see the language families that cover any data set Atlantic. Congo, like Indo-europia, was the largest, followed by, I think, Atlantic, Congo. and Austronesia, and all that. So we also distributed by Joshi class. The interesting thing I told you about Joshi class, right class 0 has no data on the web plus one they have few texts, right? So the one of the good thing with Florence data, say, is that it's actually cover a lot of low resource languages, right? Like. up to 100 of those languages included in this benchmark are low resource languages. So and also we have, this distribution across different regions of the world. We have more Asian languages, which is also nice, because we have more languages in Asia, done all other places in the world. And then we have European languages and African languages, and so on. Okay. So we also then try to find tune some baseline models like classic Mlp. and then we also fine tune 2 models Xmr was created by Meta. And actually it's a strain on other languages. So many languages are excluded in our benchmark. They have not been seen directly training. and then you can see the effect of transfer learning in naturally like performing on language that are not seen during pre-training. and we also have got 500 that's already seen a lot of these languages like I've seen like 1 77 languages in its portraying. Well, of course we also do prompting, but now I'll skip that all right. So if you look at the accuracy by language, family. one thing you'll see is that different Mlp. Which is the multi-layer perception you see that it's kind of inferior to the bed based models. This is what you expect right. But if you go to some low resource language families. So for Indo-european you have Slm large. A larger model are the best results. and if you go to topic language, you also have very good results. And on average this preaching language models also give the best result. But if you go to some very low resource languages, you find out that a simple Mlp get better results, like, for example, in the Atlantic ago. So which shows that the transfer learning works but actually is not better than a single most layer perception. So if you have. if your encoder doesn't support many of these languages, the transfer performance is also stable. so we also have accuracy by juicy classification and by region. And then you see that the region of Europe depend regardless of their classification. They often have very high performance. That means, even if they don't have a lot of representation of text. But if that language is similar to a high resource language. the transfer still work very well. Right? So if you have a language what can I like? Livonne? I know Livonne is probably endangered like Luxembourg. Lustenbourgish right? It's very close to German and some French. but it will still have a higher performance, even though it has less text on the word right? So you have a summation. Languages also have this kind of effect. But if you go to the African languages, you have much lower performance in terms of transfer performance. Okay? And so then we talk about factors affecting performance. We talk about the language coverage. If a language is not covered during pre-training, they often have lower performance, and then we have categorization, like is the language already seen? Is the script already seen, and the performance will be higher than if the language is unseen. So if the language is unseen, but that language is very similar to the same language like Moroccan, Arabic still has a high performance done. Because it's close, closer to the modern standard. Is it modern standard? Right? So, you still have a high performance while you have some languages where the language is unseen, and also the script is unseen, like the Tamashek, which is a Berber language in North Africa, and then you'll see that the performance is really poor, right, because the script is unseen, the language is unseen, it's really difficult. So we also compare prompting with 0 crosslingual transfer. And here you find out that actually, models like Chatgpt just 1 min had like much worse results than just training on English. And then do you? 0 shot transfer to all the languages. Yes, we have a question. If the script is unseen, how does the world? It's not a generation task. It's classification task. Yeah, processing it and saying, this is like a business or travel class. even though maybe it's just very randomizer. And the word embedding is learning. Yeah, that's a great question. The Tokenizer also doesn't work very well for this right? Because it's it's not saying it does not send this. So if you, Tokenizer doesn't work very well for a language. What may happen is that it will take more tokens to encode the information there. And that means it's just like you can relate this to compression. That means it's not able to get any useful information. Yes, yeah, that's a good question. So the issue is that initially. which I'm going to show you in the next couple of slides is. if a script is not supported. the 1st version of Maslinga Bert is going to give you f. 1 score accuracy of 0. Okay, but this is not interesting. All right. So people address it by using not what level tokenizer, but they use byte level tokenizer. So byte level was still kind of encoded, but the very poor way. And then this still gives you poor results. So you won't have folder site optimization now. Yes. the performance of the language, languages that were not seen by how similar those languages are not seen, or how are they themselves composed of languages that we have seen during training. Yeah? So that's a good question. So it's like, if a language is not same, and there's no relative or causing then it's gonna give you a poor result. right? So we need some analysis to to see. And that's why I could give example, like, look at Arabic way, because it has, like other Arabic scripts that are seen like Egyptian, is very prominent and modern standard Arabic, and then you could have better results. Yes. just kind of like, maybe site level quotation. Yeah, that's a good question. Yes, we do have character level models. Yeah. And we have bite level models, which I did better. Yeah. But it has not been widely adopted. Yeah, yes. yeah, for the languages that are unseen and script unseen. Yeah, there's some that are still very accurate. I'm wondering how that's sort of even a little bit possible. Oh, so, and what is accurate there. So you find out what is accurate is the Mlp. This is multilayer perceptual. So that means you don't need any language model. So you just train using circuit line. And on the text, that's it just like your assignments. So that gives better result than using a bird's model that was trained for 2 weeks on a Gpu server. Gpu. So yeah. sorry. Can you speak louder, so can you one? Oh, which we need more? That's not good. Oh, not really if you have a more popular, I would say popular language, I would say a language with more resources on the web. So if you are able to learn a self-supervised pre-training for them. You have rich representation and better encoding of the information in the language, and then it will give better performance. But for a language that you don't have a lot of text on the web just using simpler approaches like naive base or something they already appropriate. Yes, yes. Mlb, so we have. Mlp, yeah, that's a good. We have. Mlp. Smr, mlp, snmr. Uses the the tokens of ex Lemar. Why, Mlp. Engram uses engram like what you did in your summit. Alright. So last part 2 like I'm I'm very certain we'll not finish this. But yeah. So this is like, what can you do if languages are not covered. So I already showed you result that things doesn't work very well if languages are not covered, so what can you do about it? Right? So so 1st I will. I'll tell you how we develop a new model called office tomorrow by looking at some of the challenges of adapting plms to new languages. One of the issues is, many. Languages are not covered during the pre training, so when you evaluate it. you often have worse results for these languages. this is probably due to what is called cost of multilinguality. Cost of multilinguality is that if you want to cover more languages. you also need to scale the capacity of your money. which is not possible. So people now try to scale a lot like Gpt-four, and that's why it's able to learn different languages very well. but also there's other restrictions where models are just getting bigger, bigger, and then nobody can serve them. And is there a way, we can actually find a way to adapt models that in a more parameter, efficient ways. So let me show you a result of Amaric like what I told you. If you have a word based model tokenization. And then for a non-sympathy scope like Americ. if you train a embed model, you're gonna have a 4 score of 0 because you're just using one level. Actually, the 1st time I saw this. You know, I trained the model many times as I was, how can I keep getting 0? And then I have to think about what's happening in the model. And then I understand that the tokenization just fails. Right so, and then, if the language is supported. if the language is not supported, but the script is supported, it still works. For some, for example, our language is not covered in multilingual birth, but the script is covered, so you still have very good performance, and Yoruba language is covered in multilingual birth. So it has slightly better accuracy than X number of data, hey? But one thing you can do about this is to do what is called adaptive fine tuning. Basically, you can replace the vocabulary completely. That's why what we did there was just to replace the original vocabulary of X number of beta changes to that of Americ, basically trained on new Tokenizer and just replace. Just swap the vocabulary. and after swapping the vocabulary that you adapt the model by training on more on label text. And if you do this, it's going to correct this problem, and you redo the alignment. And then you are still able to use the the knowledge in the upper layers, and then you can improve the performance. So by that we're able to improve the performance. You see, for Americ we went from 0 to like 60 something. So this is already good boost in performance. What they say challenge. So that means, if a language has already seen that script going to training all your adaptation, you're still not able to reach the performance of the model. If it has already seen that script during pre-trained. So the lesson is, I try to cover as many scripts during pre-training, so that you will be it will be easier to adapt to new languages in the future. Right? So. Now you agree with me that one of the limitation of this approach of language adaptation will be, it's difficult to adapt to non-supported scripts. Another issue is that it's not parameter efficient because you create a new copy of this model when we are doing adaptation. and also, once you have adopt adapted the model, you can also reuse it for so basically, the model you have only adapted it for Americ going forward, this will only be useful for Americ. You cannot use it for English again. Any other language. Yes, alright. Did you mention? So you can train a tokenizer. It's a model itself. It's a model itself. It's very small model. Yeah. And then you can copy all these tokens. So you trade the tokenizer on. It's a heuristic way that a guardian of tokenizing the text right? And then you can copy all these tokens into into the basically so you can copy all these tokens into the original model. You just have to ensure that the size match. Right? So if the original model has 250 K. And then you can train a new Tokenizer, and saying that it has to be. give you at least 250 k. Vocabulary tokens, and then you can redo the alignments by further training. Yeah, yes, pretty late. I guess the words like tune the model for that new vocabulary. Yes, you have to worry about like changing the model too much. Yeah, you have to worry. And then things like learning rates also matters. So because then I think we I tried a very high learning rate. It doesn't work. I use the original, it doesn't work. And then I use a very, very small learning rates then what it now did was to slowly adapt the original model so that you don't have catastrophic forgetting it doesn't forget all the knowledge, because I want to retain the original rate as much as possible. But I just want to slowly remap the embedding layer and that work. Very well. Yeah, yes. yes, yes. We started with glass glasses. Transformer. Yeah. Cool? No. The tokenizer is not in your network. It's just it's it's How do I explain this. It's a heuristics to determine. What is the what is the So what is the minimal amount of tokens that can serve a language right so, and then the number of token, the number of vocabulary token is a hyper parameter. so you can train a Tokenizer to say, I just need 50,000 tokens that can serve this language, and then you have the heuristic that gives you the best 50,000 tokens. subword tokens that is good for that language. and then you can also have 200 k. But oftentimes you have to have, like something useful. For example, the original monolingual birth only has 32,000 right, while the multilingual birds. I think it originally had like 100,000. But later versions of best had, like 250,000, because they believe this is a better multilingual vocabulary to serve many languages. just trying to get as many importance as possible. And you try to get as many tokens, but not too many tokens but we are a bit guided, based on what other people have done previously. So you just don't do like a million tokens or something. Yeah. alright. So how to address this limitation of lofts which we call left? A a very simple way is just to return your own new model. So we have a new model from Africa called Africa. Beta. We have another one for India languages called Muriel, and it was just trained on like is it 22 Indian languages? But this is resource intensive, right? So another approach would be to use what is called parameter, efficient approach, where you can use things like adapters, and the idea is that you had an adapter which is like a new weight to the different blocks in your transformer model. and then, instead of modifying all the weight of the transform, but you only modify this new weight, you have added. and this new weight are smaller and more parameter, efficient. And this is the only thing you have to stop instead of storing the entire monophone. And this works very well for post language as well. So this is an example of evaluation on Indian languages where you have Smr, the original model. it's already good. But for this language, which I forgot the name of the language which is the only Austro Asia Asiatic language in India. Maybe someone knows it, and this language has very low performance and the indig bets. The 1st version had a very good performance on this language because it was covered. but it didn't cover Sino Tibetan language, and then it has worse performance. But for the Muriel bet. Similarly, it has better performance on this Sino Tibetan language. but I was resortful. the Austriatic. So just because and it's very important in India languages, because they it's like almost every language uses different scripts. Right? So if you miss a language. You miss the language. You also miss the script. and then it doesn't work right, and they have a random platforms. So So what we did then is what is called multilingual adaptive fine tuning. You pick an encoder model like Xnemaro, and then you initialize the weight of the model and just adapt it to the region of languages. So here we adapted it to African languages to several African languages. So this is very similar to what was done previously for Mbat, which is probably the topic of your reading assignment. We have Mbat. 25. So to actually increase the capacity of Mba. 25. They continue pre-training it on 25 new languages. and they show that it doesn't affect the performance of the original model right? But what we did is study different. So here they are trying to extend the model to cover more languages. Here we're trying to specialize the model just for Africa languages. So there, there will still be some catastrophic forget. And here the choice of Xmr was because it covers multiple scripts in Africa. So we didn't choose multilingual birds, because it it wouldn't work for the Amari case, I should I already described. And then we combine Irish, some Irish as language like English, French, Arabic. That also widely spoken Africa with 17 most resourced African languages. And then we just do something like continual training on Mc for Corpus and news corpus. And here we're able to cover more regions of Africa with a better multilingual representation, learning model. So to address the audio restriction. One popular approach is you do what is called knowledge distillation. So you have a teacher model, and then you want to distill the knowledge into a student model. So you can have embered, and then we have distilled. But nowadays those models are now too small that nobody cares about distilling embers again. Right? So. But what we did then is. Actually, there's another trick you can do, which is what we call vocabulary reduction. And the idea is this, think about Xmr. I told you that it was trained on very diverse scripts, very, very diverse languages, 250 k token, but because it was trained on these 250 k. Tokens and many languages. The model is also very big, and up to half of the model size now is in the embedding part of the model. right? Because it's now very big. So the question is, if you just want to specialize the model to a region of languages. do you really need all this vocabulary? So what we showed here is that actually an earlier paper already showed that if you want to specialize to 2 languages, just remove all the tokens that don't relate to these languages. Don't modify anything. Just remove all the tokens, and then your model. Still. it's good. So you don't need the performance doesn't drop. So we also did the same thing, just removing like focus that are not really African related. So, for example. we can remove things like in the script and in the tokens, because it doesn't relate to Africa at all that, and then you'll still be able to preserve your performance right? So unlike that we are the number of parameters of our model went from like 270 million parameters to like 140 million parameters without. I mean there was some drop in performance, but it was not significant at all. Okay. so this is the result. So if you do this adaptation, which is language by language, adaptation. So for every language, you adapt it to the monolingual text of that language, you always improve in performance. The only language we did not see improvement in performance is English, because the model is already good for English, so there's no need to adapt English again. And then, when we did this multilingual adaptation, what we see that we almost match the performance of the individual language adaptation. There's still some drop for some languages like Americ and Yoruba, but we will tell you how we address them so, even for languages that are not standard for training. but they have a related language. It kind of still have those languages. Okay? And for vocabulary compression, when we compress the model, we see that actually, we have a little bit drop in performance, and the biggest drop is for languages that use a different script. So while we are throwing away some tokens that we don't need, maybe we also throw away some tokens that already have languages like Amarik to perform very well. And now we observe some dropping performance. Okay, But in general, what we saw is that vocabulary compression was actually better than knowledge distillation. So if you compare models that they already perform knowledge distillation, which is a very expensive process. you'll find out that the performance of this knowledge distillation model was the worst and just doing this vocabulary. compression or reduction actually had better results on average, than the registration. so seems to be still a very attractive approach. The size of the model is remains the same, but in terms of performance, just reducing the vocabulary actually is better. Okay? So for languages that actually, we're not able to improve with this multilingual adaptation, we found out that by scaling the size of the model. So doing the adaptation on a bigger model. So we did. Xmrs 2 versions. We have the small version, 270 million parameter. We have a bigger version, 550 million parameter. So if you do the same adaptation on 500 million parameters, the performance is much better than the 270 million parameters. But also we have an additional benefit that the there's no gap in performance between a single language, adaptation and multiple multiple language adaptation. Yes. yeah. yeah, there's gotta be more weights in the model. Yeah. And here you find out. Although America was struggling before. When we have a smaller model size than when we move to a bigger model size. There was no difference in performance, right? Similar for Yoruba, actually for Yoruba. We even had better results, Kate. and currently, at least for the African languages. You know, we kind of create a better model than previous so really strong multilingual models like remed by Google and Md. Beta, by Microsoft. right? And the model is a login face. So we also did the same thing for sequence to sequence model. Right? So you can have an empty 5 model, and then you can specialize it also to region of languages. And also, you have similar effects where you can also improve the performance even for generation task. I know someone has asked for generation task. I think you asked for generation task. So this approach will still work for generation task by just adapting the model to more, to a region of languages. and then you can also have better results even for machine translation tasks. So how to adapt an English Llm. And now this is the era of large language model, and I can tell you also, this approach is still quite useful for large language models. So we have some Chinese authors that actually adapted the 1st llama 2, and they adapted it to Chinese llama, and then we had. by doing additional pre-training on just more Chinese tokens, and I think they actually did Chinese token and English token together. and by this they were able to have Chinese llama, too. and when they did instruction tuning based on Apaka, they are like Chinese Apaca. So for current land models. What you have is you 1st have for training, and then you have instruction tuning. And if you want to apply this approach you. All you have to do is just, do continue pre-training on a large amount of text for that new language, and then you do the instruction fine tuning. So this approach would still work right? And then also on Chinese mmu, you also have some boost in performance by this. Okay, so this is a second part. and the 3rd part is very, very short, which I can rush through, and the short part is, the 3rd part is cross lingual transfer learning. The question is, how do you choose the best transfer language? Alright. So so I'm going to demonstrate this with any R model, and the idea is that if you want to choose the best, any the best transfer language. this is interesting, because then we are like 21 African data sets with any our data sets. They have training development and test sets. And we have 21 non African languages. And then we want to see what would be the best source, transfer language for each of these languages. So how do you do this? You just have to train an end by end model. So you train a model on Chinese, and then you evaluate on the rest. On the remaining 41 languages. You train a model on English, anyhow. model on English, and then you evaluate on the rest 41 languages. So, and then you have like. and each map transfers call like this. And here I'm just going to display some results that are more specific to Africa. Like Arabic German, we see some interesting transfer to some African languages. So, for example, here we have languages that are more. In the Francophone region of Africa, we find that they do transfer very well to each other. Maybe they share some entities that are more interesting, maybe that relates to France or something. And they are able to perform. That means if you train on Bbj. and then you evaluate on phone. you might have better results. So we also find, like languages that are geographical proximity. For example, all the 3 Nigerian languages covered we. They have actually very good transfer performance to each other. even though they do not belong to the same They do not belong to the same family. but because they are in the same geographical location. And this is very easy to understand for Nar, for any hour you have things like personal name. Maybe they are talking about the Governor of the States, the President, so they will share the same entity. They have the same script. So this name should be flying around in different text, whether it's in Yoruba or Asa, even though they are not the same family. So we have languages like this in East Africa, which have very similar syntactic similarity and entity overlap. So this language is like Luganda, Rwanda. Swahili. They're in the same region. They are linguistically related. They also have very good transfer to each other. Then we also have languages like I told you that we have very portrait. So now you can see the result from English to Zulu. You see that the performance is very poor. We only have 45 right? But if you transfer. let's say, from Shona to Zulu, you have better result. If you transfer from Cosa to Zulu you have better result, because these languages are in the same Uguni family. They have very similar properties. They have this kind of interesting prefix before capitalization of the entity. So those language they have similar properties also transfer very well to each other. So when you want to choose a transfer language, you have to. Consider this, for example, we also have interesting transfer performance from Arabic to Swahili. And you know, in historically, Swahili has borrowed a lot of words from Arabic due to due to trade, because there's a lot of trade between East Africa and Arabia, and in many words were born into Swahili, and also the German influence is quite interesting, because there were some German people there at some point, and I don't know what happened. Why do you transfer? Very well, but I cannot explain the one for German very well, but we actually see very good transfer, for example, from Arabic to Swahili better than an African language as well, which is quite interesting. Yes, oops. this is for name. This recognition. Look so next generation you might have a generation is really big you. You might have, for example, for wholesart Zulu, very similar syntax or linguistic property. Yeah, this might also hold. Yeah. So the question is, how do you actually scale this to new languages. So let's assume you have this so you already trained this for 40 languages. What if you want to do this for another language? Do you have to repeat the same experiment and all that. So some people have been thinking about this problem. In 2019, there was a paper from Cmu on what is called language rank. and the idea is that you can train. The idea is that you can train a ranking model, a ranker model to actually predict what would be the best transfer language for a new language. Given your previous data sets, you have trained right. So the idea is that you train a set of Nlp models like what we have done. You train on English, and then you transfer to all other languages. You have all these different scores right? And then also, you use some other linguistic properties and some linguistic distance. and then you will now use this to train a ranker to predict what would be the best transfer language this is based on boosting. And some of the linguistic features are, there's a there's a linguistic vector features known as language to vec, so that means different linguistic properties have been converted to vector representation. And then you can measure what is the similarity between these different vectors or their distance, and then you can use this as features to actually build a ranker model. So we have different linguistic distance measures like geographical distance. Our geographical distance are these 2 languages. So if you have a language Indi to, let's say, Swahili, they're very far from each other. So the geographical distance will be very wide, and then you have genetic distance in their linguistic family. How far are they from each other? We have the inventory distance which is actually. I forgot the exact definition of that. We have synthetic distance. We have phonological distance. How the sound are they related to each other. We have the futura distance, which is a combination of all these distance measures. And also we have data dependent feature. What is the data size that's available. What is the entity overlap? Do? Do you have a lot of entities that actually overlap between these 2 languages and based on that, you can train your rank and model. And what we did is that we did this, and then, if we do, the brute force approach, you have the top 2 predicted transfer languages. and then we also now use a language rank by using what is called, leave one house. This guy you train on all other languages except this language, and then you can now run a prediction for this. So leave one house is a is a cross validation technique. If you don't have a lot of data for the new network. So and here you found that the language prediction are quite interesting. Sometimes they're very similar. For example, for ours. We have, like Nigerian pigeon and Yoruba with predicted if we use the brute force approach. But we still have Yoruba. That was also predicted for hours, even by the language RAM model. So that means the model is not very accurate, but sometimes it can have a good prediction. and I will show you in a couple of slides that if you use the top 2 and you co-train them together, you might actually have very better, better results than predict, using just one prediction or just using English language. So you always have better result than just using a single language or just English language. and for any, I think the 2 most important features that we discover are geographic distance. an entity overlap. So if you compare the brute force approach. You find that the English performance is the one in pink and the top one language rank. You will find out that most of the time. It's not very clear for some languages. The top one rank is better for some languages just using English is better, but if you do what is called co-training. That means if you train on the the top 2 predicted languages, you'll find out all the time. This is better than using English every time. So so that means language rank. Will is able to provide you at least stop to language. To transfer from that will be better than just using English every time. And if we compare this to the top, one best transfer language, you found that co-training the top 2 from language rank is very similar in performance than using the top one in the brute force approach. So language is cheaper. So why not just use this and just train on the top 2 languages, right? And yeah. And the last thing is which is, you can also use what is called parameter, efficient, fine tuning. So you, if you're able to get the best transfer language, you can combine it with this framework called parameter efficient fine-tuning, and this will actually boost the performance. and the idea is that you can have adapters to every transformer block. and you can train this for different languages. You can train this for English. and then you can now use this to transfer to another language. So for you to do transfer learning. So you have to train an adapter for the source language. You train an adapter for the target language and then for the task specific adaptation. You now train a model that works for the source language, and you are also works the for the source task. So in this setting, we believe that the language you're adapting to is also interested in the same source task. So the only thing you need to modify is that you only need to swap the adapter of the English language to that of the target language, Quechua. and by doing this you can use the remaining models you have trained in many weights. You have trained for the task. and this seems to work very well. And actually, this can boost your performance, and if you combine this with choosing the best transfer language, you can easily boost the performance of your postlingual transfer. So we tested this approach. There's another approach which is very similar to that which is luxury, ticketing, approach. and the idea is that it's very similar to adapter. You can check, you can get what is the best spas, fine tuning, or what is the best of network for the source language, what is the best of network for the target language, and then you can compose them together. cross language as well. So in terms of the result. This is a result on part of speed tagging. If you do not use this framework at all. This is the blue bar, and you see that this blue bar is inferior to using things like adapter or Lt. Sft. And the source language also matters, and you find out that some source languages, if, like Arabic in this case. regardless of the approach you use if you use a better source, language is already better right? So if you can combine the sauce choosing the best source language, and then you combine it with this parameter, efficient approach. You can significantly boost your performance. and, better still, if you now combine this with Co training training on multiple languages, you can even further boost the performance. And this is what we have here, which is probably our final slide is that for a language like phone. the best transfer language here seems to be wall off for are not English for Euroba. I think wallopsy transfer very well. but if you go to Zulu, all of us transfer very well to other languages, did not transfer very well to Zulu, because they have very different linguistic property. So I've been able to. Probably I don't know if I'm able to convince you that. And those little details are important. What is the best transfer language for the language? If you can get it, this can already boost your performance. I am posting watch as far. If you combine this with a different parameter, efficient approach. you can also further boost the performance. And if you combine this by training on multiple best transfer languages to enable further boost your performance. Alright. So, I guess that's the end of the ledger. Thank you for waiting till the end. Thank you. Alright. So, Jackie, with the go ahead. Fantastic from Oh, yeah, from Wednesday.
