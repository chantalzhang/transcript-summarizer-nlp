David Ifeoluwa Adelani: North campus. be discussing Engram language model. I will skip the review of last time. I'm on Slide 2 . the view of language, far, in the in the past lectures we have examined. How can we model language far in context of test classification? Then we're going to examine more statistical language modeling based on n grams. Then we'll do or maximum likelihood by relative frequency, and then we'll touch on how to evaluate our language model. I want to remind you of what is the word which you know? Award is the smallest unit that can appear in the context. if you will have a word , and then most of the time. The apostrophe and tea are often separated from the can. And here, in the 1st example, cat marks on Saturday you have 2D's, and then and then, when you're trying to say, what are the, what types that will go to your vocabulary. For for the 1st example. Because then we are just truncating the suffix for the second one. Is that is that scaling or lemmatization? What it says of democratization, because happy and happily are different. , this gets more tricky. If you want to go to the root word. , what if we have happiness? Happy if you have happiness. You always need a corpus, and of course you could also have several copperam that you will combine to form one gigantic text. A very good example that can be representative will be the English Wikipedia. English Wikipedia has the gigs of text that is big enough to capture different cases in English. Mandel Broth's law is a generalization of this law. It differs for different languages because different languages have different structures. 40% of the words appear once in a corpus. Practical implication of this most war types are very rare. Some words we only appear once, why? we appear 10,000 times. The world is morphologically rich. You have a root word, and then you can append to the left or to the how that some language is. Why do we count words when you come to what's very important for building? What is called Lovewood model? Because statistical language model is just by counting how many words appear in your couples. Given a context C, the random variable here is W, which can take any word in our dictionary. And then we have a context. What is the probability of the world equals Lamb. Using this chain rule of conditional probabilities which I can explain. using the book and maneuver of the country. You hang on as . hold on your phone 2, 1 in bidirectional oil, you can do it both in the forward direction and the backward direction. This is the this is the better formulation going down here for what you are describing. Yes, per student, the product term be from I of one to N, and then WI, plus one that I minus one. There are many applications of language model. When you use your mobile phone. you prefer that it suggests what's for you when you're typing. It's an example of a simple language model which is sentence completion. automatic speaker automatic speech recognition. that the way I'm speaking is supposed to transcribe the text, and if it's a good ASR model. what's the probability of the world? we make use of this assumption, which is called conditional independence. The last 2 words of the format. that is a trigram distribution. And if you say the cat is sitting on the mat, if you just say what's the. probability given just one previous word. That means you'll say, what's probability of Mat? Can you compute a unigram and background language model, using the following sentence? I just have a question about unicrons. If we count the probability of each word based on all the words in the purpose, aren't you always going to put those V on every word, because we use no context and use the most frequent words? You don't really have to do anything ? You don't need to do it because it works. Here, we are focusing on a more general language task. What's the probability of that? How do you compute over 5? And by 5. , it is the probability the count, of the number of times you have that which is 2 times divided by the what the count of that . And how many times do we have that? You have that 5 times ? And that is how you have 2 over 5. Also 2 or 5. , what of is this? what happens whenever the denominator that's in regards nothing. happy end of the sentence. at the end of a sentence, if nothing is happening, this is more a unigram. you can skip the last discarded, because there's nothing. 6 out of 5 not out of 6. No, it doesn't affect the calculation. It just affects how you group damage to tools. You need to divide your data set into training data and the tested data. You could also have the validation data. for us to test. What we are trying to do is to estimate what is the likelihood of generating the testcos. after you have feed the data using parameter theta on everything on your training purpose, how do you compare? What is the probability of the test compost? there are 2 ways you can evaluate. 1st we can use cross entropy and then you can also use publicity. I have an example in Slide 38. The plot of entropy versus the coin tos fairness. If there's maximum fairness that means you cannot easily predict what will happen. Then you can have a very high expected value of the information. And then, if it's biased, then you can easily determine if the probability will be 0 1. The model distribution should be close to the true distribution. Some people formulate it differently, which would be if you assume log of E. This can also be the exponential of the cross entropy. We are just saying that this will be 2 ways to power.