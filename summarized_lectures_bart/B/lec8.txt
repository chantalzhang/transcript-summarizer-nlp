Lecture 8 will continue our discussion about part of speech tagging. And today we are going to be going through some algorithms that are popularly used for this task. We examine 2 popular schemes, and then there's a universal one. Here we use nlp for proper nouns and for universal dependency. We also call it the Markov assumption. here we relate this to Markov chains. And in Markov Chains we talk about how we can decompose the joint probability which by has been removed from the board. We have the probability, the judge probability of all the observations and all the States. They are the Union variables and the o's are the observations what you observe. High equals the part of speech divided by all the possible sentences that are in your training couples. This Emily, is often calculated over your corpus. And then you have the a high J, which is computed that given. If the previous State, can you predict the previous? And we can also compute this with very similarly. The problem is that if you try to do this. you have exponentially many paths and risk to priority. The solution is that we can try to solve this using forward algorithm which is coming from dynamic programming to avoid unnecessary recalculations. we're trying to create a table of all possible state sequences and then we calculate the probability. and then you have all your observations that happen via time. once you have calculated all the state values, you can marginalize the entire values to calculate what is the probability of all given theta. the values inside the States can be referred to as what isThe probability of observing from one to T, because for every State take a random one there. The probability of Dt beginning this sentence, . the probability of a meeting, the what you observe? if you were , let's go to last. lecture. We have the initial probability. If you remember the calculation, I can open the slides, but it's very, very similar to what we did in the last calculation. where you have an initial state, you don't have transition probabilities. In the last column, once we have calculated everything for distrellis, you can. just marginalize marginalized means you sum over all the probabilities to get your Po given data because everything previously contributes. And at the last stage you can just sum up or everything. You have calculated to have the probability of O given Theta. The alphas are the are the one we computed at the forward algorithm in the backward algorithm, you compute the betters. What's the probability of T plus one to T, because you're starting from T, which is the last cell and then given R, R equals T. The last stage you just need to marginalize over what you have at the end. There's a simple trick that we use. If you have, if you have to multiply a lot of probabilities. And you need to take the Agmas most of the time. It's better to work in Logan, because, instead of multiplying all you have. to do is to just sum. and then we can have what is called this log some trick. The log sum trick is the log of the summation of exponential of probabilities. This is a very, very common trick that is used when you have to multiply a lot of probability. And also we can use it here at implementation stage. You can use the forward backward algorithm and combine this with Em algorithm. If you take the Ag mass over all the probability of Qo, given theta this is vitabi algorithm. But when you do this calculation, you have to also keep track of where the maximum entry to each cell came from. I may have to write the formulas on the board, because it's more difficult. We are supposed to multiply. and the better it is together. The only difference is that we're going to take the maximum instead of the submission. We are still at the what do you call this? because we want to go for exclamation mark at. You are going to multiply each scale probability by the observation. at the 1st State we are observing just the exclamation back. which is the this and the probabilities for each of the States. For all this establishment, 0 point 1 0 point 5 0 point 7, and the initial set probabilities are also given as 0 point 2 0 point5 0 point 3. In the last column you're going to have once you have once throughout. and you are trying to do the same calculation. But the formula is slightly different. here, here you have the A high J. what is your aid? You still have one, because your contribution from the last. column is one. And then you propagate everything to the very 1st column. and then you do the calculation. And after that you sum up everything together. In the forward backward algorithm, you predict the current state sequence using the current model. And then you update the current parameter. But you could also have this in a in a unsupervised setting where we don't have the state values. we need to guess them. And this is the idea of the em algorithm. In the east step we have 2 parameters that we want to calculate. We have a gamma I of T, which you can decompose to be this probability of Qt. I given what you observe and the parameter you are looking for. Once we estimate these values, these values will be corrected at the maximization stage where you can estimate it's based on your training data. When do you stop your Em algorithm you are likely going to stop. When your likelihood doesn't improve at some point it will stop improving, and then you can stop this is the idea of the em algorithm. We are trying to just learn everything unsupervisedly from the data. There are no labels to train on. We cannot just apply vitabi algorithm directly. You can estimate your offers and your betters, which are your offers. But in the unsupervised setting, you cannot estimate this. There's no guarantee that you're going to get very good results. There are proofs of correctness above bound Welsh correctness which you can check with additional materials. In practice, Emi guardian can be used for different tasks. We have some very popular baselines on the Wsj couples, which used to be a very popular benchmark. We also have some other sequence modeling task chunking where you find syntatic chunks in the sentence. iob 2 tagging scheme, where you clearly define the beginning of an entity and the ending of an entities. Old tax, signifying that there is no entity East is not an entity located is not a entity. You can also solve these tasks with Hmms, and as in the future lectures using crf.