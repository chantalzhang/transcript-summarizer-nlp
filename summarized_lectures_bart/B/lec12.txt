I hope you all had a good week reading week and you're enjoying the unseasonably nice weather. We are going to continue our discussion about syntax. A remember, is a formalism. It's a model of formal grammar that we can apply to model the syntax of natural languages. , that's not the bottom. OK, forget it, I'll just leave it. I posted information about both the midterm that's happening as as the final project description. please do take a look at those and start working on it with your project partners. I'll also post the reading assignments soon, and I'll Also post the programming assignment soon. This is a follow-up to last week's discussion of context-free grammars. We asked whether for any natural language can all of them be modeled as some context free ground? And why not something more expressive, a context, sensitive grammar, or the most expressive one, which is some recursively enumerable grammar? There have been two demonstrations that there are phenomena in natural in some natural languages which cannot be modeled by context-free grammars. And it's 2 particular languages. it's only Swiss, German and Babarra. It's not even standard German, OK, it's just Swiss German and Bambara. And because there's this notion, you have to remember what the M's and N's are as you're generating and accepting. In all of these arguments about what are natural languages, context-free grammars, and forth, they rely on some assumptions which you can disagree with. And the other major assumption here is this assumption that's in common with everything we're discussing today, which is that strings are either in the natural language or they're not in a natural language. You really have to buy into a particular view a particular worldview about how language works in order for these arguments to make sense. Parsing is the same thing as programming languages, but in natural language. The main idea in parsing is that you have a sentence, an input sentence, and then a grammar, a context free grammar associated with that sentence. You want to come up with a tree structure, with all the constituents that describe that particular sentence. One of the main difficulties in parsing is to find an efficient way to search through all of the plausible parse trees for some input sentence. And we're going to talk about an algorithm to do that. And then there you can talk about dependency parsing. There are algorithms that look a lot the algorithms I'm gonna talk about. There are usually multiple parses that can correspond to any sentence of a language. But here the for us, the most important ambiguity is syntactic ambiguity. We're going to talk about an algorithm to recover all parses according to a Cfg. What are the different kinds of parsing algorithms that exist? If you think about it, there are two general strategies. One, general strategy is, you start with the starting symbol. the S. The other strategy is you start from the input words and then you build ever bigger sub trees. Parsing is no longer as popular, but you still should know about it because it's very good pedagogically. We're going to cover the CYK algorithm, which is a bottom up algorithm. And the key to having an efficient parsing algorithm is to have an efficient search strategy that avoids redundant computation. We're going to convert the Cfg into an appropriate form, such that Cyk will work with it. We're then going to create a table to store all of these possible constituents. And finally, we'll read the table to recover all of the possible parses of the sentence. There are 3 possible cases of Cfg rules that do not conform to Cnf form chomsky, normal form. One non terminal rewrite to three or more hand side rules or symbols, either terminal or not terminal. By iteratively doing this, you can gradually turn all the rules into rules that have exactly 2 hand side symbols. The idea is to remove that intermediate level, that you have a rewrites to the thing directly. And this helps you get rid of these. These are called unary rules, because it involves exactly one hand side. And it has some rules that have 3 hand sides, and then it hasSome rules that mix, say terminals and non-terminals. This means in our case. every time we see a rule where N is on the left hand side. We have to copy it and then we have to. replace Replace N with NP. And you can get rid of this rule. One important thing to note is that you keep the original rules, too. you have two copies of the rules of generating I and elephant in pajamas. And then x, 1 And then X1 goes to VNP. we could replace it in. Because if I can get through. this one needs to be changed this one need to be change to. I'm gonna use P for preposition. I'll just keep it here. I deleted the rule that I needed, and I didn't put in the Oh, I, I messed up. Rules with the same hand side sequence of symbols, and that's totally fine. There's nothing to stop the grammar from doing that. We're going to set up a 2 dimensional table that will store all of the constituents that can be built from contiguous spans within the sentence. And this sounds more complicated than what it is, once you see the example, this will make sense. OK, intuitively, the entry at each cell is the list of non terminals that can span those words according to the grammar. And each cell corresponds to a particular span of the sentence and all of the constituents we can build for that span of words. The diagonals just corresponds to single words. And the cell that corresponds to the entire sentence is here. The general idea is that we're going to go from small chunks to big chunks. We're first going to fill out the constituents corresponding to single words, and then we's going to do the ones corresponding to say 2 words and the three words and four words and forth. OK, for us, the base case corresponds to individual words. And that's easy, because you can just check in your Cfg and find out all the rules that OK, the harder step is the recursive step because this step corresponds to multiple words. And the key idea here is to take advantage again of Chomsky normal form. Not all rules that produce phrases are of the form A rewrites to B&C. all we need to do is check all of the possible breakpoints in between the start and the end. There are 2 possibilities ? form Det NP or Det N? Det N there are two possibilities, ? that means we can build 2 possible constituents there. We can build either an Np or an NP there or we can either build an X 2 there. And this is the pointer way is more human interpretable. If you care about recovering all possible parses, then you have to store both. Then the second, the second constituent you've built here would be x 2, that 2 to 3 and 3 to 4. Ambiguity comes from how natural languages work. And it turns out that we don't mind and we're very comfortable dealing with ambiguity in communication. the words corresponding to the current cell. you just need to pick an order to fill out the table. as long as you filled out everything. We're going to do it together for practice and then see whether we can recover the 2 parse trees that we expect. and for pajamas it would be Np or N, And for pyjamas, it would been NP or N OK, those were our base cases and we're done our . Those were our Base Cases. And I'm gonna go bottom up and column bottom up. And by column, left to left. You have to make sure you check the 2 cells. you have to be very careful. You cannot take that as a heuristic because in this particular case, it's zero to 1 and then one to three and then zero to 2 and then two to three. It's not always going to be the cells to you that matter. You have to really reason through it and or and work through the algorithm. there's VP goes to VNP that works. it looks it's we can build a Vp. OK, it looks we can building AVP also x, 1. also X1, also X. See, I said I wouldn't miss things, oh shoot, that should be a VP. But this, this set of red edges has nothing to do with this set, just to be clear. They're they're just there to help you not get tripped up. Is there any rule for Np, and then P. to check formally? We have to check is there anyrule for NP and thenP? Is there anyRule for n, and than P? No. Is there a rule for x. 2, andthen P? Nope. is there aRule for X2, andThen P? Yes, but in the wrong order. But the basic idea is, if a cell is empty, then that's not. possible to build something there. We have to check for rules of the form. 0 to one and one to five is not possible. Zero to three and three to five, and then four to five. If you want to run the CYK algorithm exactly, there are no heuristics. You're just checking that breakpoint. Any anything that ends with my . It's unlikely you'll find a constituent if your grammar is reasonable. And this all makes intuitive sense because it's I shot the elephant in my . we have the same rules that apply. Here we have 4:00 to 7:00. The 1st possible breakpoint is 4 to 5, 5 to 7. And then 4 to 6, 6 to 7 is not possible, ? Because 4 to6 is empty. We don't have any PX twos. Two to four, four to seven, let's check that. Are there any rules of the form NPPP? Nope, but there is one for x 2 PP that creates an NP. I feel I might be missing something. But I hope I'm not. 2 to 6, 6 to 7, not possible. Do we have any rule which is BPPP? How about x, 2, or ? Yes, we have a Vp from x 1 PP. How about X2 or , X1PP? Yes, We have AVP from X1 PP. And if things worked out properly, there should be a bunch of S's here. When there are 2 Vps in one cell with the notation where you refer to Vp, and then the cell number, how do they distinguish? I don't see any 0 to 5, 5 to 7, not possible. Zero to 6, six to seven also not possible, 6 to 7 also not Possible. In the back pointers, you just keep a list of everything you build. The number of parses you. find is not just the number of s's you find in the top cell. Every time there's a decision point every time. you create where you create the same non terminal constituent, in somewhere in a. smaller span, you have to copy the structure. One of these, , I Groucho muffins in practice, I knew I said something wrong. In practice, one of these parses is much more likely than the other. No idea you have to. We have to change something about the Cfg or something. You'll see the depth ? The depth, that's a good idea, . The probability of a tree is going to be the product of all of the rules that make up that tree. And then we can define a new parsing problem which is to recover the most probable parse tree. The simplest way to do this with what we've already is to just run the Cyk algorithm as is, ignoring the probabilities. We're going to extend the CYK algorithm to keep track of probabilities in the table itself. And that'll be more efficient because then you don't have to explicitly create all of the ambiguous trees and evaluate the probabilities for all of them. you do that, and if you're able to create that match that you can create the bigger constituents. You also have to compute the new probability of that constituents. And you do that by taking the existing probabilities from each of the sub constituents and multiplying with. It's the probability you get from your PCFG for that additional new rule. If you create an Np constituent here, then you have to also figure out the new probability of the elephant, which will be 0 point 6 times 0 point 2 5 times the possibility of the rule. Np rewrites to dead End. And then, finally, once you've parsed the whole. And in this way you don't have to recover all the parse trees, you can just recover the most probable. If you have two possible ways if you have 2 possible ways or multiple ways to create the same constituent. Then you throw out all but the best. But then, when we're recovering, we That's . But then when we's recovering,. we and we take the and wetake the constituents with the highest probability and ignore other possibilities. At that point you no longer need to care about the probabilities of each of the sub constituents because the back pointers already took that into account when you were creating them. This is called a tree bank. A tree bank is a collection of trees in a in a bank, with, with a, with sentences. The most famous Tree Bank is the Wall Street Journal Corpus. Everything we talked about in that module of the course to do with smoothing still applies here, because these are also categorical distributions. The reason is because ESPY writes to . as we writes to it, suppose you have something a sentence, or even better would be a verb phrase. There are many different ways you could build that verb phrase and it seems there are different ways to do it. is a subject pronoun in English , whereas me would be the object version. Whereas me is the subject version. at the beginning of a sentence, you have an Np. this, the standard assumption of the PCFGS, does a very poor job of modeling all of these situations.