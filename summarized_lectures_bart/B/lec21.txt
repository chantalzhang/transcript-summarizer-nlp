This week we're gonna look a little bit at automatic summarization and text generation. We're going to look at how we got to the current state of generative AI from this line of work. And then, finally, we are going to end by talking a bit about summarization evaluation. There are technologies out there for a summarization that claim to do a good job in summarizing these clinical notes. This is a system by nuance called dax copilot. There are many different possible contexts in which you might want to do that. It's possible that these different contexts will lead to different possibilities. An informative summary tries to be a substitute for the original source material. An indicative summary provides a link to the source text to help users decide whether or not to read it. A critical summary might be a critical summary which provides an opinion of the source. In providing some critical summary of some material. Another way in which summarization systems can differ from each other is simply the nature of the source. If the multiple documents are by different authors, then chances are that there may be conflicting information or contradictory information between them. Some domains require the exact wording to be preserved. In the medical space there is a corpus called up to date, not a corpus, a resource called up-to-date. Or you might just decide to generate different summaries based on the background of the user. The idea behind up to date is that it's a resource where they have reference. , except it's not a wiki. It's a encyclopedia for doctors, and that they have, a documentation of all of the common things that can happen syndromes and diseases. There's some way to approximate it, using, some fmri scan or something that I don't know. But even if that's the case, it's not practical. we cannot directly measure it. instead, we have to try to model importance in a different way in text. using other heuristics or other cues. If it's very far, then it's not . we want to remove , stop words. Do we wants to remove stop words? Quite , , assignment one. You can make arguments either way that you can. If it's some notion some ways of modeling content would be. It would be such that it matters. The queue that I claim is often used in summarization systems is discourse structure. Is that the way that we write passages? And there are expectations that we have about how a piece of writing is written. And we can use those patterns themselves as to help us inform, find important information. , but that that fits here as within the score structure. But what about for event based news articles. The more important stuff tends to be at the beginning. The beginning is the picking that the opening sentences is the best predictor of important information. , within the thread, all the posts, are already ordered . either by time or and also by the number of Upvotes. But , what about inside of a post itself. The last sentence, because that's where the post original poster asks the question after giving a lot of context. This is about the relationship between our general expectations and understandings of the world and what is written in texts. What do you expect to see in an article about a natural disaster. Or the doctor kilometer range? I'm also gonna put costs. Where to donate for relief. Number that's covered in casual. what to do if you're affected suggestions and advice for those affected. All summarization systems need to perform these steps in some form or another. These days, these days, one, strategy, is to just throw everything into a pre trained, generative model. But older systems, because then they're easier to , think about and analyze and see what's going on. , think about this for 5 seconds, and then I'll ask for a vote. If you had to pick one and just one. which class of features would be more successful. Who votes for lexical features? , more people voted for discourse features, and I would agree with you. The reason for this is that this is really tricky, but if you think about it. it depends on the nature of the supervised learning system. In the nineties Lynn and Hobie did something similar. They trained us to a supervised method as . Where the input was the source text plus some human written abstracts. And then for each sentence in the human abstract they find the position in the source article that has the highest similarity to it. In news text the opening of the article acts a summary in and of itself. Another approach that people have taken is to try to implement this idea of centrality. The heuristic here is that a term is important or indicative of a document, if it appears many times within that document, but is relatively rare overall. Penguin, if it appears very rarely overall, and your whole corpus. but it appears just twice in the current article. The fact that, the you have this much larger number inside the log is usually enough to increase the weight of the penguin sufficiently of these rare words. This is based on just a very simple notion. This is really an instantiation of centrality. Because it's saying that if this word appears more than you would expect according to some background distribution, then it would score highly. And then that gives you some notion of what this article is about in the multi document case.  redundancy is both good and bad. dependency is good because it helps. If everyone's talking about it, it should be important. But it's bad in the sense that you have to be careful when you're selecting sentences to include in your summary. You don't accidentally select the same sentence many, many times. Because you , that's a good question. the question is , what if this down weights the really important words , if it was a salmon article, then you've down weighted the word salmon. But salmon was really important. that's why the squaring works. This is the genius of the Squaring. which is that if it. was a very common word in the set of articles about salmon. that salmon should have a relatively high probability. when you square it, you're decreasing it by less. then you would decrease the probability of a relatively rare word. in 2,006, Conroy et Al. used the topic signature idea introduced earlier with a sophisticated non redundancy, module, and some rules just to eliminate some parts of sentences deterministically. There's there's a metric called rouge, which I'll introduce in a bit, and then their particular system. And , and they got . very close to human level scores even back then. and then the reward signal is , how good your estimation of how good it would be to select. and then you have to define it a certain way. and if you have not seen reinforcement. Don't worry about it, but it's an alternative to supervised learning. You can think of it that way or unsupervised learning, it's a different learning paradigm. what counts as a 5 out of 5 versus a 4 out of five, or a 1 out of5. Even between cities, or even between neighborhoods. You cannot interpret the scale the same. And it also means that results do not generalize across different evaluation runs. This came after and they called it rouge. The difference between blue and rouge is that you, the denominator, is mostly defined in terms of the number of words or n-grams in the system generated translation. For summarization, because , you want to make sure you're comprehensive and capture everything in the source text. And then class, we can talk a little bit about approaches to neural abstractive summarization. And how do you do a generation test?