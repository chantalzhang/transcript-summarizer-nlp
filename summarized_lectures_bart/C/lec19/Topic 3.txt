David Ifeoluwa Adelani: There's still a lot of work to be done. we could also speed them. what's the size of if you crawl all the available web tests in that language? Oh, better tax specific one. class 0, these are . , and class one would be languages that have few texts. And then the winners are languages with sufficient amounts of labor data and legal data. Mlp. which shows that the transfer learning works but is not better than a single most layer perception. The only language we see improvement in performance is English, because the model is already good for English. And if you want to apply this approach to other languages, you have to do the same thing. continue pre-training on a large amount of text for that new language, and then do the instruction fine tuning. And then also on Chinese mmu, you also have some boost in performance by this. You train a model on English, anyhow. and each map transfers call this. And here I'm just going to display some results that are more specific to Africa. Arabic German, we see some interesting transfer to some African languages.