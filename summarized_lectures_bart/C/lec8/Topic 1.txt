We'll continue our discussion about part of speech tagging. And last time we are trying to compute, , , pr ovn they have various different symbols. And also, , we made that assumption which we call the Markov assumption. Here we relate this to Markov chains. The algorithm is very similar to what we are estimating previously. This is the transition probability from time step T to type, step t plus one. We don't know this parameter of T of Theta K. But we can estimate it with some random initial random values. and at the maximization stage you can get a better value.