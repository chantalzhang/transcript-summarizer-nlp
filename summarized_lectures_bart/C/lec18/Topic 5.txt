, , today we'll be talking about neural machine translations. That's you have different words that are equally probable. Words that you will produce? And the interesting thing is, some of these techniques have been developed. There's no need to care about the likelihood, the likelihood of fertility, . because you don't have a direct translation of that word. We're still using transformer architecture since 2017, and the title of the paper, which attention is all you need. It seems to work to. it still has some issues in modeling long context dependencies. And then you can allow flow of information from one world to the other, because everything is you're computing just mattress multiplication.