Today we'll talk about attention mechanism, , which will bridge the discussion from Rnn and Lstms to the transformer. And also we'll touch on some important concepts , how do you get your Machine translation output, because when you're trying to decode from the decoder side or any other model you use, you need some authorities. We want to learn a distribution over words to decide how important each word is in order to compute the representation of the layer. Instead of starting with one hard encoding, we start with the embedding of a different model. And this is the idea of multi-year edition. you're learning 8 different key value queries. on unlabeled text, and you have what is called self-supervised training, and the task for the for the birds model is just predict the missing token, which is a Max token. We also have a multilingual version of this that was trained on, , one on one language and a lot of data, 6.3 trillion tokens. And you can cast any task as a text generation task.