When you are trying to learn a function this, typically you have X, Theta. Theta can be your weight matrix for neural networks or a set of parameters you learn for logistic regression. likelihood is you're learning a probability, what is the best probability that can fit this data? that's the idea of likelihood and the probability, the higher the better, the closer you are to one. We typically just default to what we have, which is the words we have and then use them as features because that's all we have. And then we can do counting. Here we generalize the A1A2A3 that I told you into what is called the weight matrix. And here you can also stack different perceptions together.