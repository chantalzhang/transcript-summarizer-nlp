, , I didn't post it this morning, but I will post it after the class. We have a principle called cross validation which is also used for model selection. Theta is all your set of weights or your parameters you want to learn in the model. Some parameters cannot be learned automatically, you have to fix them. The Bayes rule is that all variables are independent, probability of AB equals to probability of A. This is what it means if you say A&B are independent given C and you assume conditional independence. And for the training, that for every after you have trained the model, you compute the likelihood over the entire data. marginal, but you can ignore it. probability of X is the marginal distribution. in that case you have to compute the joint probability of every X. typically for naive base, it's not that you cannot estimate it, but typically you don't need to. Because you need to compute this over every single features and the calculation for every feature is different. you how you will compute what is the likelihood in logistic regression in a minute. in the example I showed you here, we use a very simple example where the features are review notes, dose assignment, and ask question. If the word yo that we say yes signifies is more correlated with spam, if it appears many times in your document, it should be a good feature.