today's class lecture 9, where we'll continue on hmms, and we'll move to conditional random fields. We today we'll talk about some of the shortcomings of Ed Markov models, and then we're going to move from generative tasks to more discriminative tasks. And of course, we examine the linear chain conditional fields. The States will be the different ner tax, we examined last time. case. The one on the left hand side can be seen as the empirical distribution of the future K. This means that finding parameter estimate by gradient descent is equivalent to telling our model to predict the features in such a way that they are found in the same distribution as in the gold standard.